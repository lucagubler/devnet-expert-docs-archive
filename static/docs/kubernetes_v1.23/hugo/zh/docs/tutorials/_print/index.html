<!doctype html>
<html lang="zh" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>


<link rel="alternate" hreflang="en" href="http://localhost:1313/docs/tutorials/">
<link rel="alternate" hreflang="ko" href="http://localhost:1313/ko/docs/tutorials/">
<link rel="alternate" hreflang="ja" href="http://localhost:1313/ja/docs/tutorials/">
<link rel="alternate" hreflang="fr" href="http://localhost:1313/fr/docs/tutorials/">
<link rel="alternate" hreflang="it" href="http://localhost:1313/it/docs/tutorials/">
<link rel="alternate" hreflang="de" href="http://localhost:1313/de/docs/tutorials/">
<link rel="alternate" hreflang="pt-br" href="http://localhost:1313/pt-br/docs/tutorials/">
<link rel="alternate" hreflang="es" href="http://localhost:1313/es/docs/tutorials/">
<link rel="alternate" hreflang="id" href="http://localhost:1313/id/docs/tutorials/">
<link rel="alternate" hreflang="ru" href="http://localhost:1313/ru/docs/tutorials/">
<link rel="alternate" hreflang="pl" href="http://localhost:1313/pl/docs/tutorials/">
<link rel="alternate" hreflang="uk" href="http://localhost:1313/uk/docs/tutorials/">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.87.0" />
<link rel="canonical" type="text/html" href="http://localhost:1313/zh/docs/tutorials/">
<link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="apple-touch-icon" href="/images/kubernetes-192x192.png">
<title>教程 | Kubernetes</title><meta property="og:title" content="教程" />
<meta property="og:description" content="生产级别的容器编排系统" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/zh/docs/tutorials/" /><meta property="og:site_name" content="Kubernetes" />

<meta itemprop="name" content="教程">
<meta itemprop="description" content="生产级别的容器编排系统"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="教程"/>
<meta name="twitter:description" content="生产级别的容器编排系统"/>






<link href="/scss/main.css" rel="stylesheet">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "http://localhost:1313/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href="/css/feature-states.css">



<meta name="description" content="Kubernetes 文档的这一部分包含教程。 每个教程展示了如何完成一个比单个任务更大的目标。 通常一个教程有几个部分，每个部分都有一系列步骤。在浏览每个教程之前， 你可能希望将标准化术语表页面添加到书签，供以后参考。
基础知识   Kubernetes 基础知识 是一个深入的交互式教程，帮助你理解 Kubernetes 系统，并尝试一些基本的 Kubernetes 特性。 Kubernetes 介绍 (edX) 你好 Minikube  配置   示例：配置 Java 微服务 使用 ConfigMap 配置 Redis  无状态应用程序   公开外部 IP 地址访问集群中的应用程序 示例：使用 Redis 部署 PHP 留言板应用程序  有状态应用程序   StatefulSet 基础 示例：WordPress 和 MySQL 使用持久卷 示例：使用有状态集部署 Cassandra 运行 ZooKeeper，CP 分布式系统  服务   使用源 IP  安全   在集群级别应用 Pod 安全标准 在名字空间级别应用 Pod 安全标准 AppArmor seccomp  What&#39;s next 如果你要编写教程，请参阅内容页面类型 以获取有关教程页面类型的信息。">
<meta property="og:description" content="Kubernetes 文档的这一部分包含教程。 每个教程展示了如何完成一个比单个任务更大的目标。 通常一个教程有几个部分，每个部分都有一系列步骤。在浏览每个教程之前， 你可能希望将标准化术语表页面添加到书签，供以后参考。
基础知识   Kubernetes 基础知识 是一个深入的交互式教程，帮助你理解 Kubernetes 系统，并尝试一些基本的 Kubernetes 特性。 Kubernetes 介绍 (edX) 你好 Minikube  配置   示例：配置 Java 微服务 使用 ConfigMap 配置 Redis  无状态应用程序   公开外部 IP 地址访问集群中的应用程序 示例：使用 Redis 部署 PHP 留言板应用程序  有状态应用程序   StatefulSet 基础 示例：WordPress 和 MySQL 使用持久卷 示例：使用有状态集部署 Cassandra 运行 ZooKeeper，CP 分布式系统  服务   使用源 IP  安全   在集群级别应用 Pod 安全标准 在名字空间级别应用 Pod 安全标准 AppArmor seccomp  What&#39;s next 如果你要编写教程，请参阅内容页面类型 以获取有关教程页面类型的信息。">
<meta name="twitter:description" content="Kubernetes 文档的这一部分包含教程。 每个教程展示了如何完成一个比单个任务更大的目标。 通常一个教程有几个部分，每个部分都有一系列步骤。在浏览每个教程之前， 你可能希望将标准化术语表页面添加到书签，供以后参考。
基础知识   Kubernetes 基础知识 是一个深入的交互式教程，帮助你理解 Kubernetes 系统，并尝试一些基本的 Kubernetes 特性。 Kubernetes 介绍 (edX) 你好 Minikube  配置   示例：配置 Java 微服务 使用 ConfigMap 配置 Redis  无状态应用程序   公开外部 IP 地址访问集群中的应用程序 示例：使用 Redis 部署 PHP 留言板应用程序  有状态应用程序   StatefulSet 基础 示例：WordPress 和 MySQL 使用持久卷 示例：使用有状态集部署 Cassandra 运行 ZooKeeper，CP 分布式系统  服务   使用源 IP  安全   在集群级别应用 Pod 安全标准 在名字空间级别应用 Pod 安全标准 AppArmor seccomp  What&#39;s next 如果你要编写教程，请参阅内容页面类型 以获取有关教程页面类型的信息。">
<meta property="og:url" content="http://localhost:1313/zh/docs/tutorials/">
<meta property="og:title" content="教程">
<meta name="twitter:title" content="教程">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">

<script src="/js/script.js"></script>


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/zh/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/zh/docs/" >文档</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/blog/" >Kubernetes 博客</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/training/" >培训</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/partners/" >合作伙伴</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/community/" >社区</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/case-studies/" >案例分析</a>
			</li>
			
			
			
			<li class="nav-item dropdown">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/releases">Release Information</a>
	
	<a class="dropdown-item" href="https://kubernetes.io/zh/docs/tutorials/">v1.23</a>
	
	<a class="dropdown-item" href="https://v1-22.docs.kubernetes.io/zh/docs/tutorials/">v1.22</a>
	
	<a class="dropdown-item" href="https://v1-21.docs.kubernetes.io/zh/docs/tutorials/">v1.21</a>
	
	<a class="dropdown-item" href="https://v1-20.docs.kubernetes.io/zh/docs/tutorials/">v1.20</a>
	
	<a class="dropdown-item" href="https://v1-19.docs.kubernetes.io/zh/docs/tutorials/">v1.19</a>
	
</div>
			</li>
			
			
			<li class="nav-item dropdown">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/tutorials/">English</a>
	
	<a class="dropdown-item" href="/ko/docs/tutorials/">한국어 Korean</a>
	
	<a class="dropdown-item" href="/ja/docs/tutorials/">日本語 Japanese</a>
	
	<a class="dropdown-item" href="/fr/docs/tutorials/">Français</a>
	
	<a class="dropdown-item" href="/it/docs/tutorials/">Italiano</a>
	
	<a class="dropdown-item" href="/de/docs/tutorials/">Deutsch</a>
	
	<a class="dropdown-item" href="/pt-br/docs/tutorials/">Português</a>
	
	<a class="dropdown-item" href="/es/docs/tutorials/">Español</a>
	
	<a class="dropdown-item" href="/id/docs/tutorials/">Bahasa Indonesia</a>
	
	<a class="dropdown-item" href="/ru/docs/tutorials/">Русский</a>
	
	<a class="dropdown-item" href="/pl/docs/tutorials/">Polski</a>
	
	<a class="dropdown-item" href="/uk/docs/tutorials/">Українська</a>
	
</div>

			</li>
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/zh/docs/tutorials/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">教程</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-5e3051fff9e84735871d9fb5e7b93f33">你好，Minikube</a></li>


    
  
    
    
	
<li>2: <a href="#pg-3c83f53a74233ace9b289ac5e24c3e62">学习 Kubernetes 基础知识</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1: <a href="#pg-7df66040311338d6098ebeab43ba9afb">创建集群</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1.1: <a href="#pg-de49316920e97a82e36763cb66781ada">使用 Minikube 创建集群</a></li>


    
  
    
    
	
<li>2.1.2: <a href="#pg-323b75976001e8dfe35d67d61bc74f1a">交互式教程 - 创建集群</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.2: <a href="#pg-76d78b3fba507f7ed33cef14a35b631d">部署应用</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.2.1: <a href="#pg-2b1bba431989008c7493109a0f049ece">使用 kubectl 创建 Deployment</a></li>


    
  
    
    
	
<li>2.2.2: <a href="#pg-f8997ec143b382fa6c9621941ea62ca3">交互式教程 - 部署应用</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.3: <a href="#pg-250d620a73ec8be7e1f7d835574c4596">了解你的应用</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.3.1: <a href="#pg-2771f4e8c45321b17cb0114a2d266453">查看 pod 和工作节点</a></li>


    
  
    
    
	
<li>2.3.2: <a href="#pg-4b01eab98a9844ad91131079654199dd">交互式教程-了解你的应用</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.4: <a href="#pg-4b0e31c9e0eae68bbb0a358b4042ada9">公开地暴露你的应用</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.4.1: <a href="#pg-8ef4dad8f743b191a9e8c6f891cb191a">使用 Service 暴露您的应用</a></li>


    
  
    
    
	
<li>2.4.2: <a href="#pg-352241d22effe0714772d21c7d1b512d">交互式教程 - 暴露你的应用</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.5: <a href="#pg-be4996c93fb39c459a30b6669569d423">缩放你的应用</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.5.1: <a href="#pg-d1c15c9bd4f625adbc13149b1475287c">运行应用程序的多个实例</a></li>


    
  
    
    
	
<li>2.5.2: <a href="#pg-7bdb3fbaa1177ff5dfa3fe86bd35ef59">交互教程 - 缩放你的应用</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.6: <a href="#pg-62b8b17dadfb55f1801cf8439e944e58">更新你的应用</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.6.1: <a href="#pg-12e04355145afad615ca3c38335ba019">执行滚动更新</a></li>


    
  
    
    
	
<li>2.6.2: <a href="#pg-dddc0cb356c280e0339bcf42776987dc">交互式教程 - 更新你的应用</a></li>


    
  

    </ul>
    
  

    </ul>
    
  
    
    
	
<li>3: <a href="#pg-a3a0f1c6af19fc89ce24d8cd42c0249f">配置</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.1: <a href="#pg-e08b0be51359b976a754112b96980f54">示例：配置 java 微服务</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.1.1: <a href="#pg-025ef96f86c52822a2738b8b11b60934">使用 MicroProfile、ConfigMaps、Secrets 实现外部化应用配置</a></li>


    
  
    
    
	
<li>3.1.2: <a href="#pg-ef2047c46d3cd16631bac27403e4cfdc">互动教程 - 配置 java 微服务</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3.2: <a href="#pg-2efe621cc085b350c8c4574e6f7f1311">使用 ConfigMap 来配置 Redis</a></li>


    
  

    </ul>
    
  
    
    
	
<li>4: <a href="#pg-fe7e92bed8fb92872b139f12c4568cdb">安全</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.1: <a href="#pg-fca078b8ac6b82352ed52187a2da91b7">使用 AppArmor 限制容器对资源的访问</a></li>


    
  
    
    
	
<li>4.2: <a href="#pg-31a6c137cfc5bfea9d88f4b109109465">在名字空间级别应用 Pod 安全标准</a></li>


    
  
    
    
	
<li>4.3: <a href="#pg-d5f847bcdb6f7efbfc9c8a180d73e29a">在集群级别应用 Pod 安全标准</a></li>


    
  
    
    
	
<li>4.4: <a href="#pg-8b105172a11322c70d0223bc9dff1904">使用 seccomp 限制容器的系统调用</a></li>


    
  

    </ul>
    
  
    
    
	
<li>5: <a href="#pg-1efbbc2c3015389f835b1661d5effb29">无状态应用程序</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.1: <a href="#pg-62caf420877232190a7404b8d93c6724">公开外部 IP 地址以访问集群中应用程序</a></li>


    
  
    
    
	
<li>5.2: <a href="#pg-8c56795c6614cc5f52434ecc756448ac">示例：使用 Redis 部署 PHP 留言板应用程序</a></li>


    
  

    </ul>
    
  
    
    
	
<li>6: <a href="#pg-d6336d9712aa433eb5f0fb8cbed6bef7">有状态的应用</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>6.1: <a href="#pg-27580b3f65f3c2da07fc0f83be69da75">示例：使用 Persistent Volumes 部署 WordPress 和 MySQL</a></li>


    
  
    
    
	
<li>6.2: <a href="#pg-bf0d8e08fddd6e0282709b9fef8b5f67">示例：使用 StatefulSet 部署 Cassandra</a></li>


    
  
    
    
	
<li>6.3: <a href="#pg-4bfac214b5eb9ebddaf1f3811901d327">运行 ZooKeeper，一个分布式协调系统</a></li>


    
  
    
    
	
<li>6.4: <a href="#pg-42e39658021b706bcc9478c8cc73c4a3">StatefulSet 基础</a></li>


    
  

    </ul>
    
  
    
    
	
<li>7: <a href="#pg-97489f0aa8ac2df31a0d6b444a7bde62">Services</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>7.1: <a href="#pg-5642e8c51749e4fe2e6a2ccc207f1fab">使用 Source IP</a></li>


    
  

    </ul>
    
  

    </ul>


<div class="content">
      <!--
title: Tutorials
main_menu: true
no_list: true
weight: 60
content_type: concept
-->
<!-- overview -->
<!--
This section of the Kubernetes documentation contains tutorials.
A tutorial shows how to accomplish a goal that is larger than a single
[task](/docs/tasks/). Typically a tutorial has several sections,
each of which has a sequence of steps.
Before walking through each tutorial, you may want to bookmark the
[Standardized Glossary](/docs/reference/glossary/) page for later references.
-->
<p>Kubernetes 文档的这一部分包含教程。
每个教程展示了如何完成一个比单个<a href="/zh/docs/tasks/">任务</a>更大的目标。
通常一个教程有几个部分，每个部分都有一系列步骤。在浏览每个教程之前，
你可能希望将<a href="/zh/docs/reference/glossary/">标准化术语表</a>页面添加到书签，供以后参考。</p>
<!-- body -->
<!--
## Basics

* [Kubernetes Basics](/docs/tutorials/kubernetes-basics/) is an in-depth interactive tutorial that helps you understand the Kubernetes system and try out some basic Kubernetes features.

* [Introduction to Kubernetes (edX)](https://www.edx.org/course/introduction-kubernetes-linuxfoundationx-lfs158x#)

* [Hello Minikube](/docs/tutorials/hello-minikube/)
-->
<h2 id="basics">基础知识 </h2>
<ul>
<li><a href="/zh/docs/tutorials/Kubernetes-Basics/">Kubernetes 基础知识</a>
是一个深入的交互式教程，帮助你理解 Kubernetes 系统，并尝试一些基本的 Kubernetes 特性。</li>
<li><a href="https://www.edx.org/course/introduction-kubernetes-linuxfoundationx-lfs158x#">Kubernetes 介绍 (edX)</a></li>
<li><a href="/zh/docs/tutorials/hello-minikube/">你好 Minikube</a></li>
</ul>
<!--
## Configuration

* [Example: Configuring a Java Microservice](/docs/tutorials/configuration/configure-java-microservice/)

* [Configuring Redis Using a ConfigMap](/docs/tutorials/configuration/configure-redis-using-configmap/)
-->
<h2 id="configuration">配置 </h2>
<ul>
<li><a href="/zh/docs/tutorials/configuration/configure-java-microservice/">示例：配置 Java 微服务</a></li>
<li><a href="/zh/docs/tutorials/configuration/configure-redis-using-configmap/">使用 ConfigMap 配置 Redis</a></li>
</ul>
<!--
## Stateless Applications

* [Exposing an External IP Address to Access an Application in a Cluster](/docs/tutorials/stateless-application/expose-external-ip-address/)

* [Example: Deploying PHP Guestbook application with MongoDB](/docs/tutorials/stateless-application/guestbook/)
-->
<h2 id="stateless-applications">无状态应用程序 </h2>
<ul>
<li><a href="/zh/docs/tutorials/stateless-application/expose-external-ip-address/">公开外部 IP 地址访问集群中的应用程序</a></li>
<li><a href="/zh/docs/tutorials/stateless-application/guestbook/">示例：使用 Redis 部署 PHP 留言板应用程序</a></li>
</ul>
<!--
## Stateful Applications

* [StatefulSet Basics](/docs/tutorials/stateful-application/basic-stateful-set/)

* [Example: WordPress and MySQL with Persistent Volumes](/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/)

* [Example: Deploying Cassandra with Stateful Sets](/docs/tutorials/stateful-application/cassandra/)

* [Running ZooKeeper, A CP Distributed System](/docs/tutorials/stateful-application/zookeeper/)
-->
<h2 id="stateful-applications">有状态应用程序 </h2>
<ul>
<li><a href="/zh/docs/tutorials/stateful-application/basic-stateful-set/">StatefulSet 基础</a></li>
<li><a href="/zh/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/">示例：WordPress 和 MySQL 使用持久卷</a></li>
<li><a href="/zh/docs/tutorials/stateful-application/cassandra/">示例：使用有状态集部署 Cassandra</a></li>
<li><a href="/zh/docs/tutorials/stateful-application/zookeeper/">运行 ZooKeeper，CP 分布式系统</a></li>
</ul>
<!--
## Services

* [Using Source IP](/docs/tutorials/services/source-ip/)
-->
<h2 id="services">服务 </h2>
<ul>
<li><a href="/zh/docs/tutorials/services/source-ip/">使用源 IP</a></li>
</ul>
<!--
## Security

* [Apply Pod Security Standards at Cluster level](/docs/tutorials/security/cluster-level-pss/)
* [Apply Pod Security Standards at Namespace level](/docs/tutorials/security/ns-level-pss/)
* [AppArmor](/zh/docs/tutorials/security/apparmor/)
* [seccomp](/zh/docs/tutorials/security/seccomp/)
-->
<h2 id="security">安全 </h2>
<ul>
<li><a href="/zh/docs/tutorials/security/cluster-level-pss/">在集群级别应用 Pod 安全标准</a></li>
<li><a href="/zh/docs/tutorials/security/ns-level-pss/">在名字空间级别应用 Pod 安全标准</a></li>
<li><a href="/zh/docs/tutorials/security/apparmor/">AppArmor</a></li>
<li><a href="/zh/docs/tutorials/security/seccomp/">seccomp</a></li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
If you would like to write a tutorial, see
[Content Page Types](/docs/contribute/style/page-content-types/)
for information about the tutorial page.
-->
<p>如果你要编写教程，请参阅<a href="/zh/docs/contribute/style/page-content-types/">内容页面类型</a>
以获取有关教程页面类型的信息。</p>

</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5e3051fff9e84735871d9fb5e7b93f33">1 - 你好，Minikube</h1>
    
	<!--
title: Hello Minikube
content_type: tutorial
weight: 5
menu:
  main:
    title: "Get Started"
    weight: 10
    post: >
      <p>Ready to get your hands dirty? Build a simple Kubernetes cluster that runs a sample app.</p>
card:
  name: tutorials
  weight: 10
-->
<!-- overview -->
<!--
This tutorial shows you how to run a sample app
on Kubernetes using minikube and Katacoda.
Katacoda provides a free, in-browser Kubernetes environment.
-->
<p>本教程向你展示如何使用 Minikube 和 Katacoda
在 Kubernetes 上运行一个应用示例。Katacoda 提供免费的浏览器内 Kubernetes 环境。</p>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> You can also follow this tutorial if you've installed minikube locally.
See <a href="https://minikube.sigs.k8s.io/docs/start/">minikube start</a> for installation instructions.
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果你已在本地安装 Minikube，也可以按照本教程操作。
安装指南参阅 <a href="https://minikube.sigs.k8s.io/docs/start/">minikube start</a> 。
</div>
<h2 id="objectives">Objectives</h2>
<!--
* Deploy a sample application to minikube.
* Run the app.
* View application logs.
-->
<ul>
<li>将一个示例应用部署到 Minikube。</li>
<li>运行应用程序。</li>
<li>查看应用日志</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<!--
This tutorial provides a container image that uses NGINX to echo back all the requests.
-->
<p>本教程提供了容器镜像，使用 NGINX 来对所有请求做出回应：</p>
<!-- lessoncontent -->
<!--
## Create a minikube cluster

1. Click **Launch Terminal**
-->
<h2 id="创建-minikube-集群">创建 Minikube 集群</h2>
<ol>
<li>
<p>点击 <strong>启动终端</strong></p>
<script defer src="https://katacoda.com/embed.js"></script>
<button class="button" onclick="window.katacoda.init(); ">Launch Terminal</button>

<!-- 
If you installed minikube locally, run `minikube start`. Before you run `minikube dashboard`, you should open a new terminal, start `minikube dashboard` there, and then switch back to the main terminal.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果你在本地安装了 Minikube，运行 <code>minikube start</code>。
在运行 <code>minikube dashboard</code> 之前，你应该打开一个新终端，
在此启动 <code>minikube dashboard</code> ，然后切换回主终端。
</div>
</li>
</ol>
<!--
2. Open the Kubernetes dashboard in a browser:
-->
<ol start="2">
<li>
<p>在浏览器中打开 Kubernetes 仪表板（Dashboard）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube dashboard
</code></pre></div></li>
</ol>
<!--
3. Katacoda environment only: At the top of the terminal pane, click the plus sign, and then click **Select port to view on Host 1**.
-->
<ol start="3">
<li>仅限 Katacoda 环境：在终端窗口的顶部，单击加号，然后单击 <strong>选择要在主机 1 上查看的端口</strong>。</li>
</ol>
<!--
4. Katacoda environment only: Type `30000`, and then click **Display Port**.
-->
<ol start="4">
<li>仅限 Katacoda 环境：输入“30000”，然后单击 <strong>显示端口</strong>。</li>
</ol>
<!--
The `dashboard` command enables the dashboard add-on and opens the proxy in the default web browser. 
You can create Kubernetes resources on the dashboard such as Deployment and Service.

If you are running in an environment as root, see [Open Dashboard with URL](#open-dashboard-with-url).

By default, the dashboard is only accessible from within the internal Kubernetes virtual network.
The `dashboard` command creates a temporary proxy to make the dashboard accessible from outside the Kubernetes virtual network.

To stop the proxy, run `Ctrl+C` to exit the process.
After the command exits, the dashboard remains running in the Kubernetes cluster.
You can run the `dashboard` command again to create another proxy to access the dashboard.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p><code>dashboard</code> 命令启用仪表板插件，并在默认的 Web 浏览器中打开代理。
你可以在仪表板上创建 Kubernetes 资源，例如 Deployment 和 Service。</p>
<p>如果你以 root 用户身份在环境中运行，
请参见<a href="#open-dashboard-with-url">使用 URL 打开仪表板</a>。</p>
<p>默认情况下，仪表板只能从内部 Kubernetes 虚拟网络中访问。
<code>dashboard</code> 命令创建一个临时代理，使仪表板可以从 Kubernetes 虚拟网络外部访问。</p>
<p>要停止代理，请运行 <code>Ctrl+C</code> 退出该进程。仪表板仍在运行中。
命令退出后，仪表板仍然在 Kubernetes 集群中运行。
你可以再次运行 <code>dashboard</code> 命令创建另一个代理来访问仪表板。</p>

</div>
<!--
## Open Dashboard with URL
-->
<h2 id="使用-url-打开仪表板">使用 URL 打开仪表板</h2>
<!--
If you don't want to open a web browser, run the dashboard command with the `--url` flag to emit a URL:
-->
<p>如果你不想打开 Web 浏览器，请使用 <code>--url</code> 标志运行显示板命令以得到 URL：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube dashboard --url
</code></pre></div><!--

## Create a Deployment

A Kubernetes [*Pod*](/docs/concepts/workloads/pods/) is a group of one or more Containers,
tied together for the purposes of administration and networking. The Pod in this
tutorial has only one Container. A Kubernetes
[*Deployment*](/docs/concepts/workloads/controllers/deployment/) checks on the health of your
Pod and restarts the Pod's Container if it terminates. Deployments are the
recommended way to manage the creation and scaling of Pods.
-->
<h2 id="创建-deployment">创建 Deployment</h2>
<p>Kubernetes <a href="/zh/docs/concepts/workloads/pods/"><em>Pod</em></a> 是由一个或多个
为了管理和联网而绑定在一起的容器构成的组。 本教程中的 Pod 只有一个容器。
Kubernetes <a href="/zh/docs/concepts/workloads/controllers/deployment/"><em>Deployment</em></a>
检查 Pod 的健康状况，并在 Pod 中的容器终止的情况下重新启动新的容器。
Deployment 是管理 Pod 创建和扩展的推荐方法。</p>
<!--
1. Use the `kubectl create` command to create a Deployment that manages a Pod. The
Pod runs a Container based on the provided Docker image.
-->
<ol>
<li>
<p>使用 <code>kubectl create</code> 命令创建管理 Pod 的 Deployment。该 Pod 根据提供的 Docker
镜像运行 Container。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment hello-node --image<span style="color:#666">=</span>k8s.gcr.io/echoserver:1.4
</code></pre></div></li>
</ol>
<!--
2. View the Deployment:
-->
<ol start="2">
<li>
<p>查看 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployments
</code></pre></div><!--
The output is similar to:
-->
<p>输出结果类似于这样：</p>
<pre><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
hello-node   1/1     1            1           1m
</code></pre></li>
</ol>
<!--
3. View the Pod:
-->
<ol start="3">
<li>
<p>查看 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The output is similar to:
-->
<p>输出结果类似于这样：</p>
<pre><code>NAME                          READY     STATUS    RESTARTS   AGE
hello-node-5f76cf6ccf-br9b5   1/1       Running   0          1m
</code></pre></li>
</ol>
<!--
4. View cluster events:
-->
<ol start="4">
<li>
<p>查看集群事件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get events
</code></pre></div></li>
</ol>
<!--
5. View the `kubectl` configuration:
-->
<ol start="5">
<li>
<p>查看 <code>kubectl</code> 配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div></li>
</ol>
<!--
For more information about `kubectl`commands, see the
[kubectl overview](/docs/reference/kubectl/).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 有关 <code>kubectl</code> 命令的更多信息，请参阅 <a href="/zh/docs/reference/kubectl/">kubectl 概述</a>。
</div>
<!--
## Create a Service

By default, the Pod is only accessible by its internal IP address within the
Kubernetes cluster. To make the `hello-node` Container accessible from outside the
Kubernetes virtual network, you have to expose the Pod as a
Kubernetes [*Service*](/docs/concepts/services-networking/service/).
-->
<h2 id="创建-service">创建 Service</h2>
<p>默认情况下，Pod 只能通过 Kubernetes 集群中的内部 IP 地址访问。
要使得 <code>hello-node</code> 容器可以从 Kubernetes 虚拟网络的外部访问，你必须将 Pod
暴露为 Kubernetes <a href="/zh/docs/concepts/services-networking/service/"><em>Service</em></a>。</p>
<!--
1. Expose the Pod to the public internet using the `kubectl expose` command:
-->
<ol>
<li>
<p>使用 <code>kubectl expose</code> 命令将 Pod 暴露给公网：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose deployment hello-node --type<span style="color:#666">=</span>LoadBalancer --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</code></pre></div><!--
 The `--type=LoadBalancer` flag indicates that you want to expose your Service
 outside of the cluster.

 The application code inside the image `k8s.gcr.io/echoserver` only listens on TCP port 8080. If you used
 `kubectl expose` to expose a different port, clients could not connect to that other port.
-->
<p>这里的 <code>--type=LoadBalancer</code> 参数表明你希望将你的 Service 暴露到集群外部。</p>
<p>镜像 <code>k8s.gcr.io/echoserver</code> 中的应用程序代码仅监听 TCP 8080 端口。
如果你用 <code>kubectl expose</code> 暴露了其它的端口，客户端将不能访问其它端口。</p>
</li>
</ol>
<!--
2. View the Service you created:
-->
<ol start="2">
<li>
<p>查看你创建的 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get services
</code></pre></div><!--
The output is similar to:
-->
<p>输出结果类似于这样:</p>
<pre><code>NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
hello-node   LoadBalancer   10.108.144.78   &lt;pending&gt;     8080:30369/TCP   21s
kubernetes   ClusterIP      10.96.0.1       &lt;none&gt;        443/TCP          23m
</code></pre><!--
On cloud providers that support load balancers,
an external IP address would be provisioned to access the Service. On minikube,
the `LoadBalancer` type makes the Service accessible through the `minikube service`
command.
-->
<p>对于支持负载均衡器的云服务平台而言，平台将提供一个外部 IP 来访问该服务。
在 Minikube 上，<code>LoadBalancer</code> 使得服务可以通过命令 <code>minikube service</code> 访问。</p>
</li>
</ol>
<!--
3. Run the following command:
-->
<ol start="3">
<li>
<p>运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube service hello-node
</code></pre></div></li>
</ol>
<!--
4. Katacoda environment only: Click the plus sign, and then click **Select port to view on Host 1**.
-->
<ol start="4">
<li>仅限 Katacoda 环境：单击加号，然后单击 <strong>选择要在主机 1 上查看的端口</strong>。</li>
</ol>
<!--
5. Katacoda environment only: Note the 5-digit port number displayed opposite to `8080` in services output. This port number is randomly generated and it can be different for you. Type your number in the port number text box, then click Display Port. Using the example from earlier, you would type `30369`.

    This opens up a browser window that serves your app and shows the app's response.
-->
<ol start="5">
<li>
<p>仅限 Katacoda 环境：请注意在 service 输出中与 <code>8080</code> 对应的长度为 5 位的端口号。
此端口号是随机生成的，可能与你的不同。
在端口号文本框中输入你自己的端口号，然后单击显示端口。
对应于上面的例子，需要输入 <code>30369</code>。</p>
<p>这将打开一个浏览器窗口，为你的应用程序提供服务并显示应用的响应。</p>
</li>
</ol>
<!--
## Enable addons

The minikube tool includes a set of built-in <a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='addons'>addons</a> that can be enabled, disabled and opened in the local Kubernetes environment.

1. List the currently supported addons:
-->
<h2 id="启用插件">启用插件</h2>
<p>Minikube 有一组内置的 <a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='插件'>插件</a>，
可以在本地 Kubernetes 环境中启用、禁用和打开。</p>
<ol>
<li>
<p>列出当前支持的插件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube addons list
</code></pre></div><!--
The output is similar to:
-->
<p>输出结果类似于这样：</p>
<pre><code>addon-manager: enabled
dashboard: enabled
default-storageclass: enabled
efk: disabled
freshpod: disabled
gvisor: disabled
helm-tiller: disabled
ingress: disabled
ingress-dns: disabled
logviewer: disabled
metrics-server: disabled
nvidia-driver-installer: disabled
nvidia-gpu-device-plugin: disabled
registry: disabled
registry-creds: disabled
storage-provisioner: enabled
storage-provisioner-gluster: disabled
</code></pre></li>
</ol>
<!--
2. Enable an addon, for example, `metrics-server`:
-->
<ol start="2">
<li>
<p>启用插件，例如 <code>metrics-server</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube addons <span style="color:#a2f">enable</span> metrics-server
</code></pre></div><!--
The output is similar to:
-->
<p>输出结果类似于这样：</p>
<pre><code>The 'metrics-server' addon is enabled
</code></pre></li>
</ol>
<!--
3. View the Pod and Service you created:
-->
<ol start="3">
<li>
<p>查看创建的 Pod 和 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod,svc -n kube-system
</code></pre></div><!--
The output is similar to:
-->
<p>输出结果类似于这样：</p>
<pre><code>NAME                                        READY     STATUS    RESTARTS   AGE
pod/coredns-5644d7b6d9-mh9ll                1/1       Running   0          34m
pod/coredns-5644d7b6d9-pqd2t                1/1       Running   0          34m
pod/metrics-server-67fb648c5                1/1       Running   0          26s
pod/etcd-minikube                           1/1       Running   0          34m
pod/influxdb-grafana-b29w8                  2/2       Running   0          26s
pod/kube-addon-manager-minikube             1/1       Running   0          34m
pod/kube-apiserver-minikube                 1/1       Running   0          34m
pod/kube-controller-manager-minikube        1/1       Running   0          34m
pod/kube-proxy-rnlps                        1/1       Running   0          34m
pod/kube-scheduler-minikube                 1/1       Running   0          34m
pod/storage-provisioner                     1/1       Running   0          34m

NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/metrics-server         ClusterIP   10.96.241.45    &lt;none&gt;        80/TCP              26s
service/kube-dns               ClusterIP   10.96.0.10      &lt;none&gt;        53/UDP,53/TCP       34m
service/monitoring-grafana     NodePort    10.99.24.54     &lt;none&gt;        80:30002/TCP        26s
service/monitoring-influxdb    ClusterIP   10.111.169.94   &lt;none&gt;        8083/TCP,8086/TCP   26s
</code></pre></li>
</ol>
<!--
4. Disable `metrics-server`:
-->
<ol start="4">
<li>
<p>禁用 <code>metrics-server</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube addons disable metrics-server
</code></pre></div><!--
The output is similar to:
-->
<p>输出结果类似于这样：</p>
<pre><code>metrics-server was successfully disabled
</code></pre></li>
</ol>
<!--
## Clean up

Now you can clean up the resources you created in your cluster:
-->
<h2 id="清理">清理</h2>
<p>现在可以清理你在集群中创建的资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete service hello-node
kubectl delete deployment hello-node
</code></pre></div><!--
Optionally, stop the Minikube virtual machine (VM):
-->
<p>可选地，停止 Minikube 虚拟机（VM）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube stop
</code></pre></div><!--
Optionally, delete the Minikube VM:
-->
<p>可选地，删除 Minikube 虚拟机（VM）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube delete
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [Deployment objects](/docs/concepts/workloads/controllers/deployment/).
* Learn more about [Deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/).
* Learn more about [Service objects](/docs/concepts/services-networking/service/).
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment 对象</a>。</li>
<li>进一步了解<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">部署应用</a>。</li>
<li>进一步了解 <a href="/zh/docs/concepts/services-networking/service/">Service 对象</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3c83f53a74233ace9b289ac5e24c3e62">2 - 学习 Kubernetes 基础知识</h1>
    
	
<!DOCTYPE html>

<html lang="zh">

<body>

  <link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">

<div class="layout" id="top">

  <main class="content">

    <div class="row">
      <div class="col-md-9">
        <h2>Kubernetes 基础</h2>
        <p>本教程介绍了 Kubernetes 集群编排系统的基础知识。每个模块包含关于 Kubernetes 主要特性和概念的一些背景信息，并包括一个在线互动教程。这些互动教程让您可以自己管理一个简单的集群及其容器化应用程序。</p>
        <p>使用互动教程，您可以学习：</p>
        <ul>
        <li>在集群上部署容器化应用程序</li>
        <li>弹性部署</li>
        <li>使用新的软件版本，更新容器化应用程序</li>
        <li>调试容器化应用程序</li>
        </ul>
        <p>教程 Katacoda 在您的浏览器中运行一个虚拟终端，在浏览器中运行 Minikube，这是一个可在任何地方小规模本地部署的 Kubernetes 集群。不需要安装任何软件或进行任何配置；每个交互性教程都直接从您的网页浏览器上运行。</p>
      </div>
    </div>

    <br>

    <div class="row">
      <div class="col-md-9">
        <h2>Kubernetes 可以为您做些什么?</h2>
        <p>通过现代的 Web 服务，用户希望应用程序能够 24/7 全天候使用，开发人员希望每天可以多次发布部署新版本的应用程序。 容器化可以帮助软件包达成这些目标，使应用程序能够以简单快速的方式发布和更新，而无需停机。Kubernetes 帮助您确保这些容器化的应用程序在您想要的时间和地点运行，并帮助应用程序找到它们需要的资源和工具。Kubernetes 是一个可用于生产的开源平台，根据 Google 容器集群方面积累的经验，以及来自社区的最佳实践而设计。</p>
      </div>
    </div>

     <br>


    <div id="basics-modules" class="content__modules">
      <h2>Kubernetes 基础模块</h2>
      <div class="row">
        <div class="col-md-12">
          <div class="row">
            <div class="col-md-4">
              <div class="thumbnail">
                <a href="/zh/docs/tutorials/kubernetes-basics/create-cluster/cluster-intro/"><img src="/docs/tutorials/kubernetes-basics/public/images/module_01.svg?v=1469803628347" alt=""></a>
                <div class="caption">
                  <a href="/zh/docs/tutorials/kubernetes-basics/create-cluster/cluster-intro/"><h5>1. 创建一个 Kubernetes 集群</h5></a>
                </div>
              </div>
            </div>
            <div class="col-md-4">
              <div class="thumbnail">
                <a href="/zh/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/"><img src="/docs/tutorials/kubernetes-basics/public/images/module_02.svg?v=1469803628347" alt=""></a>
                  <div class="caption">
                    <a href="/zh/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/"><h5>2. 部署应用程序</h5></a>
                  </div>
              </div>
            </div>
            <div class="col-md-4">
              <div class="thumbnail">
                <a href="/zh/docs/tutorials/kubernetes-basics/explore/explore-intro/"><img src="/docs/tutorials/kubernetes-basics/public/images/module_03.svg?v=1469803628347" alt=""></a>
                <div class="caption">
                  <a href="/zh/docs/tutorials/kubernetes-basics/explore/explore-intro/"><h5>3. 应用程序探索</h5></a>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="col-md-12">
          <div class="row">
            <div class="col-md-4">
              <div class="thumbnail">
                <a href="/zh/docs/tutorials/kubernetes-basics/expose/expose-intro/"><img src="/docs/tutorials/kubernetes-basics/public/images/module_04.svg?v=1469803628347" alt=""></a>
                <div class="caption">
                  <a href="/zh/docs/tutorials/kubernetes-basics/expose/expose-intro/"><h5>4. 应用外部可见</h5></a>
                </div>
              </div>
            </div>
            <div class="col-md-4">
              <div class="thumbnail">
                <a href="/zh/docs/tutorials/kubernetes-basics/scale/scale-intro/"><img src="/docs/tutorials/kubernetes-basics/public/images/module_05.svg?v=1469803628347" alt=""></a>
                <div class="caption">
                  <a href="/zh/docs/tutorials/kubernetes-basics/scale/scale-intro/"><h5>5. 应用可扩展</h5></a>
                </div>
              </div>
            </div>
            <div class="col-md-4">
              <div class="thumbnail">
                <a href="/zh/docs/tutorials/kubernetes-basics/update/update-intro/"><img src="/docs/tutorials/kubernetes-basics/public/images/module_06.svg?v=1469803628347" alt=""></a>
                <div class="caption">
                  <a href="/zh/docs/tutorials/kubernetes-basics/update/update-intro/"><h5>6. 应用更新</h5></a>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
   </div>

  </main>

</div>

</body>
</html>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7df66040311338d6098ebeab43ba9afb">2.1 - 创建集群</h1>
    
	<!--
Learn about Kubernetes <a class='glossary-tooltip' title='集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cluster' target='_blank' aria-label='cluster'>cluster</a> and create a simple cluster using Minikube.
-->
<p>了解 Kubernetes <a class='glossary-tooltip' title='集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cluster' target='_blank' aria-label='集群'>集群</a>并使用 Minikube
创建一个简单的集群。</p>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-de49316920e97a82e36763cb66781ada">2.1.1 - 使用 Minikube 创建集群</h1>
    
	
<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">

<div class="layout" id="top">

    <main class="content">

        <div class="row">

      <div class="col-md-8">
          <h3>目标</h3>
                <ul>
                    <li>了解 Kubernetes 集群。</li>
                    <li>了解 Minikube 。</li>
                    <li>使用在线终端开启一个 Kubernetes 集群。</li>
                </ul>
            </div>

            <div class="col-md-8">
                <h3> Kubernetes 集群</h3>
                <p>
                <b> Kubernetes 协调一个高可用计算机集群，每个计算机作为独立单元互相连接工作。</b> Kubernetes 中的抽象允许您将容器化的应用部署到集群，而无需将它们绑定到某个特定的独立计算机。为了使用这种新的部署模型，应用需要以将应用与单个主机分离的方式打包：它们需要被容器化。与过去的那种应用直接以包的方式深度与主机集成的部署模型相比，容器化应用更灵活、更可用。<b> Kubernetes 以更高效的方式跨集群自动分发和调度应用容器。</b> Kubernetes 是一个开源平台，并且可应用于生产环境。
                </p>
                <p>一个 Kubernetes 集群包含两种类型的资源:
                    <ul>
                        <li><b> Master </b>调度整个集群</li>
                        <li><b> Nodes </b>负责运行应用</li>
                    </ul>
                </p>
            </div>

            <div class="col-md-4">
                <div class="content__box content__box_lined">
                    <h3>总结:</h3>
                    <ul>
                        <li> Kubernetes 集群</li>
                        <li> Minikube </li>
                    </ul>
                </div>
                <div class="content__box content__box_fill">
                    <p><i>
                        Kubernetes 是一个生产级别的开源平台，可协调在计算机集群内和跨计算机集群的应用容器的部署（调度）和执行.
                    </i></p>
                </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8">
                <h2 style="color: #3771e3;">集群图</h2>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8">
                <p><img src="/docs/tutorials/kubernetes-basics/public/images/module_01_cluster.svg"></p>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8">
                <p><b> Master 负责管理整个集群。</b> Master 协调集群中的所有活动，例如调度应用、维护应用的所需状态、应用扩容以及推出新的更新。</p>
                <p><b> Node 是一个虚拟机或者物理机，它在 Kubernetes 集群中充当工作机器的角色</b> 每个Node都有 Kubelet , 它管理 Node 而且是 Node 与 Master 通信的代理。 Node 还应该具有用于​​处理容器操作的工具，例如 Docker 或 rkt 。处理生产级流量的 Kubernetes 集群至少应具有三个 Node，因为如果一个 Node 出现故障其对应的 etcd 成员和控制平面实例都会丢失，并且冗余会受到影响。 你可以通过添加更多控制平面节点来降低这种风险 。</p>

            </div>
            <div class="col-md-4">
                <div class="content__box content__box_fill">
                    <p><i> Master 管理集群，Node 用于托管正在运行的应用。</i></p>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8">
                <p>在 Kubernetes 上部署应用时，您告诉 Master 启动应用容器。 Master 就编排容器在集群的 Node 上运行。<b> Node 使用 Master 暴露的 Kubernetes API 与 Master 通信。</b>终端用户也可以使用 Kubernetes API 与集群交互。</p>

                <p> Kubernetes 既可以部署在物理机上也可以部署在虚拟机上。您可以使用 Minikube 开始部署 Kubernetes 集群。 Minikube 是一种轻量级的 Kubernetes 实现，可在本地计算机上创建 VM 并部署仅包含一个节点的简单集群。 Minikube 可用于 Linux ， macOS 和 Windows 系统。Minikube CLI 提供了用于引导集群工作的多种操作，包括启动、停止、查看状态和删除。在本教程里，您可以使用预装有 Minikube 的在线终端进行体验。</p>

                <p>既然您已经知道 Kubernetes 是什么，让我们转到在线教程并启动我们的第一个 Kubernetes 集群！</p>

            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-12">
                <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/create-cluster/cluster-interactive/" role="button"> 启动交互教程 <span class="btn__next">›</span></a>
            </div>
        </div>

    </main>

</div>

</body>
</html>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-323b75976001e8dfe35d67d61bc74f1a">2.1.2 - 交互式教程 - 创建集群</h1>
    
	
<!--
---
title: Interactive Tutorial - Creating a Cluster
weight: 20
---
-->

<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">
<link href="/docs/tutorials/kubernetes-basics/public/css/overrides.css" rel="stylesheet">
<script src="https://katacoda.com/embed.js"></script>

<div class="layout" id="top">

    <main class="content katacoda-content">

        <div class="katacoda">
            <div class="katacoda__alert">
            
                <!--
                To interact with the Terminal, please use the desktop/tablet version
                -->
                要与终端交互，请使用桌面/平板
            
            </div>
            <div class="katacoda__box" id="inline-terminal-1"  data-katacoda-id="kubernetes-bootcamp/1" data-katacoda-color="326de6" data-katacoda-secondary="273d6d" data-katacoda-hideintro="false" data-katacoda-prompt="Kubernetes Bootcamp Terminal" style="height: 600px;"></div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <!--
                <a class="btn btn-lg btn-success" href="/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/" role="button">Continue to Module 2<span class="btn__next">›</span></a>
                -->
                <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/" role="button">继续阅读第二单元<span class="btn__next">›</span></a>
                
            </div>
        </div>

    </main>

</div>

</body>
</html>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-76d78b3fba507f7ed33cef14a35b631d">2.2 - 部署应用</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-2b1bba431989008c7493109a0f049ece">2.2.1 - 使用 kubectl 创建 Deployment</h1>
    
	
<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">

<div class="layout" id="top">

    <main class="content">

        <div class="row">

         <div class="col-md-8">
          <!-- <h3>Objectives</h3> -->
          <h3>目标</h3>
                <!-- <ul>
                    <li>Learn about application Deployments.</li>
                    <li>Deploy your first app on Kubernetes with kubectl.</li>
                </ul> -->
                <ul>
                  <li>学习了解应用的部署</li>
                  <li>使用 kubectl 在 Kubernetes 上部署第一个应用</li>
              </ul>
            </div>

            <div class="col-md-8">
                <!-- <h3>Kubernetes Deployments</h3> -->
                <h3>Kubernetes 部署</h3>
                <!-- <p>
                Once you have a running Kubernetes cluster, you can deploy your containerized applications on top of it.
                To do so, you create a Kubernetes <b>Deployment</b> configuration. The Deployment instructs Kubernetes
                how to create and update instances of your application. Once you've created a Deployment, the Kubernetes
                master schedules mentioned application instances onto individual Nodes in the cluster.
                </p> -->
                <p>
                  一旦运行了 Kubernetes 集群，就可以在其上部署容器化应用程序。
                  为此，您需要创建 Kubernetes <b> Deployment </b>配置。Deployment 指挥 Kubernetes 如何创建和更新应用程序的实例。创建 Deployment 后，Kubernetes master 将应用程序实例调度到集群中的各个节点上。
                  </p>

                <!-- <p>Once the application instances are created, a Kubernetes Deployment Controller continuously monitors those instances. If the Node hosting an instance goes down or is deleted, the Deployment controller replaces the instance with an instance on another Node in the cluster. <b>This provides a self-healing mechanism to address machine failure or maintenance.</b></p> -->
                <p>创建应用程序实例后，Kubernetes Deployment 控制器会持续监视这些实例。 如果托管实例的节点关闭或被删除，则 Deployment 控制器会将该实例替换为群集中另一个节点上的实例。 <b>这提供了一种自我修复机制来解决机器故障维护问题。</b></p>

                <!-- <p>In a pre-orchestration world, installation scripts would often be used to start applications, but they did not allow recovery from machine failure.  By both creating your application instances and keeping them running across Nodes, Kubernetes Deployments provide a fundamentally different approach to application management. </p> -->
                <p>在没有 Kubernetes 这种编排系统之前，安装脚本通常用于启动应用程序，但它们不允许从机器故障中恢复。通过创建应用程序实例并使它们在节点之间运行， Kubernetes Deployments 提供了一种与众不同的应用程序管理方法。</p>

            </div>

            <div class="col-md-4">
                <div class="content__box content__box_lined">
                    <!-- <h3>Summary:</h3> -->
                    <h3>总结:</h3>
                    <ul>
                        <li>Deployments</li>
                        <li>Kubectl</li>
                    </ul>
                </div>
                <div class="content__box content__box_fill">
                    <!-- <p><i>
                        A Deployment is responsible for creating and updating instances of your application
                    </i></p> -->
                    <p><i>
                      Deployment 负责创建和更新应用程序的实例
                  </i></p>
                </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8">
                <!-- <h2 style="color: #3771e3;">Deploying your first app on Kubernetes</h2> -->
                <h2 style="color: #3771e3;">部署你在 Kubernetes 上的第一个应用程序</h2>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8">
                <p><img src="/docs/tutorials/kubernetes-basics/public/images/module_02_first_app.svg"></p>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8">

                <!-- <p>You can create and manage a Deployment by using the Kubernetes command line interface, <b>Kubectl</b>. Kubectl uses the Kubernetes API to interact with the cluster. In this module, you'll learn the most common Kubectl commands needed to create Deployments that run your applications on a Kubernetes cluster.</p> -->
                <p>您可以使用 Kubernetes 命令行界面 <b>Kubectl</b> 创建和管理 Deployment。Kubectl 使用 Kubernetes API 与集群进行交互。在本单元中，您将学习创建在 Kubernetes 集群上运行应用程序的 Deployment 所需的最常见的 Kubectl 命令。</p>

                <!-- <p>When you create a Deployment, you'll need to specify the container image for your application and the number of replicas that you want to run. You can change that information later by updating your Deployment; Modules <a href="/docs/tutorials/kubernetes-basics/scale-intro/">5</a> and <a href="/docs/tutorials/kubernetes-basics/update-intro/">6</a> of the bootcamp discuss how you can scale and update your Deployments.</p> -->
                <p>创建 Deployment 时，您需要指定应用程序的容器映像以及要运行的副本数。您可以稍后通过更新 Deployment 来更改该信息; 模块 <a href="/zh/docs/tutorials/kubernetes-basics/scale-intro/">5</a> 和 <a href="/zh/docs/tutorials/kubernetes-basics/update-intro/">6</a> 讨论了如何扩展和更新 Deployments。</p>

            </div>
            <div class="col-md-4">
                <div class="content__box content__box_fill">
                    <!-- <p><i> Applications need to be packaged into one of the supported container formats in order to be deployed on Kubernetes </i></p> -->
                    <p><i> 应用程序需要打包成一种受支持的容器格式，以便部署在 Kubernetes 上 </i></p>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8">

                <!-- <p>For our first Deployment, we'll use a Node.js application packaged in a Docker container.
                    To create the Node.js application and deploy the Docker container, follow the instructions from the
                    <a href="/docs/tutorials/hello-minikube/">Hello Minikube tutorial</a>.</p>

                <p>Now that you know what Deployments are, let's go to the online tutorial and deploy our first app!</p> -->
                <p>对于我们的第一次部署，我们将使用打包在 Docker 容器中的 Node.js 应用程序。
                    要创建 Node.js 应用程序并部署 Docker 容器，请按照
                    <a href="/zh/docs/tutorials/hello-minikube/">你好 Minikube 教程</a>.</p>

                <p>现在您已经了解了 Deployment 的内容，让我们转到在线教程并部署我们的第一个应用程序！</p>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-12">
                <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/deploy-app/deploy-interactive/" role="button">开始交互式教程 <span class="btn__next">›</span></a>
            </div>
        </div>

    </main>

</div>

</body>
</html>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f8997ec143b382fa6c9621941ea62ca3">2.2.2 - 交互式教程 - 部署应用</h1>
    
	
<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">
<link href="/docs/tutorials/kubernetes-basics/public/css/overrides.css" rel="stylesheet">
<script src="https://katacoda.com/embed.js"></script>

<div class="layout" id="top">

    <main class="content katacoda-content">

        <br>
        <div class="katacoda">
            <!-- <div class="katacoda__alert">
                To interact with the Terminal, please use the desktop/tablet version
            </div> -->
            <div class="katacoda__alert">
                要与终端进行交互，请使用桌面/平板电脑版本
            </div>

            <div class="katacoda__box" id="inline-terminal-1"  data-katacoda-id="kubernetes-bootcamp/7" data-katacoda-color="326de6" data-katacoda-secondary="273d6d" data-katacoda-hideintro="false" data-katacoda-prompt="Kubernetes Bootcamp Terminal" style="height: 600px;">
            </div>

        </div>
        <div class="row">
            <div class="col-md-12">
                <!-- <a class="btn btn-lg btn-success" href="/docs/tutorials/kubernetes-basics/explore/explore-intro/" role="button">Continue to Module 3<span class="btn__next">›</span></a> -->
                <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/explore/explore-intro/" role="button">继续阅读第3单元<span class="btn__next">›</span></a>
            </div>
        </div>

    </main>

</div>

</body>
</html>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-250d620a73ec8be7e1f7d835574c4596">2.3 - 了解你的应用</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-2771f4e8c45321b17cb0114a2d266453">2.3.1 - 查看 pod 和工作节点</h1>
    
	
<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">


<div class="layout" id="top">

  <main class="content">

    <div class="row">

      <div class="col-md-8">
<!--
        <h3>Objectives</h3>
-->
        <h3>目标</h3>
        <ul>
<!--
          <li>Learn about Kubernetes Pods.</li>
          <li>Learn about Kubernetes Nodes.</li>
          <li>Troubleshoot deployed applications.</li>
-->
          <li>了解 Kubernetes Pod。</li>
          <li>了解 Kubernetes 工作节点。</li>
          <li>对已部署的应用故障排除。</li>
        </ul>
      </div>

      <div class="col-md-8">
<!--
        <h2>Kubernetes Pods</h2>
-->
        <h2>Kubernetes Pods</h2>
<!--
        <p>When you created a Deployment in Module <a href="/docs/tutorials/kubernetes-basics/deploy-intro/">2</a>, Kubernetes created a <b>Pod</b> to host your application instance. A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker), and some shared resources for those containers. Those resources include:</p>
-->
        <p>在模块 <a href="/zh/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/">2</a>创建 Deployment 时, Kubernetes 添加了一个 <b>Pod</b> 来托管你的应用实例。Pod 是 Kubernetes 抽象出来的，表示一组一个或多个应用程序容器（如 Docker），以及这些容器的一些共享资源。这些资源包括:</p>
<!--
        <ul>
          <li>Shared storage, as Volumes</li>
          <li>Networking, as a unique cluster IP address</li>
          <li>Information about how to run each container, such as the container image version or specific ports to use</li>
        </ul>
-->
        <ul>
          <li>共享存储，当作卷</li>
          <li>网络，作为唯一的集群 IP 地址</li>
          <li>有关每个容器如何运行的信息，例如容器映像版本或要使用的特定端口。</li>
        </ul>
    <!--
        <p>A Pod models an application-specific "logical host" and can contain different application containers which are relatively tightly coupled. For example, a Pod might include both the container with your Node.js app as well as a different container that feeds the data to be published by the Node.js webserver. The containers in a Pod share an IP Address and port space, are always co-located and co-scheduled, and run in a shared context on the same Node.</p>
    -->
        <p>Pod 为特定于应用程序的“逻辑主机”建模，并且可以包含相对紧耦合的不同应用容器。例如，Pod 可能既包含带有 Node.js 应用的容器，也包含另一个不同的容器，用于提供 Node.js 网络服务器要发布的数据。Pod 中的容器共享 IP 地址和端口，始终位于同一位置并且共同调度，并在同一工作节点上的共享上下文中运行。</p>
<!--
        <p>Pods are the atomic unit on the Kubernetes platform. When we create a Deployment on Kubernetes, that Deployment creates Pods with containers inside them (as opposed to creating containers directly). Each Pod is tied to the Node where it is scheduled, and remains there until termination (according to restart policy) or deletion. In case of a Node failure, identical Pods are scheduled on other available Nodes in the cluster.</p>
-->
        <p>Pod是 Kubernetes 平台上的原子单元。 当我们在 Kubernetes 上创建 Deployment 时，该 Deployment 会在其中创建包含容器的 Pod （而不是直接创建容器）。每个 Pod 都与调度它的工作节点绑定，并保持在那里直到终止（根据重启策略）或删除。 如果工作节点发生故障，则会在群集中的其他可用工作节点上调度相同的 Pod。</p>

      </div>
      <div class="col-md-4">
        <div class="content__box content__box_lined">
<!--
          <h3>Summary:</h3>
-->
          <h3>总结:</h3>
          <ul>
            <li>Pods</li>
            <li>工作节点</li>
<!--
            <li>Kubectl main commands</li>
-->
            <li>Kubectl 主要命令</li>
          </ul>
        </div>
        <div class="content__box content__box_fill">
<!--
          <p><i>
              A Pod is a group of one or more application containers (such as Docker) and includes shared storage (volumes), IP address and information about how to run them.
            </i></p>
-->
          <p><i>
              Pod 是一组一个或多个应用程序容器（例如 Docker），包括共享存储（卷), IP 地址和有关如何运行它们的信息。
            </i></p>
        </div>
      </div>
    </div>
    <br>

    <div class="row">
      <div class="col-md-8">
<!--
        <h2 style="color: #3771e3;">Pods overview</h2>
-->
        <h2 style="color: #3771e3;">Pod 概览</h2>
      </div>
    </div>

    <div class="row">
      <div class="col-md-8">
        <p><img src="/docs/tutorials/kubernetes-basics/public/images/module_03_pods.svg"></p>
      </div>
    </div>
    <br>

    <div class="row">
      <div class="col-md-8">
<!--
        <h2>Nodes</h2>
        <p>A Pod always runs on a <b>Node</b>. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the Master. A Node can have multiple pods, and the Kubernetes master automatically handles scheduling the pods across the Nodes in the cluster. The Master's automatic scheduling takes into account the available resources on each Node.</p>

        <p>Every Kubernetes Node runs at least:</p>
        <ul>
          <li>Kubelet, a process responsible for communication between the Kubernetes Master and the Node; it manages the Pods and the containers running on a machine.</li>
          <li>A container runtime (like Docker) responsible for pulling the container image from a registry, unpacking the container, and running the application.</li>
        </ul>
-->
        <h2>工作节点</h2>
        <p>一个 pod 总是运行在 <b>工作节点</b>。工作节点是 Kubernetes 中的参与计算的机器，可以是虚拟机或物理计算机，具体取决于集群。每个工作节点由主节点管理。工作节点可以有多个 pod ，Kubernetes 主节点会自动处理在群集中的工作节点上调度 pod 。 主节点的自动调度考量了每个工作节点上的可用资源。</p>

        <p>每个 Kubernetes 工作节点至少运行:</p>
        <ul>
          <li>Kubelet，负责 Kubernetes 主节点和工作节点之间通信的过程; 它管理 Pod 和机器上运行的容器。</li>
          <li>容器运行时（如 Docker）负责从仓库中提取容器镜像，解压缩容器以及运行应用程序。</li>
        </ul>
      </div>
      <div class="col-md-4">
        <div class="content__box content__box_fill">
<!--
          <p><i> Containers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk. </i></p>
-->
          <p><i>如果它们紧耦合并且需要共享磁盘等资源，这些容器应在一个 Pod 中编排。</i></p>
        </div>
      </div>
    </div>

    <br>

    <div class="row">
      <div class="col-md-8">
<!--
        <h2 style="color: #3771e3;">Node overview</h2>
-->
        <h2 style="color: #3771e3;">工作节点概览</h2>
      </div>
    </div>

    <div class="row">
      <div class="col-md-8">
        <p><img src="/docs/tutorials/kubernetes-basics/public/images/module_03_nodes.svg"></p>
      </div>
    </div>
    <br>

    <div class="row">
      <div class="col-md-8">
<!--
        <h2>Troubleshooting with kubectl</h2>
-->
        <h2>使用 kubectl 进行故障排除</h2>
<!--
        <p>In Module <a href="/docs/tutorials/kubernetes-basics/deploy/deploy-intro/">2</a>, you used Kubectl command-line interface. You'll continue to use it in Module 3 to get information about deployed applications and their environments. The most common operations can be done with the following kubectl commands:</p>
-->
        <p>在模块 <a href="/zh/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/">2</a>,您使用了 Kubectl 命令行界面。 您将继续在第3单元中使用它来获取有关已部署的应用程序及其环境的信息。 最常见的操作可以使用以下 kubectl 命令完成：</p>
<!--
        <ul>
          <li><b>kubectl get</b> - list resources</li>
          <li><b>kubectl describe</b> - show detailed information about a resource</li>
          <li><b>kubectl logs</b> - print the logs from a container in a pod</li>
          <li><b>kubectl exec</b> - execute a command on a container in a pod</li>
        </ul>
-->
        <ul>
          <li><b>kubectl get</b> - 列出资源</li>
          <li><b>kubectl describe</b> - 显示有关资源的详细信息</li>
          <li><b>kubectl logs</b> - 打印 pod 和其中容器的日志</li>
          <li><b>kubectl exec</b> - 在 pod 中的容器上执行命令</li>
        </ul>
<!--
        <p>You can use these commands to see when applications were deployed, what their current statuses are, where they are running and what their configurations are.</p>
-->
        <p>您可以使用这些命令查看应用程序的部署时间，当前状态，运行位置以及配置。</p>
<!--
        <p>Now that we know more about our cluster components and the command line, let's explore our application.</p>
-->
        <p>现在我们了解了有关集群组件和命令行的更多信息，让我们来探索一下我们的应用程序。</p>

      </div>
      <div class="col-md-4">
        <div class="content__box content__box_fill">
<!--
          <p><i> A node is a worker machine in Kubernetes and may be a VM or physical machine, depending on the cluster. Multiple Pods can run on one Node. </i></p>
-->
          <p><i>工作节点是 Kubernetes 中的负责计算的机器，可能是VM或物理计算机，具体取决于群集。多个 Pod 可以在一个工作节点上运行。 </i></p>
        </div>
      </div>
    </div>
    <br>

    <div class="row">
      <div class="col-md-12">
<!--
        <a class="btn btn-lg btn-success" href="/docs/tutorials/kubernetes-basics/explore/explore-interactive/" role="button">Start Interactive Tutorial <span class="btn__next">›</span></a>
-->
        <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/explore/explore-interactive/" role="button"> 开始交互式教程 <span class="btn__next">›</span></a>
      </div>
    </div>

  </main>

</div>

</body>
</html>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4b01eab98a9844ad91131079654199dd">2.3.2 - 交互式教程-了解你的应用</h1>
    
	
<!--
---
title: Interactive Tutorial - Exploring Your App
weight: 20
---
-->

<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">
<link href="/docs/tutorials/kubernetes-basics/public/css/overrides.css" rel="stylesheet">
<script src="https://katacoda.com/embed.js"></script>

<div class="layout" id="top">

    <main class="content katacoda-content">

        <br>
        <div class="katacoda">

<!-- 
            <div class="katacoda__alert">
                To interact with the Terminal, please use the desktop/tablet version
            </div>
-->

            <div class="katacoda__alert">
                要与终端交互，请使用桌面/平板 版本
            </div>
            
            <div class="katacoda__box" id="inline-terminal-1"  data-katacoda-id="kubernetes-bootcamp/4" data-katacoda-color="326de6" data-katacoda-secondary="273d6d" data-katacoda-hideintro="false" data-katacoda-prompt="Kubernetes Bootcamp Terminal" style="height: 600px;">
            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/expose/expose-intro/" role="button">继续阅读第4单元<span class="btn__next">›</span></a>
            </div>
        </div>

    </main>

</div>

</body>
</html>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4b0e31c9e0eae68bbb0a358b4042ada9">2.4 - 公开地暴露你的应用</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-8ef4dad8f743b191a9e8c6f891cb191a">2.4.1 - 使用 Service 暴露您的应用</h1>
    
	
<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">

<div class="layout" id="top">

	<main class="content">

		<div class="row">
			<div class="col-md-8">
<!--        <h3>Objectives</h3>-->
        <h3>目标</h3>
				<ul>
<!--          <li>Learn about a Service in Kubernetes</li>-->
<!--          <li>Understand how labels and LabelSelector objects relate to a Service</li>-->
<!--          <li>Expose an application outside a Kubernetes cluster using a Service</li>-->
					<li>了解 Kubernetes 中的 Service </li>
					<li>了解 标签(Label) 和 标签选择器(Label Selector) 对象如何与 Service 关联</li>
          <li>在 Kubernetes 集群外用 Service 暴露应用</li>
				</ul>
			</div>

			<div class="col-md-8">
<!--        <h3>Overview of Kubernetes Services</h3>-->
        <h3>Kubernetes Service 总览</h3>

<!--        <p>Kubernetes <a href="/docs/concepts/workloads/pods/pod-overview/">Pods</a> are mortal. Pods in fact have a <a href="/docs/concepts/workloads/pods/pod-lifecycle/">lifecycle</a>. When a worker node dies, the Pods running on the Node are also lost. A <a href="/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> might then dynamically drive the cluster back to desired state via creation of new Pods to keep your application running. As another example, consider an image-processing backend with 3 replicas. Those replicas are exchangeable; the front-end system should not care about backend replicas or even if a Pod is lost and recreated. That said, each Pod in a Kubernetes cluster has a unique IP address, even Pods on the same Node, so there needs to be a way of automatically reconciling changes among Pods so that your applications continue to function.</p>-->
        <p> Kubernetes <a href="/zh/docs/concepts/workloads/pods/">Pod</a> 是转瞬即逝的。 Pod 实际上拥有 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">生命周期</a>。 当一个工作 Node 挂掉后, 在 Node 上运行的 Pod 也会消亡。 <a href="/zh/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> 会自动地通过创建新的 Pod 驱动集群回到目标状态，以保证应用程序正常运行。 换一个例子，考虑一个具有3个副本数的用作图像处理的后端程序。这些副本是可替换的; 前端系统不应该关心后端副本，即使 Pod 丢失或重新创建。也就是说，Kubernetes 集群中的每个 Pod (即使是在同一个 Node 上的 Pod )都有一个唯一的 IP 地址，因此需要一种方法自动协调 Pod 之间的变更，以便应用程序保持运行。</p>

<!--        <p>A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them. Services enable a loose coupling between dependent Pods. A Service is defined using YAML <a href="/docs/concepts/configuration/overview/#general-configuration-tips">(preferred)</a> or JSON, like all Kubernetes objects. The set of Pods targeted by a Service is usually determined by a <i>LabelSelector</i> (see below for why you might want a Service without including <code>selector</code> in the spec).</p>-->
        <p> Kubernetes 中的服务(Service)是一种抽象概念，它定义了 Pod 的逻辑集和访问 Pod 的协议。Service 使从属 Pod 之间的松耦合成为可能。 和其他 Kubernetes 对象一样, Service 用 YAML <a href="/zh/docs/concepts/configuration/overview/#general-configuration-tips">(更推荐)</a> 或者 JSON 来定义. Service 下的一组 Pod 通常由 <i>LabelSelector</i> (请参阅下面的说明为什么您可能想要一个 spec 中不包含<code>selector</code>的服务)来标记。</p>

<!--        <p>Although each Pod has a unique IP address, those IPs are not exposed outside the cluster without a Service. Services allow your applications to receive traffic. Services can be exposed in different ways by specifying a <code>type</code> in the ServiceSpec:</p>-->
        <p>尽管每个 Pod 都有一个唯一的 IP 地址，但是如果没有 Service ，这些 IP 不会暴露在集群外部。Service 允许您的应用程序接收流量。Service 也可以用在 ServiceSpec 标记<code>type</code>的方式暴露</p>
			<ul>
<!--        <li><i>ClusterIP</i> (default) - Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster.</li>-->
<!--        <li><i>NodePort</i> - Exposes the Service on the same port of each selected Node in the cluster using NAT. Makes a Service accessible from outside the cluster using <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>. Superset of ClusterIP.</li>-->
<!--        <li><i>LoadBalancer</i> - Creates an external load balancer in the current cloud (if supported) and assigns a fixed, external IP to the Service. Superset of NodePort.</li>-->
<!--        <li><i>ExternalName</i> - Exposes the Service using an arbitrary name (specified by <code>externalName</code> in the spec) by returning a CNAME record with the name. No proxy is used. This type requires v1.7 or higher of <code>kube-dns</code>.</li>-->
				<li><i>ClusterIP</i> (默认) - 在集群的内部 IP 上公开 Service 。这种类型使得 Service 只能从集群内访问。</li>
				<li><i>NodePort</i> - 使用 NAT 在集群中每个选定 Node 的相同端口上公开 Service 。使用<code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code> 从集群外部访问 Service。是 ClusterIP 的超集。</li>
				<li><i>LoadBalancer</i> - 在当前云中创建一个外部负载均衡器(如果支持的话)，并为 Service 分配一个固定的外部IP。是 NodePort 的超集。</li>
				<li><i>ExternalName</i> - 通过返回带有该名称的 CNAME 记录，使用任意名称(由 spec 中的<code>externalName</code>指定)公开 Service。不使用代理。这种类型需要<code>kube-dns</code>的v1.7或更高版本。</li>
			</ul>
<!--        <p>More information about the different types of Services can be found in the <a href="/docs/tutorials/services/source-ip/">Using Source IP</a> tutorial. Also see <a href="/docs/concepts/services-networking/connect-applications-service">Connecting Applications with Services</a>.</p>-->
        <p>更多关于不同 Service 类型的信息可以在<a href="/zh/docs/tutorials/services/source-ip/">使用源 IP </a> 教程。 也请参阅 <a href="/zh/docs/concepts/services-networking/connect-applications-service">连接应用程序和 Service </a>。</p>
<!--        <p>Additionally, note that there are some use cases with Services that involve not defining <code>selector</code> in the spec. A Service created without <code>selector</code> will also not create the corresponding Endpoints object. This allows users to manually map a Service to specific endpoints. Another possibility why there may be no selector is you are strictly using <code>type: ExternalName</code>.</p>-->
        <p>另外，需要注意的是有一些 Service 的用例没有在 spec 中定义<code>selector</code>。 一个没有<code>selector</code>创建的 Service 也不会创建相应的端点对象。这允许用户手动将服务映射到特定的端点。没有 selector 的另一种可能是您严格使用<code>type: ExternalName</code>来标记。</p>
			</div>
			<div class="col-md-4">
				<div class="content__box content__box_lined">
<!--          <h3>Summary</h3>-->
          <h3>总结</h3>
					<ul>
<!--            <li>Exposing Pods to external traffic</li>-->
<!--            <li>Load balancing traffic across multiple Pods</li>-->
<!--            <li>Using labels</li>-->
						<li>将 Pod 暴露给外部通信</li>
						<li>跨多个 Pod 的负载均衡</li>
						<li>使用标签(Label)</li>
					</ul>
				</div>
				<div class="content__box content__box_fill">
<!--          <p><i>A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.</i></p>-->
          <p><i>Kubernetes 的 Service 是一个抽象层，它定义了一组 Pod 的逻辑集，并为这些 Pod 支持外部流量暴露、负载平衡和服务发现。</i></p>
				</div>
			</div>
		</div>
		<br>

		<div class="row">
			<div class="col-md-8">
				<h3>Service 和 Label</h3>
			</div>
		</div>

		<div class="row">
			<div class="col-md-8">
				<p><img src="/docs/tutorials/kubernetes-basics/public/images/module_04_services.svg" width="150%" height="150%"></p>
			</div>
		</div>

		<div class="row">
			<div class="col-md-8">
<!--        <p>A Service routes traffic across a set of Pods. Services are the abstraction that allow pods to die and replicate in Kubernetes without impacting your application. Discovery and routing among dependent Pods (such as the frontend and backend components in an application) is handled by Kubernetes Services.</p>-->
        <p>Service 通过一组 Pod 路由通信。Service 是一种抽象，它允许 Pod 死亡并在 Kubernetes 中复制，而不会影响应用程序。在依赖的 Pod (如应用程序中的前端和后端组件)之间进行发现和路由是由Kubernetes Service 处理的。</p>
<!--        <p>Services match a set of Pods using <a href="/docs/concepts/overview/working-with-objects/labels">labels and selectors</a>, a grouping primitive that allows logical operation on objects in Kubernetes. Labels are key/value pairs attached to objects and can be used in any number of ways:</p>-->
        <p>Service 匹配一组 Pod 是使用 <a href="/zh/docs/concepts/overview/working-with-objects/labels">标签(Label)和选择器(Selector)</a>, 它们是允许对 Kubernetes 中的对象进行逻辑操作的一种分组原语。标签(Label)是附加在对象上的键/值对，可以以多种方式使用:</p>
				<ul>
<!--          <li>Designate objects for development, test, and production</li>-->
<!--          <li>Embed version tags</li>-->
<!--          <li>Classify an object using tags</li>-->
					<li>指定用于开发，测试和生产的对象</li>
					<li>嵌入版本标签</li>
					<li>使用 Label 将对象进行分类</li>
				</ul>

			</div>
			<div class="col-md-4">
				<div class="content__box content__box_fill">
<!--          <p><i>You can create a Service at the same time you create a Deployment by using<br><code>&#45;&#45;expose</code> in kubectl.</i></p>-->
          <p><i>你也可以在创建 Deployment 的同时用 <code>--expose</code>创建一个 Service 。</i></p>
				</div>
			</div>
		</div>

		<br>

		<div class="row">
			<div class="col-md-8">
				<p><img src="/docs/tutorials/kubernetes-basics/public/images/module_04_labels.svg"></p>
			</div>
		</div>
		<br>
		<div class="row">
			<div class="col-md-8">
<!--        <p>Labels can be attached to objects at creation time or later on. They can be modified at any time. Let's expose our application now using a Service and apply some labels.</p>-->
        <p> 标签(Label)可以在创建时或之后附加到对象上。他们可以随时被修改。现在使用 Service 发布我们的应用程序并添加一些 Label 。</p>
			</div>
		</div>
		<br>
		<div class="row">
			<div class="col-md-12">
<!--        <a class="btn btn-lg btn-success" href="/docs/tutorials/kubernetes-basics/expose/expose-interactive/" role="button">Start Interactive Tutorial<span class="btn__next">›</span></a>-->
        <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/expose/expose-interactive/" role="button">开始交互式教程<span class="btn__next">›</span></a>
			</div>
		</div>
	</main>
</div>

</body>
</html>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-352241d22effe0714772d21c7d1b512d">2.4.2 - 交互式教程 - 暴露你的应用</h1>
    
	
<!--
---
title: Interactive Tutorial - Exposing Your App
weight: 20
---
-->

<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">
<link href="/docs/tutorials/kubernetes-basics/public/css/overrides.css" rel="stylesheet">
<script src="https://katacoda.com/embed.js"></script>

<div class="layout" id="top">

    <main class="content katacoda-content">

        <div class="katacoda">
            <div class="katacoda__alert">
            
<!--            
                To interact with the Terminal, please use the desktop/tablet version
-->
                要与终端交互，请使用台式机/平板电脑
                
            </div>
            <div class="katacoda__box" id="inline-terminal-1"  data-katacoda-id="kubernetes-bootcamp/8" data-katacoda-color="326de6" data-katacoda-secondary="273d6d" data-katacoda-hideintro="false" data-katacoda-prompt="Kubernetes Bootcamp Terminal" style="height: 600px;">
            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/scale/scale-intro/" role="button">继续阅读第5单元<span class="btn__next">›</span></a>
            </div>
        </div>

    </main>

</div>

</body>
</html>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-be4996c93fb39c459a30b6669569d423">2.5 - 缩放你的应用</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-d1c15c9bd4f625adbc13149b1475287c">2.5.1 - 运行应用程序的多个实例</h1>
    
	
<!--
---
title: Running Multiple Instances of Your App
weight: 10
---
-->

<!DOCTYPE html>
<html lang="zh">
<body>
<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">
<div class="layout" id="top">
    <main class="content">
        <div class="row">
     <div class="col-md-8">
					<!-- <h3>Objectives</h3> -->
					<h3>目标</h3>
                <ul>
										<!-- <li>Scale an app using kubectl.</li> -->
										<li>用 kubectl 扩缩应用程序</li>
                </ul>
            </div>

            <div class="col-md-8">
			 <!-- <h3>Scaling an application</h3> -->
			 <h3>扩缩应用程序</h3>

						<!-- <p>In the previous modules we created a <a href="/docs/concepts/workloads/controllers/deployment/"> Deployment</a>,
							and then exposed it publicly via a <a href="/docs/concepts/services-networking/service/">Service</a>.
							The Deployment created only one Pod for running our application.  When traffic increases, we will need to scale the application to keep up with  user demand.</p> -->
						<p>在之前的模块中，我们创建了一个 <a href="/zh/docs/concepts/workloads/controllers/deployment/"> Deployment</a>，然后通过 <a href="/zh/docs/concepts/services-networking/service/">Service</a>让其可以开放访问。Deployment 仅为跑这个应用程序创建了一个 Pod。 当流量增加时，我们需要扩容应用程序满足用户需求。</p>

						<!-- <p><b>Scaling</b> is accomplished by changing the number of replicas in a Deployment</p> -->
						<p><b>扩缩</b> 是通过改变 Deployment 中的副本数量来实现的。</p>

            </div>
            <div class="col-md-4">
                <div class="content__box content__box_lined">
										<!-- <h3>Summary:</h3> -->
										<h3>小结:</h3>
                    <ul>
												<!-- <li>Scaling a Deployment</li> -->
												<li>扩缩一个 Deployment</li>
                    </ul>
                </div>
                <div class="content__box content__box_fill">
										<!-- <p><i> You can create from the start a Deployment with multiple instances using the --replicas parameter for the kubectl run command </i></p> -->
										<p><i> 在运行 kubectl run 命令时，你可以通过设置 --replicas 参数来设置 Deployment 的副本数。</i></p>
                </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8">
								<!-- <h2 style="color: #3771e3;">Scaling overview</h2> -->
								<h2 style="color: #3771e3;">扩缩概述</h2>
            </div>
        </div>

        <div class="row">
            <div class="col-md-1"></div>
            <div class="col-md-8">
                <div id="myCarousel" class="carousel" data-ride="carousel" data-interval="3000">
                    <ol class="carousel-indicators">
                        <li data-target="#myCarousel" data-slide-to="0" class="active"></li>
                        <li data-target="#myCarousel" data-slide-to="1"></li>
                    </ol>
                      <div class="carousel-inner" role="listbox">
                        <div class="item carousel-item active">
                          <img src="/docs/tutorials/kubernetes-basics/public/images/module_05_scaling1.svg">
                        </div>

                        <div class="item carousel-item">
                          <img src="/docs/tutorials/kubernetes-basics/public/images/module_05_scaling2.svg">
                        </div>
                      </div>

                      <a class="left carousel-control" href="#myCarousel" role="button" data-slide="prev">
                        <span class="sr-only ">Previous</span>
                      </a>
                      <a class="right carousel-control" href="#myCarousel" role="button" data-slide="next">
                        <span class="sr-only">Next</span>
                      </a>

                    </div>
            </div>
        </div>

        <br>

        <div class="row">
            <div class="col-md-8">

                <!-- <p>Scaling out a Deployment will ensure new Pods are created and scheduled to Nodes with available resources. Scaling in will reduce the number of Pods to the new desired state. Kubernetes also supports
									<a href="/docs/user-guide/horizontal-pod-autoscaling/">autoscaling</a> of Pods, but it is outside of the scope of this tutorial. Scaling to zero is also possible, and it will terminate all Pods of the specified Deployment.</p> -->
								<p>扩展 Deployment 将创建新的 Pods，并将资源调度请求分配到有可用资源的节点上，收缩 会将 Pods 数量减少至所需的状态。Kubernetes 还支持 Pods 的<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale/">自动缩放</a>，但这并不在本教程的讨论范围内。将 Pods 数量收缩到0也是可以的，但这会终止 Deployment 上所有已经部署的 Pods。</p>

								<!-- <p>Running multiple instances of an application will require a way to distribute the traffic to all of them. Services have an integrated load-balancer that will distribute network traffic to all Pods of an exposed Deployment.
									Services will monitor continuously the running Pods using endpoints, to ensure the traffic is sent only to available Pods.</p> -->
								<p>运行应用程序的多个实例需要在它们之间分配流量。服务 (Service)有一种负载均衡器类型，可以将网络流量均衡分配到外部可访问的 Pods 上。服务将会一直通过端点来监视 Pods 的运行，保证流量只分配到可用的 Pods 上。</p>

            </div>
            <div class="col-md-4">
                <div class="content__box content__box_fill">
										<!-- <p><i>Scaling is accomplished by changing the number of replicas in a Deployment.</i></p> -->
										<p><i>扩缩是通过改变 Deployment 中的副本数量来实现的。</i></p>
                </div>
            </div>
        </div>

        <br>

        <div class="row">
            <div class="col-md-8">
								<!-- <p> Once you have multiple instances of an Application running, you would be able to do Rolling updates without downtime. We'll cover that in the next module. Now, let's go to the online terminal and scale our application.</p> -->
								<p>一旦有了多个应用实例，就可以没有宕机地滚动更新。我们将会在下面的模块中介绍这些。现在让我们使用在线终端来体验一下应用程序的扩缩过程。</p>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-12">
								<!-- <a class="btn btn-lg btn-success" href="/docs/tutorials/kubernetes-basics/scale/scale-interactive/" role="button">Start Interactive Tutorial <span class="btn__next">›</span></a> -->
								<a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/scale/scale-interactive/" role="button">开始互动教程 <span class="btn__next">›</span></a>
            </div>
        </div>

    </main>

</div>

</body>
</html>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7bdb3fbaa1177ff5dfa3fe86bd35ef59">2.5.2 - 交互教程 - 缩放你的应用</h1>
    
	<!--
---
title: Interactive Tutorial - Scaling Your App
weight: 20
---
-->

<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">
<link href="/docs/tutorials/kubernetes-basics/public/css/overrides.css" rel="stylesheet">
<script src="https://katacoda.com/embed.js"></script>
<div class="layout" id="top">

    <main class="content katacoda-content">
    
        <div class="katacoda">
            <div class="katacoda__alert">
                <!--
                To interact with the Terminal, please use the desktop/tablet version
                -->
                与终端交互，请使用桌面/平板电脑版本
            </div>
            <div class="katacoda__box" id="inline-terminal-1"  data-katacoda-id="kubernetes-bootcamp/5" data-katacoda-color="326de6" data-katacoda-secondary="273d6d" data-katacoda-hideintro="false" data-katacoda-prompt="Kubernetes Bootcamp Terminal" style="height: 600px;">
            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/update/update-intro/" role="button"><!--Continue to Module 6-->继续参阅第6单元<span class="btn__next">›</span></a>
            </div>
        </div>
    
    </main>

<a class="scrolltop" href="#top"></a>

</div>

</body>
</html>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-62b8b17dadfb55f1801cf8439e944e58">2.6 - 更新你的应用</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-12e04355145afad615ca3c38335ba019">2.6.1 - 执行滚动更新</h1>
    
	
<!--
---
title: Performing a Rolling Update
weight: 10
---
-->

<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet">

<div class="layout" id="top">

    <main class="content">

        <div class="row">

     <div class="col-md-8">
          <h3>Objectives</h3>
                <ul>
                <!--
                    <li>Perform a rolling update using kubectl.</li>
                -->
                    <li>使用 kubectl 执行滚动更新。</li>
                </ul>
            </div>

            <div class="col-md-8">
            <!--
            <h3>Updating an application</h3>
            -->
            <h3>更新应用程序</h3>
            
            <!--
            <p>Users expect applications to be available all the time and developers are expected to deploy new versions of them several times a day. In Kubernetes this is done with rolling updates. <b>Rolling updates</b> allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones. The new Pods will be scheduled on Nodes with available resources.</p>
            -->
            <p>用户希望应用程序始终可用，而开发人员则需要每天多次部署它们的新版本。在 Kubernetes 中，这些是通过滚动更新（Rolling Updates）完成的。 <b>滚动更新</b> 允许通过使用新的实例逐步更新 Pod 实例，零停机进行 Deployment 更新。新的 Pod 将在具有可用资源的节点上进行调度。</p>
            
            <!--
            <p>In the previous module we scaled our application to run multiple instances. This is a requirement for performing updates without affecting application availability. By default, the maximum number of Pods that can be unavailable during the update and the maximum number of new Pods that can be created, is one. Both options can be configured to either numbers or percentages (of Pods).
            In Kubernetes, updates are versioned and any Deployment update can be reverted to previous (stable) version.</p>
            -->
            <p>在前面的模块中，我们将应用程序扩展为运行多个实例。这是在不影响应用程序可用性的情况下执行更新的要求。默认情况下，更新期间不可用的 pod 的最大值和可以创建的新 pod 数都是 1。这两个选项都可以配置为（pod）数字或百分比。
            在 Kubernetes 中，更新是经过版本控制的，任何 Deployment 更新都可以恢复到以前的（稳定）版本。</p>
            
            
            </div>
            <div class="col-md-4">
                <div class="content__box content__box_lined">
                <!--
                    <h3>Summary:</h3>
                -->
                    <h3>摘要：</h3>
                    <ul>
                    <!--
                        <li>Updating an app</li>
                    -->
                        <li>更新应用</li>
                    </ul>
                </div>
                <div class="content__box content__box_fill">
                <!--
                    <p><i>Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones. </i></p>
                -->
                    <p><i>滚动更新允许通过使用新的实例逐步更新 Pod 实例从而实现 Deployments 更新，停机时间为零。</i></p>
                </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8">
            <!--
                <h2 style="color: #3771e3;">Rolling updates overview</h2>
            -->
                <h2 style="color: #3771e3;">滚动更新概述</h2>
            </div>
        </div>
        <div class="row">
            <div class="col-md-1"></div>
            <div class="col-md-8">
                <div id="myCarousel" class="carousel" data-ride="carousel" data-interval="3000">
                    <ol class="carousel-indicators">
                        <li data-target="#myCarousel" data-slide-to="0" class="active"></li>
                        <li data-target="#myCarousel" data-slide-to="1"></li>
                        <li data-target="#myCarousel" data-slide-to="2"></li>
                        <li data-target="#myCarousel" data-slide-to="3"></li>
                    </ol>
                      <div class="carousel-inner" role="listbox">
                        <div class="item carousel-item active">
                          <img src="/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates1.svg" >
                        </div>

                        <div class="item carousel-item">
                          <img src="/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates2.svg">
                        </div>

                        <div class="item carousel-item">
                          <img src="/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates3.svg">
                        </div>

                        <div class="item carousel-item">
                          <img src="/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates4.svg">
                        </div>
                      </div>

                      <a class="left carousel-control" href="#myCarousel" role="button" data-slide="prev">
                        <span class="sr-only ">Previous</span>
                      </a>
                      <a class="right carousel-control" href="#myCarousel" role="button" data-slide="next">
                        <span class="sr-only">Next</span>
                      </a>

                    </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8">
<!--
                <p>Similar to application Scaling, if a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update. An available Pod is an instance that is available to the users of the application.</p>
-->
                <p>与应用程序扩展类似，如果公开了 Deployment，服务将在更新期间仅对可用的 pod 进行负载均衡。可用 Pod 是应用程序用户可用的实例。</p>

<!--
                <p>Rolling updates allow the following actions:</p>
-->
                <p>滚动更新允许以下操作：</p>
                <ul>
                <!--
                    <li>Promote an application from one environment to another (via container image updates)</li>
                    <li>Rollback to previous versions</li>
                    <li>Continuous Integration and Continuous Delivery of applications with zero downtime</li>
                -->
                    <li>将应用程序从一个环境提升到另一个环境（通过容器镜像更新）</li>
                    <li>回滚到以前的版本</li>
                    <li>持续集成和持续交付应用程序，无需停机</li>
                </ul>

            </div>
            <div class="col-md-4">
                <div class="content__box content__box_fill">
                <!--
                    <p><i>If a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update. </i></p>
                -->
                    <p><i>如果 Deployment 是公开的，则服务将仅在更新期间对可用的 pod 进行负载均衡。 </i></p>
                </div>
            </div>
        </div>

        <br>

        <div class="row">
            <div class="col-md-8">
            <!--
                <p> In the following interactive tutorial, we'll update our application to a new version, and also perform a rollback.</p>
            -->
                <p> 在下面的交互式教程中，我们将应用程序更新为新版本，并执行回滚。</p>
            
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-12">
                <!--
                <a class="btn btn-lg btn-success" href="/docs/tutorials/kubernetes-basics/update/update-interactive/" role="button">Start Interactive Tutorial <span class="btn__next">›</span></a>
                -->
                <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/update/update-interactive/" role="button">启动交互教程<span class="btn__next">›</span></a>
                
            </div>
        </div>

    </main>

</div>

</body>
</html>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-dddc0cb356c280e0339bcf42776987dc">2.6.2 - 交互式教程 - 更新你的应用</h1>
    
	<!--
---
title: Interactive Tutorial - Updating Your App
weight: 20
---
-->


<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">
<link href="/docs/tutorials/kubernetes-basics/public/css/overrides.css" rel="stylesheet">
<script src="https://katacoda.com/embed.js"></script>
<div class="layout" id="top">

    <main class="content katacoda-content">
    
        <div class="katacoda">
            <div class="katacoda__alert">
                要与终端交互，请使用桌面/平板电脑版本
            </div>
            <div class="katacoda__box" id="inline-terminal-1"  data-katacoda-id="kubernetes-bootcamp/6" data-katacoda-color="326de6" data-katacoda-secondary="273d6d" data-katacoda-hideintro="false" data-katacoda-prompt="Kubernetes Bootcamp Terminal" style="height: 600px;">
            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <a class="btn btn-lg btn-success" href="/zh/docs/tutorials/kubernetes-basics/" role="button">回到 Kubernetes 的基础<span class="btn__next">›</span></a>
            </div>
        </div>
    </main>

</div>

</body>
</html>

</div>



    
	
  

    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a3a0f1c6af19fc89ce24d8cd42c0249f">3 - 配置</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-e08b0be51359b976a754112b96980f54">3.1 - 示例：配置 java 微服务</h1>
    
	<!-- 
---
title: "Example: Configuring a Java Microservice"
weight: 10
---
--><blockquote>
</blockquote>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-025ef96f86c52822a2738b8b11b60934">3.1.1 - 使用 MicroProfile、ConfigMaps、Secrets 实现外部化应用配置</h1>
    
	<!-- 
---
title: "Externalizing config using MicroProfile, ConfigMaps and Secrets"
content_type: tutorial
weight: 10
---
-->
<!-- overview -->
<!-- 
In this tutorial you will learn how and why to externalize your microservice’s configuration.  Specifically, you will learn how to use Kubernetes ConfigMaps and Secrets to set environment variables and then consume them using MicroProfile Config.
-->
<p>在本教程中，你会学到如何以及为什么要实现外部化微服务应用配置。
具体来说，你将学习如何使用 Kubernetes ConfigMaps 和 Secrets 设置环境变量，
然后在 MicroProfile config 中使用它们。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!-- 
### Creating Kubernetes ConfigMaps & Secrets
There are several ways to set environment variables for a Docker container in Kubernetes, including: Dockerfile, kubernetes.yml, Kubernetes ConfigMaps, and Kubernetes Secrets.  In the tutorial, you will learn how to use the latter two for setting your environment variables whose values will be injected into your microservices.  One of the benefits for using ConfigMaps and Secrets is that they can be re-used across multiple containers, including being assigned to different environment variables for the different containers.
-->
<h3 id="creating-kubernetes-configmaps-secrets">创建 Kubernetes ConfigMaps 和 Secrets </h3>
<p>在 Kubernetes 中，为 docker 容器设置环境变量有几种不同的方式，比如：
Dockerfile、kubernetes.yml、Kubernetes ConfigMaps、和 Kubernetes Secrets。
在本教程中，你将学到怎么用后两个方式去设置你的环境变量，而环境变量的值将注入到你的微服务里。
使用 ConfigMaps 和 Secrets 的一个好处是他们能在多个容器间复用，
比如赋值给不同的容器中的不同环境变量。</p>
<!-- 
ConfigMaps are API Objects that store non-confidential key-value pairs.  In the Interactive Tutorial you will learn how to use a ConfigMap to store the application's name.  For more information regarding ConfigMaps, you can find the documentation [here](/docs/tasks/configure-pod-container/configure-pod-configmap/).

Although Secrets are also used to store key-value pairs, they differ from ConfigMaps in that they're intended for confidential/sensitive information and are stored using Base64 encoding.  This makes secrets the appropriate choice for storing such things as credentials, keys, and tokens, the former of which you'll do in the Interactive Tutorial.  For more information on Secrets, you can find the documentation [here](/docs/concepts/configuration/secret/).
-->
<p>ConfigMaps 是存储非机密键值对的 API 对象。
在互动教程中，你会学到如何用 ConfigMap 来保存应用名字。
ConfigMap 的更多信息，你可以在<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">这里</a>找到文档。</p>
<p>Secrets 尽管也用来存储键值对，但区别于 ConfigMaps 的是：它针对机密/敏感数据，且存储格式为 Base64 编码。
secrets 的这种特性使得它适合于存储证书、密钥、令牌，上述内容你将在交互教程中实现。
Secrets 的更多信息，你可以在<a href="/zh/docs/concepts/configuration/secret/">这里</a>找到文档。</p>
<!-- 
### Externalizing Config from Code
Externalized application configuration is useful because configuration usually changes depending on your environment.  In order to accomplish this, we'll use Java's Contexts and Dependency Injection (CDI) and MicroProfile Config. MicroProfile Config is a feature of MicroProfile, a set of open Java technologies for developing and deploying cloud-native microservices.
-->
<h3 id="从代码外部化配置">从代码外部化配置</h3>
<p>外部化应用配置之所以有用处，是因为配置常常根据环境的不同而变化。
为了实现此功能，我们用到了 Java 上下文和依赖注入（Contexts and Dependency Injection, CDI）、MicroProfile 配置。
MicroProfile config 是 MicroProfile 的功能特性，
是一组开放 Java 技术，用于开发、部署云原生微服务。</p>
<!-- 
CDI provides a standard dependency injection capability enabling an application to be assembled from collaborating, loosely-coupled beans.  MicroProfile Config provides apps and microservices a standard way to obtain config properties from various sources, including the application, runtime, and environment.  Based on the source's defined priority, the properties are automatically combined into a single set of properties that the application can access via an API.  Together, CDI & MicroProfile will be used in the Interactive Tutorial to retrieve the externally provided properties from the Kubernetes ConfigMaps and Secrets and get injected into your application code.

Many open source frameworks and runtimes implement and support MicroProfile Config.  Throughout the interactive tutorial, you'll be using Open Liberty, a flexible open-source Java runtime for building and running cloud-native apps and microservices.  However, any MicroProfile compatible runtime could be used instead. 
-->
<p>CDI 提供一套标准的依赖注入能力，使得应用程序可以由相互协作的、松耦合的 beans 组装而成。
MicroProfile Config 为 app 和微服务提供从各种来源，比如应用、运行时、环境，获取配置参数的标准方法。
基于来源定义的优先级，属性可以自动的合并到单独一组应用可以通过 API 访问到的属性。
CDI &amp; MicroProfile 都会被用在互动教程中，
用来从 Kubernetes ConfigMaps 和 Secrets 获得外部提供的属性，并注入应用程序代码中。</p>
<p>很多开源框架、运行时支持 MicroProfile Config。
对于整个互动教程，你都可以使用开放的库、灵活的开源 Java 运行时，去构建并运行云原生的 apps 和微服务。
然而，任何 MicroProfile 兼容的运行时都可以用来做替代品。</p>
<h2 id="objectives">Objectives</h2>
<!-- 
* Create a Kubernetes ConfigMap and Secret
* Inject microservice configuration using MicroProfile Config
-->
<ul>
<li>创建 Kubernetes ConfigMap 和 Secret</li>
<li>使用 MicroProfile Config 注入微服务配置</li>
</ul>
<!-- lessoncontent -->
<!-- 
## Example: Externalizing config using MicroProfile, ConfigMaps and Secrets
### [Start Interactive Tutorial](/docs/tutorials/configuration/configure-java-microservice/configure-java-microservice-interactive/) 
-->
<h2 id="示例-使用-microprofile-configmaps-secrets-实现外部化应用配置">示例：使用 MicroProfile、ConfigMaps、Secrets 实现外部化应用配置</h2>
<h3 id="启动互动教程-zh-docs-tutorials-configuration-configure-java-microservice-configure-java-microservice-interactive"><a href="/zh/docs/tutorials/configuration/configure-java-microservice/configure-java-microservice-interactive/">启动互动教程</a></h3>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ef2047c46d3cd16631bac27403e4cfdc">3.1.2 - 互动教程 - 配置 java 微服务</h1>
    
	<!-- 
---
title: "Interactive Tutorial - Configuring a Java Microservice"
weight: 20
---
-->

<!DOCTYPE html>

<html lang="zh">

<body>

<link href="/docs/tutorials/kubernetes-basics/public/css/styles.css" rel="stylesheet">
<link href="/docs/tutorials/kubernetes-basics/public/css/overrides.css" rel="stylesheet">
<script src="https://katacoda.com/embed.js"></script>

<div class="layout" id="top">

    <main class="content katacoda-content">
        <div class="katacoda">
            <div class="katacoda__alert">
                <!-- To interact with the Terminal, please use the desktop/tablet version -->
                如需要与终端交互，请使用台式机/平板电脑版
            </div>
            <div class="katacoda__box" id="inline-terminal-1"  data-katacoda-id="kubernetes-bootcamp/9" data-katacoda-color="326de6" data-katacoda-secondary="273d6d" data-katacoda-hideintro="false" data-katacoda-prompt="Kubernetes Bootcamp Terminal" style="height: 600px;"></div>
        </div>
    </main>

</div>

</body>
</html>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2efe621cc085b350c8c4574e6f7f1311">3.2 - 使用 ConfigMap 来配置 Redis</h1>
    
	<!-- overview -->
<!--
This page provides a real world example of how to configure Redis using a ConfigMap and builds upon the [Configure Containers Using a ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/) task.
-->
<p>这篇文档基于<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">使用 ConfigMap 来配置 Containers</a> 这个任务，提供了一个使用 ConfigMap 来配置 Redis 的真实案例。</p>
<h2 id="objectives">Objectives</h2>
<!--
* Create a ConfigMap with Redis configuration values
* Create a Redis Pod that mounts and uses the created ConfigMap
* Verify that the configuration was correctly applied.
-->
<ul>
<li>使用 Redis 配置的值创建一个 ConfigMap</li>
<li>创建一个 Redis Pod，挂载并使用创建的 ConfigMap</li>
<li>验证配置已经被正确应用。</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</li>
</ul>
<!--
* The example shown on this page works with `kubectl` 1.14 and above.
* Understand [Configure Containers Using a ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/).
-->
<ul>
<li>此页面上显示的示例适用于 <code>kubectl</code> 1.14和在其以上的版本。</li>
<li>理解<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">使用ConfigMap来配置Containers</a>。</li>
</ul>
<!-- lessoncontent -->
<!--
## Real World Example: Configuring Redis using a ConfigMap

Follow the steps below to configure a Redis cache using data stored in a ConfigMap.

First create a ConfigMap with an empty configuration block:
-->
<h2 id="真实世界的案例-使用-configmap-来配置-redis">真实世界的案例：使用 ConfigMap 来配置 Redis</h2>
<p>按照下面的步骤，使用 ConfigMap 中的数据来配置 Redis 缓存。</p>
<p>首先创建一个配置模块为空的 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt;./example-redis-config.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: ConfigMap
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: example-redis-config
</span><span style="color:#b44">data:
</span><span style="color:#b44">  redis-config: &#34;&#34;
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Apply the ConfigMap created above, along with a Redis pod manifest:
-->
<p>应用上面创建的 ConfigMap 以及 Redis pod 清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f example-redis-config.yaml
kubectl apply -f https://k8s.io/examples/pods/config/redis-pod.yaml
</code></pre></div><!--
Examine the contents of the Redis pod manifest and note the following:

* A volume named `config` is created by `spec.volumes[1]`
* The `key` and `path` under `spec.volumes[1].items[0]` exposes the `redis-config` key from the 
  `example-redis-config` ConfigMap as a file named `redis.conf` on the `config` volume.
* The `config` volume is then mounted at `/redis-master` by `spec.containers[0].volumeMounts[1]`.

This has the net effect of exposing the data in `data.redis-config` from the `example-redis-config`
ConfigMap above as `/redis-master/redis.conf` inside the Pod.
-->
<p>检查 Redis pod 清单的内容，并注意以下几点：</p>
<ul>
<li>由 <code>spec.volumes[1]</code> 创建一个名为 <code>config</code> 的卷。</li>
<li><code>spec.volumes[1].items[0]</code> 下的 <code>key</code> 和 <code>path</code> 会将来自 <code>example-redis-config</code>
ConfigMap 中的 <code>redis-config</code> 密钥公开在 <code>config</code> 卷上一个名为 <code>redis-config</code> 的文件中。</li>
<li>然后 <code>config</code> 卷被 <code>spec.containers[0].volumeMounts[1]</code> 挂载在 <code>/redis-master</code>。</li>
</ul>
<p>这样做的最终效果是将上面 <code>example-redis-config</code> 配置中 <code>data.redis-config</code> 的数据作为 Pod 中的 <code>/redis-master/redis.conf</code> 公开。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/config/redis-pod.yaml" download="pods/config/redis-pod.yaml"><code>pods/config/redis-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-config-redis-pod-yaml')" title="Copy pods/config/redis-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-config-redis-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis:5.0.4<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- redis-server<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;/redis-master/redis.conf&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MASTER<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">6379</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0.1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/redis-master-data<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/redis-master<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-redis-config<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>redis-config<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>redis.conf<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 
Examine the created objects: 
-->
<p>检查创建的对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod/redis configmap/example-redis-config 
</code></pre></div><!--
You should see the following output:
-->
<p>你应该可以看到以下输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME        READY   STATUS    RESTARTS   AGE
pod/redis   1/1     Running   <span style="color:#666">0</span>          8s

NAME                             DATA   AGE
configmap/example-redis-config   <span style="color:#666">1</span>      14s
</code></pre></div><!--
Recall that we left `redis-config` key in the `example-redis-config` ConfigMap blank: 
-->
<p>回顾一下，我们在 <code>example-redis-config</code> ConfigMap 保留了空的 <code>redis-config</code> 键：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe configmap/example-redis-config
</code></pre></div><!--
You should see an empty `redis-config` key:
-->
<p>你应该可以看到一个空的 <code>redis-config</code> 键：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Name:         example-redis-config
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

<span style="color:#b8860b">Data</span>
<span style="color:#666">====</span>
redis-config:
</code></pre></div><!--
Use `kubectl exec` to enter the pod and run the `redis-cli` tool to check the current configuration:
-->
<p>使用 <code>kubectl exec</code> 进入 pod，运行 <code>redis-cli</code> 工具检查当前配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it redis -- redis-cli
</code></pre></div><!--
Check `maxmemory`:
-->
<p>查看 <code>maxmemory</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">127.0.0.1:6379&gt; CONFIG GET maxmemory
</code></pre></div><!--
It should show the default value of 0:
-->
<p>它应该显示默认值 0：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">1<span style="color:#666">)</span> <span style="color:#b44">&#34;maxmemory&#34;</span>
2<span style="color:#666">)</span> <span style="color:#b44">&#34;0&#34;</span>
</code></pre></div><!--
Similarly, check `maxmemory-policy`:
-->
<p>同样，查看 <code>maxmemory-policy</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">127.0.0.1:6379&gt; CONFIG GET maxmemory-policy
</code></pre></div><!--
Which should also yield its default value of `noeviction`:
-->
<p>它也应该显示默认值 <code>noeviction</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">1<span style="color:#666">)</span> <span style="color:#b44">&#34;maxmemory-policy&#34;</span>
2<span style="color:#666">)</span> <span style="color:#b44">&#34;noeviction&#34;</span>
</code></pre></div><!--
Now let's add some configuration values to the `example-redis-config` ConfigMap:
-->
<p>现在，向 <code>example-redis-config</code> ConfigMap 添加一些配置：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/config/example-redis-config.yaml" download="pods/config/example-redis-config.yaml"><code>pods/config/example-redis-config.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-config-example-redis-config-yaml')" title="Copy pods/config/example-redis-config.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-config-example-redis-config-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-redis-config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">redis-config</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    maxmemory 2mb
</span><span style="color:#b44;font-style:italic">    maxmemory-policy allkeys-lru</span><span style="color:#bbb">    
</span></code></pre></div>
    </div>
</div>


<!--
Apply the updated ConfigMap:
-->
<p>应用更新的 ConfigMap:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f example-redis-config.yaml
</code></pre></div><!--
Confirm that the ConfigMap was updated:
-->
<p>确认 ConfigMap 已更新：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe configmap/example-redis-config
</code></pre></div><!--
You should see the configuration values we just added:
-->
<p>你应该可以看到我们刚刚添加的配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Name:         example-redis-config
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

<span style="color:#b8860b">Data</span>
<span style="color:#666">====</span>
redis-config:
----
maxmemory 2mb
maxmemory-policy allkeys-lru
</code></pre></div><!--
Check the Redis Pod again using `redis-cli` via `kubectl exec` to see if the configuration was applied:
-->
<p>通过 <code>kubectl exec</code> 使用 <code>redis-cli</code> 再次检查 Redis Pod，查看是否已应用配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it redis -- redis-cli
</code></pre></div><!--
Check `maxmemory`:
-->
<p>查看 <code>maxmemory</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">127.0.0.1:6379&gt; CONFIG GET maxmemory
</code></pre></div><!--
It remains at the default value of 0:
-->
<p>它保持默认值 0：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">1<span style="color:#666">)</span> <span style="color:#b44">&#34;maxmemory&#34;</span>
2<span style="color:#666">)</span> <span style="color:#b44">&#34;0&#34;</span>
</code></pre></div><!--
Similarly, `maxmemory-policy` remains at the `noeviction` default setting:
-->
<p>同样，<code>maxmemory-policy</code> 保留为默认设置 <code>noeviction</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">127.0.0.1:6379&gt; CONFIG GET maxmemory-policy
</code></pre></div><!--
Returns:
-->
<p>返回：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">1<span style="color:#666">)</span> <span style="color:#b44">&#34;maxmemory-policy&#34;</span>
2<span style="color:#666">)</span> <span style="color:#b44">&#34;noeviction&#34;</span>
</code></pre></div><!--
The configuration values have not changed because the Pod needs to be restarted to grab updated
values from associated ConfigMaps. Let's delete and recreate the Pod:
-->
<p>配置值未更改，因为需要重新启动 Pod 才能从关联的 ConfigMap 中获取更新的值。
让我们删除并重新创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod redis
kubectl apply -f https://k8s.io/examples/pods/config/redis-pod.yaml
</code></pre></div><!--
Now re-check the configuration values one last time:
-->
<p>现在，最后一次重新检查配置值：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it redis -- redis-cli
</code></pre></div><!--
Check `maxmemory`:
-->
<p>查看 <code>maxmemory</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">127.0.0.1:6379&gt; CONFIG GET maxmemory
</code></pre></div><!--
It should now return the updated value of 2097152:
-->
<p>现在，它应该返回更新后的值 2097152：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">1<span style="color:#666">)</span> <span style="color:#b44">&#34;maxmemory&#34;</span>
2<span style="color:#666">)</span> <span style="color:#b44">&#34;2097152&#34;</span>
</code></pre></div><!--
Similarly, `maxmemory-policy` has also been updated:
-->
<p>同样，<code>maxmemory-policy</code> 也已更新：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">127.0.0.1:6379&gt; CONFIG GET maxmemory-policy
</code></pre></div><!--
It now reflects the desired value of `allkeys-lru`:
-->
<p>现在它反映了期望值 <code>allkeys-lru</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">1<span style="color:#666">)</span> <span style="color:#b44">&#34;maxmemory-policy&#34;</span>
2<span style="color:#666">)</span> <span style="color:#b44">&#34;allkeys-lru&#34;</span>
</code></pre></div><!--
Clean up your work by deleting the created resources:
-->
<p>删除创建的资源，清理你的工作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod/redis configmap/example-redis-config
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [ConfigMaps](/docs/tasks/configure-pod-container/configure-pod-configmap/).
-->
<ul>
<li>了解有关 <a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a>的更多信息。</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fe7e92bed8fb92872b139f12c4568cdb">4 - 安全</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-fca078b8ac6b82352ed52187a2da91b7">4.1 - 使用 AppArmor 限制容器对资源的访问</h1>
    
	<!--
title: Restrict a Container's Access to Resources with AppArmor
content_type: tutorial
weight: 10
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.4 [beta]</code>
</div>


<!-- 
AppArmor is a Linux kernel security module that supplements the standard Linux user and group based
permissions to confine programs to a limited set of resources. AppArmor can be configured for any
application to reduce its potential attack surface and provide greater in-depth defense. It is
configured through profiles tuned to allow the access needed by a specific program or container,
such as Linux capabilities, network access, file permissions, etc. Each profile can be run in either
*enforcing* mode, which blocks access to disallowed resources, or *complain* mode, which only reports
violations. 
-->
<p>AppArmor 是一个 Linux 内核安全模块，
它补充了基于标准 Linux 用户和组的权限，将程序限制在一组有限的资源中。
AppArmor 可以配置为任何应用程序减少潜在的攻击面，并且提供更加深入的防御。
它通过调整配置文件进行配置，以允许特定程序或容器所需的访问，
如 Linux 权能字、网络访问、文件权限等。
每个配置文件都可以在
<em>强制（enforcing）</em> 模式（阻止访问不允许的资源）或
<em>投诉（complain）</em> 模式（仅报告冲突）下运行。</p>
<!-- 
AppArmor can help you to run a more secure deployment by restricting what containers are allowed to
do, and/or provide better auditing through system logs. However, it is important to keep in mind
that AppArmor is not a silver bullet and can only do so much to protect against exploits in your
application code. It is important to provide good, restrictive profiles, and harden your
applications and cluster from other angles as well.
-->
<p>AppArmor 可以通过限制允许容器执行的操作，
和/或通过系统日志提供更好的审计来帮助你运行更安全的部署。
但是，重要的是要记住 AppArmor 不是灵丹妙药，
只能做部分事情来防止应用程序代码中的漏洞。
提供良好的限制性配置文件，并从其他角度强化你的应用程序和集群非常重要。</p>
<h2 id="objectives">Objectives</h2>
<!-- 
* See an example of how to load a profile on a node
* Learn how to enforce the profile on a Pod
* Learn how to check that the profile is loaded
* See what happens when a profile is violated
* See what happens when a profile cannot be loaded 
-->
<ul>
<li>查看如何在节点上加载配置文件示例</li>
<li>了解如何在 Pod 上强制执行配置文件</li>
<li>了解如何检查配置文件是否已加载</li>
<li>查看违反配置文件时会发生什么</li>
<li>查看无法加载配置文件时会发生什么</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<!-- Make sure: -->
<p>确保：</p>
<!-- 
1. Kubernetes version is at least v1.4 -- Kubernetes support for AppArmor was added in
   v1.4. Kubernetes components older than v1.4 are not aware of the new AppArmor annotations, and
   will **silently ignore** any AppArmor settings that are provided. To ensure that your Pods are
   receiving the expected protections, it is important to verify the Kubelet version of your nodes:

   ```shell
   kubectl get nodes -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.status.nodeInfo.kubeletVersion}\n{end}'
   ```
   ```
   gke-test-default-pool-239f5d02-gyn2: v1.4.0
   gke-test-default-pool-239f5d02-x1kf: v1.4.0
   gke-test-default-pool-239f5d02-xwux: v1.4.0
   ```
-->
<ol>
<li>
<p>Kubernetes 版本至少是 v1.4 —— AppArmor 在 Kubernetes v1.4 版本中才添加了对 AppArmor 的支持。
早于 v1.4 版本的 Kubernetes 组件不知道新的 AppArmor 注解
并且将会 <strong>默认忽略</strong> 提供的任何 AppArmor 设置。
为了确保你的 Pod 能够得到预期的保护，必须验证节点的 Kubelet 版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes -o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">$&#39;{range .items[*]}{@.metadata.name}: {@.status.nodeInfo.kubeletVersion}\n{end}&#39;</span>
</code></pre></div><pre><code>gke-test-default-pool-239f5d02-gyn2: v1.4.0
gke-test-default-pool-239f5d02-x1kf: v1.4.0
gke-test-default-pool-239f5d02-xwux: v1.4.0
</code></pre></li>
</ol>
<!-- 
2. AppArmor kernel module is enabled -- For the Linux kernel to enforce an AppArmor profile, the
   AppArmor kernel module must be installed and enabled. Several distributions enable the module by
   default, such as Ubuntu and SUSE, and many others provide optional support. To check whether the
   module is enabled, check the `/sys/module/apparmor/parameters/enabled` file:

   ```shell
   cat /sys/module/apparmor/parameters/enabled
   Y
   ```

   If the Kubelet contains AppArmor support (>= v1.4), it will refuse to run a Pod with AppArmor
   options if the kernel module is not enabled.

  <div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Ubuntu carries many AppArmor patches that have not been merged into the upstream Linux
kernel, including patches that add additional hooks and features. Kubernetes has only been
tested with the upstream version, and does not promise support for other features.
</div>
-->
<ol start="2">
<li>
<p>AppArmor 内核模块已启用 —— 要使 Linux 内核强制执行 AppArmor 配置文件，
必须安装并且启动 AppArmor 内核模块。默认情况下，有几个发行版支持该模块，
如 Ubuntu 和 SUSE，还有许多发行版提供可选支持。要检查模块是否已启用，请检查
<code>/sys/module/apparmor/parameters/enabled</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat /sys/module/apparmor/parameters/enabled
Y
</code></pre></div><p>如果 Kubelet 包含 AppArmor 支持（&gt;= v1.4），
但是内核模块未启用，它将拒绝运行带有 AppArmor 选项的 Pod。</p>
</li>
</ol>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Ubuntu 携带了许多没有合并到上游 Linux 内核中的 AppArmor 补丁，
包括添加附加钩子和特性的补丁。Kubernetes 只在上游版本中测试过，不承诺支持其他特性。
</div>
<!--
3. Container runtime supports AppArmor -- Currently all common Kubernetes-supported container
   runtimes should support AppArmor, like <a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a>,
   <a class='glossary-tooltip' title='专用于 Kubernetes 的轻量级容器运行时软件' data-toggle='tooltip' data-placement='top' href='https://cri-o.io/#what-is-cri-o' target='_blank' aria-label='CRI-O'>CRI-O</a> or <a class='glossary-tooltip' title='强调简单性、健壮性和可移植性的一种容器运行时' data-toggle='tooltip' data-placement='top' href='https://containerd.io/docs/' target='_blank' aria-label='containerd'>containerd</a>.
   Please refer to the corresponding runtime documentation and verify that the cluster fulfills
   the requirements to use AppArmor.
-->
<ol start="3">
<li>容器运行时支持 AppArmor —— 目前所有常见的 Kubernetes 支持的容器运行时都应该支持 AppArmor，
像 <a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a>，<a class='glossary-tooltip' title='专用于 Kubernetes 的轻量级容器运行时软件' data-toggle='tooltip' data-placement='top' href='https://cri-o.io/#what-is-cri-o' target='_blank' aria-label='CRI-O'>CRI-O</a>
或 <a class='glossary-tooltip' title='强调简单性、健壮性和可移植性的一种容器运行时' data-toggle='tooltip' data-placement='top' href='https://containerd.io/docs/' target='_blank' aria-label='containerd'>containerd</a>。
请参考相应的运行时文档并验证集群是否满足使用 AppArmor 的要求。</li>
</ol>
<!-- 
4. Profile is loaded -- AppArmor is applied to a Pod by specifying an AppArmor profile that each
   container should be run with. If any of the specified profiles is not already loaded in the
   kernel, the Kubelet (>= v1.4) will reject the Pod. You can view which profiles are loaded on a
   node by checking the `/sys/kernel/security/apparmor/profiles` file. For example:

   ```shell
   ssh gke-test-default-pool-239f5d02-gyn2 "sudo cat /sys/kernel/security/apparmor/profiles | sort"
   ```
   ```
   apparmor-test-deny-write (enforce)
   apparmor-test-audit-write (enforce)
   docker-default (enforce)
   k8s-nginx (enforce)
   ```

   For more details on loading profiles on nodes, see
   [Setting up nodes with profiles](#setting-up-nodes-with-profiles).
-->
<ol start="4">
<li>
<p>配置文件已加载 —— 通过指定每个容器都应使用的 AppArmor 配置文件，
AppArmor 会被应用到 Pod 上。如果指定的任何配置文件尚未加载到内核，
Kubelet（&gt;= v1.4） 将拒绝 Pod。
通过检查 <code>/sys/kernel/security/apparmor/profiles</code> 文件，
可以查看节点加载了哪些配置文件。例如:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ssh gke-test-default-pool-239f5d02-gyn2 <span style="color:#b44">&#34;sudo cat /sys/kernel/security/apparmor/profiles | sort&#34;</span>
</code></pre></div><pre><code>apparmor-test-deny-write (enforce)
apparmor-test-audit-write (enforce)
docker-default (enforce)
k8s-nginx (enforce)
</code></pre><p>有关在节点上加载配置文件的详细信息，请参见<a href="#setting-up-nodes-with-profiles">使用配置文件设置节点</a>。</p>
</li>
</ol>
<!-- 
As long as the Kubelet version includes AppArmor support (>= v1.4), the Kubelet will reject a Pod
with AppArmor options if any of the prerequisites are not met. You can also verify AppArmor support
on nodes by checking the node ready condition message (though this is likely to be removed in a
later release): 
-->
<p>只要 Kubelet 版本包含 AppArmor 支持(&gt;=v1.4)，
如果不满足这些先决条件，Kubelet 将拒绝带有 AppArmor 选项的 Pod。
你还可以通过检查节点就绪状况消息来验证节点上的 AppArmor 支持（尽管这可能会在以后的版本中删除）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes -o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">$&#39;{range .items[*]}{@.metadata.name}: {.status.conditions[?(@.reason==&#34;KubeletReady&#34;)].message}\n{end}&#39;</span>
</code></pre></div><pre><code>gke-test-default-pool-239f5d02-gyn2: kubelet is posting ready status. AppArmor enabled
gke-test-default-pool-239f5d02-x1kf: kubelet is posting ready status. AppArmor enabled
gke-test-default-pool-239f5d02-xwux: kubelet is posting ready status. AppArmor enabled
</code></pre><!-- lessoncontent -->
<!-- ## Securing a Pod -->
<h2 id="securing-a-pod">保护 Pod</h2>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
AppArmor is currently in beta, so options are specified as annotations. Once support graduates to
general availability, the annotations will be replaced with first-class fields (more details in
[Upgrade path to GA](#upgrade-path-to-general-availability)).
-->
<p>AppArmor 目前处于 Beta 阶段，因此选项以注解形式设定。
一旦 AppArmor 支持进入正式发布阶段，注解将被替换为一阶的资源字段
（更多详情参见<a href="#upgrade-path-to-general-availability">升级到 GA 的途径</a>）。
</div>
<!--
AppArmor profiles are specified *per-container*. To specify the AppArmor profile to run a Pod
container with, add an annotation to the Pod's metadata: 
-->
<p>AppArmor 配置文件是按 <em>逐个容器</em> 的形式来设置的。
要指定用来运行 Pod 容器的 AppArmor 配置文件，请向 Pod 的 metadata 添加注解：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">container.apparmor.security.beta.kubernetes.io/&lt;container_name&gt;</span>:<span style="color:#bbb"> </span>&lt;profile_ref&gt;<span style="color:#bbb">
</span></code></pre></div><!-- 
Where `<container_name>` is the name of the container to apply the profile to, and `<profile_ref>`
specifies the profile to apply. The `profile_ref` can be one of: 
-->
<p><code>&lt;container_name&gt;</code> 的名称是配置文件所针对的容器的名称，<code>&lt;profile_def&gt;</code> 则设置要应用的配置文件。
<code>&lt;profile_ref&gt;</code> 可以是以下取值之一：</p>
<!-- 
* `runtime/default` to apply the runtime's default profile
* `localhost/<profile_name>` to apply the profile loaded on the host with the name `<profile_name>`
* `unconfined` to indicate that no profiles will be loaded 
-->
<ul>
<li><code>runtime/default</code> 应用运行时的默认配置</li>
<li><code>localhost/&lt;profile_name&gt;</code> 应用在主机上加载的名为 <code>&lt;profile_name&gt;</code> 的配置文件</li>
<li><code>unconfined</code> 表示不加载配置文件</li>
</ul>
<!-- 
See the [API Reference](#api-reference) for the full details on the annotation and profile name formats.
-->
<p>有关注解和配置文件名称格式的详细信息，请参阅<a href="#api-reference">API 参考</a>。</p>
<!-- 
Kubernetes AppArmor enforcement works by first checking that all the prerequisites have been
met, and then forwarding the profile selection to the container runtime for enforcement. If the
prerequisites have not been met, the Pod will be rejected, and will not run. 
-->
<p>Kubernetes AppArmor 强制执行机制首先检查所有先决条件都已满足，
然后将所选的配置文件转发到容器运行时进行强制执行。
如果未满足先决条件，Pod 将被拒绝，并且不会运行。</p>
<!-- 
To verify that the profile was applied, you can look for the AppArmor security option listed in the container created event: 
-->
<p>要验证是否应用了配置文件，可以在容器创建事件中查找所列出的 AppArmor 安全选项：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get events | grep Created
</code></pre></div><pre><code>22s        22s         1         hello-apparmor     Pod       spec.containers{hello}   Normal    Created     {kubelet e2e-test-stclair-node-pool-31nt}   Created container with docker id 269a53b202d3; Security:[seccomp=unconfined apparmor=k8s-apparmor-example-deny-write]
</code></pre><!-- 
You can also verify directly that the container's root process is running with the correct profile by checking its proc attr: 
-->
<p>你还可以通过检查容器的 proc attr，直接验证容器的根进程是否以正确的配置文件运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> &lt;pod_name&gt; cat /proc/1/attr/current
</code></pre></div><pre><code>k8s-apparmor-example-deny-write (enforce)
</code></pre><!-- ## Example -->
<h2 id="example">举例</h2>
<!-- *This example assumes you have already set up a cluster with AppArmor support.* -->
<p><em>本例假设你已经设置了一个集群使用 AppArmor 支持。</em></p>
<!-- 
First, we need to load the profile we want to use onto our nodes. This profile denies all file writes: 
-->
<p>首先，我们需要将要使用的配置文件加载到节点上。配置文件拒绝所有文件写入：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic">#include &lt;tunables/global&gt;</span>

profile k8s-apparmor-example-deny-write <span style="color:#b8860b">flags</span><span style="color:#666">=(</span>attach_disconnected<span style="color:#666">)</span> <span style="color:#666">{</span>
  <span style="color:#080;font-style:italic">#include &lt;abstractions/base&gt;</span>

  file,

  <span style="color:#080;font-style:italic"># Deny all file writes.</span>
  deny /** w,
<span style="color:#666">}</span>
</code></pre></div><!-- 
Since we don't know where the Pod will be scheduled, we'll need to load the profile on all our
nodes. For this example we'll use SSH to install the profiles, but other approaches are
discussed in [Setting up nodes with profiles](#setting-up-nodes-with-profiles). 
-->
<p>由于我们不知道 Pod 将被调度到哪里，我们需要在所有节点上加载配置文件。
在本例中，我们将使用 SSH 来安装概要文件，
但是在<a href="#setting-up-nodes-with-profiles">使用配置文件设置节点</a>中讨论了其他方法。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">NODES</span><span style="color:#666">=(</span>
    <span style="color:#080;font-style:italic"># The SSH-accessible domain names of your nodes</span>
    gke-test-default-pool-239f5d02-gyn2.us-central1-a.my-k8s
    gke-test-default-pool-239f5d02-x1kf.us-central1-a.my-k8s
    gke-test-default-pool-239f5d02-xwux.us-central1-a.my-k8s<span style="color:#666">)</span>
<span style="color:#a2f;font-weight:bold">for</span> NODE in <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODES</span>[*]<span style="color:#b68;font-weight:bold">}</span>; <span style="color:#a2f;font-weight:bold">do</span> ssh <span style="color:#b8860b">$NODE</span> <span style="color:#b44">&#39;sudo apparmor_parser -q &lt;&lt;EOF
</span><span style="color:#b44">#include &lt;tunables/global&gt;
</span><span style="color:#b44">
</span><span style="color:#b44">profile k8s-apparmor-example-deny-write flags=(attach_disconnected) {
</span><span style="color:#b44">  #include &lt;abstractions/base&gt;
</span><span style="color:#b44">
</span><span style="color:#b44">  file,
</span><span style="color:#b44">
</span><span style="color:#b44">  # Deny all file writes.
</span><span style="color:#b44">  deny /** w,
</span><span style="color:#b44">}
</span><span style="color:#b44">EOF&#39;</span>
<span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!-- Next, we'll run a simple "Hello AppArmor" pod with the deny-write profile: -->
<p>接下来，我们将运行一个带有拒绝写入配置文件的简单 “Hello AppArmor” Pod：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/hello-apparmor.yaml" download="pods/security/hello-apparmor.yaml"><code>pods/security/hello-apparmor.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-hello-apparmor-yaml')" title="Copy pods/security/hello-apparmor.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-hello-apparmor-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello-apparmor<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Tell Kubernetes to apply the AppArmor profile &#34;k8s-apparmor-example-deny-write&#34;.</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Note that this is ignored if the Kubernetes node is not running version 1.4 or greater.</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">container.apparmor.security.beta.kubernetes.io/hello</span>:<span style="color:#bbb"> </span>localhost/k8s-apparmor-example-deny-write<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;echo &#39;Hello AppArmor!&#39; &amp;&amp; sleep 1h&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./hello-apparmor.yaml
</code></pre></div><!-- 
If we look at the pod events, we can see that the Pod container was created with the AppArmor
profile "k8s-apparmor-example-deny-write": 
-->
<p>如果我们查看 Pod 事件，我们可以看到 Pod 容器是用 AppArmor
配置文件 “k8s-apparmor-example-deny-write” 所创建的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get events | grep hello-apparmor
</code></pre></div><pre><code>14s        14s         1         hello-apparmor   Pod                                Normal    Scheduled   {default-scheduler }                           Successfully assigned hello-apparmor to gke-test-default-pool-239f5d02-gyn2
14s        14s         1         hello-apparmor   Pod       spec.containers{hello}   Normal    Pulling     {kubelet gke-test-default-pool-239f5d02-gyn2}   pulling image &quot;busybox&quot;
13s        13s         1         hello-apparmor   Pod       spec.containers{hello}   Normal    Pulled      {kubelet gke-test-default-pool-239f5d02-gyn2}   Successfully pulled image &quot;busybox&quot;
13s        13s         1         hello-apparmor   Pod       spec.containers{hello}   Normal    Created     {kubelet gke-test-default-pool-239f5d02-gyn2}   Created container with docker id 06b6cd1c0989; Security:[seccomp=unconfined apparmor=k8s-apparmor-example-deny-write]
13s        13s         1         hello-apparmor   Pod       spec.containers{hello}   Normal    Started     {kubelet gke-test-default-pool-239f5d02-gyn2}   Started container with docker id 06b6cd1c0989
</code></pre><!-- We can verify that the container is actually running with that profile by checking its proc attr: -->
<p>我们可以通过检查该配置文件的 proc attr 来验证容器是否实际使用该配置文件运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> hello-apparmor -- cat /proc/1/attr/current
</code></pre></div><pre><code>k8s-apparmor-example-deny-write (enforce)
</code></pre><!-- Finally, we can see what happens if we try to violate the profile by writing to a file: -->
<p>最后，我们可以看到，如果我们尝试通过写入文件来违反配置文件会发生什么：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> hello-apparmor -- touch /tmp/test
</code></pre></div><pre><code>touch: /tmp/test: Permission denied
error: error executing remote command: command terminated with non-zero exit code: Error executing in Docker Container: 1
</code></pre><!-- To wrap up, let's look at what happens if we try to specify a profile that hasn't been loaded: -->
<p>最后，让我们看看如果我们试图指定一个尚未加载的配置文件会发生什么：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f /dev/stdin &lt;&lt;EOF
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello-apparmor-2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">container.apparmor.security.beta.kubernetes.io/hello</span>:<span style="color:#bbb"> </span>localhost/k8s-apparmor-example-allow-write<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;echo &#39;Hello AppArmor!&#39; &amp;&amp; sleep 1h&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span>EOF<span style="color:#bbb">
</span><span style="color:#bbb"></span>pod/hello-apparmor-2 created<span style="color:#bbb">
</span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod hello-apparmor-2
</code></pre></div><pre><code>Name:          hello-apparmor-2
Namespace:     default
Node:          gke-test-default-pool-239f5d02-x1kf/
Start Time:    Tue, 30 Aug 2016 17:58:56 -0700
Labels:        &lt;none&gt;
Annotations:   container.apparmor.security.beta.kubernetes.io/hello=localhost/k8s-apparmor-example-allow-write
Status:        Pending
Reason:        AppArmor
Message:       Pod Cannot enforce AppArmor: profile &quot;k8s-apparmor-example-allow-write&quot; is not loaded
IP:
Controllers:   &lt;none&gt;
Containers:
  hello:
    Container ID:
    Image:     busybox
    Image ID:
    Port:
    Command:
      sh
      -c
      echo 'Hello AppArmor!' &amp;&amp; sleep 1h
    State:              Waiting
      Reason:           Blocked
    Ready:              False
    Restart Count:      0
    Environment:        &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-dnz7v (ro)
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  default-token-dnz7v:
    Type:    Secret (a volume populated by a Secret)
    SecretName:    default-token-dnz7v
    Optional:   false
QoS Class:      BestEffort
Node-Selectors: &lt;none&gt;
Tolerations:    &lt;none&gt;
Events:
  FirstSeen    LastSeen    Count    From                        SubobjectPath    Type        Reason        Message
  ---------    --------    -----    ----                        -------------    --------    ------        -------
  23s          23s         1        {default-scheduler }                         Normal      Scheduled     Successfully assigned hello-apparmor-2 to e2e-test-stclair-node-pool-t1f5
  23s          23s         1        {kubelet e2e-test-stclair-node-pool-t1f5}             Warning        AppArmor    Cannot enforce AppArmor: profile &quot;k8s-apparmor-example-allow-write&quot; is not loaded
</code></pre><!-- 
Note the pod status is Pending, with a helpful error message: `Pod Cannot enforce AppArmor: profile
"k8s-apparmor-example-allow-write" is not loaded`. An event was also recorded with the same message. 
-->
<p>注意 Pod 呈现 Pending 状态，并且显示一条有用的错误信息：
<code>Pod Cannot enforce AppArmor: profile &quot;k8s-apparmor-example-allow-write&quot; is not loaded</code>。
还用相同的消息记录了一个事件。</p>
<!-- ## Administration -->
<h2 id="administration">管理</h2>
<!-- ### Setting up nodes with profiles -->
<h3 id="setting-up-nodes-with-profiles">使用配置文件设置节点</h3>
<!-- 
Kubernetes does not currently provide any native mechanisms for loading AppArmor profiles onto
nodes. There are lots of ways to setup the profiles though, such as: 
-->
<p>Kubernetes 目前不提供任何本地机制来将 AppArmor 配置文件加载到节点上。
有很多方法可以设置配置文件，例如：</p>
<!-- 
* Through a [DaemonSet](/docs/concepts/workloads/controllers/daemonset/) that runs a Pod on each node to
  ensure the correct profiles are loaded. An example implementation can be found
  [here](https://git.k8s.io/kubernetes/test/images/apparmor-loader).
* At node initialization time, using your node initialization scripts (e.g. Salt, Ansible, etc.) or
  image.
* By copying the profiles to each node and loading them through SSH, as demonstrated in the
  [Example](#example). 
-->
<ul>
<li>通过在每个节点上运行 Pod 的
<a href="/zh/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>来确保加载了正确的配置文件。
可以在<a href="https://git.k8s.io/kubernetes/test/images/apparmor-loader">这里</a>找到实现示例。</li>
<li>在节点初始化时，使用节点初始化脚本(例如 Salt、Ansible 等)或镜像。</li>
<li>通过将配置文件复制到每个节点并通过 SSH 加载它们，如<a href="#example">示例</a>。</li>
</ul>
<!-- 
The scheduler is not aware of which profiles are loaded onto which node, so the full set of profiles
must be loaded onto every node.  An alternative approach is to add a node label for each profile (or
class of profiles) on the node, and use a
[node selector](/docs/concepts/configuration/assign-pod-node/) to ensure the Pod is run on a
node with the required profile. 
-->
<p>调度程序不知道哪些配置文件加载到哪个节点上，因此必须将全套配置文件加载到每个节点上。
另一种方法是为节点上的每个配置文件（或配置文件类）添加节点标签，
并使用<a href="/zh/docs/concepts/configuration/assign-pod-node/">节点选择器</a>确保
Pod 在具有所需配置文件的节点上运行。</p>
<!-- ### Restricting profiles with the PodSecurityPolicy -->
<h3 id="restricting-profiles-with-the-podsecuritypolicy">使用 PodSecurityPolicy 限制配置文件</h3>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
PodSecurityPolicy is deprecated in Kubernetes v1.21, and will be removed in v1.25.
See [PodSecurityPolicy](/docs/concepts/security/pod-security-policy/) documentation for more information.
-->
<p>PodSecurityPolicy 在 Kubernetes v1.21 版本中已被废弃，将在 v1.25 版本移除。
查看 <a href="/zh/docs/concepts/security/pod-security-policy/">PodSecurityPolicy</a> 文档获取更多信息。
</div>
<!-- 
If the PodSecurityPolicy extension is enabled, cluster-wide AppArmor restrictions can be applied. To
enable the PodSecurityPolicy, the following flag must be set on the `apiserver`: 
-->
<p>如果启用了 PodSecurityPolicy 扩展，则可以应用群集范围的 AppArmor 限制。
要启用 PodSecurityPolicy，必须在 <code>apiserver</code> 上设置以下标志：</p>
<pre><code>--enable-admission-plugins=PodSecurityPolicy[,others...]
</code></pre><!-- The AppArmor options can be specified as annotations on the PodSecurityPolicy: -->
<p>AppArmor 选项可以指定为 PodSecurityPolicy 上的注解：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apparmor.security.beta.kubernetes.io/defaultProfileName</span>:<span style="color:#bbb"> </span>&lt;profile_ref&gt;<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apparmor.security.beta.kubernetes.io/allowedProfileNames</span>:<span style="color:#bbb"> </span>&lt;profile_ref&gt;[,others...]<span style="color:#bbb">
</span></code></pre></div><!-- 
The default profile name option specifies the profile to apply to containers by default when none is
specified. The allowed profile names option specifies a list of profiles that Pod containers are
allowed to be run with. If both options are provided, the default must be allowed. The profiles are
specified in the same format as on containers. See the [API Reference](#api-reference) for the full
specification. 
-->
<p>默认配置文件名选项指定默认情况下在未指定任何配置文件时应用于容器的配置文件。
所允许的配置文件名称选项指定允许 Pod 容器运行期间所对应的配置文件列表。
如果同时提供了这两个选项，则必须允许默认值。
配置文件的指定格式与容器上的相同。有关完整规范，请参阅 <a href="#api-reference">API 参考</a>。</p>
<!-- ### Disabling AppArmor -->
<h3 id="disabling-apparmor">禁用 AppArmor</h3>
<!-- If you do not want AppArmor to be available on your cluster, it can be disabled by a command-line flag: -->
<p>如果你不希望 AppArmor 在集群上可用，可以通过命令行标志禁用它：</p>
<pre><code>--feature-gates=AppArmor=false
</code></pre><!-- 
When disabled, any Pod that includes an AppArmor profile will fail validation with a "Forbidden"
error. 
-->
<p>禁用时，任何包含 AppArmor 配置文件的 Pod 都将导致验证失败，且返回 “Forbidden” 错误。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Even if the Kubernetes feature is disabled, runtimes may still enforce the default profile. The
option to disable the AppArmor feature will be removed when AppArmor graduates to general
availability (GA).
-->
<p>即使此 Kubernetes 特性被禁用，运行时仍可能强制执行默认配置文件。
当 AppArmor 升级为正式版 (GA) 时，禁用 AppArmor 功能的选项将被删除。
</div>
<!-- ## Authoring Profiles -->
<h2 id="authoring-profiles">编写配置文件</h2>
<!-- 
Getting AppArmor profiles specified correctly can be a tricky business. Fortunately there are some
tools to help with that: 
-->
<p>获得正确指定的 AppArmor 配置文件可能是一件棘手的事情。幸运的是，有一些工具可以帮助你做到这一点：</p>
<!-- 
* `aa-genprof` and `aa-logprof` generate profile rules by monitoring an application's activity and
  logs, and admitting the actions it takes. Further instructions are provided by the
  [AppArmor documentation](https://gitlab.com/apparmor/apparmor/wikis/Profiling_with_tools).
* [bane](https://github.com/jfrazelle/bane) is an AppArmor profile generator for Docker that uses a
  simplified profile language. 
-->
<ul>
<li><code>aa-genprof</code> 和 <code>aa-logprof</code>
通过监视应用程序的活动和日志并准许它所执行的操作来生成配置文件规则。
<a href="https://gitlab.com/apparmor/apparmor/wikis/Profiling_with_tools">AppArmor 文档</a>提供了进一步的指导。</li>
<li><a href="https://github.com/jfrazelle/bane">bane</a>
是一个用于 Docker的 AppArmor 配置文件生成器，它使用一种简化的画像语言（profile language）</li>
</ul>
<!-- 
To debug problems with AppArmor, you can check the system logs to see what, specifically, was
denied. AppArmor logs verbose messages to `dmesg`, and errors can usually be found in the system
logs or through `journalctl`. More information is provided in
[AppArmor failures](https://gitlab.com/apparmor/apparmor/wikis/AppArmor_Failures). 
-->
<p>想要调试 AppArmor 的问题，你可以检查系统日志，查看具体拒绝了什么。
AppArmor 将详细消息记录到 <code>dmesg</code>，
错误通常可以在系统日志中或通过 <code>journalctl</code> 找到。
更多详细信息见 <a href="https://gitlab.com/apparmor/apparmor/wikis/AppArmor_Failures">AppArmor 失败</a>。</p>
<!-- ## API Reference -->
<h2 id="api-reference">API 参考</h2>
<!-- ### Pod Annotation -->
<h3 id="pod-annotation">Pod 注解</h3>
<!-- Specifying the profile a container will run with: -->
<p>指定容器将使用的配置文件：</p>
<!-- 
- **key**: `container.apparmor.security.beta.kubernetes.io/<container_name>`
  Where `<container_name>` matches the name of a container in the Pod.
  A separate profile can be specified for each container in the Pod.
- **value**: a profile reference, described below 
-->
<ul>
<li><strong>键名</strong>: <code>container.apparmor.security.beta.kubernetes.io/&lt;container_name&gt;</code>
，其中 <code>&lt;container_name&gt;</code> 与 Pod 中某容器的名称匹配。
可以为 Pod 中的每个容器指定单独的配置文件。</li>
<li><strong>键值</strong>: 对配置文件的引用，如下所述</li>
</ul>
<!-- ### Profile Reference -->
<h3 id="profile-reference">配置文件引用</h3>
<!-- 
- `runtime/default`: Refers to the default runtime profile.
  - Equivalent to not specifying a profile (without a PodSecurityPolicy default), except it still
    requires AppArmor to be enabled.
  - In practice, many container runtimes use the same OCI default profile, defined here:
    https://github.com/containers/common/blob/main/pkg/apparmor/apparmor_linux_template.go
- `localhost/<profile_name>`: Refers to a profile loaded on the node (localhost) by name.
  - The possible profile names are detailed in the
    [core policy reference](https://gitlab.com/apparmor/apparmor/wikis/AppArmor_Core_Policy_Reference#profile-names-and-attachment-specifications).
- `unconfined`: This effectively disables AppArmor on the container. 
-->
<ul>
<li><code>runtime/default</code>: 指默认运行时配置文件。
<ul>
<li>等同于不指定配置文件（没有 PodSecurityPolicy 默认值），只是它仍然需要启用 AppArmor。</li>
<li>实际上，许多容器运行时使用相同的 OCI 默认配置文件，在此处定义：
<a href="https://github.com/containers/common/blob/main/pkg/apparmor/apparmor_linux_template.go">https://github.com/containers/common/blob/main/pkg/apparmor/apparmor_linux_template.go</a></li>
</ul>
</li>
<li><code>localhost/&lt;profile_name&gt;</code>: 按名称引用加载到节点（localhost）上的配置文件。
<ul>
<li>可能的配置文件名在<a href="https://gitlab.com/apparmor/apparmor/wikis/AppArmor_Core_Policy_Reference#profile-names-and-attachment-specifications">核心策略参考</a>。</li>
</ul>
</li>
<li><code>unconfined</code>: 这相当于为容器禁用 AppArmor。</li>
</ul>
<!-- Any other profile reference format is invalid. -->
<p>任何其他配置文件引用格式无效。</p>
<!-- ### PodSecurityPolicy Annotations -->
<h3 id="podsecuritypolicy-annotations">PodSecurityPolicy 注解</h3>
<!-- Specifying the default profile to apply to containers when none is provided: -->
<p>指定在未提供容器时应用于容器的默认配置文件：</p>
<!-- 
* **key**: `apparmor.security.beta.kubernetes.io/defaultProfileName`
* **value**: a profile reference, described above 
-->
<ul>
<li><strong>键名</strong>: <code>apparmor.security.beta.kubernetes.io/defaultProfileName</code></li>
<li><strong>键值</strong>: 如上述文件参考所述</li>
</ul>
<!-- Specifying the list of profiles Pod containers is allowed to specify: -->
<p>上面描述的指定配置文件，Pod 容器列表的配置文件引用允许指定：</p>
<!-- 
* **key**: `apparmor.security.beta.kubernetes.io/allowedProfileNames`
* **value**: a comma-separated list of profile references (described above)
  - Although an escaped comma is a legal character in a profile name, it cannot be explicitly
    allowed here. 
-->
<ul>
<li><strong>键名</strong>: <code>apparmor.security.beta.kubernetes.io/allowedProfileNames</code></li>
<li><strong>键值</strong>: 配置文件引用的逗号分隔列表（如上所述）
<ul>
<li>尽管转义逗号是配置文件名中的合法字符，但此处不能显式允许。</li>
</ul>
</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!-- Additional resources: -->
<p>其他资源：</p>
<!-- 
* [Quick guide to the AppArmor profile language](https://gitlab.com/apparmor/apparmor/wikis/QuickProfileLanguage)
* [AppArmor core policy reference](https://gitlab.com/apparmor/apparmor/wikis/Policy_Layout) 
-->
<ul>
<li><a href="https://gitlab.com/apparmor/apparmor/wikis/QuickProfileLanguage">Apparmor 配置文件语言快速指南</a></li>
<li><a href="https://gitlab.com/apparmor/apparmor/wikis/Policy_Layout">Apparmor 核心策略参考</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-31a6c137cfc5bfea9d88f4b109109465">4.2 - 在名字空间级别应用 Pod 安全标准</h1>
    
	<!--
title: Apply Pod Security Standards at the Namespace Level
content_type: tutorial
weight: 10
-->


<div class="alert alert-primary" role="alert">
<h4 class="alert-heading">Note</h4>

    <!-- This tutorial applies only for new clusters. -->
<p>本教程仅适用于新集群。

</div>

<!--
Pod Security admission (PSA) is enabled by default in v1.23 and later, as it
[graduated to beta](/blog/2021/12/09/pod-security-admission-beta/). Pod Security Admission
is an admission controller that applies 
[Pod Security Standards](/docs/concepts/security/pod-security-standards/) 
when pods are created. In this tutorial, you will enforce the `baseline` Pod Security Standard,
one namespace at a time.

You can also apply Pod Security Standards to multiple namespaces at once at the cluster
level. For instructions, refer to 
[Apply Pod Security Standards at the cluster level](/docs/tutorials/security/cluster-level-pss).
-->
<p>Pod 安全准入（PSA）在 v1.23 及更高版本默认启用，
因为它<a href="/blog/2021/12/09/pod-security-admission-beta/">升级到测试版（beta）</a>。
Pod 安全准入是在创建 Pod 时应用
<a href="/zh/docs/concepts/security/pod-security-standards/">Pod 安全标准</a>的准入控制器。
在本教程中，你将应用 <code>baseline</code> Pod 安全标准，每次一个名字空间。</p>
<p>你还可以在集群级别一次将 Pod 安全标准应用于多个名称空间。
有关说明，请参阅<a href="/zh/docs/tutorials/security/cluster-level-pss">在集群级别应用 Pod 安全标准</a>。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!-- 
Install the following on your workstation:

- [KinD](https://kind.sigs.k8s.io/docs/user/quick-start/#installation)
- [kubectl](/docs/tasks/tools/)
-->
<p>在你的工作站中安装以下内容：</p>
<ul>
<li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">KinD</a></li>
<li><a href="/zh/docs/tasks/tools/">kubectl</a></li>
</ul>
<!--
## Create cluster

1. Create a `KinD` cluster as follows:
-->
<h2 id="create-cluster">创建集群 </h2>
<ol start="2">
<li>
<p>按照如下方式创建一个 <code>KinD</code> 集群：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kind create cluster --name psa-ns-level --image kindest/node:v1.23.0
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Creating cluster &quot;psa-ns-level&quot; ...
 ✓ Ensuring node image (kindest/node:v1.23.0) 🖼 
 ✓ Preparing nodes 📦  
 ✓ Writing configuration 📜 
 ✓ Starting control-plane 🕹️ 
 ✓ Installing CNI 🔌 
 ✓ Installing StorageClass 💾 
Set kubectl context to &quot;kind-psa-ns-level&quot;
You can now use your cluster with:

kubectl cluster-info --context kind-psa-ns-level

Not sure what to do next? 😅  Check out https://kind.sigs.k8s.io/docs/user/quick-start/
</code></pre></li>
</ol>
<!-- 1. Set the kubectl context to the new cluster: -->
<ol>
<li>将 kubectl 上下文设置为新集群：
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info --context kind-psa-ns-level
</code></pre></div> <!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Kubernetes control plane is running at https://127.0.0.1:50996
CoreDNS is running at https://127.0.0.1:50996/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</code></pre></li>
</ol>
<!--
## Create a namespace

Create a new namespace called `example`:
-->
<h2 id="create-a-namespace">创建名字空间 </h2>
<p>创建一个名为 <code>example</code> 的新名字空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create ns example
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>namespace/example created
</code></pre><!-- 
## Apply Pod Security Standards

1. Enable Pod Security Standards on this namespace using labels supported by
   built-in Pod Security Admission. In this step we will warn on baseline pod
   security standard as per the latest version (default value)
-->
<h2 id="apply-pod-security-standards">应用 Pod 安全标准 </h2>
<ol>
<li>
<p>使用内置 Pod 安全准入所支持的标签在此名字空间上启用 Pod 安全标准。
在这一步中，我们将根据最新版本（默认值）对基线 Pod 安全标准发出警告。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl label --overwrite ns example <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  pod-security.kubernetes.io/warn<span style="color:#666">=</span>baseline <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  pod-security.kubernetes.io/warn-version<span style="color:#666">=</span>latest
</code></pre></div></li>
</ol>
<!-- 
2. Multiple pod security standards can be enabled on any namespace, using labels.
   Following command will `enforce` the `baseline` Pod Security Standard, but
   `warn` and `audit` for `restricted` Pod Security Standards as per the latest
   version (default value)
-->
<ol start="2">
<li>
<p>可以使用标签在任何名字空间上启用多个 Pod 安全标准。
以下命令将强制（<code>enforce</code>） 执行基线（<code>baseline</code>）Pod 安全标准，
但根据最新版本（默认值）对受限（<code>restricted</code>）Pod 安全标准执行警告（<code>warn</code>）和审核（<code>audit</code>）。</p>
<pre><code>kubectl label --overwrite ns example \
  pod-security.kubernetes.io/enforce=baseline \
  pod-security.kubernetes.io/enforce-version=latest \
  pod-security.kubernetes.io/warn=restricted \
  pod-security.kubernetes.io/warn-version=latest \
  pod-security.kubernetes.io/audit=restricted \
  pod-security.kubernetes.io/audit-version=latest
</code></pre></li>
</ol>
<!-- 
## Verify the Pod Security Standards

1. Create a minimal pod in `example` namespace:
-->
<h2 id="verify-the-pod-security-standards">验证 Pod 安全标准 </h2>
<ol>
<li>
<p>在 <code>example</code> 名字空间中创建一个最小的 pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt; /tmp/pss/nginx-pod.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: Pod
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  containers:
</span><span style="color:#b44">    - image: nginx
</span><span style="color:#b44">      name: nginx
</span><span style="color:#b44">      ports:
</span><span style="color:#b44">        - containerPort: 80
</span><span style="color:#b44">EOF</span>
</code></pre></div></li>
</ol>
<!-- 
2. Apply the pod spec to the cluster in `example` namespace: 
-->
<ol>
<li>将 Pod 规约应用到集群中的 <code>example</code> 名字空间中：
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -n example -f /tmp/pss/nginx-pod.yaml
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Warning: would violate PodSecurity &quot;restricted:latest&quot;: allowPrivilegeEscalation != false (container &quot;nginx&quot; must set securityContext allowPrivilegeEscalation=false), unrestricted capabilities (container &quot;nginx&quot; must set securityContext.capabilities.drop=[&quot;ALL&quot;]), runAsNonRoot != true (pod or container &quot;nginx&quot; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &quot;nginx&quot; must set securityContext seccompProfile.type to &quot;RuntimeDefault&quot; or &quot;Localhost&quot;)
pod/nginx created
</code></pre></li>
</ol>
<!-- 
1. Apply the pod spec to the cluster in `default` namespace:
-->
<ol start="3">
<li>
<p>将 Pod 规约应用到集群中的 <code>default</code> 名字空间中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -n default -f /tmp/pss/nginx-pod.yaml
</code></pre></div><!-- Output is similar to this: -->
<p>输出类似于：</p>
<pre><code>pod/nginx created
</code></pre></li>
</ol>
<!-- 
The Pod Security Standards were applied only to the `example`
namespace. You could create the same Pod in the `default` namespace
with no warnings.
-->
<p>以上 Pod 安全标准仅被应用到 <code>example</code> 名字空间。
你可以在没有警告的情况下在 <code>default</code> 名字空间中创建相同的 Pod。</p>
<!-- 
## Clean up

Run `kind delete cluster -name psa-ns-level` to delete the cluster created.
-->
<h2 id="clean-up">清理 </h2>
<p>运行 <code>kind delete cluster -name psa-ns-level</code> 删除创建的集群。</p>
<h2 id="what-s-next">What's next</h2>
<!-- 
- Run a
  [shell script](/examples/security/kind-with-namespace-level-baseline-pod-security.sh)
  to perform all the preceding steps all at once.

  1. Create KinD cluster
  2. Create new namespace
  3. Apply `baseline` Pod Security Standard in `enforce` mode while applying
     `restricted` Pod Security Standard also in `warn` and `audit` mode.
  4. Create a new pod with the following pod security standards applied
- [Pod Security Admission](/docs/concepts/security/pod-security-admission/)
- [Pod Security Standards](/docs/concepts/security/pod-security-standards/)
- [Apply Pod Security Standards at the cluster level](/docs/tutorials/security/cluster-level-pss/)
-->
<ul>
<li>
<p>运行一个 <a href="/examples/security/kind-with-namespace-level-baseline-pod-security.sh">shell 脚本</a>
一次执行所有前面的步骤。</p>
<ol>
<li>创建 KinD 集群</li>
<li>创建新的名字空间</li>
<li>在 <code>enforce</code> 模式下应用 <code>baseline</code> Pod 安全标准，
同时在 <code>warn</code> 和 <code>audit</code> 模式下应用 <code>restricted</code> Pod 安全标准。</li>
<li>创建一个应用以下 Pod 安全标准的新 Pod</li>
</ol>
</li>
<li>
<p><a href="/zh/docs/concepts/security/pod-security-admission/">Pod 安全准入</a></p>
</li>
<li>
<p><a href="/zh/docs/concepts/security/pod-security-standards/">Pod 安全标准</a></p>
</li>
<li>
<p><a href="/zh/docs/tutorials/security/cluster-level-pss/">在集群级别应用 Pod 安全标准</a></p>
</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d5f847bcdb6f7efbfc9c8a180d73e29a">4.3 - 在集群级别应用 Pod 安全标准</h1>
    
	<!-- 
title: Apply Pod Security Standards at the Cluster Level
content_type: tutorial
weight: 10
-->


<div class="alert alert-primary" role="alert">
<h4 class="alert-heading">Note</h4>

    <!-- This tutorial applies only for new clusters. -->
<p>本教程仅适用于新集群。

</div>

<!-- 
Pod Security admission (PSA) is enabled by default in v1.23 and later, as it has
[graduated to beta](/blog/2021/12/09/pod-security-admission-beta/).
Pod Security
is an admission controller that carries out checks against the Kubernetes
[Pod Security Standards](/docs/concepts/security/pod-security-standards/) when new pods are
created. This tutorial shows you how to enforce the `baseline` Pod Security
Standard at the cluster level which applies a standard configuration
to all namespaces in a cluster.

To apply Pod Security Standards to specific namespaces, refer to [Apply Pod Security Standards at the namespace level](/docs/tutorials/security/ns-level-pss).
-->
<p>Pod 安全准入（PSA）在 v1.23 及更高版本默认启用，
因为它<a href="/blog/2021/12/09/pod-security-admission-beta/">升级到测试版（beta）</a>。
Pod 安全准入是在创建 Pod 时应用
<a href="/zh/docs/concepts/security/pod-security-standards/">Pod 安全标准</a>的准入控制器。
本教程将向你展示如何在集群级别实施 <code>baseline</code> Pod 安全标准，
该标准将标准配置应用于集群中的所有名称空间。</p>
<p>要将 Pod 安全标准应用于特定名字空间，
请参阅<a href="/zh/docs/tutorials/security/ns-level-pss">在名字空间级别应用 Pod 安全标准</a>。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!-- 
Install the following on your workstation:

- [KinD](https://kind.sigs.k8s.io/docs/user/quick-start/#installation)
- [kubectl](https://kubernetes.io/docs/tasks/tools/)
-->
<p>在你的工作站中安装以下内容：</p>
<ul>
<li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">KinD</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a></li>
</ul>
<!--
## Choose the right Pod Security Standard to apply

[Pod Security Admission](/docs/concepts/security/pod-security-admission/)
lets you apply built-in [Pod Security Standards](/docs/concepts/security/pod-security-standards/)
with the following modes: `enforce`, `audit`, and `warn`.

To gather information that helps you to choose the Pod Security Standards
that are most appropriate for your configuration, do the following: 
-->
<h2 id="choose-the-right-pod-security-standard-to-apply">正确选择要应用的 Pod 安全标准 </h2>
<p><a href="/zh/docs/concepts/security/pod-security-admission/">Pod 安全准入</a>
允许你使用以下模式应用内置的
<a href="/zh/docs/concepts/security/pod-security-standards/">Pod 安全标准</a>:
<code>enforce</code>、<code>audit</code> 和 <code>warn</code>。</p>
<p>要收集信息以便选择最适合你的配置的 Pod 安全标准，请执行以下操作：</p>
<!-- 
1. Create a cluster with no Pod Security Standards applied:
 -->
<ol>
<li>
<p>创建一个没有应用 Pod 安全标准的集群：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kind create cluster --name psa-wo-cluster-pss --image kindest/node:v1.23.0
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Creating cluster &quot;psa-wo-cluster-pss&quot; ...
✓ Ensuring node image (kindest/node:v1.23.0) 🖼
✓ Preparing nodes 📦  
✓ Writing configuration 📜
✓ Starting control-plane 🕹️
✓ Installing CNI 🔌
✓ Installing StorageClass 💾
Set kubectl context to &quot;kind-psa-wo-cluster-pss&quot;
You can now use your cluster with:

kubectl cluster-info --context kind-psa-wo-cluster-pss

Thanks for using kind! 😊

</code></pre></li>
</ol>
<!-- 
1. Set the kubectl context to the new cluster:
-->
<ol start="2">
<li>
<p>将 kubectl 上下文设置为新集群：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info --context kind-psa-wo-cluster-pss
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Kubernetes control plane is running at https://127.0.0.1:61350 
CoreDNS is running at https://127.0.0.1:61350/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</code></pre></li>
</ol>
<!-- 
1. Get a list of namespaces in the cluster:
-->
<ol start="3">
<li>
<p>获取集群中的名字空间列表：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get ns
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>NAME                 STATUS   AGE
default              Active   9m30s
kube-node-lease      Active   9m32s
kube-public          Active   9m32s
kube-system          Active   9m32s
local-path-storage   Active   9m26s
</code></pre></li>
</ol>
<!-- 
1. Use `--dry-run=server` to understand what happens when different Pod Security Standards
   are applied:
 -->
<ol start="4">
<li>
<p>使用 <code>--dry-run=server</code> 来了解应用不同的 Pod 安全标准时会发生什么：</p>
<ol>
<li>
<p>Privileged</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl label --dry-run<span style="color:#666">=</span>server --overwrite ns --all <span style="color:#b62;font-weight:bold">\ </span>                   
pod-security.kubernetes.io/enforce<span style="color:#666">=</span>privileged
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>namespace/default labeled
namespace/kube-node-lease labeled
namespace/kube-public labeled
namespace/kube-system labeled
namespace/local-path-storage labeled
</code></pre></li>
<li>
<p>Baseline</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl label --dry-run<span style="color:#666">=</span>server --overwrite ns --all <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>pod-security.kubernetes.io/enforce<span style="color:#666">=</span>baseline
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>namespace/default labeled
namespace/kube-node-lease labeled
namespace/kube-public labeled
Warning: existing pods in namespace &quot;kube-system&quot; violate the new PodSecurity enforce level &quot;baseline:latest&quot;
Warning: etcd-psa-wo-cluster-pss-control-plane (and 3 other pods): host namespaces, hostPath volumes
Warning: kindnet-vzj42: non-default capabilities, host namespaces, hostPath volumes
Warning: kube-proxy-m6hwf: host namespaces, hostPath volumes, privileged
namespace/kube-system labeled
namespace/local-path-storage labeled
</code></pre></li>
<li>
<p>Restricted</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl label --dry-run<span style="color:#666">=</span>server --overwrite ns --all <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>pod-security.kubernetes.io/enforce<span style="color:#666">=</span>restricted
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>namespace/default labeled
namespace/kube-node-lease labeled
namespace/kube-public labeled
Warning: existing pods in namespace &quot;kube-system&quot; violate the new PodSecurity enforce level &quot;restricted:latest&quot;
Warning: coredns-7bb9c7b568-hsptc (and 1 other pod): unrestricted capabilities, runAsNonRoot != true, seccompProfile
Warning: etcd-psa-wo-cluster-pss-control-plane (and 3 other pods): host namespaces, hostPath volumes, allowPrivilegeEscalation != false, unrestricted capabilities, restricted volume types, runAsNonRoot != true
Warning: kindnet-vzj42: non-default capabilities, host namespaces, hostPath volumes, allowPrivilegeEscalation != false, unrestricted capabilities, restricted volume types, runAsNonRoot != true, seccompProfile
Warning: kube-proxy-m6hwf: host namespaces, hostPath volumes, privileged, allowPrivilegeEscalation != false, unrestricted capabilities, restricted volume types, runAsNonRoot != true, seccompProfile
namespace/kube-system labeled
Warning: existing pods in namespace &quot;local-path-storage&quot; violate the new PodSecurity enforce level &quot;restricted:latest&quot;
Warning: local-path-provisioner-d6d9f7ffc-lw9lh: allowPrivilegeEscalation != false, unrestricted capabilities, runAsNonRoot != true, seccompProfile
namespace/local-path-storage labeled
</code></pre></li>
</ol>
</li>
</ol>
<!-- 
From the previous output, you'll notice that applying the `privileged` Pod Security Standard shows no warnings
for any namespaces. However, `baseline` and `restricted` standards both have
warnings, specifically in the `kube-system` namespace.
-->
<p>从前面的输出中，你会注意到应用 <code>privileged</code> Pod 安全标准不会显示任何名字空间的警告。
然而，<code>baseline</code> 和 <code>restricted</code> 标准都有警告，特别是在 <code>kube-system</code> 名字空间中。</p>
<!-- 
## Set modes, versions and standards

In this section, you apply the following Pod Security Standards to the `latest` version:

* `baseline` standard in `enforce` mode.
* `restricted` standard in `warn` and `audit` mode.
-->
<h2 id="set-modes-versions-and-standards">设置模式、版本和标准 </h2>
<p>在本节中，你将以下 Pod 安全标准应用于最新（<code>latest</code>）版本：</p>
<ul>
<li>在 <code>enforce</code> 模式下的 <code>baseline</code> 标准。</li>
<li><code>warn</code> 和 <code>audit</code> 模式下的 <code>restricted</code> 标准。</li>
</ul>
<!-- 
The `baseline` Pod Security Standard provides a convenient
middle ground that allows keeping the exemption list short and prevents known
privilege escalations.

Additionally, to prevent pods from failing in `kube-system`, you'll exempt the namespace
from having Pod Security Standards applied.

When you implement Pod Security Admission in your own environment, consider the
following:
-->
<p><code>baseline</code> Pod 安全标准提供了一个方便的中间立场，能够保持豁免列表简短并防止已知的特权升级。</p>
<p>此外，为了防止 <code>kube-system</code> 中的 Pod 失败，你将免除该名字空间应用 Pod 安全标准。</p>
<p>在你自己的环境中实施 Pod 安全准入时，请考虑以下事项：</p>
<!-- 
1. Based on the risk posture applied to a cluster, a stricter Pod Security
   Standard like `restricted` might be a better choice.
1. Exempting the `kube-system` namespace allows pods to run as
   `privileged` in this namespace. For real world use, the Kubernetes project
   strongly recommends that you apply strict RBAC
   policies that limit access to `kube-system`, following the principle of least
   privilege.
   To implement the preceding standards, do the following:
1. Create a configuration file that can be consumed by the Pod Security
   Admission Controller to implement these Pod Security Standards:
-->
<ol>
<li>
<p>根据应用于集群的风险状况，更严格的 Pod 安全标准（如 <code>restricted</code>）可能是更好的选择。</p>
</li>
<li>
<p>对 <code>kube-system</code> 名字空间进行赦免会允许 Pod 在其中以 <code>privileged</code> 模式运行。
对于实际使用，Kubernetes 项目强烈建议你应用严格的 RBAC 策略来限制对 <code>kube-system</code> 的访问，
遵循最小特权原则。</p>
</li>
<li>
<p>创建一个配置文件，Pod 安全准入控制器可以使用该文件来实现这些 Pod 安全标准：</p>
<pre><code>mkdir -p /tmp/pss
cat &lt;&lt;EOF &gt; /tmp/pss/cluster-level-pss.yaml 
apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
- name: PodSecurity
  configuration:
    apiVersion: pod-security.admission.config.k8s.io/v1beta1
    kind: PodSecurityConfiguration
    defaults:
      enforce: &quot;baseline&quot;
      enforce-version: &quot;latest&quot;
      audit: &quot;restricted&quot;
      audit-version: &quot;latest&quot;
      warn: &quot;restricted&quot;
      warn-version: &quot;latest&quot;
    exemptions:
      usernames: []
      runtimeClasses: []
      namespaces: [kube-system]
EOF
</code></pre></li>
</ol>
<!-- 
1. Configure the API server to consume this file during cluster creation:
-->
<ol start="4">
<li>
<p>在创建集群时配置 API 服务器使用此文件：</p>
<pre><code>cat &lt;&lt;EOF &gt; /tmp/pss/cluster-config.yaml 
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
        extraArgs:
          admission-control-config-file: /etc/config/cluster-level-pss.yaml
        extraVolumes:
          - name: accf
            hostPath: /etc/config
            mountPath: /etc/config
            readOnly: false
            pathType: &quot;DirectoryOrCreate&quot;
  extraMounts:
  - hostPath: /tmp/pss
    containerPath: /etc/config
    # optional: if set, the mount is read-only.
    # default false
    readOnly: false
    # optional: if set, the mount needs SELinux relabeling.
    # default false
    selinuxRelabel: false
    # optional: set propagation mode (None, HostToContainer or Bidirectional)
    # see https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation
    # default None
    propagation: None
EOF
</code></pre><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
   If you use Docker Desktop with KinD on macOS, you can
   add `/tmp` as a Shared Directory under the menu item
   **Preferences > Resources > File Sharing**.
   -->
<p>如果你在 macOS 上使用 Docker Desktop 和 KinD，
你可以在菜单项 <strong>Preferences &gt; Resources &gt; File Sharing</strong>
下添加 <code>/tmp</code> 作为共享目录。
</div>
</li>
</ol>
<!-- 
1. Create a cluster that uses Pod Security Admission to apply
   these Pod Security Standards:
-->
<ol start="5">
<li>
<p>创建一个使用 Pod 安全准入的集群来应用这些 Pod 安全标准：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kind create cluster --name psa-with-cluster-pss --image kindest/node:v1.23.0 --config /tmp/pss/cluster-config.yaml
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Creating cluster &quot;psa-with-cluster-pss&quot; ...
 ✓ Ensuring node image (kindest/node:v1.23.0) 🖼 
 ✓ Preparing nodes 📦  
 ✓ Writing configuration 📜 
 ✓ Starting control-plane 🕹️ 
 ✓ Installing CNI 🔌 
 ✓ Installing StorageClass 💾 
Set kubectl context to &quot;kind-psa-with-cluster-pss&quot;
You can now use your cluster with:

kubectl cluster-info --context kind-psa-with-cluster-pss

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂
</code></pre></li>
</ol>
<!-- 
1. Point kubectl to the cluster
-->
<ol start="6">
<li>
<p>将 kubectl 指向集群</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info --context kind-psa-with-cluster-pss
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Kubernetes control plane is running at https://127.0.0.1:63855
CoreDNS is running at https://127.0.0.1:63855/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</code></pre></li>
</ol>
<!-- 
1. Create the following Pod specification for a minimal configuration in the default namespace:
-->
<ol start="7">
<li>
<p>创建以下 Pod 规约作为在 default 名字空间中的一个最小配置：</p>
<pre><code>cat &lt;&lt;EOF &gt; /tmp/pss/nginx-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
    - image: nginx
      name: nginx
      ports:
        - containerPort: 80
EOF
</code></pre></li>
</ol>
<!-- 
1. Create the Pod in the cluster:
-->
<ol start="8">
<li>
<p>在集群中创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f /tmp/pss/nginx-pod.yaml
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Warning: would violate PodSecurity &quot;restricted:latest&quot;: allowPrivilegeEscalation != false (container &quot;nginx&quot; must set securityContext allowPrivilegeEscalation=false), unrestricted capabilities (container &quot;nginx&quot; must set securityContext.capabilities.drop=[&quot;ALL&quot;]), runAsNonRoot != true (pod or container &quot;nginx&quot; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container &quot;nginx&quot; must set securityContext seccompProfile.type to &quot;RuntimeDefault&quot; or &quot;Localhost&quot;)
pod/nginx created
</code></pre></li>
</ol>
<!-- 
## Clean up

Run `kind delete cluster --name psa-with-cluster-pss` and
`kind delete cluster --name psa-wo-cluster-pss` to delete the clusters you
created.
-->
<h2 id="clean-up">清理 </h2>
<p>运行 <code>kind delete cluster --name psa-with-cluster-pss</code> 和
<code>kind delete cluster --name psa-wo-cluster-pss</code> 来删除你创建的集群。</p>
<h2 id="what-s-next">What's next</h2>
<!-- 
- Run a
  [shell script](/examples/security/kind-with-cluster-level-baseline-pod-security.sh)
  to perform all the preceding steps at once:
  1. Create a Pod Security Standards based cluster level Configuration
  2. Create a file to let API server consumes this configuration
  3. Create a cluster that creates an API server with this configuration
  4. Set kubectl context to this new cluster
  5. Create a minimal pod yaml file
  6. Apply this file to create a Pod in the new cluster
- [Pod Security Admission](/docs/concepts/security/pod-security-admission/)
- [Pod Security Standards](/docs/concepts/security/pod-security-standards/)
- [Apply Pod Security Standards at the namespace level](/docs/tutorials/security/ns-level-pss/)
-->
<ul>
<li>运行一个 <a href="/zh/examples/security/kind-with-cluster-level-baseline-pod-security.sh">shell 脚本</a>
一次执行前面的所有步骤：
<ol>
<li>创建一个基于 Pod 安全标准的集群级别配置</li>
<li>创建一个文件让 API 服务器消费这个配置</li>
<li>创建一个集群，用这个配置创建一个 API 服务器</li>
<li>设置 kubectl 上下文为这个新集群</li>
<li>创建一个最小的 Pod yaml 文件</li>
<li>应用这个文件，在新集群中创建一个 Pod</li>
</ol>
</li>
<li><a href="/zh/docs/concepts/security/pod-security-admission/">Pod 安全准入</a></li>
<li><a href="/zh/docs/concepts/security/pod-security-standards/">Pod 安全标准</a></li>
<li><a href="/zh/docs/tutorials/security/ns-level-pss/">在名字空间级别应用 Pod 安全标准</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8b105172a11322c70d0223bc9dff1904">4.4 - 使用 seccomp 限制容器的系统调用</h1>
    
	<!-- 
reviewers:
- hasheddan
- pjbgf
- saschagrunert
title: Restrict a Container's Syscalls with seccomp
content_type: tutorial
weight: 20
min-kubernetes-server-version: v1.22
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code>
</div>


<!-- 
Seccomp stands for secure computing mode and has been a feature of the Linux
kernel since version 2.6.12. It can be used to sandbox the privileges of a
process, restricting the calls it is able to make from userspace into the
kernel. Kubernetes lets you automatically apply seccomp profiles loaded onto a
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a> to your Pods and containers.

Identifying the privileges required for your workloads can be difficult. In this
tutorial, you will go through how to load seccomp profiles into a local
Kubernetes cluster, how to apply them to a Pod, and how you can begin to craft
profiles that give only the necessary privileges to your container processes.
-->
<p>Seccomp 代表安全计算（Secure Computing）模式，自 2.6.12 版本以来，一直是 Linux 内核的一个特性。
它可以用来沙箱化进程的权限，限制进程从用户态到内核态的调用。
Kubernetes 能使你自动将加载到 <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>上的
seccomp 配置文件应用到你的 Pod 和容器。</p>
<p>识别你的工作负载所需要的权限是很困难的。在本篇教程中，
你将了解如何将 seccomp 配置文件加载到本地的 Kubernetes 集群中，
如何将它们应用到 Pod，以及如何开始制作只为容器进程提供必要的权限的配置文件。</p>
<h2 id="objectives">Objectives</h2>
<!-- 
* Learn how to load seccomp profiles on a node
* Learn how to apply a seccomp profile to a container
* Observe auditing of syscalls made by a container process
* Observe behavior when a missing profile is specified
* Observe a violation of a seccomp profile
* Learn how to create fine-grained seccomp profiles
* Learn how to apply a container runtime default seccomp profile
-->
<ul>
<li>了解如何在节点上加载 seccomp 配置文件</li>
<li>了解如何将 seccomp 配置文件应用到容器上</li>
<li>观察容器进程对系统调用的审计</li>
<li>观察指定的配置文件缺失时的行为</li>
<li>观察违反 seccomp 配置文件的行为</li>
<li>了解如何创建细粒度的 seccomp 配置文件</li>
<li>了解如何应用容器运行时所默认的 seccomp 配置文件</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<!-- 
In order to complete all steps in this tutorial, you must install
[kind](/docs/tasks/tools/#kind) and [kubectl](/docs/tasks/tools/#kubectl).

This tutorial shows some examples that are still alpha (since v1.22) and
others that use only generally available seccomp functionality. You should
make sure that your cluster is
[configured correctly](https://kind.sigs.k8s.io/docs/user/quick-start/#setting-kubernetes-version)
for the version you are using.

The tutorial also uses the `curl` tool for downloading examples to your computer.
You can adapt the steps to use a different tool if you prefer.
-->
<p>为了完成本篇教程中的所有步骤，你必须安装 <a href="/zh/docs/tasks/tools/#kind">kind</a>
和 <a href="/zh/docs/tasks/tools/#kubectl">kubectl</a>。</p>
<p>本篇教程演示的某些示例仍然是 alpha 状态（自 v1.22 起），另一些示例则仅使用 seccomp 正式发布的功能。
你应该确保，针对你使用的版本，
<a href="https://kind.sigs.k8s.io/docs/user/quick-start/#setting-kubernetes-version">正确配置</a>了集群。</p>
<p>本篇教程也使用了 <code>curl</code> 工具来下载示例到你的计算机上。
你可以使用其他自己偏好的工具来自适应这些步骤。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
It is not possible to apply a seccomp profile to a container running with
`privileged: true` set in the container's `securityContext`. Privileged containers always
run as `Unconfined`.
-->
<p>无法将 seccomp 配置文件应用于在容器的 <code>securityContext</code> 中设置了 <code>privileged: true</code> 的容器。
特权容器始终以 <code>Unconfined</code> 的方式运行。
</div>
<!-- steps -->
<!-- 
## Download example seccomp profiles {#download-profiles}

The contents of these profiles will be explored later on, but for now go ahead
and download them into a directory named `profiles/` so that they can be loaded
into the cluster.
-->
<h2 id="download-profiles">下载示例 seccomp 配置文件 </h2>
<p>这些配置文件的内容将在稍后进行分析，
现在先将它们下载到名为 <code>profiles/</code> 的目录中，以便将它们加载到集群中。</p>
<ul class="nav nav-tabs" id="tab-with-code" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tab-with-code-0" role="tab" aria-controls="tab-with-code-0" aria-selected="true">audit.json</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-with-code-1" role="tab" aria-controls="tab-with-code-1">violation.json</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-with-code-2" role="tab" aria-controls="tab-with-code-2">fine-grained.json</a></li></ul>
<div class="tab-content" id="tab-with-code"><div id="tab-with-code-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tab-with-code-0">

<p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/seccomp/profiles/audit.json" download="pods/security/seccomp/profiles/audit.json"><code>pods/security/seccomp/profiles/audit.json</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-seccomp-profiles-audit-json')" title="Copy pods/security/seccomp/profiles/audit.json to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-seccomp-profiles-audit-json">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#008000;font-weight:bold">&#34;defaultAction&#34;</span>: <span style="color:#b44">&#34;SCMP_ACT_LOG&#34;</span>
}</code></pre></div>
    </div>
</div>


</div>
  <div id="tab-with-code-1" class="tab-pane" role="tabpanel" aria-labelledby="tab-with-code-1">

<p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/seccomp/profiles/violation.json" download="pods/security/seccomp/profiles/violation.json"><code>pods/security/seccomp/profiles/violation.json</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-seccomp-profiles-violation-json')" title="Copy pods/security/seccomp/profiles/violation.json to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-seccomp-profiles-violation-json">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#008000;font-weight:bold">&#34;defaultAction&#34;</span>: <span style="color:#b44">&#34;SCMP_ACT_ERRNO&#34;</span>
}</code></pre></div>
    </div>
</div>


</div>
  <div id="tab-with-code-2" class="tab-pane" role="tabpanel" aria-labelledby="tab-with-code-2">

<p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/seccomp/profiles/fine-grained.json" download="pods/security/seccomp/profiles/fine-grained.json"><code>pods/security/seccomp/profiles/fine-grained.json</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-seccomp-profiles-fine-grained-json')" title="Copy pods/security/seccomp/profiles/fine-grained.json to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-seccomp-profiles-fine-grained-json">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#008000;font-weight:bold">&#34;defaultAction&#34;</span>: <span style="color:#b44">&#34;SCMP_ACT_ERRNO&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;architectures&#34;</span>: [
        <span style="color:#b44">&#34;SCMP_ARCH_X86_64&#34;</span>,
        <span style="color:#b44">&#34;SCMP_ARCH_X86&#34;</span>,
        <span style="color:#b44">&#34;SCMP_ARCH_X32&#34;</span>
    ],
    <span style="color:#008000;font-weight:bold">&#34;syscalls&#34;</span>: [
        {
            <span style="color:#008000;font-weight:bold">&#34;names&#34;</span>: [
                <span style="color:#b44">&#34;accept4&#34;</span>,
                <span style="color:#b44">&#34;epoll_wait&#34;</span>,
                <span style="color:#b44">&#34;pselect6&#34;</span>,
                <span style="color:#b44">&#34;futex&#34;</span>,
                <span style="color:#b44">&#34;madvise&#34;</span>,
                <span style="color:#b44">&#34;epoll_ctl&#34;</span>,
                <span style="color:#b44">&#34;getsockname&#34;</span>,
                <span style="color:#b44">&#34;setsockopt&#34;</span>,
                <span style="color:#b44">&#34;vfork&#34;</span>,
                <span style="color:#b44">&#34;mmap&#34;</span>,
                <span style="color:#b44">&#34;read&#34;</span>,
                <span style="color:#b44">&#34;write&#34;</span>,
                <span style="color:#b44">&#34;close&#34;</span>,
                <span style="color:#b44">&#34;arch_prctl&#34;</span>,
                <span style="color:#b44">&#34;sched_getaffinity&#34;</span>,
                <span style="color:#b44">&#34;munmap&#34;</span>,
                <span style="color:#b44">&#34;brk&#34;</span>,
                <span style="color:#b44">&#34;rt_sigaction&#34;</span>,
                <span style="color:#b44">&#34;rt_sigprocmask&#34;</span>,
                <span style="color:#b44">&#34;sigaltstack&#34;</span>,
                <span style="color:#b44">&#34;gettid&#34;</span>,
                <span style="color:#b44">&#34;clone&#34;</span>,
                <span style="color:#b44">&#34;bind&#34;</span>,
                <span style="color:#b44">&#34;socket&#34;</span>,
                <span style="color:#b44">&#34;openat&#34;</span>,
                <span style="color:#b44">&#34;readlinkat&#34;</span>,
                <span style="color:#b44">&#34;exit_group&#34;</span>,
                <span style="color:#b44">&#34;epoll_create1&#34;</span>,
                <span style="color:#b44">&#34;listen&#34;</span>,
                <span style="color:#b44">&#34;rt_sigreturn&#34;</span>,
                <span style="color:#b44">&#34;sched_yield&#34;</span>,
                <span style="color:#b44">&#34;clock_gettime&#34;</span>,
                <span style="color:#b44">&#34;connect&#34;</span>,
                <span style="color:#b44">&#34;dup2&#34;</span>,
                <span style="color:#b44">&#34;epoll_pwait&#34;</span>,
                <span style="color:#b44">&#34;execve&#34;</span>,
                <span style="color:#b44">&#34;exit&#34;</span>,
                <span style="color:#b44">&#34;fcntl&#34;</span>,
                <span style="color:#b44">&#34;getpid&#34;</span>,
                <span style="color:#b44">&#34;getuid&#34;</span>,
                <span style="color:#b44">&#34;ioctl&#34;</span>,
                <span style="color:#b44">&#34;mprotect&#34;</span>,
                <span style="color:#b44">&#34;nanosleep&#34;</span>,
                <span style="color:#b44">&#34;open&#34;</span>,
                <span style="color:#b44">&#34;poll&#34;</span>,
                <span style="color:#b44">&#34;recvfrom&#34;</span>,
                <span style="color:#b44">&#34;sendto&#34;</span>,
                <span style="color:#b44">&#34;set_tid_address&#34;</span>,
                <span style="color:#b44">&#34;setitimer&#34;</span>,
                <span style="color:#b44">&#34;writev&#34;</span>
            ],
            <span style="color:#008000;font-weight:bold">&#34;action&#34;</span>: <span style="color:#b44">&#34;SCMP_ACT_ALLOW&#34;</span>
        }
    ]
}</code></pre></div>
    </div>
</div>


</div></div>

<!-- Run these commands: -->
<p>执行这些命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">mkdir ./profiles
curl -L -o profiles/audit.json https://k8s.io/examples/pods/security/seccomp/profiles/audit.json
curl -L -o profiles/violation.json https://k8s.io/examples/pods/security/seccomp/profiles/violation.json
curl -L -o profiles/fine-grained.json https://k8s.io/examples/pods/security/seccomp/profiles/fine-grained.json
ls profiles
</code></pre></div><!-- You should see three profiles listed at the end of the final step: -->
<p>你应该看到在最后一步的末尾列出有三个配置文件：</p>
<pre><code>audit.json  fine-grained.json  violation.json
</code></pre><!-- 
## Create a local Kubernetes cluster with kind

For simplicity, [kind](https://kind.sigs.k8s.io/) can be used to create a single
node cluster with the seccomp profiles loaded. Kind runs Kubernetes in Docker,
so each node of the cluster is a container. This allows for files
to be mounted in the filesystem of each container similar to loading files
onto a node.
-->
<h2 id="create-a-local-kubernetes-cluster-with-kind">使用 kind 创建本地 Kubernetes 集群</h2>
<p>为简单起见，<a href="https://kind.sigs.k8s.io/">kind</a> 可用来创建加载了 seccomp 配置文件的单节点集群。
Kind 在 Docker 中运行 Kubernetes，因此集群的每个节点都是一个容器。
这允许将文件挂载到每个容器的文件系统中，类似于将文件加载到节点上。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/seccomp/kind.yaml" download="pods/security/seccomp/kind.yaml"><code>pods/security/seccomp/kind.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-seccomp-kind-yaml')" title="Copy pods/security/seccomp/kind.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-seccomp-kind-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kind.x-k8s.io/v1alpha4<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Cluster<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">nodes</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>control-plane<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">extraMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;./profiles&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">containerPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/var/lib/kubelet/seccomp/profiles&#34;</span></code></pre></div>
    </div>
</div>


<!-- 
Download that example kind configuration, and save it to a file named `kind.yaml`:
-->
<p>下载该示例 kind 配置，并将其保存到名为 <code>kind.yaml</code> 的文件中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -L -O https://k8s.io/examples/pods/security/seccomp/kind.yaml
</code></pre></div><!-- 
You can set a specific Kubernetes version by setting the node's container image.
See [Nodes](https://kind.sigs.k8s.io/docs/user/configuration/#nodes) within the
kind documentation about configuration for more details on this.
This tutorial assumes you are using Kubernetes v1.23.
-->
<p>你可以通过设置节点的容器镜像来设置特定的 Kubernetes 版本。
有关此类配置的更多信息，
参阅 kind 文档中<a href="https://kind.sigs.k8s.io/docs/user/configuration/#nodes">节点</a>小节。
本篇教程假定你正在使用 Kubernetes v1.23。</p>
<!-- 
As an alpha feature, you can configure Kubernetes to use the profile that the
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>
prefers by default, rather than falling back to `Unconfined`.
If you want to try that, see
[enable the use of `RuntimeDefault` as the default seccomp profile for all workloads](#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads)
before you continue.
-->
<p>作为 alpha 特性，你可以将 Kubernetes 配置为使用
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>
默认首选的配置文件，而不是回退到 <code>Unconfined</code>。
如果你想尝试，请在继续之前参阅
<a href="#enable-runtimedefault-as-default">启用使用 <code>RuntimeDefault</code> 作为所有工作负载的默认 seccomp 配置文件</a></p>
<!--
Once you have a kind configuration in place, create the kind cluster with
that configuration: 
-->
<p>有了 kind 配置后，使用该配置创建 kind 集群：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kind create cluster --config<span style="color:#666">=</span>kind.yaml
</code></pre></div><!--
After the new Kubernetes cluster is ready, identify the Docker container running
as the single node cluster:
-->
<p>新的 Kubernetes 集群准备就绪后，找出作为单节点集群运行的 Docker 容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker ps
</code></pre></div><!--
You should see output indicating that a container is running with name
`kind-control-plane`. The output is similar to:
-->
<p>你应该看到输出中名为 <code>kind-control-plane</code> 的容器正在运行。
输出类似于：</p>
<pre><code>CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                       NAMES
6a96207fed4b        kindest/node:v1.18.2   &quot;/usr/local/bin/entr…&quot;   27 seconds ago      Up 24 seconds       127.0.0.1:42223-&gt;6443/tcp   kind-control-plane
</code></pre><!--
If observing the filesystem of that container, you should see that the
`profiles/` directory has been successfully loaded into the default seccomp path
of the kubelet. Use `docker exec` to run a command in the Pod:
-->
<p>如果观察该容器的文件系统，
你应该会看到 <code>profiles/</code> 目录已成功加载到 kubelet 的默认 seccomp 路径中。
使用 <code>docker exec</code> 在 Pod 中运行命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 6a96207fed4b 更改为你从 “docker ps” 看到的容器 ID</span>
docker <span style="color:#a2f">exec</span> -it 6a96207fed4b ls /var/lib/kubelet/seccomp/profiles
</code></pre></div><pre><code>audit.json  fine-grained.json  violation.json
</code></pre><!-- 
You have verified that these seccomp profiles are available to the kubelet
running within kind.
-->
<p>你已验证这些 seccomp 配置文件可用于在 kind 中运行的 kubelet。</p>
<!-- 
## Enable the use of `RuntimeDefault` as the default seccomp profile for all workloads
-->
<h2 id="enable-runtimedefault-as-default">启用使用 <code>RuntimeDefault</code> 作为所有工作负载的默认 seccomp 配置文件</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>


<!-- 
`SeccompDefault` is an optional kubelet
[feature gate](/docs/reference/command-line-tools-reference/feature-gates) as
well as corresponding `--seccomp-default`
[command line flag](/docs/reference/command-line-tools-reference/kubelet).
Both have to be enabled simultaneously to use the feature.
-->
<p><code>SeccompDefault</code> 是一个可选的 kubelet <a href="/zh/docs/reference/command-line-tools-reference/feature-gates">特性门控</a>
以及相应的 <code>--seccomp-default</code> <a href="/zh/docs/reference/command-line-tools-reference/kubelet">命令行标志</a>。
两者必须同时启用才能使用该功能。</p>
<!-- 
If enabled, the kubelet will use the `RuntimeDefault` seccomp profile by default, which is
defined by the container runtime, instead of using the `Unconfined` (seccomp disabled) mode.
The default profiles aim to provide a strong set
of security defaults while preserving the functionality of the workload. It is
possible that the default profiles differ between container runtimes and their
release versions, for example when comparing those from CRI-O and containerd.
-->
<p>如果启用，kubelet 将会默认使用 <code>RuntimeDefault</code> seccomp 配置文件，
（这一配置文明是由容器运行时定义的），而不是使用 <code>Unconfined</code>（禁用 seccomp）模式。
默认的配置文件旨在提供一组限制性较强且能保留工作负载功能的安全默认值。
不同容器运行时及其不同发布版本之间的默认配置文件可能有所不同，
例如在比较来自 CRI-O 和 containerd 的配置文件时。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
Enabling the feature will neither change the Kubernetes
`securityContext.seccompProfile` API field nor add the deprecated annotations of
the workload. This provides users the possibility to rollback anytime without
actually changing the workload configuration. Tools like
[`crictl inspect`](https://github.com/kubernetes-sigs/cri-tools) can be used to
verify which seccomp profile is being used by a container.
-->
<p>启用该功能既不会更改 Kubernetes <code>securityContext.seccompProfile</code> API 字段，
也不会添加已弃用的工作负载注解。
这为用户提供了随时回滚的可能性，而且无需实际更改工作负载配置。
<a href="https://github.com/kubernetes-sigs/cri-tools"><code>crictl inspect</code></a>
之类的工具可用于验证容器正在使用哪个 seccomp 配置文件。
</div>
<!-- 
Some workloads may require a lower amount of syscall restrictions than others.
This means that they can fail during runtime even with the `RuntimeDefault`
profile. To mitigate such a failure, you can:

- Run the workload explicitly as `Unconfined`.
- Disable the `SeccompDefault` feature for the nodes. Also making sure that
  workloads get scheduled on nodes where the feature is disabled.
- Create a custom seccomp profile for the workload.
-->
<p>与其他工作负载相比，某些工作负载可能需要更少的系统调用限制。
这意味着即使使用 <code>RuntimeDefault</code> 配置文件，它们也可能在运行时失败。
要应对此类故障，你可以：</p>
<ul>
<li>将工作负载显式运行为 <code>Unconfined</code>。</li>
<li>禁用节点的 <code>SeccompDefault</code> 功能。还要确保工作负载被调度到禁用该功能的节点上。</li>
<li>为工作负载创建自定义 seccomp 配置文件。</li>
</ul>
<!-- 
If you were introducing this feature into production-like cluster, the Kubernetes project
recommends that you enable this feature gate on a subset of your nodes and then
test workload execution before rolling the change out cluster-wide.

More detailed information about a possible upgrade and downgrade strategy can be
found in the [related Kubernetes Enhancement Proposal (KEP)](https://github.com/kubernetes/enhancements/tree/a70cc18/keps/sig-node/2413-seccomp-by-default#upgrade--downgrade-strategy).
-->
<p>如果你将此功能引入到类似生产的集群中，
Kubernetes 项目建议你在部分节点上启用此特性门控，
然后在整个集群范围内推出更改之前，测试工作负载执行情况。</p>
<p>有关可能的升级和降级策略的更多详细信息，
请参阅<a href="https://github.com/kubernetes/enhancements/tree/a70cc18/keps/sig-node/2413-seccomp-by-default#upgrade--downgrade-strategy">相关的 Kubernetes 增强提案 (KEP)</a>。</p>
<!-- 
Since the feature is in alpha state it is disabled per default. To enable it,
pass the flags `--feature-gates=SeccompDefault=true --seccomp-default` to the
`kubelet` CLI or enable it via the [kubelet configuration
file](/docs/tasks/administer-cluster/kubelet-config-file/). To enable the
feature gate in [kind](https://kind.sigs.k8s.io), ensure that `kind` provides
the minimum required Kubernetes version and enables the `SeccompDefault` feature
[in the kind configuration](https://kind.sigs.k8s.io/docs/user/quick-start/#enable-feature-gates-in-your-cluster):
-->
<p>由于此特性处于 alpha 阶段，默认是被禁用的。
要启用它，传递标志 <code>--feature-gates=SeccompDefault=true --seccomp-default</code> 到
kubelet CLI 或者通过 <a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet 配置文件</a>启用。
要在 <a href="https://kind.sigs.k8s.io">kind</a> 启用特性门控，
请确保 <code>kind</code> 提供所需的最低 Kubernetes 版本，
并<a href="https://kind.sigs.k8s.io/docs/user/quick-start/#enable-feature-gates-in-your-cluster">在 kind 配置中</a>
启用了 <code>SeccompDefault</code> 特性：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Cluster<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kind.x-k8s.io/v1alpha4<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">featureGates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">SeccompDefault</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">nodes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>control-plane<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>kindest/node:v1.23.0@sha256:49824ab1727c04e56a21a5d8372a402fcd32ea51ac96a2706a12af38934f81ac<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubeadmConfigPatches</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- |<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">        kind: JoinConfiguration
</span><span style="color:#b44;font-style:italic">        nodeRegistration:
</span><span style="color:#b44;font-style:italic">          kubeletExtraArgs:
</span><span style="color:#b44;font-style:italic">            seccomp-default: &#34;true&#34;</span><span style="color:#bbb">        
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>worker<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>kindest/node:v1.23.0@sha256:49824ab1727c04e56a21a5d8372a402fcd32ea51ac96a2706a12af38934f81ac<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubeadmConfigPatches</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- |<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">        kind: JoinConfiguration
</span><span style="color:#b44;font-style:italic">        nodeRegistration:
</span><span style="color:#b44;font-style:italic">          kubeletExtraArgs:
</span><span style="color:#b44;font-style:italic">            feature-gates: SeccompDefault=true
</span><span style="color:#b44;font-style:italic">            seccomp-default: &#34;true&#34;</span><span style="color:#bbb">        
</span></code></pre></div><!-- If the cluster is ready, then running a pod: -->
<p>如果集群已就绪，则运行一个 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run --rm -it --restart<span style="color:#666">=</span>Never --image<span style="color:#666">=</span>alpine alpine -- sh
</code></pre></div><!-- 
Should now have the default seccomp profile attached. This can be verified by
using `docker exec` to run `crictl inspect` for the container on the kind
worker:
-->
<p>现在应该附加了默认的 seccomp 配置文件。
这可以通过使用 <code>docker exec</code> 为 kind 上的容器运行 <code>crictl inspect</code> 来验证：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker <span style="color:#a2f">exec</span> -it kind-worker bash -c <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    <span style="color:#b44">&#39;crictl inspect $(crictl ps --name=alpine -q) | jq .info.runtimeSpec.linux.seccomp&#39;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;defaultAction&#34;</span>: <span style="color:#b44">&#34;SCMP_ACT_ERRNO&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;architectures&#34;</span>: [<span style="color:#b44">&#34;SCMP_ARCH_X86_64&#34;</span>, <span style="color:#b44">&#34;SCMP_ARCH_X86&#34;</span>, <span style="color:#b44">&#34;SCMP_ARCH_X32&#34;</span>],
  <span style="color:#008000;font-weight:bold">&#34;syscalls&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;names&#34;</span>: [<span style="color:#b44">&#34;...&#34;</span>]
    }
  ]
}
</code></pre></div><!-- 
## Create a Pod with a seccomp profile for syscall auditing

To start off, apply the `audit.json` profile, which will log all syscalls of the
process, to a new Pod.

Here's a manifest for that Pod:
-->
<h2 id="create-a-pod-with-a-seccomp-profile-for-syscall-auditing">使用 seccomp 配置文件创建 Pod 以进行系统调用审计</h2>
<p>首先，将 <code>audit.json</code> 配置文件应用到新的 Pod 上，该配置文件将记录进程的所有系统调用。</p>
<p>这是该 Pod 的清单：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/seccomp/ga/audit-pod.yaml" download="pods/security/seccomp/ga/audit-pod.yaml"><code>pods/security/seccomp/ga/audit-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-seccomp-ga-audit-pod-yaml')" title="Copy pods/security/seccomp/ga/audit-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-seccomp-ga-audit-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>audit-pod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>audit-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">seccompProfile</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Localhost<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">localhostProfile</span>:<span style="color:#bbb"> </span>profiles/audit.json<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>hashicorp/http-echo:0.2.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;-text=just made some syscalls!&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">allowPrivilegeEscalation</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span></code></pre></div>
    </div>
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
The functional support for the already deprecated seccomp annotations
`seccomp.security.alpha.kubernetes.io/pod` (for the whole pod) and
`container.seccomp.security.alpha.kubernetes.io/[name]` (for a single container)
is going to be removed with the release of Kubernetes v1.25. Please always use
the native API fields in favor of the annotations.
-->
<p>已弃用的 seccomp 注解 <code>seccomp.security.alpha.kubernetes.io/pod</code>（针对整个 Pod）和
<code>container.seccomp.security.alpha.kubernetes.io/[name]</code>（针对单个容器）
将随着 Kubernetes v1.25 的发布而被删除。
请在可能的情况下使用原生 API 字段而不是注解。
</div>
<!-- Create the Pod in the cluster: -->
<p>在集群中创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/audit-pod.yaml
</code></pre></div><!-- 
This profile does not restrict any syscalls, so the Pod should start
successfully.
-->
<p>此配置文件不限制任何系统调用，因此 Pod 应该成功启动。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod/audit-pod
</code></pre></div><pre><code>NAME        READY   STATUS    RESTARTS   AGE
audit-pod   1/1     Running   0          30s
</code></pre><!-- 
In order to be able to interact with this endpoint exposed by this
container, create a NodePort <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a>
that allows access to the endpoint from inside the kind control plane container.
-->
<p>为了能够与容器暴露的端点交互，
创建一个 NodePort 类型的 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>，
允许从 kind 控制平面容器内部访问端点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose pod audit-pod --type NodePort --port <span style="color:#666">5678</span>
</code></pre></div><!-- Check what port the Service has been assigned on the node. -->
<p>检查 Service 在节点上分配的端口。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service audit-pod
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre><code>NAME        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
audit-pod   NodePort   10.111.36.142   &lt;none&gt;        5678:32373/TCP   72s
</code></pre><!-- 
Now you can use `curl` to access that endpoint from inside the kind control plane container,
at the port exposed by this Service. Use `docker exec` to run the `curl` command within the
container belonging to that control plane container:
-->
<p>现在，你可以使用 <code>curl</code> 从 kind 控制平面容器内部访问该端点，位于该服务所公开的端口上。
使用 <code>docker exec</code> 在属于该控制平面容器的容器中运行 <code>curl</code> 命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 6a96207fed4b 更改为你从 “docker ps” 看到的控制平面容器 ID</span>
docker <span style="color:#a2f">exec</span> -it 6a96207fed4b curl localhost:32373
</code></pre></div><pre><code>just made some syscalls!
</code></pre><!-- 
You can see that the process is running, but what syscalls did it actually make?
Because this Pod is running in a local cluster, you should be able to see those
in `/var/log/syslog`. Open up a new terminal window and `tail` the output for
calls from `http-echo`:
-->
<p>你可以看到该进程正在运行，但它实际上进行了哪些系统调用？
因为这个 Pod 在本地集群中运行，你应该能够在 <code>/var/log/syslog</code> 中看到它们。
打开一个新的终端窗口并 <code>tail</code> 来自 <code>http-echo</code> 的调用的输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">tail -f /var/log/syslog | grep <span style="color:#b44">&#39;http-echo&#39;</span>
</code></pre></div><!-- 
You should already see some logs of syscalls made by `http-echo`, and if you
`curl` the endpoint in the control plane container you will see more written.

For example:
-->
<p>你应该已经看到了一些由 <code>http-echo</code> 进行的系统调用的日志，
如果你在控制平面容器中 <code>curl</code> 端点，你会看到更多的写入。</p>
<p>例如：</p>
<pre><code>Jul  6 15:37:40 my-machine kernel: [369128.669452] audit: type=1326 audit(1594067860.484:14536): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm=&quot;http-echo&quot; exe=&quot;/http-echo&quot; sig=0 arch=c000003e syscall=51 compat=0 ip=0x46fe1f code=0x7ffc0000
Jul  6 15:37:40 my-machine kernel: [369128.669453] audit: type=1326 audit(1594067860.484:14537): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm=&quot;http-echo&quot; exe=&quot;/http-echo&quot; sig=0 arch=c000003e syscall=54 compat=0 ip=0x46fdba code=0x7ffc0000
Jul  6 15:37:40 my-machine kernel: [369128.669455] audit: type=1326 audit(1594067860.484:14538): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm=&quot;http-echo&quot; exe=&quot;/http-echo&quot; sig=0 arch=c000003e syscall=202 compat=0 ip=0x455e53 code=0x7ffc0000
Jul  6 15:37:40 my-machine kernel: [369128.669456] audit: type=1326 audit(1594067860.484:14539): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm=&quot;http-echo&quot; exe=&quot;/http-echo&quot; sig=0 arch=c000003e syscall=288 compat=0 ip=0x46fdba code=0x7ffc0000
Jul  6 15:37:40 my-machine kernel: [369128.669517] audit: type=1326 audit(1594067860.484:14540): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm=&quot;http-echo&quot; exe=&quot;/http-echo&quot; sig=0 arch=c000003e syscall=0 compat=0 ip=0x46fd44 code=0x7ffc0000
Jul  6 15:37:40 my-machine kernel: [369128.669519] audit: type=1326 audit(1594067860.484:14541): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm=&quot;http-echo&quot; exe=&quot;/http-echo&quot; sig=0 arch=c000003e syscall=270 compat=0 ip=0x4559b1 code=0x7ffc0000
Jul  6 15:38:40 my-machine kernel: [369188.671648] audit: type=1326 audit(1594067920.488:14559): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm=&quot;http-echo&quot; exe=&quot;/http-echo&quot; sig=0 arch=c000003e syscall=270 compat=0 ip=0x4559b1 code=0x7ffc0000
Jul  6 15:38:40 my-machine kernel: [369188.671726] audit: type=1326 audit(1594067920.488:14560): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm=&quot;http-echo&quot; exe=&quot;/http-echo&quot; sig=0 arch=c000003e syscall=202 compat=0 ip=0x455e53 code=0x7ffc0000
</code></pre><!-- 
You can begin to understand the syscalls required by the `http-echo` process by
looking at the `syscall=` entry on each line. While these are unlikely to
encompass all syscalls it uses, it can serve as a basis for a seccomp profile
for this container.

Clean up that Pod and Service before moving to the next section:
-->
<p>通过查看每一行的 <code>syscall=</code> 条目，你可以开始了解 <code>http-echo</code> 进程所需的系统调用。
虽然这些不太可能包含它使用的所有系统调用，但它可以作为此容器的 seccomp 配置文件的基础。</p>
<p>在转到下一部分之前清理该 Pod 和 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete service audit-pod --wait
kubectl delete pod audit-pod --wait --now
</code></pre></div><!-- 
## Create Pod with seccomp profile that causes violation

For demonstration, apply a profile to the Pod that does not allow for any
syscalls.

The manifest for this demonstration is:
-->
<h2 id="create-pod-with-seccomp-profile-that-causes-violation">使用导致违规的 seccomp 配置文件创建 Pod</h2>
<p>出于演示目的，将配置文件应用于不允许任何系统调用的 Pod 上。</p>
<p>此演示的清单是：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/seccomp/ga/violation-pod.yaml" download="pods/security/seccomp/ga/violation-pod.yaml"><code>pods/security/seccomp/ga/violation-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-seccomp-ga-violation-pod-yaml')" title="Copy pods/security/seccomp/ga/violation-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-seccomp-ga-violation-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>violation-pod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>violation-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">seccompProfile</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Localhost<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">localhostProfile</span>:<span style="color:#bbb"> </span>profiles/violation.json<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>hashicorp/http-echo:0.2.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;-text=just made some syscalls!&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">allowPrivilegeEscalation</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span></code></pre></div>
    </div>
</div>


<!-- Attempt to create the Pod in the cluster: -->
<p>尝试在集群中创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/violation-pod.yaml
</code></pre></div><!-- 
The Pod creates, but there is an issue.
If you check the status of the Pod, you should see that it failed to start.
-->
<p>Pod 创建，但存在问题。
如果你检查 Pod 状态，你应该看到它没有启动。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod/violation-pod
</code></pre></div><pre><code>NAME            READY   STATUS             RESTARTS   AGE
violation-pod   0/1     CrashLoopBackOff   1          6s
</code></pre><!-- 
As seen in the previous example, the `http-echo` process requires quite a few
syscalls. Here seccomp has been instructed to error on any syscall by setting
`"defaultAction": "SCMP_ACT_ERRNO"`. This is extremely secure, but removes the
ability to do anything meaningful. What you really want is to give workloads
only the privileges they need.

Clean up that Pod before moving to the next section:
-->
<p>如上例所示，<code>http-echo</code> 进程需要相当多的系统调用。
这里 seccomp 已通过设置 <code>&quot;defaultAction&quot;: &quot;SCMP_ACT_ERRNO&quot;</code> 被指示为在发生任何系统调用时报错。
这是非常安全的，但消除了做任何有意义的事情的能力。
你真正想要的是只给工作负载它们所需要的权限。</p>
<p>在转到下一部分之前清理该 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod violation-pod --wait --now
</code></pre></div><!-- 
## Create Pod with seccomp profile that only allows necessary syscalls

If you take a look at the `fine-grained.json` profile, you will notice some of the syscalls
seen in syslog of the first example where the profile set `"defaultAction":
"SCMP_ACT_LOG"`. Now the profile is setting `"defaultAction": "SCMP_ACT_ERRNO"`,
but explicitly allowing a set of syscalls in the `"action": "SCMP_ACT_ALLOW"`
block. Ideally, the container will run successfully and you will see no messages
sent to `syslog`.

The manifest for this example is:
-->
<h2 id="create-pod-with-seccomp-profile-that-only-allows-necessary-syscalls">使用只允许必要的系统调用的 seccomp 配置文件创建 Pod</h2>
<p>如果你看一看 <code>fine-grained.json</code> 配置文件，
你会注意到第一个示例的 syslog 中看到的一些系统调用，
其中配置文件设置为 <code>&quot;defaultAction&quot;: &quot;SCMP_ACT_LOG&quot;</code>。
现在的配置文件设置 <code>&quot;defaultAction&quot;: &quot;SCMP_ACT_ERRNO&quot;</code>,
但在 <code>&quot;action&quot;: &quot;SCMP_ACT_ALLOW&quot;</code> 块中明确允许一组系统调用。
理想情况下，容器将成功运行，并且你看到没有消息发送到 <code>syslog</code>。</p>
<p>此示例的清单是：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/seccomp/ga/fine-pod.yaml" download="pods/security/seccomp/ga/fine-pod.yaml"><code>pods/security/seccomp/ga/fine-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-seccomp-ga-fine-pod-yaml')" title="Copy pods/security/seccomp/ga/fine-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-seccomp-ga-fine-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fine-pod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>fine-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">seccompProfile</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Localhost<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">localhostProfile</span>:<span style="color:#bbb"> </span>profiles/fine-grained.json<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>hashicorp/http-echo:0.2.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;-text=just made some syscalls!&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">allowPrivilegeEscalation</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span></code></pre></div>
    </div>
</div>


<!-- Create the Pod in your cluster: -->
<p>在你的集群中创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/fine-pod.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod fine-pod
</code></pre></div><!-- The Pod should be showing as having started successfully: -->
<p>此 Pod 应该显示为已成功启动：</p>
<pre><code>NAME        READY   STATUS    RESTARTS   AGE
fine-pod   1/1     Running   0          30s
</code></pre><!-- 
Open up a new terminal window and use `tail` to monitor for log entries that
mention calls from `http-echo`:
-->
<p>打开一个新的终端窗口并使用 <code>tail</code> 来监视提到来自 <code>http-echo</code> 的调用的日志条目：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 你计算机上的日志路径可能与 “/var/log/syslog” 不同</span>
tail -f /var/log/syslog | grep <span style="color:#b44">&#39;http-echo&#39;</span>
</code></pre></div><!-- Next, expose the Pod with a NodePort Service: -->
<p>接着，使用 NodePort Service 公开 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose pod fine-pod --type NodePort --port <span style="color:#666">5678</span>
</code></pre></div><!-- Check what port the Service has been assigned on the node: -->
<p>检查节点上的 Service 分配了什么端口：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service fine-pod
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre><code>NAME        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
fine-pod    NodePort   10.111.36.142   &lt;none&gt;        5678:32373/TCP   72s
</code></pre><!-- Use `curl` to access that endpoint from inside the kind control plane container: -->
<p>使用 <code>curl</code> 从 kind 控制平面容器内部访问端点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 6a96207fed4b 更改为你从 “docker ps” 看到的控制平面容器 ID</span>
docker <span style="color:#a2f">exec</span> -it 6a96207fed4b curl localhost:32373
</code></pre></div><pre><code>just made some syscalls!
</code></pre><!-- 
You should see no output in the `syslog`. This is because the profile allowed all
necessary syscalls and specified that an error should occur if one outside of
the list is invoked. This is an ideal situation from a security perspective, but
required some effort in analyzing the program. It would be nice if there was a
simple way to get closer to this security without requiring as much effort.

Clean up that Pod and Service before moving to the next section:
-->
<p>你应该在 <code>syslog</code> 中看不到任何输出。
这是因为配置文件允许所有必要的系统调用，并指定如果调用列表之外的系统调用应发生错误。
从安全角度来看，这是一种理想的情况，但需要在分析程序时付出一些努力。
如果有一种简单的方法可以在不需要太多努力的情况下更接近这种安全性，那就太好了。</p>
<p>在转到下一部分之前清理该 Pod 和服务：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete service fine-pod --wait
kubectl delete pod fine-pod --wait --now
</code></pre></div><!--
## Create Pod that uses the container runtime default seccomp profile

Most container runtimes provide a sane set of default syscalls that are allowed
or not. You can adopt these defaults for your workload by setting the seccomp
type in the security context of a pod or container to `RuntimeDefault`. 
-->
<h2 id="create-pod-that-uses-the-container-runtime-default-seccomp-profile">创建使用容器运行时默认 seccomp 配置文件的 Pod</h2>
<p>大多数容器运行时都提供了一组合理的默认系统调用，以及是否允许执行这些系统调用。
你可以通过将 Pod 或容器的安全上下文中的 seccomp 类型设置为 <code>RuntimeDefault</code>
来为你的工作负载采用这些默认值。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
If you have the `SeccompDefault` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) enabled, then Pods use the `RuntimeDefault` seccomp profile whenever
no other seccomp profile is specified. Otherwise, the default is `Unconfined`.
-->
<p>如果你已经启用了 <code>SeccompDefault</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>，
只要没有指定其他 seccomp 配置文件，那么 Pod 就会使用 <code>SeccompDefault</code> 的 seccomp 配置文件。
否则，默认值为 <code>Unconfined</code>。
</div>
<!-- 
Here's a manifest for a Pod that requests the `RuntimeDefault` seccomp profile
for all its containers:
-->
<p>这是一个 Pod 的清单，它要求其所有容器使用 <code>RuntimeDefault</code> seccomp 配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/seccomp/ga/default-pod.yaml" download="pods/security/seccomp/ga/default-pod.yaml"><code>pods/security/seccomp/ga/default-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-seccomp-ga-default-pod-yaml')" title="Copy pods/security/seccomp/ga/default-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-seccomp-ga-default-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-pod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>default-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">seccompProfile</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>RuntimeDefault<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>hashicorp/http-echo:0.2.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;-text=just made some more syscalls!&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">allowPrivilegeEscalation</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span></code></pre></div>
    </div>
</div>


<!-- Create that Pod: -->
<p>创建此 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/default-pod.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod default-pod
</code></pre></div><!-- The Pod should be showing as having started successfully: -->
<p>此 Pod 应该显示为成功启动：</p>
<pre><code>NAME        READY   STATUS    RESTARTS   AGE
default-pod 1/1     Running   0          20s
</code></pre><!-- Finally, now that you saw that work OK, clean up: -->
<p>最后，你看到一切正常之后，请清理：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod default-pod --wait --now
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!-- 
You can learn more about Linux seccomp:

* [A seccomp Overview](https://lwn.net/Articles/656307/)
* [Seccomp Security Profiles for Docker](https://docs.docker.com/engine/security/seccomp/)
-->
<p>你可以了解有关 Linux seccomp 的更多信息：</p>
<ul>
<li><a href="https://lwn.net/Articles/656307/">seccomp 概述</a></li>
<li><a href="https://docs.docker.com/engine/security/seccomp/">Docker 的 Seccomp 安全配置文件</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1efbbc2c3015389f835b1661d5effb29">5 - 无状态应用程序</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-62caf420877232190a7404b8d93c6724">5.1 - 公开外部 IP 地址以访问集群中应用程序</h1>
    
	<!--
title: Exposing an External IP Address to Access an Application in a Cluster
content_type: tutorial
weight: 10
-->
<!-- overview -->
<!--
This page shows how to create a Kubernetes Service object that exposes an
external IP address.
-->
<p>此页面显示如何创建公开外部 IP 地址的 Kubernetes 服务对象。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
* Install [kubectl](/docs/tasks/tools/).
* Use a cloud provider like Google Kubernetes Engine or Amazon Web Services to
  create a Kubernetes cluster. This tutorial creates an
  [external load balancer](/docs/tasks/access-application-cluster/create-external-load-balancer/),
  which requires a cloud provider.
* Configure `kubectl` to communicate with your Kubernetes API server. For instructions, see the
  documentation for your cloud provider.
-->
<ul>
<li>安装 <a href="/zh/docs/tasks/tools/">kubectl</a>.</li>
<li>使用 Google Kubernetes Engine 或 Amazon Web Services 等云供应商创建 Kubernetes 集群。
本教程创建了一个<a href="/zh/docs/tasks/access-application-cluster/create-external-load-balancer/">外部负载均衡器</a>，
需要云供应商。</li>
<li>配置 <code>kubectl</code> 与 Kubernetes API 服务器通信。有关说明，请参阅云供应商文档。</li>
</ul>
<h2 id="objectives">Objectives</h2>
<!--
* Run five instances of a Hello World application.
* Create a Service object that exposes an external IP address.
* Use the Service object to access the running application.
-->
<ul>
<li>运行 Hello World 应用程序的五个实例。</li>
<li>创建一个公开外部 IP 地址的 Service 对象。</li>
<li>使用 Service 对象访问正在运行的应用程序。</li>
</ul>
<!-- lessoncontent -->
<!--
## Creating a service for an application running in five pods
-->
<h2 id="为一个在五个-pod-中运行的应用程序创建服务">为一个在五个 pod 中运行的应用程序创建服务</h2>
<!--
1. Run a Hello World application in your cluster:
-->
<ol>
<li>
<p>在集群中运行 Hello World 应用程序：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/load-balancer-example.yaml" download="service/load-balancer-example.yaml"><code>service/load-balancer-example.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-load-balancer-example-yaml')" title="Copy service/load-balancer-example.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-load-balancer-example-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>load-balancer-example<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello-world<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>load-balancer-example<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>load-balancer-example<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/node-hello:1.0<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello-world<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml
</code></pre></div><!--
The preceding command creates a
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>
and an associated
<a class='glossary-tooltip' title='ReplicaSet 是下一代副本控制器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/replicaset/' target='_blank' aria-label='ReplicaSet'>ReplicaSet</a>.
The ReplicaSet has five
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>
each of which runs the Hello World application.
-->
<p>前面的命令创建一个
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>
对象和一个关联的
<a class='glossary-tooltip' title='ReplicaSet 是下一代副本控制器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/replicaset/' target='_blank' aria-label='ReplicaSet'>ReplicaSet</a> 对象。
ReplicaSet 有五个 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>，
每个都运行 Hello World 应用程序。</p>
</li>
</ol>
<!--
1. Display information about the Deployment:
-->
<ol start="2">
<li>
<p>显示有关 Deployment 的信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployments hello-world
kubectl describe deployments hello-world
</code></pre></div></li>
</ol>
<!--
1. Display information about your ReplicaSet objects:
-->
<ol start="3">
<li>
<p>显示有关 ReplicaSet 对象的信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get replicasets
kubectl describe replicasets
</code></pre></div></li>
</ol>
<!--
1. Create a Service object that exposes the deployment:
-->
<ol start="4">
<li>
<p>创建公开 Deployment 的 Service 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose deployment hello-world --type<span style="color:#666">=</span>LoadBalancer --name<span style="color:#666">=</span>my-service
</code></pre></div></li>
</ol>
<!--
1. Display information about the Service:
-->
<ol start="5">
<li>
<p>显示有关 Service 的信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get services my-service
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code class="language-console" data-lang="console">NAME         TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)    AGE
my-service   LoadBalancer   10.3.245.137   104.198.205.71   8080/TCP   54s
</code></pre><!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> The <code>type=LoadBalancer</code> service is backed by external cloud providers, which is not covered in this example, please refer to <a href="/docs/concepts/services-networking/service/#loadbalancer">this page</a> for the details.
</div>
-->
<p>提示：<code>type=LoadBalancer</code> 服务由外部云服务提供商提供支持，本例中不包含此部分，
详细信息请参考<a href="/docs/concepts/services-networking/service/#loadbalancer">此页</a></p>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> If the external IP address is shown as &lt;pending&gt;, wait for a minute and enter the same command again.
</div>
-->
<p>提示：如果外部 IP 地址显示为 &lt;pending&gt;，请等待一分钟再次输入相同的命令。</p>
</li>
</ol>
<!--
1. Display detailed information about the Service:
-->
<ol start="6">
<li>
<p>显示有关 Service 的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe services my-service
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code class="language-console" data-lang="console">Name:           my-service
Namespace:      default
Labels:         app.kubernetes.io/name=load-balancer-example
Annotations:    &lt;none&gt;
Selector:       app.kubernetes.io/name=load-balancer-example
Type:           LoadBalancer
IP:             10.3.245.137
LoadBalancer Ingress:   104.198.205.71
Port:           &lt;unset&gt; 8080/TCP
NodePort:       &lt;unset&gt; 32377/TCP
Endpoints:      10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more...
Session Affinity:   None
Events:         &lt;none&gt;
</code></pre><!--
Make a note of the external IP address (`LoadBalancer Ingress`) exposed by
your service. In this example, the external IP address is 104.198.205.71.
Also note the value of `Port` and `NodePort`. In this example, the `Port`
is 8080 and the `NodePort` is 32377.
-->
<p>记下服务公开的外部 IP 地址（<code>LoadBalancer Ingress</code>)。
在本例中，外部 IP 地址是 104.198.205.71。还要注意 <code>Port</code> 和 <code>NodePort</code> 的值。
在本例中，<code>Port</code> 是 8080，<code>NodePort</code> 是32377。</p>
</li>
</ol>
<!--
1. In the preceding output, you can see that the service has several endpoints:
   10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more. These are internal
   addresses of the pods that are running the Hello World application. To
   verify these are pod addresses, enter this command:
-->
<ol start="7">
<li>
<p>在前面的输出中，您可以看到服务有几个端点：
10.0.0.6:8080、10.0.1.6:8080、10.0.1.7:8080 和另外两个，
这些都是正在运行 Hello World 应用程序的 pod 的内部地址。
要验证这些是 pod 地址，请输入以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --output<span style="color:#666">=</span>wide
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code class="language-console" data-lang="console">NAME                         ...  IP         NODE
hello-world-2895499144-1jaz9 ...  10.0.1.6   gke-cluster-1-default-pool-e0b8d269-1afc
hello-world-2895499144-2e5uh ...  10.0.1.8   gke-cluster-1-default-pool-e0b8d269-1afc
hello-world-2895499144-9m4h1 ...  10.0.0.6   gke-cluster-1-default-pool-e0b8d269-5v7a
hello-world-2895499144-o4z13 ...  10.0.1.7   gke-cluster-1-default-pool-e0b8d269-1afc
hello-world-2895499144-segjf ...  10.0.2.5   gke-cluster-1-default-pool-e0b8d269-cpuc
</code></pre></li>
</ol>
<!--
1. Use the external IP address (`LoadBalancer Ingress`) to access the Hello
   World application:
-->
<ol start="8">
<li>
<p>使用外部 IP 地址（<code>LoadBalancer Ingress</code>）访问 Hello World 应用程序:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl http://&lt;external-ip&gt;:&lt;port&gt;
</code></pre></div><!--
where `<external-ip>` is the external IP address (`LoadBalancer Ingress`)
of your Service, and `<port>` is the value of `Port` in your Service
description.
If you are using minikube, typing `minikube service my-service` will
automatically open the Hello World application in a browser.
-->
<p>其中 <code>&lt;external-ip&gt;</code> 是您的服务的外部 IP 地址（<code>LoadBalancer Ingress</code>），
<code>&lt;port&gt;</code> 是您的服务描述中的 <code>port</code> 的值。
如果您正在使用 minikube，输入 <code>minikube service my-service</code> 将在浏览器中自动打开 Hello World 应用程序。</p>
<!--
The response to a successful request is a hello message:
-->
<p>成功请求的响应是一条问候消息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Hello Kubernetes!
</code></pre></div></li>
</ol>
<h2 id="cleaning-up">Cleaning up</h2>
<!--
To delete the Service, enter this command:
-->
<p>要删除服务，请输入以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete services my-service
</code></pre></div><!--
To delete the Deployment, the ReplicaSet, and the Pods that are running
the Hello World application, enter this command:
-->
<p>要删除正在运行 Hello World 应用程序的 Deployment，ReplicaSet 和 Pod，请输入以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment hello-world
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
Learn more about
[connecting applications with services](/docs/concepts/services-networking/connect-applications-service/).
-->
<p>进一步了解<a href="/zh/docs/concepts/services-networking/connect-applications-service/">将应用程序与服务连接</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8c56795c6614cc5f52434ecc756448ac">5.2 - 示例：使用 Redis 部署 PHP 留言板应用程序</h1>
    
	<!--
title: "Example: Deploying PHP Guestbook application with Redis"
reviewers:
- ahmetb
content_type: tutorial
weight: 20
card:
  name: tutorials
  weight: 30
  title: "Stateless Example: PHP Guestbook with Redis"
min-kubernetes-server-version: v1.14
-->
<!-- overview -->
<!--
This tutorial shows you how to build and deploy a simple _(not production ready)_, multi-tier web application using Kubernetes and [Docker](https://www.docker.com/). This example consists of the following components:
-->
<p>本教程向您展示如何使用 Kubernetes 和 <a href="https://www.docker.com/">Docker</a> 构建和部署
一个简单的 <em>(非面向生产的)</em> 多层 web 应用程序。本例由以下组件组成：</p>
<!--
* A single-instance [Redis](https://www.redis.io/) to store guestbook entries
* Multiple web frontend instances
-->
<ul>
<li>单实例 <a href="https://www.redis.io/">Redis</a> 以保存留言板条目</li>
<li>多个 web 前端实例</li>
</ul>
<h2 id="objectives">Objectives</h2>
<!--
* Start up a Redis leader.
* Start up two Redis followers.
* Start up the guestbook frontend.
* Expose and view the Frontend Service.
* Clean up.
-->
<ul>
<li>启动 Redis 领导者（Leader）</li>
<li>启动两个 Redis 跟随者（Follower）</li>
<li>公开并查看前端服务</li>
<li>清理</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>



Your Kubernetes server must be at or later than version v1.14.
 To check the version, enter <code>kubectl version</code>.

<!-- lessoncontent -->
<!--
## Start up the Redis Database
-->
<h2 id="启动-redis-数据库">启动 Redis 数据库</h2>
<!--
The guestbook application uses Redis to store its data.
-->
<p>留言板应用程序使用 Redis 存储数据。</p>
<!--
### Creating the Redis Deployment
-->
<h3 id="创建-redis-deployment">创建 Redis Deployment</h3>
<!--
The manifest file, included below, specifies a Deployment controller that runs a single replica Redis Pod.
-->
<p>下面包含的清单文件指定了一个 Deployment 控制器，该控制器运行一个 Redis Pod 副本。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/guestbook/redis-leader-deployment.yaml" download="application/guestbook/redis-leader-deployment.yaml"><code>application/guestbook/redis-leader-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-guestbook-redis-leader-deployment-yaml')" title="Copy application/guestbook/redis-leader-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-guestbook-redis-leader-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis-leader<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>leader<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>leader<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>leader<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;docker.io/redis:6.0.5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>100m<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">6379</span></code></pre></div>
    </div>
</div>


<!--
1. Launch a terminal window in the directory you downloaded the manifest files.
1. Apply the Redis Deployment from the `redis-leader-deployment.yaml` file:
-->
<ol>
<li>
<p>在下载清单文件的目录中启动终端窗口。</p>
</li>
<li>
<p>从 <code>redis-leader-deployment.yaml</code> 文件中应用 Redis Deployment：</p>
<!---
for local testing of the content via relative file path
kubectl apply -f ./content/en/examples/application/guestbook/redis-leader-deployment.yaml
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/guestbook/redis-leader-deployment.yaml
</code></pre></div></li>
</ol>
<!--
1. Query the list of Pods to verify that the Redis Pod is running:
-->
<ol start="3">
<li>
<p>查询 Pod 列表以验证 Redis Pod 是否正在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The response should be similar to this:
-->
<p>响应应该与此类似：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                           READY   STATUS    RESTARTS   AGE
redis-leader-fb76b4755-xjr2n   1/1     Running   <span style="color:#666">0</span>          13s
</code></pre></div></li>
</ol>
<!--
1. Run the following command to view the logs from the Redis leader Pod:
-->
<ol start="4">
<li>
<p>运行以下命令查看 Redis Deployment 中的日志：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs -f deployment/redis-leader
</code></pre></div></li>
</ol>
<!--
### Creating the Redis leader Service
-->
<h3 id="创建-redis-领导者服务">创建 Redis 领导者服务</h3>
<!--
The guestbook application needs to communicate to the Redis to write its data. You need to apply a [Service](/docs/concepts/services-networking/service/) to proxy the traffic to the Redis Pod. A Service defines a policy to access the Pods.
-->
<p>留言板应用程序需要往 Redis 中写数据。因此，需要创建
<a href="/zh/docs/concepts/services-networking/service/">Service</a> 来转发 Redis Pod
的流量。Service 定义了访问 Pod 的策略。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/guestbook/redis-leader-service.yaml" download="application/guestbook/redis-leader-service.yaml"><code>application/guestbook/redis-leader-service.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-guestbook-redis-leader-service-yaml')" title="Copy application/guestbook/redis-leader-service.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-guestbook-redis-leader-service-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis-leader<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>leader<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">6379</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">6379</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>leader<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend</code></pre></div>
    </div>
</div>


<!--
1. Apply the Redis Service from the following `redis-leader-service.yaml` file:
-->
<ol>
<li>
<p>使用下面的 <code>redis-leader-service.yaml</code> 文件创建 Redis的服务：</p>
<!---
for local testing of the content via relative file path
kubectl apply -f ./content/en/examples/application/guestbook/redis-leader-service.yaml
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/guestbook/redis-leader-service.yaml
</code></pre></div></li>
</ol>
<!--
1. Query the list of Services to verify that the Redis Service is running:
-->
<ol start="2">
<li>
<p>查询服务列表验证 Redis 服务是否正在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service
</code></pre></div><!--
The response should be similar to this:
-->
<p>响应应该与此类似：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT<span style="color:#666">(</span>S<span style="color:#666">)</span>    AGE
kubernetes     ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP    1m
redis-leader   ClusterIP   10.103.78.24 &lt;none&gt;        6379/TCP   16s
</code></pre></div></li>
</ol>
<!--
This manifest file creates a Service named `redis-leader` with a set of labels that match the labels previously defined, so the Service routes network traffic to the Redis Pod.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 这个清单文件创建了一个名为 <code>redis-leader</code> 的 Service，其中包含一组
与前面定义的标签匹配的标签，因此服务将网络流量路由到 Redis Pod 上。
</div>
<!--
### Set up Redis followers

Although the Redis leader is a single Pod, you can make it highly available and meet traffic demands by adding a few Redis followers, or replicas.
-->
<h3 id="设置-redis-跟随者">设置 Redis 跟随者</h3>
<p>尽管 Redis 领导者只有一个 Pod，你可以通过添加若干 Redis 跟随者来将其配置为高可用状态，
以满足流量需求。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/guestbook/redis-follower-deployment.yaml" download="application/guestbook/redis-follower-deployment.yaml"><code>application/guestbook/redis-follower-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-guestbook-redis-follower-deployment-yaml')" title="Copy application/guestbook/redis-follower-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-guestbook-redis-follower-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis-follower<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>follower<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>follower<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>follower<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google_samples/gb-redis-follower:v2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>100m<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">6379</span></code></pre></div>
    </div>
</div>


<!--
1. Apply the Redis Service from the following `redis-follower-deployment.yaml` file:
-->
<ol>
<li>
<p>应用下面的 <code>redis-follower-deployment.yaml</code> 文件创建 Redis Deployment：</p>
<!---
for local testing of the content via relative file path
kubectl apply -f ./content/en/examples/application/guestbook/redis-follower-deployment.yaml
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/guestbook/redis-follower-deployment.yaml
</code></pre></div></li>
</ol>
<!--
1. Verify that the two Redis follower replicas are running by querying the list of Pods:
-->
<ol start="2">
<li>
<p>通过查询 Pods 列表，验证两个 Redis 跟随者副本在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The response should be similar to this:
-->
<p>响应应该类似于这样：</p>
<pre><code>NAME                             READY   STATUS    RESTARTS   AGE
redis-follower-dddfbdcc9-82sfr   1/1     Running   0          37s
redis-follower-dddfbdcc9-qrt5k   1/1     Running   0          38s
redis-leader-fb76b4755-xjr2n     1/1     Running   0          11m
</code></pre></li>
</ol>
<!--
### Creating the Redis follower service

The guestbook application needs to communicate with the Redis followers to read data. To make the Redis followers discoverable, you must set up another [Service](/docs/concepts/services-networking/service/).
-->
<h3 id="创建-redis-跟随者服务">创建 Redis 跟随者服务</h3>
<p>Guestbook 应用需要与 Redis 跟随者通信以读取数据。
为了让 Redis 跟随者可被发现，你必须创建另一个
<a href="/zh/docs/concepts/services-networking/service/">Service</a>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/guestbook/redis-follower-service.yaml" download="application/guestbook/redis-follower-service.yaml"><code>application/guestbook/redis-follower-service.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-guestbook-redis-follower-service-yaml')" title="Copy application/guestbook/redis-follower-service.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-guestbook-redis-follower-service-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis-follower<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>follower<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># the port that this service should serve on</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">6379</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>follower<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend</code></pre></div>
    </div>
</div>


<!--
1. Apply the Redis Service from the following `redis-follower-service.yaml` file:
-->
<ol>
<li>
<p>应用如下所示 <code>redis-follower-service.yaml</code> 文件中的 Redis Service：</p>
<!---
for local testing of the content via relative file path
kubectl apply -f ./content/en/examples/application/guestbook/redis-follower-service.yaml
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/guestbook/redis-follower-service.yaml
</code></pre></div></li>
</ol>
<!--
1. Query the list of Services to verify that the Redis Service is running:
-->
<ol start="2">
<li>
<p>查询 Service 列表，验证 Redis 服务在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service
</code></pre></div><!--
The response should be similar to this:
-->
<p>响应应该类似于这样：</p>
<pre><code>NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes       ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    3d19h
redis-follower   ClusterIP   10.110.162.42   &lt;none&gt;        6379/TCP   9s
redis-leader     ClusterIP   10.103.78.24    &lt;none&gt;        6379/TCP   6m10s
</code></pre></li>
</ol>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
This manifest file creates a Service named `redis-follower` with a set of labels that match the labels previously defined, so the Service routes network traffic to the Redis Pod.
-->
<p>清单文件创建了一个名为 <code>redis-follower</code> 的 Service，该 Service
具有一些与之前所定义的标签相匹配的标签，因此该 Service 能够将网络流量
路由到 Redis Pod 之上。
</div>
<!--
## Set up and Expose the Guestbook Frontend
-->
<h2 id="设置并公开留言板前端">设置并公开留言板前端</h2>
<!-- 
Now that you have the Redis storage of your guestbook up and running, start the guestbook web servers. Like the Redis followers, the frontend is deployed using a Kubernetes Deployment.

The guestbook app uses a PHP frontend. It is configured to communicate with either the Redis follower or leader Services, depending on whether the request is a read or a write. The frontend exposes a JSON interface, and serves a jQuery-Ajax-based UX.
-->
<p>现在你有了一个为 Guestbook 应用配置的 Redis 存储处于运行状态，
接下来可以启动 Guestbook 的 Web 服务器了。
与 Redis 跟随者类似，前端也是使用 Kubernetes Deployment 来部署的。</p>
<p>Guestbook 应用使用 PHP 前端。该前端被配置成与后端的 Redis 跟随者或者
领导者服务通信，具体选择哪个服务取决于请求是读操作还是写操作。
前端对外暴露一个 JSON 接口，并提供基于 jQuery-Ajax 的用户体验。</p>
<!--
### Creating the Guestbook Frontend Deployment
-->
<h3 id="创建-guestbook-前端-deployment">创建 Guestbook 前端 Deployment</h3>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/guestbook/frontend-deployment.yaml" download="application/guestbook/frontend-deployment.yaml"><code>application/guestbook/frontend-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-guestbook-frontend-deployment-yaml')" title="Copy application/guestbook/frontend-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-guestbook-frontend-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-redis<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google_samples/gb-frontend:v5<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>GET_HOSTS_FROM<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;dns&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>100m<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span></code></pre></div>
    </div>
</div>


<!--
1. Apply the frontend Deployment from the `frontend-deployment.yaml` file:
-->
<ol>
<li>
<p>应用来自 <code>frontend-deployment.yaml</code> 文件的前端 Deployment：</p>
<!---
for local testing of the content via relative file path
kubectl apply -f ./content/en/examples/application/guestbook/frontend-deployment.yaml
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
</code></pre></div></li>
</ol>
<!--
1. Query the list of Pods to verify that the three frontend replicas are running:
-->
<ol start="2">
<li>
<p>查询 Pod 列表，验证三个前端副本正在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>guestbook -l <span style="color:#b8860b">tier</span><span style="color:#666">=</span>frontend
</code></pre></div><!--
The response should be similar to this:
-->
<p>响应应该与此类似：</p>
<pre><code>NAME                        READY   STATUS    RESTARTS   AGE
frontend-85595f5bf9-5tqhb   1/1     Running   0          47s
frontend-85595f5bf9-qbzwm   1/1     Running   0          47s
frontend-85595f5bf9-zchwc   1/1     Running   0          47s
</code></pre></li>
</ol>
<!--
### Creating the Frontend Service
-->
<h3 id="创建前端服务">创建前端服务</h3>
<!--
The `Redis` Services you applied is only accessible within the Kubernetes cluster because the default type for a Service is [ClusterIP](/docs/concepts/services-networking/service/#publishing-services-service-types). `ClusterIP` provides a single IP address for the set of Pods the Service is pointing to. This IP address is accessible only within the cluster.
-->
<p>应用的 <code>Redis</code> 服务只能在 Kubernetes 集群中访问，因为服务的默认类型是
<a href="/zh/docs/concepts/services-networking/service/#publishing-services-service-types">ClusterIP</a>。
<code>ClusterIP</code> 为服务指向的 Pod 集提供一个 IP 地址。这个 IP 地址只能在集群中访问。</p>
<!--
If you want guests to be able to access your guestbook, you must configure the 
frontend Service to be externally visible, so a client can request the Service 
from outside the Kubernetes cluster. However a Kubernetes user can use
`kubectl port-forward` to access the service even though it uses a 
`ClusterIP`.
-->
<p>如果你希望访客能够访问你的 Guestbook，你必须将前端服务配置为外部可见的，
以便客户端可以从 Kubernetes 集群之外请求服务。
然而即便使用了 <code>ClusterIP</code>，Kubernetes 用户仍可以通过
<code>kubectl port-forward</code> 访问服务。</p>
<!--
Some cloud providers, like Google Compute Engine or Google Kubernetes Engine, support external load balancers. If your cloud provider supports load balancers and you want to use it, uncomment `type: LoadBalancer`.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 一些云提供商，如 Google Compute Engine 或 Google Kubernetes Engine，
支持外部负载均衡器。如果你的云提供商支持负载均衡器，并且你希望使用它，
只需取消注释 <code>type: LoadBalancer</code>。
</div>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/guestbook/frontend-service.yaml" download="application/guestbook/frontend-service.yaml"><code>application/guestbook/frontend-service.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-guestbook-frontend-service-yaml')" title="Copy application/guestbook/frontend-service.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-guestbook-frontend-service-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># if your cluster supports it, uncomment the following to automatically create</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># an external load-balanced IP for the frontend service.</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># type: LoadBalancer</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic">#type: LoadBalancer</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># the port that this service should serve on</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend</code></pre></div>
    </div>
</div>


<!--
1. Apply the frontend Service from the `frontend-service.yaml` file:
-->
<ol>
<li>
<p>应用来自 <code>frontend-service.yaml</code> 文件中的前端服务：</p>
<!---
for local testing of the content via relative file path
kubectl apply -f ./content/en/examples/application/guestbook/frontend-service.yaml
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml
</code></pre></div></li>
</ol>
<!--
1. Query the list of Services to verify that the frontend Service is running:
-->
<ol start="2">
<li>
<p>查询 Service 列表以验证前端服务正在运行:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get services
</code></pre></div><!--
The response should be similar to this:
-->
<p>响应应该与此类似：</p>
<pre><code>NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
frontend         ClusterIP   10.97.28.230    &lt;none&gt;        80/TCP     19s
kubernetes       ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    3d19h
redis-follower   ClusterIP   10.110.162.42   &lt;none&gt;        6379/TCP   5m48s
redis-leader     ClusterIP   10.103.78.24    &lt;none&gt;        6379/TCP   11m
</code></pre></li>
</ol>
<!--
### Viewing the Frontend Service via `kubectl port-forward`
-->
<h3 id="通过-kubectl-port-forward-查看前端服务">通过 <code>kubectl port-forward</code> 查看前端服务</h3>
<!--
1. Run the following command to forward port `8080` on your local machine to port `80` on the service.
-->
<ol>
<li>
<p>运行以下命令将本机的 <code>8080</code> 端口转发到服务的 <code>80</code> 端口。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl port-forward svc/frontend 8080:80
</code></pre></div><!--
The response should be similar to this:
-->
<p>响应应该与此类似：</p>
<pre><code>Forwarding from 127.0.0.1:8080 -&gt; 80
Forwarding from [::1]:8080 -&gt; 80
</code></pre></li>
</ol>
<!--
1. load the page [http://localhost:8080](http://localhost:8080) in your browser to view your guestbook.
-->
<ol start="2">
<li>在浏览器中加载 <a href="http://localhost:8080">http://localhost:8080</a>
页面以查看 Guestbook。</li>
</ol>
<!--
### Viewing the Frontend Service via `LoadBalancer`
-->
<h3 id="通过-loadbalancer-查看前端服务">通过 <code>LoadBalancer</code> 查看前端服务</h3>
<!--
If you deployed the `frontend-service.yaml` manifest with type: `LoadBalancer` you need to find the IP address to view your Guestbook.
-->
<p>如果你部署了 <code>frontend-service.yaml</code>，需要找到用来查看 Guestbook 的
IP 地址。</p>
<!--
1. Run the following command to get the IP address for the frontend Service.
-->
<ol>
<li>
<p>运行以下命令以获取前端服务的 IP 地址。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service frontend
</code></pre></div><!--
The response should be similar to this:
-->
<p>响应应该与此类似：</p>
<pre><code>NAME       TYPE           CLUSTER-IP      EXTERNAL-IP        PORT(S)        AGE
frontend   LoadBalancer   10.51.242.136   109.197.92.229     80:32372/TCP   1m
</code></pre></li>
</ol>
<!--
1. Copy the external IP address, and load the page in your browser to view your guestbook.
-->
<ol start="2">
<li>复制这里的外部 IP 地址，然后在浏览器中加载页面以查看留言板。</li>
</ol>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Try adding some guestbook entries by typing in a message, and clicking Submit. The message you typed appears in the frontend. This message indicates that data is successfully added to Redis through the Services you created earlier.
-->
<p>尝试通过输入消息并点击 Submit 来添加一些留言板条目。
你所输入的消息会在前端显示。这一消息表明数据被通过你
之前所创建的 Service 添加到 Redis 存储中。
</div>
<!--
## Scale the Web Frontend
-->
<h2 id="扩展-web-前端">扩展 Web 前端</h2>
<!--
You can scale up or down as needed because your servers are defined as a Service that uses a Deployment controller.
-->
<p>你可以根据需要执行伸缩操作，这是因为服务器本身被定义为使用一个
Deployment 控制器的 Service。</p>
<!--
1. Run the following command to scale up the number of frontend Pods:
-->
<ol>
<li>
<p>运行以下命令扩展前端 Pod 的数量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale deployment frontend --replicas<span style="color:#666">=</span><span style="color:#666">5</span>
</code></pre></div></li>
</ol>
<!--
1. Query the list of Pods to verify the number of frontend Pods running:
-->
<ol start="2">
<li>
<p>查询 Pod 列表验证正在运行的前端 Pod 的数量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The response should look similar to this:
-->
<p>响应应该类似于这样：</p>
<pre><code>NAME                             READY   STATUS    RESTARTS   AGE
frontend-85595f5bf9-5df5m        1/1     Running   0          83s
frontend-85595f5bf9-7zmg5        1/1     Running   0          83s
frontend-85595f5bf9-cpskg        1/1     Running   0          15m
frontend-85595f5bf9-l2l54        1/1     Running   0          14m
frontend-85595f5bf9-l9c8z        1/1     Running   0          14m
redis-follower-dddfbdcc9-82sfr   1/1     Running   0          97m
redis-follower-dddfbdcc9-qrt5k   1/1     Running   0          97m
redis-leader-fb76b4755-xjr2n     1/1     Running   0          108m
</code></pre></li>
</ol>
<!--
1. Run the following command to scale down the number of frontend Pods:
-->
<ol start="3">
<li>
<p>运行以下命令缩小前端 Pod 的数量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale deployment frontend --replicas<span style="color:#666">=</span><span style="color:#666">2</span>
</code></pre></div></li>
</ol>
<!--
1. Query the list of Pods to verify the number of frontend Pods running:
-->
<ol start="4">
<li>
<p>查询 Pod 列表验证正在运行的前端 Pod 的数量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The response should look similar to this:
-->
<p>响应应该类似于这样：</p>
<pre><code>NAME                             READY   STATUS    RESTARTS   AGE
frontend-85595f5bf9-cpskg        1/1     Running   0          16m
frontend-85595f5bf9-l9c8z        1/1     Running   0          15m
redis-follower-dddfbdcc9-82sfr   1/1     Running   0          98m
redis-follower-dddfbdcc9-qrt5k   1/1     Running   0          98m
redis-leader-fb76b4755-xjr2n     1/1     Running   0          109m
</code></pre></li>
</ol>
<h2 id="cleaning-up">Cleaning up</h2>
<!--
Deleting the Deployments and Services also deletes any running Pods. Use labels to delete multiple resources with one command.
-->
<p>删除 Deployments 和服务还会删除正在运行的 Pod。
使用标签用一个命令删除多个资源。</p>
<!--
1. Run the following commands to delete all Pods, Deployments, and Services.
-->
<ol>
<li>
<p>运行以下命令以删除所有 Pod，Deployments 和 Services。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>redis
kubectl delete service -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>redis
kubectl delete deployment frontend
kubectl delete service frontend
</code></pre></div><!--
The responses should be:
-->
<p>响应应该是：</p>
<pre><code>deployment.apps &quot;redis-follower&quot; deleted
deployment.apps &quot;redis-leader&quot; deleted
deployment.apps &quot;frontend&quot; deleted
service &quot;frontend&quot; deleted
</code></pre></li>
</ol>
<!--
1. Query the list of Pods to verify that no Pods are running:
-->
<ol start="2">
<li>
<p>查询 Pod 列表，确认没有 Pod 在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The response should be this:
-->
<p>响应应该是：</p>
<pre><code>
No resources found in default namespace.
</code></pre></li>
</ol>
<h2 id="what-s-next">What's next</h2>
<!--
* Complete the [Kubernetes Basics](/docs/tutorials/kubernetes-basics/) Interactive Tutorials
* Use Kubernetes to create a blog using [Persistent Volumes for MySQL and Wordpress](/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#visit-your-new-wordpress-blog)
* Read more about [connecting applications](/docs/concepts/services-networking/connect-applications-service/)
* Read more about [Managing Resources](/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively)
-->
<ul>
<li>完成 <a href="/zh/docs/tutorials/kubernetes-basics/">Kubernetes 基础</a> 交互式教程</li>
<li>使用 Kubernetes 创建一个博客，使用
<a href="/zh/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/#visit-your-new-wordpress-blog">MySQL 和 Wordpress 的持久卷</a></li>
<li>进一步阅读<a href="/zh/docs/concepts/services-networking/connect-applications-service/">连接应用程序</a></li>
<li>进一步阅读<a href="/zh/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively">管理资源</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d6336d9712aa433eb5f0fb8cbed6bef7">6 - 有状态的应用</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-27580b3f65f3c2da07fc0f83be69da75">6.1 - 示例：使用 Persistent Volumes 部署 WordPress 和 MySQL</h1>
    
	<!--
title: "Example: Deploying WordPress and MySQL with Persistent Volumes"
reviewers:
- ahmetb
content_type: tutorial
weight: 20
card: 
  name: tutorials
  weight: 40
  title: "Stateful Example: Wordpress with Persistent Volumes"
-->
<!-- overview -->
<!--
This tutorial shows you how to deploy a WordPress site and a MySQL database using Minikube. Both applications use PersistentVolumes and PersistentVolumeClaims to store data.
-->
<p>本示例描述了如何通过 Minikube 在 Kubernetes 上安装 WordPress 和 MySQL。这两个应用都使用 PersistentVolumes 和 PersistentVolumeClaims 保存数据。</p>
<!--
 A [PersistentVolume](/docs/concepts/storage/persistent-volumes/)(PV)is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a [StorageClass](/docs/concepts/storage/storage-classes).  A [PersistentVolumeClaim](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)(PVC)is a request for storage by a user that can be fulfilled by a PV. PersistentVolumes and PersistentVolumeClaims are independent from Pod lifecycles and preserve data through restarting, rescheduling, and even deleting Pods.
 -->
<p><a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolume</a>（PV）是一块集群里由管理员手动提供，或 kubernetes 通过 <a href="/zh/docs/concepts/storage/storage-classes">StorageClass</a> 动态创建的存储。
<a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim</a>（PVC）是一个满足对 PV 存储需要的请求。PersistentVolumes 和 PersistentVolumeClaims 是独立于 Pod 生命周期而在 Pod 重启，重新调度甚至删除过程中保存数据。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
 This deployment is not suitable for production use cases, as it uses single instance WordPress and MySQL Pods. Consider using [WordPress Helm Chart](https://github.com/kubernetes/charts/tree/master/stable/wordpress) to deploy WordPress in production.
 -->
<p>这种部署并不适合生产场景，它使用单实例 WordPress 和 MySQL Pods。考虑使用 <a href="https://github.com/kubernetes/charts/tree/master/stable/wordpress">WordPress Helm Chart</a> 在生产场景中部署 WordPress。
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
 The files provided in this tutorial are using GA Deployment APIs and are specific to kubernetes version 1.9 and later. If you wish to use this tutorial with an earlier version of Kubernetes, please update the API version appropriately, or reference earlier versions of this tutorial.
-->
<p>本教程中提供的文件使用 GA Deployment API，并且特定于 kubernetes 1.9 或更高版本。如果您希望将本教程与 Kubernetes 的早期版本一起使用，请相应地更新 API 版本，或参考本教程的早期版本。
</div>
<h2 id="objectives">Objectives</h2>
<!--
* Create PersistentVolumeClaims and PersistentVolumes
* Create a `kustomization.yaml` with
  * a Secret generator
  * MySQL resource configs
  * WordPress resource configs
* Apply the kustomization directory by `kubectl apply -k ./`
* Clean up
-->
<ul>
<li>创建 PersistentVolumeClaims 和 PersistentVolumes</li>
<li>创建 <code>kustomization.yaml</code> 使用
<ul>
<li>Secret 生成器</li>
<li>MySQL 资源配置</li>
<li>WordPress 资源配置</li>
</ul>
</li>
<li>应用整个 kustomization 目录 <code>kubectl apply -k ./</code></li>
<li>清理</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
The example shown on this page works with `kubectl` 1.14 and above.

Download the following configuration files:

1. [mysql-deployment.yaml](/examples/application/wordpress/mysql-deployment.yaml)

1. [wordpress-deployment.yaml](/examples/application/wordpress/wordpress-deployment.yaml)
-->
<p>此例在<code>kubectl</code> 1.14 或者更高版本有效。</p>
<p>下载下面的配置文件：</p>
<ol>
<li>
<p><a href="/examples/application/wordpress/mysql-deployment.yaml">mysql-deployment.yaml</a></p>
</li>
<li>
<p><a href="/examples/application/wordpress/wordpress-deployment.yaml">wordpress-deployment.yaml</a></p>
</li>
</ol>
<!-- lessoncontent -->
<!--
## Create PersistentVolumeClaims and PersistentVolumes
-->
<h2 id="创建-persistentvolumeclaims-和-persistentvolumes">创建 PersistentVolumeClaims 和 PersistentVolumes</h2>
<!-- MySQL and Wordpress each require a PersistentVolume to store data.  Their PersistentVolumeClaims will be created at the deployment step.

Many cluster environments have a default StorageClass installed.  When a StorageClass is not specified in the PersistentVolumeClaim, the cluster's default StorageClass is used instead.

When a PersistentVolumeClaim is created, a PersistentVolume is dynamically provisioned based on the StorageClass configuration.
-->
<p>MySQL 和 Wordpress 都需要一个 PersistentVolume 来存储数据。他们的 PersistentVolumeClaims 将在部署步骤中创建。</p>
<p>许多群集环境都安装了默认的 StorageClass。如果在 PersistentVolumeClaim 中未指定 StorageClass，则使用群集的默认 StorageClass。</p>
<p>创建 PersistentVolumeClaim 时，将根据 StorageClass 配置动态设置 PersistentVolume。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
In local clusters, the default StorageClass uses the `hostPath` provisioner.  `hostPath` volumes are only suitable for development and testing. With `hostPath` volumes, your data lives in `/tmp` on the node the Pod is scheduled onto and does not move between nodes. If a Pod dies and gets scheduled to another node in the cluster, or the node is rebooted, the data is lost.
-->
<p>在本地群集中，默认的 StorageClass 使用<code>hostPath</code>供应器。 <code>hostPath</code>卷仅适用于开发和测试。使用 <code>hostPath</code> 卷，您的数据位于 Pod 调度到的节点上的<code>/tmp</code>中，并且不会在节点之间移动。如果 Pod 死亡并被调度到群集中的另一个节点，或者该节点重新启动，则数据将丢失。
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you are bringing up a cluster that needs to use the `hostPath` provisioner, the `--enable-hostpath-provisioner` flag must be set in the `controller-manager` component.
-->
<p>如果要建立需要使用<code>hostPath</code>设置程序的集群，则必须在 controller-manager 组件中设置<code>--enable-hostpath-provisioner</code>标志。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- If you have a Kubernetes cluster running on Google Kubernetes Engine, please follow [this guide](https://cloud.google.com/kubernetes-engine/docs/tutorials/persistent-disk). -->
<p>如果你已经有运行在 Google Kubernetes Engine 的集群，请参考 <a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/persistent-disk">this guide</a>。
</div>
<!--
## Create a kustomization.yaml
-->
<h2 id="创建-kustomization-yaml">创建 kustomization.yaml</h2>
<!--
### Add a Secret generator
-->
<h3 id="创建-secret-生成器">创建 Secret 生成器</h3>
<!--
A [Secret](/docs/concepts/configuration/secret/) is an object that stores a piece of sensitive data like a password or key. Since 1.14, `kubectl` supports the management of Kubernetes objects using a kustomization file. You can create a Secret by generators in `kustomization.yaml`.

Add a Secret generator in `kustomization.yaml` from the following command. You will need to replace `YOUR_PASSWORD` with the password you want to use.
-->
<p>A <a href="/zh/docs/concepts/configuration/secret/">Secret</a> 是存储诸如密码或密钥之类的敏感数据的对象。从 1.14 开始，<code>kubectl</code>支持使用 kustomization 文件管理 Kubernetes 对象。您可以通过<code>kustomization.yaml</code>中的生成器创建一个 Secret。</p>
<p>通过以下命令在<code>kustomization.yaml</code>中添加一个 Secret 生成器。您需要用您要使用的密码替换<code>YOUR_PASSWORD</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">secretGenerator:
</span><span style="color:#b44">- name: mysql-pass
</span><span style="color:#b44">  literals:
</span><span style="color:#b44">  - password=YOUR_PASSWORD
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
## Add resource configs for MySQL and WordPress
-->
<h2 id="补充-mysql-和-wordpress-的资源配置">补充 MySQL 和 WordPress 的资源配置</h2>
<!--
The following manifest describes a single-instance MySQL Deployment. The MySQL container mounts the PersistentVolume at /var/lib/mysql. The `MYSQL_ROOT_PASSWORD` environment variable sets the database password from the Secret.
-->
<p>以下 manifest 文件描述了单实例 MySQL 部署。MySQL 容器将 PersistentVolume 挂载在<code>/var/lib/mysql</code>。 <code>MYSQL_ROOT_PASSWORD</code>环境变量设置来自 Secret 的数据库密码。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/wordpress/mysql-deployment.yaml" download="application/wordpress/mysql-deployment.yaml"><code>application/wordpress/mysql-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-wordpress-mysql-deployment-yaml')" title="Copy application/wordpress/mysql-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-wordpress-mysql-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress-mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">3306</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>20Gi<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress-mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Recreate<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mysql:5.6<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MYSQL_ROOT_PASSWORD<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">secretKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-pass<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>password<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">3306</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-persistent-storage<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/mysql<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-persistent-storage<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>mysql-pv-claim<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
The following manifest describes a single-instance WordPress Deployment. The WordPress container mounts the
PersistentVolume at `/var/www/html` for website data files. The `WORDPRESS_DB_HOST` environment variable sets
the name of the MySQL Service defined above, and WordPress will access the database by Service. The
`WORDPRESS_DB_PASSWORD` environment variable sets the database password from the Secret kustomize generated.
-->
<p>以下 manifest 文件描述了单实例 WordPress 部署。WordPress 容器将网站数据文件位于<code>/var/www/html</code>的 PersistentVolume。<code>WORDPRESS_DB_HOST</code>环境变量集上面定义的 MySQL Service 的名称，WordPress 将通过 Service 访问数据库。<code>WORDPRESS_DB_PASSWORD</code>环境变量设置从 Secret kustomize 生成的数据库密码。


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/wordpress/wordpress-deployment.yaml" download="application/wordpress/wordpress-deployment.yaml"><code>application/wordpress/wordpress-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-wordpress-wordpress-deployment-yaml')" title="Copy application/wordpress/wordpress-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-wordpress-wordpress-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wp-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>20Gi<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Recreate<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>wordpress:4.8-apache<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>WORDPRESS_DB_HOST<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>wordpress-mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>WORDPRESS_DB_PASSWORD<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">secretKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-pass<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>password<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress-persistent-storage<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/www/html<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress-persistent-storage<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>wp-pv-claim<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>

</p>
<!--
1. Download the MySQL deployment configuration file.

      ```shell
      curl -LO https://k8s.io/examples/application/wordpress/mysql-deployment.yaml
      ```

2. Download the WordPress configuration file.

      ```shell
      curl -LO https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml
      ```

3. Add them to `kustomization.yaml` file.

      ```shell
      cat <<EOF >>./kustomization.yaml
      resources:
        - mysql-deployment.yaml
        - wordpress-deployment.yaml
      EOF
      ```
-->
<ol>
<li>
<p>下载 MySQL deployment 配置文件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -LO https://k8s.io/examples/application/wordpress/mysql-deployment.yaml
</code></pre></div></li>
<li>
<p>下载 WordPress 配置文件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -LO https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml
</code></pre></div></li>
<li>
<p>补充到 <code>kustomization.yaml</code> 文件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt;&gt;./kustomization.yaml
</span><span style="color:#b44">resources:
</span><span style="color:#b44">  - mysql-deployment.yaml
</span><span style="color:#b44">  - wordpress-deployment.yaml
</span><span style="color:#b44">EOF</span>
</code></pre></div></li>
</ol>
<!--
## Apply and Verify
-->
<h2 id="应用和验证">应用和验证</h2>
<!--
The `kustomization.yaml` contains all the resources for deploying a WordPress site and a
MySQL database. You can apply the directory by
```shell
kubectl apply -k ./
```

Now you can verify that all objects exist.

1. Verify that the Secret exists by running the following command:

      ```shell
      kubectl get secrets
      ```

      The response should be like this:

      ```shell
      NAME                    TYPE                                  DATA   AGE
      mysql-pass-c57bb4t7mf   Opaque                                1      9s
      ```

2. Verify that a PersistentVolume got dynamically provisioned.

      ```shell
      kubectl get pvc
      ```

      <div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> It can take up to a few minutes for the PVs to be provisioned and bound.
</div>

      The response should be like this:

      ```shell
      NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
      mysql-pv-claim   Bound     pvc-8cbd7b2e-4044-11e9-b2bb-42010a800002   20Gi       RWO            standard           77s
      wp-pv-claim      Bound     pvc-8cd0df54-4044-11e9-b2bb-42010a800002   20Gi       RWO            standard           77s
      ```

3. Verify that the Pod is running by running the following command:

      ```shell
      kubectl get pods
      ```

      <div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> It can take up to a few minutes for the Pod's Status to be <code>RUNNING</code>.
</div>

      The response should be like this:

      ```
      NAME                               READY     STATUS    RESTARTS   AGE
      wordpress-mysql-1894417608-x5dzt   1/1       Running   0          40s
      ```

4. Verify that the Service is running by running the following command:

      ```shell
      kubectl get services wordpress
      ```

      The response should be like this:

      ```
      NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
      wordpress   ClusterIP   10.0.0.89    <pending>     80:32406/TCP   4m
      ```

      <div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Minikube can only expose Services through <code>NodePort</code>. The EXTERNAL-IP is always pending.
</div>

5. Run the following command to get the IP Address for the WordPress Service:

      ```shell
      minikube service wordpress --url
      ```

      The response should be like this:

      ```
      http://1.2.3.4:32406
      ```

6. Copy the IP address, and load the page in your browser to view your site.

   You should see the WordPress set up page similar to the following screenshot.

   ![wordpress-init](https://raw.githubusercontent.com/kubernetes/examples/master/mysql-wordpress-pd/WordPress.png)
-->
<p><code>kustomization.yaml</code>包含用于部署 WordPress 网站的所有资源以及 MySQL 数据库。您可以通过以下方式应用目录</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -k ./
</code></pre></div><p>现在，您可以验证所有对象是否存在。</p>
<ol>
<li>
<p>通过运行以下命令验证 Secret 是否存在：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secrets
</code></pre></div><p>响应应如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                    TYPE                                  DATA   AGE
mysql-pass-c57bb4t7mf   Opaque                                <span style="color:#666">1</span>      9s
</code></pre></div></li>
<li>
<p>验证是否已动态配置 PersistentVolume：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pvc
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 设置和绑定 PV 可能要花费几分钟。
</div>
<p>响应应如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
mysql-pv-claim   Bound     pvc-8cbd7b2e-4044-11e9-b2bb-42010a800002   20Gi       RWO            standard           77s
wp-pv-claim      Bound     pvc-8cd0df54-4044-11e9-b2bb-42010a800002   20Gi       RWO            standard           77s
</code></pre></div></li>
<li>
<p>通过运行以下命令来验证 Pod 是否正在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 等待 Pod 状态变成<code>RUNNING</code>可能会花费几分钟。
</div>
<p>响应应如下所示：</p>
<pre><code>NAME                               READY     STATUS    RESTARTS   AGE
wordpress-mysql-1894417608-x5dzt   1/1       Running   0          40s
</code></pre></li>
<li>
<p>通过运行以下命令来验证 Service 是否正在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get services wordpress
</code></pre></div><p>响应应如下所示：</p>
<pre><code>NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
wordpress   ClusterIP   10.0.0.89    &lt;pending&gt;     80:32406/TCP   4m
</code></pre><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Minikube 只能通过 NodePort 公开服务。EXTERNAL-IP 始终处于挂起状态
</div>
</li>
<li>
<p>运行以下命令以获取 WordPress 服务的 IP 地址：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube service wordpress --url
</code></pre></div><p>响应应如下所示：</p>
<pre><code>http://1.2.3.4:32406
</code></pre></li>
<li>
<p>复制 IP 地址，然后将页面加载到浏览器中来查看您的站点。</p>
<p>您应该看到类似于以下屏幕截图的 WordPress 设置页面。</p>
<p><img src="https://raw.githubusercontent.com/kubernetes/examples/master/mysql-wordpress-pd/WordPress.png" alt="wordpress-init"></p>
</li>
</ol>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
Do not leave your WordPress installation on this page. If another user finds it, they can set up a website on your instance and use it to serve malicious content. <br/><br/>Either install WordPress by creating a username and password or delete your instance.
-->
<p>不要在此页面上保留 WordPress 安装。如果其他用户找到了它，他们可以在您的实例上建立一个网站并使用它来提供恶意内容。<br/><br/>通过创建用户名和密码来安装 WordPress 或删除您的实例。
</div>


<h2 id="cleaning-up">Cleaning up</h2>
<!--
1. Run the following command to delete your Secret, Deployments, Services and PersistentVolumeClaims:

      ```shell
      kubectl delete -k ./
      ```
-->
<ol>
<li>
<p>运行一下命令删除您的 Secret，Deployments，Services and PersistentVolumeClaims：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete -k ./
</code></pre></div></li>
</ol>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [Introspection and Debugging](/docs/tasks/debug/debug-application)
* Learn more about [Jobs](/docs/concepts/workloads/controllers/jobs-run-to-completion/)
* Learn more about [Port Forwarding](/docs/tasks/access-application-cluster/port-forward-access-application-cluster/)
* Learn how to [Get a Shell to a Container](/docs/tasks/debug/debug-application/get-shell-running-container/)
-->
<ul>
<li>了解更多关于 <a href="/zh/docs/tasks/debug/debug-application">Introspection and Debugging</a></li>
<li>了解更多关于 <a href="/zh/docs/concepts/workloads/controllers/jobs-run-to-completion/">Jobs</a></li>
<li>了解更多关于 <a href="/zh/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">Port Forwarding</a></li>
<li>了解如何 <a href="/zh/docs/tasks/debug/debug-application/get-shell-running-container/">Get a Shell to a Container</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bf0d8e08fddd6e0282709b9fef8b5f67">6.2 - 示例：使用 StatefulSet 部署 Cassandra</h1>
    
	<!--
title: "Example: Deploying Cassandra with a StatefulSet"
reviewers:
- ahmetb
content_type: tutorial
weight: 30
-->
<!-- overview -->
<!--
This tutorial shows you how to run [Apache Cassandra](https://cassandra.apache.org/) on Kubernetes.
Cassandra, a database, needs persistent storage to provide data durability (application _state_).
In this example, a custom Cassandra seed provider lets the database discover new Cassandra instances as they join the Cassandra cluster.
-->
<p>本教程描述拉如何在 Kubernetes 上运行 <a href="https://cassandra.apache.org/">Apache Cassandra</a>。
数据库 Cassandra 需要永久性存储提供数据持久性（应用“状态”）。
在此示例中，自定义 Cassandra seed provider 使数据库在加入 Cassandra
集群时发现新的 Cassandra 实例。</p>
<!--
*StatefulSets* make it easier to deploy stateful applications into your Kubernetes cluster.
For more information on the features used in this tutorial, see
[StatefulSet](/docs/concepts/workloads/controllers/statefulset/).
-->
<p>使用&quot;StatefulSets&quot;可以更轻松地将有状态的应用程序部署到你的 Kubernetes 集群中。
有关本教程中使用的功能的更多信息，
参阅 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Cassandra and Kubernetes both use the term _node_ to mean a member of a cluster. In this
tutorial, the Pods that belong to the StatefulSet are Cassandra nodes and are members
of the Cassandra cluster (called a _ring_). When those Pods run in your Kubernetes cluster,
the Kubernetes control plane schedules those Pods onto Kubernetes
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='Nodes'>Nodes</a>.
-->
<p>Cassandra 和 Kubernetes 都使用术语“节点（node）”来表示集群的成员。
在本教程中，属于 StatefulSet 的 Pod 是 Cassandra 节点，并且是 Cassandra 集群的成员（称为 “ring”）。
当这些 Pod 在你的 Kubernetes 集群中运行时，Kubernetes 控制平面会将这些 Pod 调度到 Kubernetes 的
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>上。</p>
<!--
When a Cassandra node starts, it uses a _seed list_ to bootstrap discovery of other
nodes in the ring.
This tutorial deploys a custom Cassandra seed provider that lets the database discover
new Cassandra Pods as they appear inside your Kubernetes cluster.
-->
<p>当 Cassandra 节点启动时，使用 <em>seed列表</em> 来引导发现 ring 中其他节点。
本教程部署了一个自定义的 Cassandra seed provider，使数据库可以发现新的 Cassandra Pod
出现在 Kubernetes 集群中。</p>

</div>
<h2 id="objectives">Objectives</h2>
<!--
* Create and validate a Cassandra headless <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>.
* Use a <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a> to create a Cassandra ring.
* Validate the StatefulSet.
* Modify the StatefulSet.
* Delete the StatefulSet and its <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>.
-->
<ul>
<li>创建并验证 Cassandra 无头（headless）<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>..</li>
<li>使用 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a> 创建一个 Cassandra ring。</li>
<li>验证 StatefulSet。</li>
<li>修改 StatefulSet。</li>
<li>删除 StatefulSet 及其 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>.</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!--
To complete this tutorial, you should already have a basic familiarity with
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>,
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a>, and
<a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSets'>StatefulSets</a>.
-->
<p>要完成本教程，你应该已经熟悉 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>，
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> 和
<a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>。</p>
<!--
### Additional Minikube setup instructions

<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p><a href="https://minikube.sigs.k8s.io/docs/">Minikube</a> defaults to 2048MB of memory and 2 CPU.
Running Minikube with the default resource configuration results in insufficient resource
errors during this tutorial. To avoid these errors, start Minikube with the following settings:
--&gt;</p>
<h3 id="额外的-minikube-设置说明">额外的 Minikube 设置说明</h3>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p><a href="https://minikube.sigs.k8s.io/docs/">Minikube</a>默认为 2048MB 内存和 2 个 CPU。
在本教程中，使用默认资源配置运行 Minikube 会导致资源不足的错误。为避免这些错误，请使用以下设置启动 Minikube：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube start --memory <span style="color:#666">5120</span> --cpus<span style="color:#666">=</span><span style="color:#666">4</span>
</code></pre></div>
</div>
<!-- lessoncontent -->
<!--
## Creating a headless Service for Cassandra {#creating-a-cassandra-headless-service}

In Kubernetes, a <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> describes a set of
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> that perform the same task.

The following Service is used for DNS lookups between Cassandra Pods and clients within your cluster:

Create a Service to track all Cassandra StatefulSet members from the `cassandra-service.yaml` file:
-->
<h2 id="creating-a-cassandra-headless-service">为 Cassandra 创建无头（headless） Services</h2>
<p>在 Kubernetes 中，一个 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>
描述了一组执行相同任务的 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>。</p>
<p>以下 Service 用于在 Cassandra Pod 和集群中的客户端之间进行 DNS 查找：</p>
<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/cassandra/cassandra-service.yaml" download="application/cassandra/cassandra-service.yaml"><code>application/cassandra/cassandra-service.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-cassandra-cassandra-service-yaml')" title="Copy application/cassandra/cassandra-service.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-cassandra-cassandra-service-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">9042</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>
<p>创建一个 Service 来跟踪 <code>cassandra-service.yaml</code> 文件中的所有 Cassandra StatefulSet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-service.yaml
</code></pre></div><!--
### Validating (optional) {#validating}

Get the Cassandra Service.
-->
<h3 id="validating">验证(可选)</h3>
<p>获取 Cassandra Service。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc cassandra
</code></pre></div><!-- 
The response is 
-->
<p>响应是：</p>
<pre><code>NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
cassandra   ClusterIP   None         &lt;none&gt;        9042/TCP   45s
</code></pre><!--
If you don't see a Service named `cassandra`, that means creation failed. Read
[Debug Services](/docs/tasks/debug/debug-application/debug-service/)
for help troubleshooting common issues.
-->
<p>如果没有看到名为 <code>cassandra</code> 的服务，则表示创建失败。
请阅读<a href="/zh/docs/tasks/debug/debug-application/debug-service/">调试服务</a>，以解决常见问题。</p>
<!--
## Using a StatefulSet to create a Cassandra ring

The StatefulSet manifest, included below, creates a Cassandra ring that consists of three Pods.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> This example uses the default provisioner for Minikube.
Please update the following StatefulSet for the cloud you are working with.
</div>
-->
<h2 id="使用-statefulset-创建-cassandra-ring">使用 StatefulSet 创建 Cassandra Ring</h2>
<p>下面包含的 StatefulSet 清单创建了一个由三个 Pod 组成的 Cassandra ring。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 本示例使用 Minikube 的默认配置程序。
请为正在使用的云更新以下 StatefulSet。
</div>
<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/cassandra/cassandra-statefulset.yaml" download="application/cassandra/cassandra-statefulset.yaml"><code>application/cassandra/cassandra-statefulset.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-cassandra-cassandra-statefulset-yaml')" title="Copy application/cassandra/cassandra-statefulset.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-cassandra-cassandra-statefulset-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">1800</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/cassandra:v13<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">7000</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>intra-node<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">7001</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>tls-intra-node<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">7199</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>jmx<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9042</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">capabilities</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">add</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- IPC_LOCK<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">lifecycle</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">preStop</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> 
</span><span style="color:#bbb">              </span>- /bin/sh<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- nodetool drain<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MAX_HEAP_SIZE<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>512M<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>HEAP_NEWSIZE<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>100M<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>CASSANDRA_SEEDS<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;cassandra-0.cassandra.default.svc.cluster.local&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>CASSANDRA_CLUSTER_NAME<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;K8Demo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>CASSANDRA_DC<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;DC1-K8Demo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>CASSANDRA_RACK<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Rack1-K8Demo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>POD_IP<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>status.podIP<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- /bin/bash<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- /ready-probe.sh<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># These volume mounts are persistent. They are like inline claims,</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># but not exactly because the names need to match exactly one of</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># the stateful pod volumes.</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra-data<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/cassandra_data<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># These are converted to volume claims by the controller</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># and mounted at the paths mentioned above.</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># do not use these in production until ssd GCEPersistentDisk or other ssd pd</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra-data<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>k8s.io/minikube-hostpath<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>pd-ssd<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>
<!--
Create the Cassandra StatefulSet from the `cassandra-statefulset.yaml` file:
-->
<p>使用 <code>cassandra-statefulset.yaml</code> 文件创建 Cassandra StatefulSet ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 如果你能未经修改地 apply cassandra-statefulset.yaml，请使用此命令</span>
kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml
</code></pre></div><!--
If you need to modify `cassandra-statefulset.yaml` to suit your cluster, download
https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml and then apply
that manifest, from the folder you saved the modified version into:
-->
<p>如果你为了适合你的集群需要修改 <code>cassandra-statefulset.yaml</code>，
下载 <a href="https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml">https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml</a>，
然后 apply 修改后的清单。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 如果使用本地的 cassandra-statefulset.yaml ，请使用此命令</span>
kubectl apply -f cassandra-statefulset.yaml
</code></pre></div><!--
## Validating the Cassandra StatefulSet

1. Get the Cassandra StatefulSet:
-->
<h2 id="验证-cassandra-statefulset">验证 Cassandra StatefulSet</h2>
<ol>
<li>
<p>获取 Cassandra StatefulSet:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get statefulset cassandra
</code></pre></div><!--
The response should be similar to:
-->
<p>响应应该与此类似：</p>
<pre><code>NAME        DESIRED   CURRENT   AGE
cassandra   3         0         13s
</code></pre><!--
The `StatefulSet` resource deploys Pods sequentially.
-->
<p><code>StatefulSet</code> 资源会按顺序部署 Pod。</p>
</li>
</ol>
<!--
1. Get the Pods to see the ordered creation status:
-->
<ol start="2">
<li>
<p>获取 Pod 查看已排序的创建状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l<span style="color:#666">=</span><span style="color:#b44">&#34;app=cassandra&#34;</span>
</code></pre></div><!--
The response should be similar to:
-->
<p>响应应该与此类似：</p>
<pre><code>NAME          READY     STATUS              RESTARTS   AGE
cassandra-0   1/1       Running             0          1m
cassandra-1   0/1       ContainerCreating   0          8s
</code></pre><!--
It can take several minutes for all three Pods to deploy. Once they are deployed, the same command
returns output similar to:
-->
<p>这三个 Pod 要花几分钟的时间才能部署。部署之后，相同的命令将返回类似于以下的输出：</p>
<pre><code>NAME          READY     STATUS    RESTARTS   AGE
cassandra-0   1/1       Running   0          10m
cassandra-1   1/1       Running   0          9m
cassandra-2   1/1       Running   0          8m
</code></pre></li>
</ol>
<!--
3. Run the Cassandra [nodetool](https://cwiki.apache.org/confluence/display/CASSANDRA2/NodeTool) inside the first Pod, to
   display the status of the ring.
-->
<ol start="3">
<li>
<p>运行第一个 Pod 中的 Cassandra <a href="https://cwiki.apache.org/confluence/display/CASSANDRA2/NodeTool">nodetool</a>，
以显示 ring 的状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it cassandra-0 -- nodetool status
</code></pre></div><!--
The response should be similar to:
-->
<p>响应应该与此类似：</p>
<pre><code>Datacenter: DC1-K8Demo
======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address     Load       Tokens       Owns (effective)  Host ID                               Rack
UN  172.17.0.5  83.57 KiB  32           74.0%             e2dd09e6-d9d3-477e-96c5-45094c08db0f  Rack1-K8Demo
UN  172.17.0.4  101.04 KiB  32           58.8%             f89d6835-3a42-4419-92b3-0e62cae1479c  Rack1-K8Demo
UN  172.17.0.6  84.74 KiB  32           67.1%             a6a1e8c2-3dc5-4417-b1a0-26507af2aaad  Rack1-K8Demo
</code></pre></li>
</ol>
<!--
## Modifying the Cassandra StatefulSet

Use `kubectl edit` to modify the size of a Cassandra StatefulSet.

1. Run the following command:
-->
<h2 id="修改-cassandra-statefulset">修改 Cassandra StatefulSet</h2>
<p>使用 <code>kubectl edit</code> 修改 Cassandra StatefulSet 的大小。</p>
<ol>
<li>
<p>运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit statefulset cassandra
</code></pre></div><!--
This command opens an editor in your terminal. The line you need to change is the `replicas` field.
The following sample is an excerpt of the StatefulSet file:
-->
<p>此命令你的终端中打开一个编辑器。需要更改的是 <code>replicas</code> 字段。下面是 StatefulSet 文件的片段示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># and an empty file will abort the edit. If an error occurs while saving this file will be</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># reopened with the relevant failures.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2016-08-13T18:40:58Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">generation</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;323&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>7a219483-6185-11e6-a910-42010a8a0fc0<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
1. Change the number of replicas to 4, and then save the manifest.

   The StatefulSet now scales to run with 4 Pods.

1. Get the Cassandra StatefulSet to verify your change:
-->
<ol start="2">
<li>
<p>将副本数（replicas）更改为 4，然后保存清单。</p>
<p>StatefulSet 现在可以扩展到运行 4 个 Pod。</p>
</li>
<li>
<p>获取 Cassandra StatefulSet 验证更改：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get statefulset cassandra
</code></pre></div><!--
The response should be similar to:
-->
<p>响应应该与此类似：</p>
<pre><code>NAME        DESIRED   CURRENT   AGE
cassandra   4         4         36m
</code></pre></li>
</ol>
<h2 id="cleaning-up">Cleaning up</h2>
<!--
Deleting or scaling a StatefulSet down does not delete the volumes associated with the StatefulSet.
This setting is for your safety because your data is more valuable than automatically purging all related StatefulSet resources.
-->
<p>删除或缩小 StatefulSet 不会删除与 StatefulSet 关联的卷。
这个设置是出于安全考虑，因为你的数据比自动清除所有相关的 StatefulSet 资源更有价值。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
Depending on the storage class and reclaim policy, deleting the *PersistentVolumeClaims* may cause the associated volumes
to also be deleted. Never assume you'll be able to access data if its volume claims are deleted.
-->
<p>根据存储类和回收策略，删除 <em>PersistentVolumeClaims</em> 可能导致关联的卷也被删除。
千万不要认为其容量声明被删除，你就能访问数据。
</div>
<!--
1. Run the following commands (chained together into a single command) to delete everything in the Cassandra StatefulSet:
-->
<ol>
<li>
<p>运行以下命令（连在一起成为一个单独的命令）删除 Cassandra StatefulSet 中的所有内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">grace</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get pod cassandra-0 -o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.spec.terminationGracePeriodSeconds}&#39;</span><span style="color:#a2f;font-weight:bold">)</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#666">&amp;&amp;</span> kubectl delete statefulset -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>cassandra <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#666">&amp;&amp;</span> <span style="color:#a2f">echo</span> <span style="color:#b44">&#34;Sleeping </span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">grace</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44"> seconds&#34;</span> 1&gt;&amp;<span style="color:#666">2</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#666">&amp;&amp;</span> sleep <span style="color:#b8860b">$grace</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#666">&amp;&amp;</span> kubectl delete persistentvolumeclaim -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>cassandra
</code></pre></div></li>
</ol>
<!--
1. Run the following command to delete the Service you set up for Cassandra:
-->
<ol start="2">
<li>
<p>运行以下命令，删除你为 Cassandra 设置的 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete service -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>cassandra
</code></pre></div></li>
</ol>
<!--
## Cassandra container environment variables

The Pods in this tutorial use the [`gcr.io/google-samples/cassandra:v13`](https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile)
image from Google's [container registry](https://cloud.google.com/container-registry/docs/).
The Docker image above is based on [debian-base](https://github.com/kubernetes/release/tree/master/images/build/debian-base)
and includes OpenJDK 8.

This image includes a standard Cassandra installation from the Apache Debian repo.
By using environment variables you can change values that are inserted into `cassandra.yaml`.
-->
<h2 id="cassandra-容器环境变量">Cassandra 容器环境变量</h2>
<p>本教程中的 Pod 使用来自 Google <a href="https://cloud.google.com/container-registry/docs/">容器镜像库</a>
的 <a href="https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile"><code>gcr.io/google-samples/cassandra:v13</code></a>
镜像。上面的 Docker 镜像基于 <a href="https://github.com/kubernetes/release/tree/master/images/build/debian-base">debian-base</a>，
并且包含 OpenJDK 8。</p>
<p>该映像包括来自 Apache Debian 存储库的标准 Cassandra 安装。
通过使用环境变量，您可以更改插入到 <code>cassandra.yaml</code> 中的值。</p>
<table>
<thead>
<tr>
<th>环境变量</th>
<th style="text-align:center">默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CASSANDRA_CLUSTER_NAME</code></td>
<td style="text-align:center"><code>'Test Cluster'</code></td>
</tr>
<tr>
<td><code>CASSANDRA_NUM_TOKENS</code></td>
<td style="text-align:center"><code>32</code></td>
</tr>
<tr>
<td><code>CASSANDRA_RPC_ADDRESS</code></td>
<td style="text-align:center"><code>0.0.0.0</code></td>
</tr>
</tbody>
</table>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn how to [Scale a StatefulSet](/docs/tasks/run-application/scale-stateful-set/).
* Learn more about the [*KubernetesSeedProvider*](https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java)
* See more custom [Seed Provider Configurations](https://git.k8s.io/examples/cassandra/java/README.md)
-->
<ul>
<li>了解如何<a href="/docs/tasks/run-application/scale-stateful-set/">扩缩 StatefulSet</a>。</li>
<li>了解有关 <a href="https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java"><em>KubernetesSeedProvider</em></a> 的更多信息</li>
<li>查看更多自定义 <a href="https://git.k8s.io/examples/cassandra/java/README.md">Seed Provider Configurations</a></li>
</ul>

</div>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4bfac214b5eb9ebddaf1f3811901d327">6.3 - 运行 ZooKeeper，一个分布式协调系统</h1>
    
	<!--
reviewers:
- bprashanth
- enisoc
- erictune
- foxish
- janetkuo
- kow3ns
- smarterclayton
title: Running ZooKeeper, A Distributed System Coordinator
content_type: tutorial
weight: 40
-->
<!-- overview -->
<!--
This tutorial demonstrates running [Apache Zookeeper](https://zookeeper.apache.org) on
Kubernetes using [StatefulSets](/docs/concepts/workloads/controllers/statefulset/),
[PodDisruptionBudgets](/docs/concepts/workloads/pods/disruptions/#pod-disruption-budget),
and [PodAntiAffinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity).
-->
<p>本教程展示了在 Kubernetes 上使用
<a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>，
<a href="/zh/docs/concepts/workloads/pods/disruptions/#pod-disruption-budget">PodDisruptionBudget</a> 和
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#%E4%BA%B2%E5%92%8C%E4%B8%8E%E5%8F%8D%E4%BA%B2%E5%92%8C">PodAntiAffinity</a>
特性运行 <a href="https://zookeeper.apache.org">Apache Zookeeper</a>。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
Before starting this tutorial, you should be familiar with the following
Kubernetes concepts.
-->
<p>在开始本教程前，你应该熟悉以下 Kubernetes 概念。</p>
<ul>
<li><a href="/zh/docs/concepts/workloads/pods/">Pods</a></li>
<li><a href="/zh/docs/concepts/services-networking/dns-pod-service/">集群 DNS</a></li>
<li><a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务（Headless Service）</a></li>
<li><a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/main/staging/persistent-volume-provisioning/">PersistentVolume 制备</a></li>
<li><a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a></li>
<li><a href="/zh/docs/concepts/workloads/pods/disruptions/#pod-disruption-budget">PodDisruptionBudget</a></li>
<li><a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#%E4%BA%B2%E5%92%8C%E4%B8%8E%E5%8F%8D%E4%BA%B2%E5%92%8C">PodAntiAffinity</a></li>
<li><a href="/zh/docs/reference/kubectl/kubectl/">kubectl CLI</a></li>
</ul>
<!--
You must have a cluster with at least four nodes, and each node requires at least 2 CPUs and 4 GiB of memory. In this tutorial you will cordon and drain the cluster's nodes. **This means that the cluster will terminate and evict all Pods on its nodes, and the nodes will temporarily become unschedulable.** You should use a dedicated cluster for this tutorial, or you should ensure that the disruption you cause will not interfere with other tenants.
-->
<p>你需要一个至少包含四个节点的集群，每个节点至少 2 CPUs 和 4 GiB 内存。
在本教程中你将会隔离（Cordon）和腾空（Drain ）集群的节点。
<strong>这意味着集群节点上所有的 Pods 将会被终止并移除。这些节点也会暂时变为不可调度</strong>。
在本教程中你应该使用一个独占的集群，或者保证你造成的干扰不会影响其它租户。</p>
<!--
This tutorial assumes that you have configured your cluster to dynamically provision
PersistentVolumes. If your cluster is not configured to do so, you
will have to manually provision three 20 GiB volumes before starting this
tutorial.
-->
<p>本教程假设你的集群配置为动态的提供 PersistentVolumes。
如果你的集群没有配置成这样，在开始本教程前，你需要手动准备三个 20 GiB 的卷。</p>
<h2 id="objectives">Objectives</h2>
<!--
After this tutorial, you will know the following.

- How to deploy a ZooKeeper ensemble using StatefulSet.
- How to consistently configure the ensemble.
- How to spread the deployment of ZooKeeper servers in the ensemble.
- How to use PodDisruptionBudgets to ensure service availability during planned maintenance.
-->
<p>在学习本教程后，你将熟悉下列内容。</p>
<ul>
<li>如何使用 StatefulSet 部署一个 ZooKeeper ensemble。</li>
<li>如何一致性配置 ensemble。</li>
<li>如何在 ensemble 中 分布 ZooKeeper 服务器的部署。</li>
<li>如何在计划维护中使用 PodDisruptionBudgets 确保服务可用性。</li>
</ul>
<!-- lessoncontent -->
<!-- 
### ZooKeeper

[Apache ZooKeeper](https://zookeeper.apache.org/doc/current/) is a
distributed, open-source coordination service for distributed applications.
ZooKeeper allows you to read, write, and observe updates to data. Data are
organized in a file system like hierarchy and replicated to all ZooKeeper
servers in the ensemble (a set of ZooKeeper servers). All operations on data
are atomic and sequentially consistent. ZooKeeper ensures this by using the
[Zab](https://pdfs.semanticscholar.org/b02c/6b00bd5dbdbd951fddb00b906c82fa80f0b3.pdf)
consensus protocol to replicate a state machine across all servers in the ensemble.
-->
<h3 id="zookeeper-basics">ZooKeeper  </h3>
<p><a href="https://zookeeper.apache.org/doc/current/">Apache ZooKeeper</a>
是一个分布式的开源协调服务，用于分布式系统。
ZooKeeper 允许你读取、写入数据和发现数据更新。
数据按层次结构组织在文件系统中，并复制到 ensemble（一个 ZooKeeper 服务器的集合）
中所有的 ZooKeeper 服务器。对数据的所有操作都是原子的和顺序一致的。
ZooKeeper 通过
<a href="https://pdfs.semanticscholar.org/b02c/6b00bd5dbdbd951fddb00b906c82fa80f0b3.pdf">Zab</a>
一致性协议在 ensemble 的所有服务器之间复制一个状态机来确保这个特性。</p>
<!--
The ensemble uses the Zab protocol to elect a leader, and the ensemble cannot write data until that election is complete. Once complete, the ensemble uses Zab to ensure that it replicates all writes to a quorum before it acknowledges and makes them visible to clients. Without respect to weighted quorums, a quorum is a majority component of the ensemble containing the current leader. For instance, if the ensemble has three servers, a component that contains the leader and one other server constitutes a quorum. If the ensemble can not achieve a quorum, the ensemble cannot write data.
-->
<p>Ensemble 使用 Zab 协议选举一个领导者，在选举出领导者前不能写入数据。
一旦选举出了领导者，ensemble 使用 Zab 保证所有写入被复制到一个 quorum，
然后这些写入操作才会被确认并对客户端可用。
如果没有遵照加权 quorums，一个 quorum 表示包含当前领导者的 ensemble 的多数成员。
例如，如果 ensemble 有 3 个服务器，一个包含领导者的成员和另一个服务器就组成了一个
quorum。
如果 ensemble 不能达成一个 quorum，数据将不能被写入。</p>
<!--
ZooKeeper servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on storage media. When a server crashes, it can recover its previous state by replaying the WAL. To prevent the WAL from growing without bound, ZooKeeper servers will periodically snapshot them in memory state to storage media. These snapshots can be loaded directly into memory, and all WAL entries that preceded the snapshot may be discarded.
-->
<p>ZooKeeper 在内存中保存它们的整个状态机，但是每个改变都被写入一个在存储介质上的
持久 WAL（Write Ahead Log）。
当一个服务器出现故障时，它能够通过回放 WAL 恢复之前的状态。
为了防止 WAL 无限制的增长，ZooKeeper 服务器会定期的将内存状态快照保存到存储介质。
这些快照能够直接加载到内存中，所有在这个快照之前的 WAL 条目都可以被安全的丢弃。</p>
<!--
## Creating a ZooKeeper Ensemble

The manifest below contains a
[Headless Service](/docs/concepts/services-networking/service/#headless-services),
a [Service](/docs/concepts/services-networking/service/),
a [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets),
and a [StatefulSet](/docs/concepts/workloads/controllers/statefulset/).
-->
<h2 id="创建一个-zookeeper-ensemble">创建一个 ZooKeeper Ensemble</h2>
<p>下面的清单包含一个
<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>，
一个 <a href="/zh/docs/concepts/services-networking/service/">Service</a>，
一个 <a href="/zh/docs/concepts/workloads/pods/disruptions/#specifying-a-poddisruptionbudget">PodDisruptionBudget</a>，
和一个 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/zookeeper/zookeeper.yaml" download="application/zookeeper/zookeeper.yaml"><code>application/zookeeper/zookeeper.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-zookeeper-zookeeper-yaml')" title="Copy application/zookeeper/zookeeper.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-zookeeper-zookeeper-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk-hs<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">2888</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>server<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">3888</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>leader-election<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk-cs<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">2181</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>client<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>policy/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PodDisruptionBudget<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk-pdb<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">maxUnavailable</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span>zk-hs<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">updateStrategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>RollingUpdate<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podManagementPolicy</span>:<span style="color:#bbb"> </span>OrderedReady<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">podAntiAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;app&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">                    </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">                    </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                    </span>- zk<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kubernetes-zookeeper<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0.5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">2181</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>client<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">2888</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>server<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">3888</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>leader-election<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#b44">&#34;start-zookeeper \
</span><span style="color:#b44">          --servers=3 \
</span><span style="color:#b44">          --data_dir=/var/lib/zookeeper/data \
</span><span style="color:#b44">          --data_log_dir=/var/lib/zookeeper/data/log \
</span><span style="color:#b44">          --conf_dir=/opt/zookeeper/conf \
</span><span style="color:#b44">          --client_port=2181 \
</span><span style="color:#b44">          --election_port=3888 \
</span><span style="color:#b44">          --server_port=2888 \
</span><span style="color:#b44">          --tick_time=2000 \
</span><span style="color:#b44">          --init_limit=10 \
</span><span style="color:#b44">          --sync_limit=5 \
</span><span style="color:#b44">          --heap=512M \
</span><span style="color:#b44">          --max_client_cnxns=60 \
</span><span style="color:#b44">          --snap_retain_count=3 \
</span><span style="color:#b44">          --purge_interval=12 \
</span><span style="color:#b44">          --max_session_timeout=40000 \
</span><span style="color:#b44">          --min_session_timeout=4000 \
</span><span style="color:#b44">          --log_level=INFO&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#b44">&#34;zookeeper-ready 2181&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#b44">&#34;zookeeper-ready 2181&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>datadir<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/zookeeper<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">runAsUser</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">fsGroup</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>datadir<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Open a terminal, and use the
[`kubectl apply`](/docs/reference/generated/kubectl/kubectl-commands/#apply) command to create the
manifest.
-->
<p>打开一个命令行终端，使用命令
<a href="/docs/reference/generated/kubectl/kubectl-commands/#apply"><code>kubectl apply</code></a>
创建这个清单。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/zookeeper/zookeeper.yaml
</code></pre></div><!--
This creates the `zk-hs` Headless Service, the `zk-cs` Service,
the `zk-pdb` PodDisruptionBudget, and the `zk` StatefulSet.
-->
<p>这个操作创建了 <code>zk-hs</code> 无头服务、<code>zk-cs</code> 服务、<code>zk-pdb</code> PodDisruptionBudget
和 <code>zk</code> StatefulSet。</p>
<pre><code>service/zk-hs created
service/zk-cs created
poddisruptionbudget.policy/zk-pdb created
statefulset.apps/zk created
</code></pre><!--
Use [`kubectl get`](/docs/reference/generated/kubectl/kubectl-commands/#get) to watch the
StatefulSet controller create the StatefulSet's Pods.
-->
<p>使用命令
<a href="/docs/reference/generated/kubectl/kubectl-commands/#get"><code>kubectl get</code></a>
查看 StatefulSet 控制器创建的 Pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
Once the `zk-2` Pod is Running and Ready, use `CTRL-C` to terminate kubectl.
-->
<p>一旦  <code>zk-2</code> Pod 变成 Running 和 Ready 状态，使用 <code>CRTL-C</code> 结束 kubectl。</p>
<pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      0/1       Pending   0          0s
zk-0      0/1       Pending   0         0s
zk-0      0/1       ContainerCreating   0         0s
zk-0      0/1       Running   0         19s
zk-0      1/1       Running   0         40s
zk-1      0/1       Pending   0         0s
zk-1      0/1       Pending   0         0s
zk-1      0/1       ContainerCreating   0         0s
zk-1      0/1       Running   0         18s
zk-1      1/1       Running   0         40s
zk-2      0/1       Pending   0         0s
zk-2      0/1       Pending   0         0s
zk-2      0/1       ContainerCreating   0         0s
zk-2      0/1       Running   0         19s
zk-2      1/1       Running   0         40s
</code></pre><!--
The StatefulSet controller creates three Pods, and each Pod has a container with
a [ZooKeeper](https://www-us.apache.org/dist/zookeeper/stable/) server.
-->
<p>StatefulSet 控制器创建 3 个 Pods，每个 Pod 包含一个
<a href="https://www-us.apache.org/dist/zookeeper/stable/">ZooKeeper</a> 服务容器。</p>
<!--
### Facilitating Leader Election

Because there is no terminating algorithm for electing a leader in an anonymous network, Zab requires explicit membership configuration to perform leader election. Each server in the ensemble needs to have a unique identifier, all servers need to know the global set of identifiers, and each identifier needs to be associated with a network address.

Use [`kubectl exec`](/docs/reference/generated/kubectl/kubectl-commands/#exec) to get the hostnames
of the Pods in the `zk` StatefulSet.
-->
<h3 id="facilitating-leader-election">促成 Leader 选举 </h3>
<p>由于在匿名网络中没有用于选举 leader 的终止算法，Zab 要求显式的进行成员关系配置，
以执行 leader 选举。Ensemble 中的每个服务器都需要具有一个独一无二的标识符，
所有的服务器均需要知道标识符的全集，并且每个标识符都需要和一个网络地址相关联。</p>
<p>使用命令
<a href="/docs/reference/generated/kubectl/kubectl-commands/#exec"><code>kubectl exec</code></a>
获取 <code>zk</code> StatefulSet 中 Pods 的主机名。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> zk-<span style="color:#b8860b">$i</span> -- hostname; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
The StatefulSet controller provides each Pod with a unique hostname based on its ordinal index. The hostnames take the form of `<statefulset name>-<ordinal index>`. Because the `replicas` field of the `zk` StatefulSet is set to `3`, the Set's controller creates three Pods with their hostnames set to `zk-0`, `zk-1`, and
`zk-2`.
-->
<p>StatefulSet 控制器基于每个 Pod 的序号索引为它们各自提供一个唯一的主机名。
主机名采用 <code>&lt;statefulset 名称&gt;-&lt;序数索引&gt;</code> 的形式。
由于 <code>zk</code> StatefulSet 的 <code>replicas</code> 字段设置为 3，这个集合的控制器将创建
3 个 Pods，主机名为：<code>zk-0</code>、<code>zk-1</code> 和 <code>zk-2</code>。</p>
<pre><code>zk-0
zk-1
zk-2
</code></pre><!--
The servers in a ZooKeeper ensemble use natural numbers as unique identifiers, and store each server's identifier in a file called `myid` in the server's data directory.

To examine the contents of the `myid` file for each server use the following command.
-->
<p>ZooKeeper ensemble 中的服务器使用自然数作为唯一标识符，
每个服务器的标识符都保存在服务器的数据目录中一个名为 <code>myid</code> 的文件里。</p>
<p>检查每个服务器的 <code>myid</code> 文件的内容。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> <span style="color:#a2f">echo</span> <span style="color:#b44">&#34;myid zk-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span>;kubectl <span style="color:#a2f">exec</span> zk-<span style="color:#b8860b">$i</span> -- cat /var/lib/zookeeper/data/myid; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
Because the identifiers are natural numbers and the ordinal indices are non-negative integers, you can generate an identifier by adding 1 to the ordinal.
-->
<p>由于标识符为自然数并且序号索引是非负整数，你可以在序号上加 1 来生成一个标识符。</p>
<pre><code>myid zk-0
1
myid zk-1
2
myid zk-2
3
</code></pre><!--
To get the Fully Qualified Domain Name (FQDN) of each Pod in the `zk` StatefulSet use the following command.
-->
<p>获取 <code>zk</code> StatefulSet 中每个 Pod 的全限定域名（Fully Qualified Domain Name，FQDN）。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> zk-<span style="color:#b8860b">$i</span> -- hostname -f; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
The `zk-hs` Service creates a domain for all of the Pods,
`zk-hs.default.svc.cluster.local`.
-->
<p><code>zk-hs</code> Service 为所有 Pods 创建了一个域：<code>zk-hs.default.svc.cluster.local</code>。</p>
<pre><code>zk-0.zk-hs.default.svc.cluster.local
zk-1.zk-hs.default.svc.cluster.local
zk-2.zk-hs.default.svc.cluster.local
</code></pre><!--
The A records in [Kubernetes DNS](/docs/concepts/services-networking/dns-pod-service/) resolve the FQDNs to the Pods' IP addresses. If Kubernetes reschedules the Pods, it will update the A records with the Pods' new IP addresses, but the A records names will not change.

ZooKeeper stores its application configuration in a file named `zoo.cfg`. Use `kubectl exec` to view the contents of the `zoo.cfg` file in the `zk-0` Pod.
-->
<p><a href="/zh/docs/concepts/services-networking/dns-pod-service/">Kubernetes DNS</a>
中的 A 记录将 FQDNs 解析成为 Pods 的 IP 地址。
如果 Pods 被调度，这个 A 记录将会使用 Pods 的新 IP 地址完成更新，
但 A 记录的名称不会改变。</p>
<p>ZooKeeper 在一个名为 <code>zoo.cfg</code> 的文件中保存它的应用配置。
使用 <code>kubectl exec</code> 在  <code>zk-0</code> Pod 中查看 <code>zoo.cfg</code> 文件的内容。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 -- cat /opt/zookeeper/conf/zoo.cfg
</code></pre></div><!--
In the `server.1`, `server.2`, and `server.3` properties at the bottom of
the file, the `1`, `2`, and `3` correspond to the identifiers in the
ZooKeeper servers' `myid` files. They are set to the FQDNs for the Pods in
the `zk` StatefulSet.
-->
<p>文件底部为 <code>server.1</code>、<code>server.2</code> 和 <code>server.3</code>，其中的 <code>1</code>、<code>2</code> 和 <code>3</code>
分别对应 ZooKeeper 服务器的 <code>myid</code> 文件中的标识符。
它们被设置为 <code>zk</code> StatefulSet 中的 Pods 的 FQDNs。</p>
<pre><code>clientPort=2181
dataDir=/var/lib/zookeeper/data
dataLogDir=/var/lib/zookeeper/log
tickTime=2000
initLimit=10
syncLimit=2000
maxClientCnxns=60
minSessionTimeout= 4000
maxSessionTimeout= 40000
autopurge.snapRetainCount=3
autopurge.purgeInterval=0
server.1=zk-0.zk-hs.default.svc.cluster.local:2888:3888
server.2=zk-1.zk-hs.default.svc.cluster.local:2888:3888
server.3=zk-2.zk-hs.default.svc.cluster.local:2888:3888
</code></pre><!--
### Achieving consensus

Consensus protocols require that the identifiers of each participant be unique. No two participants in the Zab protocol should claim the same unique identifier. This is necessary to allow the processes in the system to agree on which processes have committed which data. If two Pods are launched with the same ordinal, two ZooKeeper servers would both identify themselves as the same server.
-->
<h3 id="achieving-consensus">达成共识  </h3>
<p>一致性协议要求每个参与者的标识符唯一。
在 Zab 协议里任何两个参与者都不应该声明相同的唯一标识符。
对于让系统中的进程协商哪些进程已经提交了哪些数据而言，这是必须的。
如果有两个 Pods 使用相同的序号启动，这两个 ZooKeeper 服务器
会将自己识别为相同的服务器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      0/1       Pending   0          0s
zk-0      0/1       Pending   0         0s
zk-0      0/1       ContainerCreating   0         0s
zk-0      0/1       Running   0         19s
zk-0      1/1       Running   0         40s
zk-1      0/1       Pending   0         0s
zk-1      0/1       Pending   0         0s
zk-1      0/1       ContainerCreating   0         0s
zk-1      0/1       Running   0         18s
zk-1      1/1       Running   0         40s
zk-2      0/1       Pending   0         0s
zk-2      0/1       Pending   0         0s
zk-2      0/1       ContainerCreating   0         0s
zk-2      0/1       Running   0         19s
zk-2      1/1       Running   0         40s
</code></pre><!--
The A records for each Pod are entered when the Pod becomes Ready. Therefore,
the FQDNs of the ZooKeeper servers will resolve to a single endpoint, and that
endpoint will be the unique ZooKeeper server claiming the identity configured
in its `myid` file.
-->
<p>每个 Pod 的 A 记录仅在 Pod 变成 Ready状态时被录入。
因此，ZooKeeper 服务器的 FQDNs 只会解析到一个端点，而那个端点将会是
一个唯一的 ZooKeeper 服务器，这个服务器声明了配置在它的 <code>myid</code>
文件中的标识符。</p>
<pre><code>zk-0.zk-hs.default.svc.cluster.local
zk-1.zk-hs.default.svc.cluster.local
zk-2.zk-hs.default.svc.cluster.local
</code></pre><!--
This ensures that the `servers` properties in the ZooKeepers' `zoo.cfg` files
represents a correctly configured ensemble.
-->
<p>这保证了 ZooKeepers 的 <code>zoo.cfg</code> 文件中的 <code>servers</code> 属性代表了
一个正确配置的 ensemble。</p>
<pre><code>server.1=zk-0.zk-hs.default.svc.cluster.local:2888:3888
server.2=zk-1.zk-hs.default.svc.cluster.local:2888:3888
server.3=zk-2.zk-hs.default.svc.cluster.local:2888:3888
</code></pre><!--
When the servers use the Zab protocol to attempt to commit a value, they will either achieve consensus and commit the value (if leader election has succeeded and at least two of the Pods are Running and Ready), or they will fail to do so (if either of the conditions are not met). No state will arise where one server acknowledges a write on behalf of another.
-->
<p>当服务器使用 Zab 协议尝试提交一个值的时候，它们会达成一致并成功提交这个值
（如果领导者选举成功并且至少有两个 Pods 处于 Running 和 Ready状态），
或者将会失败（如果没有满足上述条件中的任意一条）。
当一个服务器承认另一个服务器的代写时不会有状态产生。</p>
<!--
### Sanity Testing the Ensemble

The most basic sanity test is to write data to one ZooKeeper server and
to read the data from another.

The command below executes the `zkCli.sh` script to write `world` to the path `/hello` on the `zk-0` Pod in the ensemble.
-->
<h3 id="ensemble-健康检查">Ensemble 健康检查</h3>
<p>最基本的健康检查是向一个 ZooKeeper 服务器写入一些数据，然后从
另一个服务器读取这些数据。</p>
<p>使用 <code>zkCli.sh</code> 脚本在 <code>zk-0</code> Pod 上写入 <code>world</code> 到路径 <code>/hello</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 zkCli.sh create /hello world
</code></pre></div><pre><code>WATCHER::

WatchedEvent state:SyncConnected type:None path:null
Created /hello
</code></pre><!--
To get the data from the `zk-1` Pod use the following command.
-->
<p>使用下面的命令从 <code>zk-1</code> Pod 获取数据。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-1 zkCli.sh get /hello
</code></pre></div><!--
The data that you created on `zk-0` is available on all the servers in the
ensemble.
-->
<p>你在 <code>zk-0</code> 上创建的数据在 ensemble 中所有的服务器上都是可用的。</p>
<pre><code>WATCHER::

WatchedEvent state:SyncConnected type:None path:null
world
cZxid = 0x100000002
ctime = Thu Dec 08 15:13:30 UTC 2016
mZxid = 0x100000002
mtime = Thu Dec 08 15:13:30 UTC 2016
pZxid = 0x100000002
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 5
numChildren = 0
</code></pre><!--
### Providing Durable Storage

As mentioned in the [ZooKeeper Basics](#zookeeper-basics) section,
ZooKeeper commits all entries to a durable WAL, and periodically writes snapshots
in memory state, to storage media. Using WALs to provide durability is a common
technique for applications that use consensus protocols to achieve a replicated
state machine.

Use the [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands/#delete) command to delete the
`zk` StatefulSet.
-->
<h3 id="提供持久存储">提供持久存储</h3>
<p>如同在 <a href="#zookeeper-basics">ZooKeeper</a> 一节所提到的，ZooKeeper 提交
所有的条目到一个持久 WAL，并周期性的将内存快照写入存储介质。
对于使用一致性协议实现一个复制状态机的应用来说，使用 WALs 提供持久化
是一种常用的技术，对于普通的存储应用也是如此。</p>
<p>使用 <a href="/docs/reference/generated/kubectl/kubectl-commands/#delete"><code>kubectl delete</code></a>
删除 <code>zk</code> StatefulSet。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete statefulset zk
</code></pre></div><pre><code>statefulset.apps &quot;zk&quot; deleted
</code></pre><!--
Watch the termination of the Pods in the StatefulSet.
-->
<p>观察 StatefulSet 中的 Pods 变为终止状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
When `zk-0` if fully terminated, use `CTRL-C` to terminate kubectl.
-->
<p>当 <code>zk-0</code> 完全终止时，使用 <code>CRTL-C</code> 结束 kubectl。</p>
<pre><code>zk-2      1/1       Terminating   0         9m
zk-0      1/1       Terminating   0         11m
zk-1      1/1       Terminating   0         10m
zk-2      0/1       Terminating   0         9m
zk-2      0/1       Terminating   0         9m
zk-2      0/1       Terminating   0         9m
zk-1      0/1       Terminating   0         10m
zk-1      0/1       Terminating   0         10m
zk-1      0/1       Terminating   0         10m
zk-0      0/1       Terminating   0         11m
zk-0      0/1       Terminating   0         11m
zk-0      0/1       Terminating   0         11m
</code></pre><!--
Reapply the manifest in `zookeeper.yaml`.
-->
<p>重新应用 <code>zookeeper.yaml</code> 中的清单。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/zookeeper/zookeeper.yaml
</code></pre></div><!--
This creates the `zk` StatefulSet object, but the other API objects in the manifest are not modified because they already exist.

Watch the StatefulSet controller recreate the StatefulSet's Pods.
-->
<p><code>zk</code> StatefulSet 将会被创建。由于清单中的其他 API 对象已经存在，所以它们不会被修改。</p>
<p>观察 StatefulSet 控制器重建 StatefulSet 的 Pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
Once the `zk-2` Pod is Running and Ready, use `CTRL-C` to terminate kubectl.
-->
<p>一旦 <code>zk-2</code> Pod 处于 Running 和 Ready 状态，使用 <code>CRTL-C</code> 停止 kubectl命令。</p>
<pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      0/1       Pending   0          0s
zk-0      0/1       Pending   0         0s
zk-0      0/1       ContainerCreating   0         0s
zk-0      0/1       Running   0         19s
zk-0      1/1       Running   0         40s
zk-1      0/1       Pending   0         0s
zk-1      0/1       Pending   0         0s
zk-1      0/1       ContainerCreating   0         0s
zk-1      0/1       Running   0         18s
zk-1      1/1       Running   0         40s
zk-2      0/1       Pending   0         0s
zk-2      0/1       Pending   0         0s
zk-2      0/1       ContainerCreating   0         0s
zk-2      0/1       Running   0         19s
zk-2      1/1       Running   0         40s
</code></pre><!--
Use the command below to get the value you entered during the [sanity test](#sanity-testing-the-ensemble),
from the `zk-2` Pod.
-->
<p>从 <code>zk-2</code> Pod 中获取你在<a href="#Ensemble-%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5">健康检查</a>中输入的值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-2 zkCli.sh get /hello
</code></pre></div><!--
Even though you terminated and recreated all of the Pods in the `zk` StatefulSet, the ensemble still serves the original value.
-->
<p>尽管 <code>zk</code> StatefulSet 中所有的 Pods 都已经被终止并重建过，ensemble
仍然使用原来的数值提供服务。</p>
<pre><code>WATCHER::

WatchedEvent state:SyncConnected type:None path:null
world
cZxid = 0x100000002
ctime = Thu Dec 08 15:13:30 UTC 2016
mZxid = 0x100000002
mtime = Thu Dec 08 15:13:30 UTC 2016
pZxid = 0x100000002
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 5
numChildren = 0
</code></pre><!--
The `volumeClaimTemplates` field of the `zk` StatefulSet's `spec` specifies a PersistentVolume provisioned for each Pod.
-->
<p><code>zk</code> StatefulSet 的 <code>spec</code> 中的 <code>volumeClaimTemplates</code> 字段标识了
将要为每个 Pod 准备的 PersistentVolume。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>datadir<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volume.alpha.kubernetes.io/storage-class</span>:<span style="color:#bbb"> </span>anything<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>20Gi<span style="color:#bbb">
</span></code></pre></div><!--
The `StatefulSet` controller generates a `PersistentVolumeClaim` for each Pod in
the `StatefulSet`.

Use the following command to get the `StatefulSet`'s `PersistentVolumeClaims`.
-->
<p><code>StatefulSet</code> 控制器为 <code>StatefulSet</code> 中的每个 Pod 生成一个 <code>PersistentVolumeClaim</code>。</p>
<p>获取 <code>StatefulSet</code> 的 <code>PersistentVolumeClaim</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pvc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
When the `StatefulSet` recreated its Pods, it remounts the Pods' PersistentVolumes.
-->
<p>当 <code>StatefulSet</code> 重新创建它的 Pods 时，Pods 的 PersistentVolumes 会被重新挂载。</p>
<pre><code>NAME           STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
datadir-zk-0   Bound     pvc-bed742cd-bcb1-11e6-994f-42010a800002   20Gi       RWO           1h
datadir-zk-1   Bound     pvc-bedd27d2-bcb1-11e6-994f-42010a800002   20Gi       RWO           1h
datadir-zk-2   Bound     pvc-bee0817e-bcb1-11e6-994f-42010a800002   20Gi       RWO           1h
</code></pre><!--
The `volumeMounts` section of the `StatefulSet`'s container `template` mounts the PersistentVolumes in the ZooKeeper servers' data directories.
-->
<p>StatefulSet 的容器 <code>template</code> 中的 <code>volumeMounts</code> 一节使得
PersistentVolumes 被挂载到 ZooKeeper 服务器的数据目录。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>datadir<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/zookeeper<span style="color:#bbb">
</span></code></pre></div><!--
When a Pod in the `zk` `StatefulSet` is (re)scheduled, it will always have the
same `PersistentVolume` mounted to the ZooKeeper server's data directory.
Even when the Pods are rescheduled, all the writes made to the ZooKeeper
servers' WALs, and all their snapshots, remain durable.
-->
<p>当 <code>zk</code> <code>StatefulSet</code> 中的一个 Pod 被（重新）调度时，它总是拥有相同的 PersistentVolume，
挂载到 ZooKeeper 服务器的数据目录。
即使在 Pods 被重新调度时，所有对 ZooKeeper 服务器的 WALs 的写入和它们的
全部快照都仍然是持久的。</p>
<!--
## Ensuring consistent configuration

As noted in the [Facilitating Leader Election](#facilitating-leader-election) and
[Achieving Consensus](#achieving-consensus) sections, the servers in a
ZooKeeper ensemble require consistent configuration to elect a leader
and form a quorum. They also require consistent configuration of the Zab protocol
in order for the protocol to work correctly over a network. In our example we
achieve consistent configuration by embedding the configuration directly into
the manifest.

Get the `zk` StatefulSet.
-->
<h2 id="确保一致性配置">确保一致性配置</h2>
<p>如同在<a href="#facilitating-leader-election">促成领导者选举</a> 和<a href="#achieving-consensus">达成一致</a>
小节中提到的，ZooKeeper ensemble 中的服务器需要一致性的配置来选举一个领导者并形成一个
quorum。它们还需要 Zab 协议的一致性配置来保证这个协议在网络中正确的工作。
在这次的示例中，我们通过直接将配置写入代码清单中来达到该目的。</p>
<p>获取 <code>zk</code> StatefulSet。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get sts zk -o yaml
</code></pre></div><pre><code>    ...
    command:
      - sh
      - -c
      - &quot;start-zookeeper \
        --servers=3 \
        --data_dir=/var/lib/zookeeper/data \
        --data_log_dir=/var/lib/zookeeper/data/log \
        --conf_dir=/opt/zookeeper/conf \
        --client_port=2181 \
        --election_port=3888 \
        --server_port=2888 \
        --tick_time=2000 \
        --init_limit=10 \
        --sync_limit=5 \
        --heap=512M \
        --max_client_cnxns=60 \
        --snap_retain_count=3 \
        --purge_interval=12 \
        --max_session_timeout=40000 \
        --min_session_timeout=4000 \
        --log_level=INFO&quot;
...
</code></pre><!--
The command used to start the ZooKeeper servers passed the configuration as command line parameter. You can also use environment variables to pass configuration to the ensemble.
-->
<p>用于启动 ZooKeeper 服务器的命令将这些配置作为命令行参数传给了 ensemble。
你也可以通过环境变量来传入这些配置。</p>
<!--
### Configuring Logging

One of the files generated by the `zkGenConfig.sh` script controls ZooKeeper's logging.
ZooKeeper uses [Log4j](https://logging.apache.org/log4j/2.x/), and, by default,
it uses a time and size based rolling file appender for its logging configuration.

Use the command below to get the logging configuration from one of Pods in the `zk` `StatefulSet`.
-->
<h3 id="configuring-logging">配置日志  </h3>
<p><code>zkGenConfig.sh</code> 脚本产生的一个文件控制了 ZooKeeper 的日志行为。
ZooKeeper 使用了 <a href="http://logging.apache.org/log4j/2.x/">Log4j</a> 并默认使用
基于文件大小和时间的滚动文件追加器作为日志配置。</p>
<p>从 <code>zk</code> StatefulSet 的一个 Pod 中获取日志配置。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 cat /usr/etc/zookeeper/log4j.properties
</code></pre></div><!--
The logging configuration below will cause the ZooKeeper process to write all
of its logs to the standard output file stream.
-->
<p>下面的日志配置会使 ZooKeeper 进程将其所有的日志写入标志输出文件流中。</p>
<pre><code>zookeeper.root.logger=CONSOLE
zookeeper.console.threshold=INFO
log4j.rootLogger=${zookeeper.root.logger}
log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
log4j.appender.CONSOLE.Threshold=${zookeeper.console.threshold}
log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n
</code></pre><!--
This is the simplest possible way to safely log inside the container.
Because the applications write logs to standard out, Kubernetes will handle log rotation for you.
Kubernetes also implements a sane retention policy that ensures application logs written to
standard out and standard error do not exhaust local storage media.

Use [`kubectl logs`](/docs/reference/generated/kubectl/kubectl-commands/#logs) to retrieve the last 20 log lines from one of the Pods.
-->
<p>这是在容器里安全记录日志的最简单的方法。
由于应用的日志被写入标准输出，Kubernetes 将会为你处理日志轮转。
Kubernetes 还实现了一个智能保存策略，保证写入标准输出和标准错误流
的应用日志不会耗尽本地存储媒介。</p>
<p>使用命令 <a href="/docs/reference/generated/kubectl/kubectl-commands/#logs"><code>kubectl logs</code></a>
从一个 Pod 中取回最后 20 行日志。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs zk-0 --tail <span style="color:#666">20</span>
</code></pre></div><!--
You can view application logs written to standard out or standard error using `kubectl logs` and from the Kubernetes Dashboard.
-->
<p>使用 <code>kubectl logs</code> 或者从 Kubernetes Dashboard 可以查看写入到标准输出和标准错误流中的应用日志。</p>
<pre><code>2016-12-06 19:34:16,236 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52740
2016-12-06 19:34:16,237 [myid:1] - INFO  [Thread-1136:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52740 (no session established for client)
2016-12-06 19:34:26,155 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52749
2016-12-06 19:34:26,155 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52749
2016-12-06 19:34:26,156 [myid:1] - INFO  [Thread-1137:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52749 (no session established for client)
2016-12-06 19:34:26,222 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52750
2016-12-06 19:34:26,222 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52750
2016-12-06 19:34:26,226 [myid:1] - INFO  [Thread-1138:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52750 (no session established for client)
2016-12-06 19:34:36,151 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52760
2016-12-06 19:34:36,152 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52760
2016-12-06 19:34:36,152 [myid:1] - INFO  [Thread-1139:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52760 (no session established for client)
2016-12-06 19:34:36,230 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52761
2016-12-06 19:34:36,231 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52761
2016-12-06 19:34:36,231 [myid:1] - INFO  [Thread-1140:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52761 (no session established for client)
2016-12-06 19:34:46,149 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52767
2016-12-06 19:34:46,149 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52767
2016-12-06 19:34:46,149 [myid:1] - INFO  [Thread-1141:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52767 (no session established for client)
2016-12-06 19:34:46,230 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52768
2016-12-06 19:34:46,230 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52768
2016-12-06 19:34:46,230 [myid:1] - INFO  [Thread-1142:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52768 (no session established for client)
</code></pre><!--
Kubernetes integrates with many logging solutions. You can choose a logging solution
that best fits your cluster and applications. For cluster-level logging and aggregation,
consider deploying a [sidecar container](/docs/concepts/cluster-administration/logging#sidecar-container-with-logging-agent) to rotate and ship your logs.
-->
<p>Kubernetes 支持与多种日志方案集成。你可以选择一个最适合你的集群和应用
的日志解决方案。对于集群级别的日志输出与整合，可以考虑部署一个
<a href="/zh/docs/concepts/cluster-administration/logging#sidecar-container-with-logging-agent">边车容器</a>
来轮转和提供日志数据。</p>
<!--
### Configuring a non-privileged user

The best practices to allow an application to run as a privileged
user inside of a container are a matter of debate. If your organization requires
that applications run as a non-privileged user you can use a
[SecurityContext](/docs/tasks/configure-pod-container/security-context/) to control the user that
the entry point runs as.

The `zk` `StatefulSet`'s Pod `template` contains a `SecurityContext`.
-->
<h3 id="配置非特权用户">配置非特权用户</h3>
<p>在容器中允许应用以特权用户运行这条最佳实践是值得商讨的。
如果你的组织要求应用以非特权用户运行，你可以使用
<a href="/zh/docs/tasks/configure-pod-container/security-context/">SecurityContext</a>
控制运行容器入口点所使用的用户。</p>
<p><code>zk</code> StatefulSet 的 Pod 的 <code>template</code> 包含了一个 <code>SecurityContext</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">runAsUser</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">fsGroup</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span></code></pre></div><!--
In the Pods' containers, UID 1000 corresponds to the zookeeper user and GID 1000
corresponds to the zookeeper group.

Get the ZooKeeper process information from the `zk-0` Pod.
-->
<p>在 Pods 的容器内部，UID 1000 对应用户 zookeeper，GID 1000 对应用户组 zookeeper。</p>
<p>从 <code>zk-0</code> Pod 获取 ZooKeeper 进程信息。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 -- ps -elf
</code></pre></div><!--
As the `runAsUser` field of the `securityContext` object is set to 1000,
instead of running as root, the ZooKeeper process runs as the zookeeper user.
-->
<p>由于 <code>securityContext</code> 对象的 <code>runAsUser</code> 字段被设置为 1000 而不是 root，
ZooKeeper 进程将以 zookeeper 用户运行。</p>
<pre><code>F S UID        PID  PPID  C PRI  NI ADDR SZ WCHAN  STIME TTY          TIME CMD
4 S zookeep+     1     0  0  80   0 -  1127 -      20:46 ?        00:00:00 sh -c zkGenConfig.sh &amp;&amp; zkServer.sh start-foreground
0 S zookeep+    27     1  0  80   0 - 1155556 -    20:46 ?        00:00:19 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.9.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper: -Xmx2G -Xms2G -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/bin/../etc/zookeeper/zoo.cfg
</code></pre><!--
By default, when the Pod's PersistentVolumes is mounted to the ZooKeeper server's data directory, it is only accessible by the root user. This configuration prevents the ZooKeeper process from writing to its WAL and storing its snapshots.

Use the command below to get the file permissions of the ZooKeeper data directory on the `zk-0` Pod.
-->
<p>默认情况下，当 Pod 的 PersistentVolume 被挂载到 ZooKeeper 服务器的数据目录时，
它只能被 root 用户访问。这个配置将阻止 ZooKeeper 进程写入它的 WAL 及保存快照。</p>
<p>在 <code>zk-0</code> Pod 上获取 ZooKeeper 数据目录的文件权限。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -ti zk-0 -- ls -ld /var/lib/zookeeper/data
</code></pre></div><!--
Because the `fsGroup` field of the `securityContext` object is set to 1000, the ownership of the Pods' PersistentVolumes is set to the zookeeper group, and the ZooKeeper process is able to read and write its data.
-->
<p>由于 <code>securityContext</code> 对象的 <code>fsGroup</code> 字段设置为 1000，Pods 的
PersistentVolumes 的所有权属于 zookeeper 用户组，因而 ZooKeeper
进程能够成功地读写数据。</p>
<pre><code>drwxr-sr-x 3 zookeeper zookeeper 4096 Dec  5 20:45 /var/lib/zookeeper/data
</code></pre><!--
## Managing the ZooKeeper Process

The [ZooKeeper documentation](https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_supervision)
mentions that "You will want to have a supervisory process that
manages each of your ZooKeeper server processes (JVM)." Utilizing a watchdog
(supervisory process) to restart failed processes in a distributed system is a
common pattern. When deploying an application in Kubernetes, rather than using
an external utility as a supervisory process, you should use Kubernetes as the
watchdog for your application.
-->
<h2 id="管理-zookeeper-进程">管理 ZooKeeper 进程</h2>
<p><a href="https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_supervision">ZooKeeper 文档</a>
指出“你将需要一个监管程序用于管理每个 ZooKeeper 服务进程（JVM）”。
在分布式系统中，使用一个看门狗（监管程序）来重启故障进程是一种常用的模式。</p>
<!--
### Updating the ensemble

The `zk` `StatefulSet` is configured to use the `RollingUpdate` update strategy.

You can use `kubectl patch` to update the number of `cpus` allocated to the servers.
-->
<h3 id="更新-ensemble">更新 Ensemble</h3>
<p><code>zk</code> <code>StatefulSet</code> 的更新策略被设置为了 <code>RollingUpdate</code>。</p>
<p>你可以使用 <code>kubectl patch</code> 更新分配给每个服务器的 <code>cpus</code> 的数量。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch sts zk --type<span style="color:#666">=</span><span style="color:#b44">&#39;json&#39;</span> -p<span style="color:#666">=</span><span style="color:#b44">&#39;[{&#34;op&#34;: &#34;replace&#34;, &#34;path&#34;: &#34;/spec/template/spec/containers/0/resources/requests/cpu&#34;, &#34;value&#34;:&#34;0.3&#34;}]&#39;</span>
</code></pre></div><pre><code>statefulset.apps/zk patched
</code></pre><!--
Use `kubectl rollout status` to watch the status of the update.
-->
<p>使用 <code>kubectl rollout status</code> 观测更新状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status sts/zk
</code></pre></div><pre><code>waiting for statefulset rolling update to complete 0 pods at revision zk-5db4499664...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
waiting for statefulset rolling update to complete 1 pods at revision zk-5db4499664...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
waiting for statefulset rolling update to complete 2 pods at revision zk-5db4499664...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
statefulset rolling update complete 3 pods at revision zk-5db4499664...
</code></pre><!--
This terminates the Pods, one at a time, in reverse ordinal order, and recreates them with the new configuration. This ensures that quorum is maintained during a rolling update.

Use the `kubectl rollout history` command to view a history or previous configurations.

The output is similar to this:
-->
<p>这项操作会逆序地依次终止每一个 Pod，并用新的配置重新创建。
这样做确保了在滚动更新的过程中 quorum 依旧保持工作。</p>
<p>使用 <code>kubectl rollout history</code> 命令查看历史或先前的配置。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout <span style="color:#a2f">history</span> sts/zk
</code></pre></div><p>输出类似于：</p>
<pre><code>statefulsets &quot;zk&quot;
REVISION
1
2
</code></pre><!--
Use the `kubectl rollout undo` command to roll back the modification.

The output is similar to this:
-->
<p>使用 <code>kubectl rollout undo</code> 命令撤销这次的改动。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout undo sts/zk
</code></pre></div><p>输出类似于：</p>
<pre><code>statefulset.apps/zk rolled back
</code></pre><!--
### Handling process failure

[Restart Policies](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) control how
Kubernetes handles process failures for the entry point of the container in a Pod.
For Pods in a `StatefulSet`, the only appropriate `RestartPolicy` is Always, and this
is the default value. For stateful applications you should **never** override
the default policy.

Use the following command to examine the process tree for the ZooKeeper server running in the `zk-0` Pod.
-->
<h3 id="处理进程故障">处理进程故障</h3>
<p><a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">重启策略</a>
控制 Kubernetes 如何处理一个 Pod 中容器入口点的进程故障。
对于 StatefulSet 中的 Pods 来说，Always 是唯一合适的 RestartPolicy，也是默认值。
你应该<strong>绝不</strong>覆盖有状态应用的默认策略。</p>
<p>检查 <code>zk-0</code> Pod 中运行的 ZooKeeper 服务器的进程树。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 -- ps -ef
</code></pre></div><!--
The command used as the container's entry point has PID 1, and
the ZooKeeper process, a child of the entry point, has PID 27.
-->
<p>作为容器入口点的命令的 PID 为 1，Zookeeper 进程是入口点的子进程，
PID 为 27。</p>
<pre><code>UID        PID  PPID  C STIME TTY          TIME CMD
zookeep+     1     0  0 15:03 ?        00:00:00 sh -c zkGenConfig.sh &amp;&amp; zkServer.sh start-foreground
zookeep+    27     1  0 15:03 ?        00:00:03 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.9.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper: -Xmx2G -Xms2G -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/bin/../etc/zookeeper/zoo.cfg
</code></pre><!--
In another terminal watch the Pods in the `zk` `StatefulSet` with the following command.
-->
<p>在一个终端观察 <code>zk</code> <code>StatefulSet</code> 中的 Pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
In another terminal, terminate the ZooKeeper process in Pod `zk-0` with the following command.
-->
<p>在另一个终端杀掉 Pod <code>zk-0</code> 中的 ZooKeeper 进程。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"> kubectl <span style="color:#a2f">exec</span> zk-0 -- pkill java
</code></pre></div><!--
The termination of the ZooKeeper process caused its parent process to terminate. Because the `RestartPolicy` of the container is Always, it restarted the parent process.
-->
<p>ZooKeeper 进程的终结导致了它父进程的终止。由于容器的 <code>RestartPolicy</code>
是 Always，父进程被重启。</p>
<pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      1/1       Running   0          21m
zk-1      1/1       Running   0          20m
zk-2      1/1       Running   0          19m
NAME      READY     STATUS    RESTARTS   AGE
zk-0      0/1       Error     0          29m
zk-0      0/1       Running   1         29m
zk-0      1/1       Running   1         29m
</code></pre><!--
If your application uses a script (such as `zkServer.sh`) to launch the process
that implements the application's business logic, the script must terminate with the
child process. This ensures that Kubernetes will restart the application's
container when the process implementing the application's business logic fails.
-->
<p>如果你的应用使用一个脚本（例如 <code>zkServer.sh</code>）来启动一个实现了应用业务逻辑的进程，
这个脚本必须和子进程一起结束。这保证了当实现应用业务逻辑的进程故障时，
Kubernetes 会重启这个应用的容器。</p>
<!--
### Testing for liveness

Configuring your application to restart failed processes is not enough to
keep a distributed system healthy. There are scenarios where
a system's processes can be both alive and unresponsive, or otherwise
unhealthy. You should use liveness probes to notify Kubernetes
that your application's processes are unhealthy and it should restart them.

The Pod `template` for the `zk` `StatefulSet` specifies a liveness probe.
-->
<h3 id="存活性测试">存活性测试</h3>
<p>你的应用配置为自动重启故障进程，但这对于保持一个分布式系统的健康来说是不够的。
许多场景下，一个系统进程可以是活动状态但不响应请求，或者是不健康状态。
你应该使用存活性探针来通知 Kubernetes 你的应用进程处于不健康状态，需要被重启。</p>
<p><code>zk</code> <code>StatefulSet</code> 的 Pod 的 <code>template</code> 一节指定了一个存活探针。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;zookeeper-ready 2181&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></code></pre></div><!--
The probe calls a bash script that uses the ZooKeeper `ruok` four letter
word to test the server's health.
-->
<p>这个探针调用一个简单的 Bash 脚本，使用 ZooKeeper 的四字缩写 <code>ruok</code>
来测试服务器的健康状态。</p>
<pre><code>OK=$(echo ruok | nc 127.0.0.1 $1)
if [ &quot;$OK&quot; == &quot;imok&quot; ]; then
    exit 0
else
    exit 1
fi
</code></pre><!--
In one terminal window, use the following command to watch the Pods in the `zk` StatefulSet.
-->
<p>在一个终端窗口中使用下面的命令观察 <code>zk</code> StatefulSet 中的 Pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
In another window, using the following command to delete the `zookeeper-ready` script from the file system of Pod `zk-0`.
-->
<p>在另一个窗口中，从 Pod <code>zk-0</code> 的文件系统中删除 <code>zookeeper-ready</code> 脚本。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 -- rm /opt/zookeeper/bin/zookeeper-ready
</code></pre></div><!--
When the liveness probe for the ZooKeeper process fails, Kubernetes will
automatically restart the process for you, ensuring that unhealthy processes in
the ensemble are restarted.
-->
<p>当 ZooKeeper 进程的存活探针探测失败时，Kubernetes 将会为你自动重启这个进程，
从而保证 ensemble 中不健康状态的进程都被重启。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      1/1       Running   0          1h
zk-1      1/1       Running   0          1h
zk-2      1/1       Running   0          1h
NAME      READY     STATUS    RESTARTS   AGE
zk-0      0/1       Running   0          1h
zk-0      0/1       Running   1         1h
zk-0      1/1       Running   1         1h
</code></pre><!--
### Testing for readiness

Readiness is not the same as liveness. If a process is alive, it is scheduled
and healthy. If a process is ready, it is able to process input. Liveness is
a necessary, but not sufficient, condition for readiness. There are cases,
particularly during initialization and termination, when a process can be
alive but not ready.
-->
<h3 id="就绪性测试">就绪性测试</h3>
<p>就绪不同于存活。如果一个进程是存活的，它是可调度和健康的。
如果一个进程是就绪的，它应该能够处理输入。存活是就绪的必要非充分条件。
在许多场景下，特别是初始化和终止过程中，一个进程可以是存活但没有就绪的。</p>
<!--
If you specify a readiness probe, Kubernetes will ensure that your application's
processes will not receive network traffic until their readiness checks pass.

For a ZooKeeper server, liveness implies readiness.  Therefore, the readiness
probe from the `zookeeper.yaml` manifest is identical to the liveness probe.
-->
<p>如果你指定了一个就绪探针，Kubernetes 将保证在就绪检查通过之前，
你的应用不会接收到网络流量。</p>
<p>对于一个 ZooKeeper 服务器来说，存活即就绪。
因此 <code>zookeeper.yaml</code> 清单中的就绪探针和存活探针完全相同。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;zookeeper-ready 2181&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></code></pre></div><!--
Even though the liveness and readiness probes are identical, it is important
to specify both. This ensures that only healthy servers in the ZooKeeper
ensemble receive network traffic.
-->
<p>虽然存活探针和就绪探针是相同的，但同时指定它们两者仍然重要。
这保证了 ZooKeeper ensemble 中只有健康的服务器能接收网络流量。</p>
<!--
## Tolerating node failure

ZooKeeper needs a quorum of servers to successfully commit mutations
to data. For a three server ensemble, two servers must be healthy for
writes to succeed. In quorum based systems, members are deployed across failure
domains to ensure availability. To avoid an outage, due to the loss of an
individual machine, best practices preclude co-locating multiple instances of the
application on the same machine.
-->
<h2 id="容忍节点故障">容忍节点故障</h2>
<p>ZooKeeper 需要一个 quorum 来提交数据变动。对于一个拥有 3 个服务器的 ensemble 来说，
必须有两个服务器是健康的，写入才能成功。
在基于 quorum 的系统里，成员被部署在多个故障域中以保证可用性。
为了防止由于某台机器断连引起服务中断，最佳实践是防止应用的多个实例在相同的机器上共存。</p>
<!--
By default, Kubernetes may co-locate Pods in a `StatefulSet` on the same node.
For the three server ensemble you created, if two servers are on the same node, and that node fails,
the clients of your ZooKeeper service will experience an outage until at least one of the Pods can be rescheduled.
-->
<p>默认情况下，Kubernetes 可以把 <code>StatefulSet</code> 的 Pods 部署在相同节点上。
对于你创建的 3 个服务器的 ensemble 来说，如果有两个服务器并存于
相同的节点上并且该节点发生故障时，ZooKeeper 服务将中断，
直至至少一个 Pods 被重新调度。</p>
<!--
You should always provision additional capacity to allow the processes of critical
systems to be rescheduled in the event of node failures. If you do so, then the
outage will only last until the Kubernetes scheduler reschedules one of the ZooKeeper
servers. However, if you want your service to tolerate node failures with no downtime,
you should set `podAntiAffinity`.

Use the command below to get the nodes for Pods in the `zk` `StatefulSet`.
-->
<p>你应该总是提供多余的容量以允许关键系统进程在节点故障时能够被重新调度。
如果你这样做了，服务故障就只会持续到 Kubernetes 调度器重新调度某个
ZooKeeper 服务器为止。
但是，如果希望你的服务在容忍节点故障时无停服时间，你应该设置 <code>podAntiAffinity</code>。</p>
<p>使用下面的命令获取 <code>zk</code> <code>StatefulSet</code> 中的 Pods 的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl get pod zk-<span style="color:#b8860b">$i</span> --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span>; <span style="color:#a2f">echo</span> <span style="color:#b44">&#34;&#34;</span>; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
All of the Pods in the `zk` `StatefulSet` are deployed on different nodes.
-->
<p><code>zk</code> <code>StatefulSet</code> 中所有的 Pods 都被部署在不同的节点。</p>
<pre><code>kubernetes-node-cxpk
kubernetes-node-a5aq
kubernetes-node-2g2d
</code></pre><!--
This is because the Pods in the `zk` `StatefulSet` have a `PodAntiAffinity` specified.
-->
<p>这是因为 <code>zk</code> <code>StatefulSet</code> 中的 Pods 指定了 <code>PodAntiAffinity</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podAntiAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;app&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span>- zk<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
The `requiredDuringSchedulingIgnoredDuringExecution` field tells the
Kubernetes Scheduler that it should never co-locate two Pods which have `app` label
as `zk` in the domain defined by the `topologyKey`. The `topologyKey`
`kubernetes.io/hostname` indicates that the domain is an individual node. Using
different rules, labels, and selectors, you can extend this technique to spread
your ensemble across physical, network, and power failure domains.
-->
<p><code>requiredDuringSchedulingIgnoredDuringExecution</code> 告诉 Kubernetes 调度器，
在以 <code>topologyKey</code> 指定的域中，绝对不要把带有键为 <code>app</code>、值为 <code>zk</code> 的标签
的两个 Pods 调度到相同的节点。<code>topologyKey</code> <code>kubernetes.io/hostname</code> 表示
这个域是一个单独的节点。
使用不同的规则、标签和选择算符，你能够通过这种技术把你的 ensemble 分布
在不同的物理、网络和电力故障域之间。</p>
<!--
## Surviving maintenance

**In this section you will cordon and drain nodes. If you are using this tutorial
on a shared cluster, be sure that this will not adversely affect other tenants.**

The previous section showed you how to spread your Pods across nodes to survive
unplanned node failures, but you also need to plan for temporary node failures
that occur due to planned maintenance.

Use this command to get the nodes in your cluster.
-->
<h2 id="节点维护期间保持应用可用">节点维护期间保持应用可用</h2>
<p><strong>在本节中你将会隔离（Cordon）和腾空（Drain）节点。
如果你是在一个共享的集群里使用本教程，请保证不会影响到其他租户。</strong></p>
<p>上一小节展示了如何在节点之间分散 Pods 以在计划外的节点故障时保证服务存活。
但是你也需要为计划内维护引起的临时节点故障做准备。</p>
<p>使用此命令获取你的集群中的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><!--
Use [`kubectl cordon`](/docs/reference/generated/kubectl/kubectl-commands/#cordon) to
cordon all but four of the nodes in your cluster.
-->
<p>使用 <a href="/docs/reference/generated/kubectl/kubectl-commands/#cordon"><code>kubectl cordon</code></a>
隔离你的集群中除 4 个节点以外的所有节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cordon &lt;node-name&gt;
</code></pre></div><!--
Use this command to get the `zk-pdb` `PodDisruptionBudget`.
-->
<p>使用下面的命令获取 <code>zk-pdb</code> <code>PodDisruptionBudget</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pdb zk-pdb
</code></pre></div><!--
The `max-unavailable` field indicates to Kubernetes that at most one Pod from
`zk` `StatefulSet` can be unavailable at any time.
-->
<p><code>max-unavailable</code> 字段指示 Kubernetes 在任何时候，<code>zk</code> <code>StatefulSet</code>
至多有一个 Pod 是不可用的。</p>
<pre><code>NAME      MIN-AVAILABLE   MAX-UNAVAILABLE   ALLOWED-DISRUPTIONS   AGE
zk-pdb    N/A             1                 1
</code></pre><!--
In one terminal, use this command to watch the Pods in the `zk` `StatefulSet`.
-->
<p>在一个终端中，使用下面的命令观察 <code>zk</code> <code>StatefulSet</code> 中的 Pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
In another terminal, use this command to get the nodes that the Pods are currently scheduled on.
-->
<p>在另一个终端中，使用下面的命令获取 Pods 当前调度的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl get pod zk-<span style="color:#b8860b">$i</span> --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span>; <span style="color:#a2f">echo</span> <span style="color:#b44">&#34;&#34;</span>; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>kubernetes-node-pb41
kubernetes-node-ixsl
kubernetes-node-i4c4
</code></pre><!--
Use [`kubectl drain`](/docs/reference/generated/kubectl/kubectl-commands/#drain) to cordon and
drain the node on which the `zk-0` Pod is scheduled.

The output is similar to this:
-->
<p>使用 <a href="/docs/reference/generated/kubectl/kubectl-commands/#drain"><code>kubectl drain</code></a>
来隔离和腾空 <code>zk-0</code> Pod 调度所在的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain <span style="color:#a2f;font-weight:bold">$(</span>kubectl get pod zk-0 --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span><span style="color:#a2f;font-weight:bold">)</span> --ignore-daemonsets --force --delete-emptydir-data
</code></pre></div><p>输出类似于：</p>
<pre><code>node &quot;kubernetes-node-pb41&quot; cordoned

WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-pb41, kube-proxy-kubernetes-node-pb41; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-o5elz
pod &quot;zk-0&quot; deleted
node &quot;kubernetes-node-pb41&quot; drained
</code></pre><!--
As there are four nodes in your cluster, `kubectl drain`, succeeds and the
`zk-0` is rescheduled to another node.
-->
<p>由于你的集群中有 4 个节点, <code>kubectl drain</code> 执行成功，<code>zk-0</code> 被调度到其它节点。</p>
<pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      1/1       Running   2          1h
zk-1      1/1       Running   0          1h
zk-2      1/1       Running   0          1h
NAME      READY     STATUS        RESTARTS   AGE
zk-0      1/1       Terminating   2          2h
zk-0      0/1       Terminating   2         2h
zk-0      0/1       Terminating   2         2h
zk-0      0/1       Terminating   2         2h
zk-0      0/1       Pending   0         0s
zk-0      0/1       Pending   0         0s
zk-0      0/1       ContainerCreating   0         0s
zk-0      0/1       Running   0         51s
zk-0      1/1       Running   0         1m
</code></pre><!--
Keep watching the `StatefulSet`'s Pods in the first terminal and drain the node on which
`zk-1` is scheduled.

The output is similar to this:
-->
<p>在第一个终端中持续观察 <code>StatefulSet</code> 的 Pods 并腾空 <code>zk-1</code> 调度所在的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain <span style="color:#a2f;font-weight:bold">$(</span>kubectl get pod zk-1 --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span><span style="color:#a2f;font-weight:bold">)</span> --ignore-daemonsets --force --delete-emptydir-data
</code></pre></div><p>输出类似于：</p>
<pre><code>kubernetes-node-ixsl&quot; cordoned
WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-ixsl, kube-proxy-kubernetes-node-ixsl; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-voc74
pod &quot;zk-1&quot; deleted
node &quot;kubernetes-node-ixsl&quot; drained
</code></pre><!--
The `zk-1` Pod cannot be scheduled because the `zk` `StatefulSet` contains a `PodAntiAffinity` rule preventing
co-location of the Pods, and as only two nodes are schedulable, the Pod will remain in a Pending state.

The output is similar to this:
-->
<p><code>zk-1</code> Pod 不能被调度，这是因为 <code>zk</code> <code>StatefulSet</code> 包含了一个防止 Pods
共存的 <code>PodAntiAffinity</code> 规则，而且只有两个节点可用于调度，
这个 Pod 将保持在 Pending 状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><p>输出类似于：</p>
<pre><code>NAME      READY     STATUS              RESTARTS   AGE
zk-0      1/1       Running             2          1h
zk-1      1/1       Running             0          1h
zk-2      1/1       Running             0          1h
NAME      READY     STATUS              RESTARTS   AGE
zk-0      1/1       Terminating         2          2h
zk-0      0/1       Terminating         2          2h
zk-0      0/1       Terminating         2          2h
zk-0      0/1       Terminating         2          2h
zk-0      0/1       Pending             0          0s
zk-0      0/1       Pending             0          0s
zk-0      0/1       ContainerCreating   0          0s
zk-0      0/1       Running             0          51s
zk-0      1/1       Running             0          1m
zk-1      1/1       Terminating         0          2h
zk-1      0/1       Terminating         0          2h
zk-1      0/1       Terminating         0          2h
zk-1      0/1       Terminating         0          2h
zk-1      0/1       Pending             0          0s
zk-1      0/1       Pending             0          0s
</code></pre><!--
Continue to watch the Pods of the StatefulSet, and drain the node on which
`zk-2` is scheduled.

The output is similar to this:
-->
<p>继续观察 StatefulSet 中的 Pods 并腾空 <code>zk-2</code> 调度所在的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain <span style="color:#a2f;font-weight:bold">$(</span>kubectl get pod zk-2 --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span><span style="color:#a2f;font-weight:bold">)</span> --ignore-daemonsets --force --delete-emptydir-data
</code></pre></div><p>输出类似于：</p>
<pre><code>node &quot;kubernetes-node-i4c4&quot; cordoned

WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-i4c4, kube-proxy-kubernetes-node-i4c4; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog
WARNING: Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog; Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-i4c4, kube-proxy-kubernetes-node-i4c4
There are pending pods when an error occurred: Cannot evict pod as it would violate the pod's disruption budget.
pod/zk-2
</code></pre><!--
Use `CTRL-C` to terminate to kubectl.

You cannot drain the third node because evicting `zk-2` would violate `zk-budget`. However, the node will remain cordoned.

Use `zkCli.sh` to retrieve the value you entered during the sanity test from `zk-0`.
-->
<p>使用 <code>CTRL-C</code> 终止 kubectl。</p>
<p>你不能腾空第三个节点，因为驱逐 <code>zk-2</code> 将和 <code>zk-budget</code> 冲突。
然而这个节点仍然处于隔离状态（Cordoned）。</p>
<p>使用 <code>zkCli.sh</code> 从 <code>zk-0</code> 取回你的健康检查中输入的数值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 zkCli.sh get /hello
</code></pre></div><!--
The service is still available because its `PodDisruptionBudget` is respected.
-->
<p>由于遵守了 <code>PodDisruptionBudget</code>，服务仍然可用。</p>
<pre><code>WatchedEvent state:SyncConnected type:None path:null
world
cZxid = 0x200000002
ctime = Wed Dec 07 00:08:59 UTC 2016
mZxid = 0x200000002
mtime = Wed Dec 07 00:08:59 UTC 2016
pZxid = 0x200000002
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 5
numChildren = 0
</code></pre><!--
Use [`kubectl uncordon`](/docs/reference/generated/kubectl/kubectl-commands/#uncordon) to uncordon the first node.

The output is similar to this:
-->
<p>使用 <a href="/docs/reference/generated/kubectl/kubectl-commands/#uncordon"><code>kubectl uncordon</code></a>
来取消对第一个节点的隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl uncordon kubernetes-node-pb41
</code></pre></div><p>输出类似于：</p>
<pre><code>node &quot;kubernetes-node-pb41&quot; uncordoned
</code></pre><!--
`zk-1` is rescheduled on this node. Wait until `zk-1` is Running and Ready.

The output is similar to this:
-->
<p><code>zk-1</code> 被重新调度到了这个节点。等待 <code>zk-1</code> 变为 Running 和 Ready 状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><p>输出类似于：</p>
<pre><code>NAME      READY     STATUS             RESTARTS  AGE
zk-0      1/1       Running            2         1h
zk-1      1/1       Running            0         1h
zk-2      1/1       Running            0         1h
NAME      READY     STATUS             RESTARTS  AGE
zk-0      1/1       Terminating        2         2h
zk-0      0/1       Terminating        2         2h
zk-0      0/1       Terminating        2         2h
zk-0      0/1       Terminating        2         2h
zk-0      0/1       Pending            0         0s
zk-0      0/1       Pending            0         0s
zk-0      0/1       ContainerCreating  0         0s
zk-0      0/1       Running            0         51s
zk-0      1/1       Running            0         1m
zk-1      1/1       Terminating        0         2h
zk-1      0/1       Terminating        0         2h
zk-1      0/1       Terminating        0         2h
zk-1      0/1       Terminating        0         2h
zk-1      0/1       Pending            0         0s
zk-1      0/1       Pending            0         0s
zk-1      0/1       Pending            0         12m
zk-1      0/1       ContainerCreating  0         12m
zk-1      0/1       Running            0         13m
zk-1      1/1       Running            0         13m
</code></pre><!--
Attempt to drain the node on which `zk-2` is scheduled.
-->
<p>尝试腾空 <code>zk-2</code> 调度所在的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain <span style="color:#a2f;font-weight:bold">$(</span>kubectl get pod zk-2 --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span><span style="color:#a2f;font-weight:bold">)</span> --ignore-daemonsets --force --delete-emptydir-data
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>node &quot;kubernetes-node-i4c4&quot; already cordoned
WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-i4c4, kube-proxy-kubernetes-node-i4c4; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog
pod &quot;heapster-v1.2.0-2604621511-wht1r&quot; deleted
pod &quot;zk-2&quot; deleted
node &quot;kubernetes-node-i4c4&quot; drained
</code></pre><!--
This time `kubectl drain` succeeds.

Uncordon the second node to allow `zk-2` to be rescheduled.

The output is similar to this:
-->
<p>这次 <code>kubectl drain</code> 执行成功。</p>
<p>取消第二个节点的隔离，以允许 <code>zk-2</code> 被重新调度。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl uncordon kubernetes-node-ixsl
</code></pre></div><p>输出类似于：</p>
<pre><code>node &quot;kubernetes-node-ixsl&quot; uncordoned
</code></pre><!--
You can use `kubectl drain` in conjunction with `PodDisruptionBudgets` to ensure that your services remain available during maintenance.
If drain is used to cordon nodes and evict pods prior to taking the node offline for maintenance,
services that express a disruption budget will have that budget respected.
You should always allocate additional capacity for critical services so that their Pods can be immediately rescheduled.
-->
<p>你可以同时使用 <code>kubectl drain</code> 和 <code>PodDisruptionBudgets</code> 来保证你的服务
在维护过程中仍然可用。如果使用了腾空操作来隔离节点并在节点离线之前驱逐了 pods，
那么设置了干扰预算的服务将会遵守该预算。
你应该总是为关键服务分配额外容量，这样它们的 Pods 就能够迅速的重新调度。</p>
<h2 id="cleaning-up">Cleaning up</h2>
<!--
- Use `kubectl uncordon` to uncordon all the nodes in your cluster.
- You must delete the persistent storage media for the PersistentVolumes used in this tutorial.
  Follow the necessary steps, based on your environment, storage configuration,
  and provisioning method, to ensure that all storage is reclaimed.
-->
<ul>
<li>使用 <code>kubectl uncordon</code> 解除你集群中所有节点的隔离。</li>
<li>你需要删除在本教程中使用的 PersistentVolumes 的持久存储媒介。
请遵循必须的步骤，基于你的环境、存储配置和制备方法，保证回收所有的存储。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-42e39658021b706bcc9478c8cc73c4a3">6.4 - StatefulSet 基础</h1>
    
	<!-- overview -->
<!--
This tutorial provides an introduction to managing applications with
[StatefulSets](/docs/concepts/workloads/controllers/statefulset/). It
demonstrates how to create, delete, scale, and update the Pods of StatefulSets.
-->
<p>本教程介绍了如何使用 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a> 来管理应用。
演示了如何创建、删除、扩容/缩容和更新 StatefulSets 的 Pods。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
Before you begin this tutorial, you should familiarize yourself with the
following Kubernetes concepts.
-->
<p>在开始本教程之前，你应该熟悉以下 Kubernetes 的概念：</p>
<ul>
<li><a href="/zh/docs/concepts/workloads/pods/">Pods</a></li>
<li><a href="/zh/docs/concepts/services-networking/dns-pod-service/">Cluster DNS</a></li>
<li><a href="/zh/docs/concepts/services-networking/service/#headless-services">Headless Services</a></li>
<li><a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/">PersistentVolume Provisioning</a></li>
<li><a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a></li>
<li><a href="/zh/docs/user-guide/kubectl/">kubectl CLI</a></li>
</ul>
<!--
This tutorial assumes that your cluster is configured to dynamically provision
PersistentVolumes. If your cluster is not configured to do so, you
will have to manually provision two 1 GiB volumes prior to starting this
tutorial.
-->
<p>本教程假设你的集群被配置为动态的提供 PersistentVolumes。如果没有这样配置，在开始本教程之前，你需要手动准备 2 个 1 GiB 的存储卷。</p>
<h2 id="objectives">Objectives</h2>
<!--
StatefulSets are intended to be used with stateful applications and distributed
systems. However, the administration of stateful applications and
distributed systems on Kubernetes is a broad, complex topic. In order to
demonstrate the basic features of a StatefulSet, and not to conflate the former
topic with the latter, you will deploy a simple web application using a StatefulSet.

After this tutorial, you will be familiar with the following.

* How to create a StatefulSet
* How a StatefulSet manages its Pods
* How to delete a StatefulSet
* How to scale a StatefulSet
* How to update a StatefulSet's Pods
-->
<p>StatefulSets 旨在与有状态的应用及分布式系统一起使用。然而在 Kubernetes 上管理有状态应用和分布式系统是一个宽泛而复杂的话题。
为了演示 StatefulSet 的基本特性，并且不使前后的主题混淆，你将会使用 StatefulSet 部署一个简单的 web 应用。</p>
<p>在阅读本教程后，你将熟悉以下内容：</p>
<ul>
<li>如何创建 StatefulSet</li>
<li>StatefulSet 怎样管理它的 Pods</li>
<li>如何删除 StatefulSet</li>
<li>如何对 StatefulSet 进行扩容/缩容</li>
<li>如何更新一个 StatefulSet 的 Pods</li>
</ul>
<!-- lessoncontent -->
<!--
## Creating a StatefulSet

Begin by creating a StatefulSet using the example below. It is similar to the
example presented in the
[StatefulSets](/docs/concepts/workloads/controllers/statefulset/) concept.
It creates a [headless Service](/docs/concepts/services-networking/service/#headless-services),
`nginx`, to publish the IP addresses of Pods in the StatefulSet, `web`.
-->
<h2 id="创建-statefulset">创建 StatefulSet</h2>
<p>作为开始，使用如下示例创建一个 StatefulSet。它和 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a> 概念中的示例相似。
它创建了一个 <a href="/zh/docs/concepts/services-networking/service/#headless-services">Headless Service</a> <code>nginx</code> 用来发布 StatefulSet <code>web</code> 中的 Pod 的 IP 地址。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/web/web.yaml" download="application/web/web.yaml"><code>application/web/web.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-web-web-yaml')" title="Copy application/web/web.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-web-web-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;nginx&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/nginx-slim:0.8<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Download the example above, and save it to a file named `web.yaml`

You will need to use two terminal windows. In the first terminal, use
[`kubectl get`](/docs/reference/generated/kubectl/kubectl-commands/#get) to watch the creation
of the StatefulSet's Pods.
-->
<p>下载上面的例子并保存为文件 <code>web.yaml</code>。</p>
<p>你需要使用两个终端窗口。 在第一个终端中，使用 <a href="/zh/docs/user-guide/kubectl/v1.23/#get"><code>kubectl get</code></a>  来查看 StatefulSet 的 Pods 的创建情况。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In the second terminal, use
[`kubectl apply`](/docs/reference/generated/kubectl/kubectl-commands/#apply) to create the
Headless Service and StatefulSet defined in `web.yaml`.
-->
<p>在另一个终端中，使用 <a href="/zh/docs/reference/generated/kubectl/kubectl-commands/#apply"><code>kubectl apply</code></a>来创建定义在 <code>web.yaml</code> 中的 Headless Service 和 StatefulSet。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f web.yaml
</code></pre></div><pre><code>service/nginx created
statefulset.apps/web created
</code></pre><!--
The command above creates two Pods, each running an
[NGINX](https://www.nginx.com) webserver. Get the `nginx` Service and the
`web` StatefulSet to verify that they were created successfully.
-->
<p>上面的命令创建了两个 Pod，每个都运行了一个 <a href="https://www.nginx.com">NGINX</a> web 服务器。
获取 <code>nginx</code> Service 和 <code>web</code> StatefulSet 来验证是否成功的创建了它们。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service nginx
</code></pre></div><pre><code>NAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     ClusterIP    None         &lt;none&gt;        80/TCP    12s
</code></pre><!--
...then get the `web` StatefulSet, to verify that both were created successfully:
-->
<p>...然后获取 <code>web</code> StatefulSet，以验证两者均已成功创建：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get statefulset web
</code></pre></div><pre><code>NAME      DESIRED   CURRENT   AGE
web       2         1         20s
</code></pre><!--

### Ordered Pod Creation

For a StatefulSet with N replicas, when Pods are being deployed, they are
created sequentially, in order from {0..N-1}. Examine the output of the
`kubectl get` command in the first terminal. Eventually, the output will
look like the example below.
-->
<h3 id="顺序创建-pod">顺序创建 Pod</h3>
<p>对于一个拥有 N 个副本的 StatefulSet，Pod 被部署时是按照 {0 …… N-1} 的序号顺序创建的。
在第一个终端中使用 <code>kubectl get</code> 检查输出。这个输出最终将看起来像下面的样子。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         19s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         18s
</code></pre><!--
Notice that the `web-1` Pod is not launched until the `web-0` Pod is
[Running and Ready](/docs/user-guide/pod-states).
-->
<p>请注意在 <code>web-0</code> Pod 处于 <a href="/zh/docs/user-guide/pod-states">Running和Ready</a> 状态后 <code>web-1</code> Pod 才会被启动。</p>
<!--
## Pods in a StatefulSet


Pods in a StatefulSet have a unique ordinal index and a stable network identity.

### Examining the Pod's Ordinal Index

Get the StatefulSet's Pods.
-->
<h2 id="statefulset-中的-pod">StatefulSet 中的 Pod</h2>
<p>StatefulSet 中的 Pod 拥有一个唯一的顺序索引和稳定的网络身份标识。</p>
<h3 id="检查-pod-的顺序索引">检查 Pod 的顺序索引</h3>
<p>获取 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          1m
web-1     1/1       Running   0          1m
</code></pre><!--
As mentioned in the [StatefulSets](/docs/concepts/workloads/controllers/statefulset/)
concept, the Pods in a StatefulSet have a sticky, unique identity. This identity
is based on a unique ordinal index that is assigned to each Pod by the
StatefulSet controller. The Pods' names take the form
`<statefulset name>-<ordinal index>`. Since the `web` StatefulSet has two
replicas, it creates two Pods, `web-0` and `web-1`.

### Using Stable Network Identities

Each Pod has a stable hostname based on its ordinal index. Use
[`kubectl exec`](/docs/reference/generated/kubectl/kubectl-commands/#exec) to execute the
`hostname` command in each Pod.
-->
<p>如同 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a> 概念中所提到的，
StatefulSet 中的 Pod 拥有一个具有黏性的、独一无二的身份标志。
这个标志基于 StatefulSet 控制器分配给每个 Pod 的唯一顺序索引。
Pod 的名称的形式为<code>&lt;statefulset name&gt;-&lt;ordinal index&gt;</code>。
<code>web</code>StatefulSet 拥有两个副本，所以它创建了两个 Pod：<code>web-0</code>和<code>web-1</code>。</p>
<h3 id="使用稳定的网络身份标识">使用稳定的网络身份标识</h3>
<p>每个 Pod 都拥有一个基于其顺序索引的稳定的主机名。使用<a href="/zh/docs/reference/generated/kubectl/kubectl-commands/#exec"><code>kubectl exec</code></a>在每个 Pod 中执行<code>hostname</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span> -- sh -c <span style="color:#b44">&#39;hostname&#39;</span>; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>web-0
web-1
</code></pre><!--
Use [`kubectl run`](/docs/reference/generated/kubectl/kubectl-commands/#run) to execute
a container that provides the `nslookup` command from the `dnsutils` package.
Using `nslookup` on the Pods' hostnames, you can examine their in-cluster DNS
addresses.
-->
<p>使用 <a href="/zh/docs/reference/generated/kubectl/kubectl-commands/#run"><code>kubectl run</code></a>
运行一个提供 <code>nslookup</code> 命令的容器，该命令来自于 <code>dnsutils</code> 包。
通过对 Pod 的主机名执行 <code>nslookup</code>，你可以检查他们在集群内部的 DNS 地址。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run -i --tty --image busybox:1.28 dns-test --restart<span style="color:#666">=</span>Never --rm
</code></pre></div><!--
which starts a new shell. In that new shell, run:
-->
<p>这将启动一个新的 shell。在新 shell 中，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># Run this in the dns-test container shell</span>
nslookup web-0.nginx
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.6

nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.6
</code></pre><!--
The CNAME of the headless service points to SRV records (one for each Pod that
is Running and Ready). The SRV records point to A record entries that
contain the Pods' IP addresses.

In one terminal, watch the StatefulSet's Pods.
-->
<p>headless service 的 CNAME 指向 SRV 记录（记录每个 Running 和 Ready 状态的 Pod）。
SRV 记录指向一个包含 Pod IP 地址的记录表项。</p>
<p>在一个终端中查看 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In a second terminal, use
[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands/#delete) to delete all
the Pods in the StatefulSet.
-->
<p>在另一个终端中使用 <a href="/zh/docs/reference/generated/kubectl/kubectl-commands/#delete"><code>kubectl delete</code></a> 删除 StatefulSet 中所有的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>pod &quot;web-0&quot; deleted
pod &quot;web-1&quot; deleted
</code></pre><!--
Wait for the StatefulSet to restart them, and for both Pods to transition to
Running and Ready.
-->
<p>等待 StatefulSet 重启它们，并且两个 Pod 都变成 Running 和 Ready 状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         34s
</code></pre><!--
Use `kubectl exec` and `kubectl run` to view the Pods hostnames and in-cluster
DNS entries.
-->
<p>使用 <code>kubectl exec</code> 和 <code>kubectl run</code> 查看 Pod 的主机名和集群内部的 DNS 表项。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> web-<span style="color:#b8860b">$i</span> -- sh -c <span style="color:#b44">&#39;hostname&#39;</span>; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>web-0
web-1
</code></pre><!--
then, run:
-->
<p>然后，运行：</p>
<pre><code>kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh
</code></pre><!--
which starts a new shell.  
In that new shell, run:
-->
<p>这将启动一个新的 shell。在新 shell 中，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># Run this in the dns-test container shell</span>
nslookup web-0.nginx
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.7

nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.8
</code></pre><!--
The Pods' ordinals, hostnames, SRV records, and A record names have not changed,
but the IP addresses associated with the Pods may have changed. In the cluster
used for this tutorial, they have. This is why it is important not to configure
other applications to connect to Pods in a StatefulSet by IP address.


If you need to find and connect to the active members of a StatefulSet, you
should query the CNAME of the Headless Service
(`nginx.default.svc.cluster.local`). The SRV records associated with the
CNAME will contain only the Pods in the StatefulSet that are Running and
Ready.

If your application already implements connection logic that tests for
liveness and readiness, you can use the SRV records of the Pods (
`web-0.nginx.default.svc.cluster.local`,
`web-1.nginx.default.svc.cluster.local`), as they are stable, and your
application will be able to discover the Pods' addresses when they transition
to Running and Ready.
-->
<p>Pod 的序号、主机名、SRV 条目和记录名称没有改变，但和 Pod 相关联的 IP 地址可能发生了改变。
在本教程中使用的集群中它们就改变了。这就是为什么不要在其他应用中使用 StatefulSet 中的 Pod 的 IP 地址进行连接，这点很重要。</p>
<p>如果你需要查找并连接一个 StatefulSet 的活动成员，你应该查询 Headless Service 的 CNAME。
和 CNAME 相关联的 SRV 记录只会包含 StatefulSet 中处于 Running 和 Ready 状态的 Pod。</p>
<p>如果你的应用已经实现了用于测试 liveness 和 readiness 的连接逻辑，你可以使用 Pod 的 SRV 记录（<code>web-0.nginx.default.svc.cluster.local</code>，
<code>web-1.nginx.default.svc.cluster.local</code>）。因为他们是稳定的，并且当你的 Pod 的状态变为 Running 和 Ready 时，你的应用就能够发现它们的地址。</p>
<!--
### Writing to Stable Storage

Get the PersistentVolumeClaims for `web-0` and `web-1`.
-->
<h3 id="写入稳定的存储">写入稳定的存储</h3>
<p>获取 <code>web-0</code> 和 <code>web-1</code> 的 PersistentVolumeClaims。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pvc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s
</code></pre><!--
The StatefulSet controller created two PersistentVolumeClaims that are
bound to two [PersistentVolumes](/docs/concepts/storage/persistent-volumes/). As the cluster used in this tutorial is configured to dynamically provision
PersistentVolumes, the PersistentVolumes were created and bound automatically.

The NGINX webservers, by default, will serve an index file at
`/usr/share/nginx/html/index.html`. The `volumeMounts` field in the
StatefulSets `spec` ensures that the `/usr/share/nginx/html` directory is
backed by a PersistentVolume.

Write the Pods' hostnames to their `index.html` files and verify that the NGINX
webservers serve the hostnames.
-->
<p>StatefulSet 控制器创建了两个 PersistentVolumeClaims，绑定到两个 <a href="/zh/docs/concepts/storage/volumes/">PersistentVolumes</a>。由于本教程使用的集群配置为动态提供 PersistentVolume，所有的 PersistentVolume 都是自动创建和绑定的。</p>
<p>NGINX web 服务器默认会加载位于 <code>/usr/share/nginx/html/index.html</code> 的 index 文件。
StatefulSets <code>spec</code> 中的 <code>volumeMounts</code> 字段保证了 <code>/usr/share/nginx/html</code> 文件夹由一个 PersistentVolume 支持。</p>
<p>将 Pod 的主机名写入它们的<code>index.html</code>文件并验证 NGINX web 服务器使用该主机名提供服务。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span> -- sh -c <span style="color:#b44">&#39;echo &#34;$(hostname)&#34; &gt; /usr/share/nginx/html/index.html&#39;</span>; <span style="color:#a2f;font-weight:bold">done</span>

<span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> -i -t <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span> -- curl http://localhost/; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>web-0
web-1
</code></pre><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you instead see **403 Forbidden** responses for the above curl command,
you will need to fix the permissions of the directory mounted by the `volumeMounts`
(due to a [bug when using hostPath volumes](https://github.com/kubernetes/kubernetes/issues/2630)),
by running:
-->
<p>请注意，如果你看见上面的 curl 命令返回了 <strong>403 Forbidden</strong> 的响应，你需要像这样修复使用 <code>volumeMounts</code>
（原因归咎于<a href="https://github.com/kubernetes/kubernetes/issues/2630">使用 hostPath 卷时存在的缺陷</a>）
挂载的目录的权限
运行：</p>
<p><code>for i in 0 1; do kubectl exec web-$i -- chmod 755 /usr/share/nginx/html; done</code></p>
<!--
before retrying the `curl` command above.
-->
<p>在你重新尝试上面的 <code>curl</code> 命令之前。</p>

</div>
<!--
In one terminal, watch the StatefulSet's Pods.
-->
<p>在一个终端查看 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In a second terminal, delete all of the StatefulSet's Pods.
-->
<p>在另一个终端删除 StatefulSet 所有的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>pod &quot;web-0&quot; deleted
pod &quot;web-1&quot; deleted
</code></pre><!--
Examine the output of the `kubectl get` command in the first terminal, and wait
for all of the Pods to transition to Running and Ready.
-->
<p>在第一个终端里检查 <code>kubectl get</code> 命令的输出，等待所有 Pod 变成 Running 和 Ready 状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         34s
</code></pre><!--
Verify the web servers continue to serve their hostnames.
-->
<p>验证所有 web 服务器在继续使用它们的主机名提供服务。</p>
<pre><code>for i in 0 1; do kubectl exec -i -t &quot;web-$i&quot; -- curl http://localhost/; done
</code></pre><pre><code>web-0
web-1
</code></pre><!--
Even though `web-0` and `web-1` were rescheduled, they continue to serve their
hostnames because the PersistentVolumes associated with their
PersistentVolumeClaims are remounted to their `volumeMounts`. No matter what
node `web-0`and `web-1` are scheduled on, their PersistentVolumes will be
mounted to the appropriate mount points.

## Scaling a StatefulSet
Scaling a StatefulSet refers to increasing or decreasing the number of replicas.
This is accomplished by updating the `replicas` field. You can use either
[`kubectl scale`](/docs/reference/generated/kubectl/kubectl-commands/#scale) or
[`kubectl patch`](/docs/reference/generated/kubectl/kubectl-commands/#patch) to scale a StatefulSet.

### Scaling Up

In one terminal window, watch the Pods in the StatefulSet.
-->
<p>虽然 <code>web-0</code> 和 <code>web-1</code> 被重新调度了，但它们仍然继续监听各自的主机名，因为和它们的 PersistentVolumeClaim 相关联的 PersistentVolume 被重新挂载到了各自的 <code>volumeMount</code> 上。
不管 <code>web-0</code> 和 <code>web-1</code> 被调度到了哪个节点上，它们的 PersistentVolumes 将会被挂载到合适的挂载点上。</p>
<h2 id="扩容-缩容-statefulset">扩容/缩容 StatefulSet</h2>
<p>扩容/缩容 StatefulSet 指增加或减少它的副本数。这通过更新 <code>replicas</code> 字段完成。
你可以使用<a href="/zh/docs/user-guide/kubectl/v1.23/#scale"><code>kubectl scale</code></a>
或者<a href="/zh/docs/user-guide/kubectl/v1.23/#patch"><code>kubectl patch</code></a>来扩容/缩容一个 StatefulSet。</p>
<h3 id="扩容">扩容</h3>
<p>在一个终端窗口观察 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In another terminal window, use `kubectl scale` to scale the number of replicas
to 5.-->
<p>在另一个终端窗口使用 <code>kubectl scale</code> 扩展副本数为 5。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale sts web --replicas<span style="color:#666">=</span><span style="color:#666">5</span>
</code></pre></div><pre><code>statefulset.apps/web scaled
</code></pre><!--
Examine the output of the `kubectl get` command in the first terminal, and wait
for the three additional Pods to transition to Running and Ready.
-->
<p>在第一个 终端中检查 <code>kubectl get</code> 命令的输出，等待增加的 3 个 Pod 的状态变为 Running 和 Ready。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2h
web-1     1/1       Running   0          2h
NAME      READY     STATUS    RESTARTS   AGE
web-2     0/1       Pending   0          0s
web-2     0/1       Pending   0         0s
web-2     0/1       ContainerCreating   0         0s
web-2     1/1       Running   0         19s
web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         0s
web-3     0/1       ContainerCreating   0         0s
web-3     1/1       Running   0         18s
web-4     0/1       Pending   0         0s
web-4     0/1       Pending   0         0s
web-4     0/1       ContainerCreating   0         0s
web-4     1/1       Running   0         19s
</code></pre><!--
The StatefulSet controller scaled the number of replicas. As with
[StatefulSet creation](#ordered-pod-creation), the StatefulSet controller
created each Pod sequentially with respect to its ordinal index, and it
waited for each Pod's predecessor to be Running and Ready before launching the
subsequent Pod.

### Scaling Down

In one terminal, watch the StatefulSet's Pods.
-->
<p>StatefulSet 控制器扩展了副本的数量。
如同<a href="#%E9%A1%BA%E5%BA%8F%E5%88%9B%E5%BB%BApod">创建 StatefulSet</a> 所述，StatefulSet 按序号索引顺序的创建每个 Pod，并且会等待前一个 Pod 变为 Running 和 Ready 才会启动下一个 Pod。</p>
<h3 id="缩容">缩容</h3>
<p>在一个终端观察 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In another terminal, use `kubectl patch` to scale the StatefulSet back down to
three replicas.
-->
<p>在另一个终端使用 <code>kubectl patch</code> 将 StatefulSet 缩容回三个副本。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch sts web -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;replicas&#34;:3}}&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
Wait for `web-4` and `web-3` to transition to Terminating.
-->
<p>等待 <code>web-4</code> 和 <code>web-3</code> 状态变为 Terminating。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          3h
web-1     1/1       Running             0          3h
web-2     1/1       Running             0          55s
web-3     1/1       Running             0          36s
web-4     0/1       ContainerCreating   0          18s
NAME      READY     STATUS    RESTARTS   AGE
web-4     1/1       Running   0          19s
web-4     1/1       Terminating   0         24s
web-4     1/1       Terminating   0         24s
web-3     1/1       Terminating   0         42s
web-3     1/1       Terminating   0         42s
</code></pre><!--
### Ordered Pod Termination

The controller deleted one Pod at a time, in reverse order with respect to its
ordinal index, and it waited for each to be completely shutdown before
deleting the next.

Get the StatefulSet's PersistentVolumeClaims.
-->
<h3 id="顺序终止-pod">顺序终止 Pod</h3>
<p>控制器会按照与 Pod 序号索引相反的顺序每次删除一个 Pod。在删除下一个 Pod 前会等待上一个被完全关闭。</p>
<p>获取 StatefulSet 的 PersistentVolumeClaims。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pvc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-2   Bound     pvc-e1125b27-b508-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-3   Bound     pvc-e1176df6-b508-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-4   Bound     pvc-e11bb5f8-b508-11e6-932f-42010a800002   1Gi        RWO           13h

</code></pre><!--
There are still five PersistentVolumeClaims and five PersistentVolumes.
When exploring a Pod's [stable storage](#writing-to-stable-storage), we saw that the PersistentVolumes mounted to the Pods of a StatefulSet are not deleted when the StatefulSet's Pods are deleted. This is still true when Pod deletion is caused by scaling the StatefulSet down.

## Updating StatefulSets

In Kubernetes 1.7 and later, the StatefulSet controller supports automated updates.  The
strategy used is determined by the `spec.updateStrategy` field of the
StatefulSet API Object. This feature can be used to upgrade the container
images, resource requests and/or limits, labels, and annotations of the Pods in a
StatefulSet. There are two valid update strategies, `RollingUpdate` and
`OnDelete`.

`RollingUpdate` update strategy is the default for StatefulSets.
-->
<p>五个 PersistentVolumeClaims 和五个 PersistentVolumes 仍然存在。
查看 Pod 的 <a href="#stable-storage">稳定存储</a>，我们发现当删除 StatefulSet 的 Pod 时，挂载到 StatefulSet 的 Pod 的 PersistentVolumes 不会被删除。
当这种删除行为是由 StatefulSet 缩容引起时也是一样的。</p>
<h2 id="更新-statefulset">更新 StatefulSet</h2>
<p>Kubernetes 1.7 版本的 StatefulSet 控制器支持自动更新。
更新策略由 StatefulSet API Object 的<code>spec.updateStrategy</code> 字段决定。这个特性能够用来更新一个 StatefulSet 中的 Pod 的 container images，resource requests，以及 limits，labels 和 annotations。
<code>RollingUpdate</code>滚动更新是 StatefulSets 默认策略。</p>
<!--
The `RollingUpdate` update strategy will update all Pods in a StatefulSet, in
reverse ordinal order, while respecting the StatefulSet guarantees.

Patch the `web` StatefulSet to apply the `RollingUpdate` update strategy.
-->
<h3 id="rolling-update-策略">Rolling Update 策略</h3>
<p><code>RollingUpdate</code> 更新策略会更新一个 StatefulSet 中所有的 Pod，采用与序号索引相反的顺序并遵循 StatefulSet 的保证。</p>
<p>Patch <code>web</code> StatefulSet 来执行 <code>RollingUpdate</code> 更新策略。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;updateStrategy&#34;:{&#34;type&#34;:&#34;RollingUpdate&#34;}}}&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
In one terminal window, patch the `web` StatefulSet to change the container
image again.
-->
<p>在一个终端窗口中 patch <code>web</code> StatefulSet 来再次的改变容器镜像。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web --type<span style="color:#666">=</span><span style="color:#b44">&#39;json&#39;</span> -p<span style="color:#666">=</span><span style="color:#b44">&#39;[{&#34;op&#34;: &#34;replace&#34;, &#34;path&#34;: &#34;/spec/template/spec/containers/0/image&#34;, &#34;value&#34;:&#34;gcr.io/google_containers/nginx-slim:0.8&#34;}]&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
In another terminal, watch the Pods in the StatefulSet.
-->
<p>在另一个终端监控 StatefulSet 中的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get po -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          7m
web-1     1/1       Running   0          7m
web-2     1/1       Running   0          8m
web-2     1/1       Terminating   0         8m
web-2     1/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Pending   0         0s
web-2     0/1       Pending   0         0s
web-2     0/1       ContainerCreating   0         0s
web-2     1/1       Running   0         19s
web-1     1/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         6s
web-0     1/1       Terminating   0         7m
web-0     1/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Pending   0         0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         10s
</code></pre><!--
The Pods in the StatefulSet are updated in reverse ordinal order. The
StatefulSet controller terminates each Pod, and waits for it to transition to Running and
Ready prior to updating the next Pod. Note that, even though the StatefulSet
controller will not proceed to update the next Pod until its ordinal successor
is Running and Ready, it will restore any Pod that fails during the update to
its current version. Pods that have already received the update will be
restored to the updated version, and Pods that have not yet received the
update will be restored to the previous version. In this way, the controller
attempts to continue to keep the application healthy and the update consistent
in the presence of intermittent failures.

Get the Pods to view their container images.
-->
<p>StatefulSet 里的 Pod 采用和序号相反的顺序更新。在更新下一个 Pod 前，StatefulSet 控制器终止每个 Pod 并等待它们变成 Running 和 Ready。
请注意，虽然在顺序后继者变成 Running 和 Ready 之前 StatefulSet 控制器不会更新下一个 Pod，但它仍然会重建任何在更新过程中发生故障的 Pod，使用的是它们当前的版本。
已经接收到更新请求的 Pod 将会被恢复为更新的版本，没有收到请求的 Pod 则会被恢复为之前的版本。
像这样，控制器尝试继续使应用保持健康并在出现间歇性故障时保持更新的一致性。</p>
<p>获取 Pod 来查看他们的容器镜像。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> p in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl get pod <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$p</span><span style="color:#b44">&#34;</span> --template <span style="color:#b44">&#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}&#39;</span>; echo; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>k8s.gcr.io/nginx-slim:0.8
k8s.gcr.io/nginx-slim:0.8
k8s.gcr.io/nginx-slim:0.8

</code></pre><!--
All the Pods in the StatefulSet are now running the previous container image.

**Tip** You can also use `kubectl rollout status sts/<name>` to view
the status of a rolling update.

#### Staging an Update
You can stage an update to a StatefulSet by using the `partition` parameter of
the `RollingUpdate` update strategy. A staged update will keep all of the Pods
in the StatefulSet at the current version while allowing mutations to the
StatefulSet's `.spec.template`.

Patch the `web` StatefulSet to add a partition to the `updateStrategy` field.
-->
<p>StatefulSet 中的所有 Pod 现在都在运行之前的容器镜像。</p>
<p><strong>小窍门</strong>：你还可以使用 <code>kubectl rollout status sts/&lt;name&gt;</code> 来查看 rolling update 的状态。</p>
<h4 id="分段更新">分段更新</h4>
<p>你可以使用 <code>RollingUpdate</code> 更新策略的 <code>partition</code> 参数来分段更新一个 StatefulSet。
分段的更新将会使 StatefulSet 中的其余所有 Pod 保持当前版本的同时仅允许改变 StatefulSet 的  <code>.spec.template</code>。</p>
<p>Patch <code>web</code> StatefulSet 来对 <code>updateStrategy</code> 字段添加一个分区。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;updateStrategy&#34;:{&#34;type&#34;:&#34;RollingUpdate&#34;,&#34;rollingUpdate&#34;:{&#34;partition&#34;:3}}}}&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
Patch the StatefulSet again to change the container's image.
-->
<p>再次 Patch StatefulSet 来改变容器镜像。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web --type<span style="color:#666">=</span><span style="color:#b44">&#39;json&#39;</span> -p<span style="color:#666">=</span><span style="color:#b44">&#39;[{&#34;op&#34;: &#34;replace&#34;, &#34;path&#34;: &#34;/spec/template/spec/containers/0/image&#34;, &#34;value&#34;:&#34;k8s.gcr.io/nginx-slim:0.7&#34;}]&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
Delete a Pod in the StatefulSet.
-->
<p>删除 StatefulSet 中的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod web-2
</code></pre></div><pre><code>pod &quot;web-2&quot; deleted
</code></pre><!--
Wait for the Pod to be Running and Ready.
-->
<p>等待 Pod 变成 Running 和 Ready。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s
</code></pre><!--
Get the Pod's container.
-->
<p>获取 Pod 的容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod web-2 --template <span style="color:#b44">&#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}&#39;</span>
</code></pre></div><pre><code>k8s.gcr.io/nginx-slim:0.8
</code></pre><!--
Notice that, even though the update strategy is `RollingUpdate` the StatefulSet
controller restored the Pod with its original container. This is because the
ordinal of the Pod is less than the `partition` specified by the
`updateStrategy`.

#### Rolling Out a Canary
You can roll out a canary to test a modification by decrementing the `partition`
you specified [above](#staging-an-update).

Patch the StatefulSet to decrement the partition.
-->
<p>请注意，虽然更新策略是 <code>RollingUpdate</code>，StatefulSet 控制器还是会使用原始的容器恢复 Pod。
这是因为 Pod 的序号比 <code>updateStrategy</code> 指定的 <code>partition</code> 更小。</p>
<h4 id="灰度发布">灰度发布</h4>
<p>你可以通过减少 <a href="#%E5%88%86%E6%AE%B5%E6%9B%B4%E6%96%B0">上文</a>指定的 <code>partition</code> 来进行灰度发布，以此来测试你的程序的改动。</p>
<p>通过 patch 命令修改 StatefulSet 来减少分区。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;updateStrategy&#34;:{&#34;type&#34;:&#34;RollingUpdate&#34;,&#34;rollingUpdate&#34;:{&#34;partition&#34;:2}}}}&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
Wait for `web-2` to be Running and Ready.
-->
<p>等待 <code>web-2</code> 变成 Running 和 Ready。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s
</code></pre><!--
Get the Pod's container.
-->
<p>获取 Pod 的容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod web-2 --template <span style="color:#b44">&#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}&#39;</span>
</code></pre></div><pre><code>k8s.gcr.io/nginx-slim:0.7

</code></pre><!--
When you changed the `partition`, the StatefulSet controller automatically
updated the `web-2` Pod because the Pod's ordinal was greater than or equal to
the `partition`.

Delete the `web-1` Pod.
-->
<p>当你改变 <code>partition</code> 时，StatefulSet 会自动的更新 <code>web-2</code> Pod，这是因为 Pod 的序号大于或等于 <code>partition</code>。</p>
<p>删除 <code>web-1</code> Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod web-1
</code></pre></div><pre><code>pod &quot;web-1&quot; deleted
</code></pre><!--
Wait for the `web-1` Pod to be Running and Ready.
-->
<p>等待 <code>web-1</code> 变成 Running 和 Ready。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>NAME      READY     STATUS        RESTARTS   AGE
web-0     1/1       Running       0          6m
web-1     0/1       Terminating   0          6m
web-2     1/1       Running       0          2m
web-1     0/1       Terminating   0         6m
web-1     0/1       Terminating   0         6m
web-1     0/1       Terminating   0         6m
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         18s
</code></pre><!--
Get the `web-1` Pods container.
-->
<p>获取 <code>web-1</code> Pod 的容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod web-1 --template <span style="color:#b44">&#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}&#39;</span>
</code></pre></div><pre><code>k8s.gcr.io/nginx-slim:0.8
</code></pre><!--
`web-1` was restored to its original configuration because the Pod's ordinal
was less than the partition. When a partition is specified, all Pods with an
ordinal that is greater than or equal to the partition will be updated when the
StatefulSet's `.spec.template` is updated. If a Pod that has an ordinal less
than the partition is deleted or otherwise terminated, it will be restored to
its original configuration.

#### Phased Roll Outs
You can perform a phased roll out (e.g. a linear, geometric, or exponential
roll out) using a partitioned rolling update in a similar manner to how you
rolled out a [canary](#rolling-out-a-canary). To perform a phased roll out, set
the `partition` to the ordinal at which you want the controller to pause the
update.

The partition is currently set to `2`. Set the partition to `0`.
-->
<p><code>web-1</code> 被按照原来的配置恢复，因为 Pod 的序号小于分区。当指定了分区时，如果更新了 StatefulSet 的 <code>.spec.template</code>，则所有序号大于或等于分区的 Pod 都将被更新。
如果一个序号小于分区的 Pod 被删除或者终止，它将被按照原来的配置恢复。</p>
<h4 id="分阶段的发布">分阶段的发布</h4>
<p>你可以使用类似<a href="#%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83">灰度发布</a>的方法执行一次分阶段的发布（例如一次线性的、等比的或者指数形式的发布）。
要执行一次分阶段的发布，你需要设置 <code>partition</code> 为希望控制器暂停更新的序号。</p>
<p>分区当前为<code>2</code>。请将分区设置为<code>0</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;updateStrategy&#34;:{&#34;type&#34;:&#34;RollingUpdate&#34;,&#34;rollingUpdate&#34;:{&#34;partition&#34;:0}}}}&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
Wait for all of the Pods in the StatefulSet to become Running and Ready.
-->
<p>等待 StatefulSet 中的所有 Pod 变成 Running 和 Ready。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          3m
web-1     0/1       ContainerCreating   0          11s
web-2     1/1       Running             0          2m
web-1     1/1       Running   0         18s
web-0     1/1       Terminating   0         3m
web-0     1/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Pending   0         0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         3s
</code></pre><!--
Get the Pod's containers.
-->
<p>获取 Pod 的容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> p in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl get pod <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$p</span><span style="color:#b44">&#34;</span> --template <span style="color:#b44">&#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}&#39;</span>; echo; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>k8s.gcr.io/nginx-slim:0.7
k8s.gcr.io/nginx-slim:0.7
k8s.gcr.io/nginx-slim:0.7
</code></pre><!--
By moving the `partition` to `0`, you allowed the StatefulSet controller to
continue the update process.

### On Delete

The `OnDelete` update strategy implements the legacy (1.6 and prior) behavior,
When you select this update strategy, the StatefulSet controller will not
automatically update Pods when a modification is made to the StatefulSet's
`.spec.template` field. This strategy can be selected by setting the
`.spec.template.updateStrategy.type` to `OnDelete`.


## Deleting StatefulSets

StatefulSet supports both Non-Cascading and Cascading deletion. In a
Non-Cascading Delete, the StatefulSet's Pods are not deleted when the StatefulSet is deleted. In a Cascading Delete, both the StatefulSet and its Pods are
deleted.

### Non-Cascading Delete

In one terminal window, watch the Pods in the StatefulSet.
-->
<p>将 <code>partition</code> 改变为 <code>0</code> 以允许 StatefulSet 控制器继续更新过程。</p>
<h3 id="on-delete-策略">On Delete 策略</h3>
<p><code>OnDelete</code> 更新策略实现了传统（1.7 之前）行为，它也是默认的更新策略。
当你选择这个更新策略并修改 StatefulSet 的 <code>.spec.template</code> 字段时，StatefulSet 控制器将不会自动的更新 Pod。</p>
<h2 id="删除-statefulset">删除 StatefulSet</h2>
<p>StatefulSet 同时支持级联和非级联删除。使用非级联方式删除 StatefulSet 时，StatefulSet 的 Pod 不会被删除。使用级联删除时，StatefulSet 和它的 Pod 都会被删除。</p>
<h3 id="非级联删除">非级联删除</h3>
<p>在一个终端窗口查看 StatefulSet 中的 Pod。</p>
<pre><code>kubectl get pods -w -l app=nginx
</code></pre><!--
Use [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands/#delete) to delete the
StatefulSet. Make sure to supply the `--cascade=orphan` parameter to the
command. This parameter tells Kubernetes to only delete the StatefulSet, and to
not delete any of its Pods.
-->
<p>使用 <a href="/zh/docs/reference/generated/kubectl/kubectl-commands/#delete"><code>kubectl delete</code></a> 删除 StatefulSet。
请确保提供了 <code>--cascade=orphan</code> 参数给命令。这个参数告诉 Kubernetes 只删除 StatefulSet 而不要删除它的任何 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete statefulset web --cascade<span style="color:#666">=</span>orphan
</code></pre></div><pre><code>statefulset.apps &quot;web&quot; deleted
</code></pre><!--
Get the Pods to examine their status.
-->
<p>获取 Pod 来检查他们的状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          6m
web-1     1/1       Running   0          7m
web-2     1/1       Running   0          5m
</code></pre><!--
Even though `web` has been deleted, all of the Pods are still Running and Ready.
Delete `web-0`.
-->
<p>虽然 <code>web</code>  已经被删除了，但所有 Pod 仍然处于 Running 和 Ready 状态。
删除 <code>web-0</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod web-0
</code></pre></div><pre><code>pod &quot;web-0&quot; deleted
</code></pre><!--
Get the StatefulSet's Pods.
-->
<p>获取 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-1     1/1       Running   0          10m
web-2     1/1       Running   0          7m
</code></pre><!--
As the `web` StatefulSet has been deleted, `web-0` has not been relaunched.

In one terminal, watch the StatefulSet's Pods.
-->
<p>由于 <code>web</code> StatefulSet 已经被删除，<code>web-0</code>没有被重新启动。</p>
<p>在一个终端监控 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In a second terminal, recreate the StatefulSet. Note that, unless
you deleted the `nginx` Service ( which you should not have ), you will see
an error indicating that the Service already exists.
-->
<p>在另一个终端里重新创建 StatefulSet。请注意，除非你删除了 <code>nginx</code> Service （你不应该这样做），你将会看到一个错误，提示 Service 已经存在。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f web.yaml
</code></pre></div><pre><code>statefulset.apps/web created
service/nginx unchanged
</code></pre><!--
Ignore the error. It only indicates that an attempt was made to create the nginx
Headless Service even though that Service already exists.

Examine the output of the `kubectl get` command running in the first terminal.
-->
<p>请忽略这个错误。它仅表示 kubernetes 进行了一次创建 nginx Headless Service 的尝试，尽管那个 Service 已经存在。</p>
<p>在第一个终端中运行并检查 <code>kubectl get</code> 命令的输出。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-1     1/1       Running   0          16m
web-2     1/1       Running   0          2m
NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         18s
web-2     1/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m
</code></pre><!--
When the `web` StatefulSet was recreated, it first relaunched `web-0`.
Since `web-1` was already Running and Ready, when `web-0` transitioned to
Running and Ready, it adopted this Pod. Since you recreated the StatefulSet
with `replicas` equal to 2, once `web-0` had been recreated, and once
`web-1` had been determined to already be Running and Ready, `web-2` was
terminated.

Let's take another look at the contents of the `index.html` file served by the
Pods' webservers:
-->
<p>当重新创建 <code>web</code> StatefulSet 时，<code>web-0</code> 被第一个重新启动。
由于 <code>web-1</code> 已经处于 Running 和 Ready 状态，当 <code>web-0</code> 变成 Running 和 Ready 时，
StatefulSet 会接收这个 Pod。由于你重新创建的 StatefulSet 的 <code>replicas</code> 等于 2，
一旦 <code>web-0</code> 被重新创建并且 <code>web-1</code> 被认为已经处于 Running 和 Ready 状态时，<code>web-2</code> 将会被终止。</p>
<p>让我们再看看被 Pod 的 web 服务器加载的 <code>index.html</code> 的内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> -i -t <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span> -- curl http://localhost/; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>web-0
web-1
</code></pre><!--
Even though you deleted both the StatefulSet and the `web-0` Pod, it still
serves the hostname originally entered into its `index.html` file. This is
because the StatefulSet never deletes the PersistentVolumes associated with a
Pod. When you recreated the StatefulSet and it relaunched `web-0`, its original
PersistentVolume was remounted.

### Cascading Delete

In one terminal window, watch the Pods in the StatefulSet.
-->
<p>尽管你同时删除了 StatefulSet 和 <code>web-0</code> Pod，但它仍然使用最初写入 <code>index.html</code> 文件的主机名进行服务。
这是因为 StatefulSet 永远不会删除和一个 Pod 相关联的 PersistentVolumes。
当你重建这个 StatefulSet 并且重新启动了 <code>web-0</code> 时，它原本的 PersistentVolume 会被重新挂载。</p>
<h3 id="级联删除">级联删除</h3>
<p>在一个终端窗口观察 StatefulSet 里的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In another terminal, delete the StatefulSet again. This time, omit the
`--cascade=orphan` parameter.
-->
<p>在另一个窗口中再次删除这个 StatefulSet。这次省略 <code>--cascade=orphan</code> 参数。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete statefulset web
</code></pre></div><pre><code>statefulset.apps &quot;web&quot; deleted
</code></pre><!--
Examine the output of the `kubectl get` command running in the first terminal,
and wait for all of the Pods to transition to Terminating.
-->
<p>在第一个终端检查 <code>kubectl get</code> 命令的输出，并等待所有的 Pod 变成 Terminating 状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          11m
web-1     1/1       Running   0          27m
NAME      READY     STATUS        RESTARTS   AGE
web-0     1/1       Terminating   0          12m
web-1     1/1       Terminating   0         29m
web-0     0/1       Terminating   0         12m
web-0     0/1       Terminating   0         12m
web-0     0/1       Terminating   0         12m
web-1     0/1       Terminating   0         29m
web-1     0/1       Terminating   0         29m
web-1     0/1       Terminating   0         29m

</code></pre><!--
As you saw in the [Scaling Down](#scaling-down) section, the Pods
are terminated one at a time, with respect to the reverse order of their ordinal
indices. Before terminating a Pod, the StatefulSet controller waits for
the Pod's successor to be completely terminated.

Note that, while a cascading delete will delete the StatefulSet and its Pods,
it will not delete the Headless Service associated with the StatefulSet. You
must delete the `nginx` Service manually.
-->
<p>如同你在<a href="#ordered-pod-termination">缩容</a>一节看到的，Pod 按照和他们序号索引相反的顺序每次终止一个。
在终止一个 Pod 前，StatefulSet 控制器会等待 Pod 后继者被完全终止。</p>
<p>请注意，虽然级联删除会删除 StatefulSet 和它的 Pod，但它并不会删除和 StatefulSet 关联的 Headless Service。你必须手动删除<code>nginx</code> Service。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete service nginx
</code></pre></div><pre><code>service &quot;nginx&quot; deleted
</code></pre><!--
Recreate the StatefulSet and Headless Service one more time.
-->
<p>再一次重新创建 StatefulSet 和 Headless Service。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f web.yaml
</code></pre></div><pre><code>service/nginx created
statefulset.apps/web created
</code></pre><!--
When all of the StatefulSet's Pods transition to Running and Ready, retrieve
the contents of their `index.html` files.
-->
<p>当 StatefulSet 所有的 Pod 变成 Running 和 Ready 时，获取它们的 <code>index.html</code> 文件的内容。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> -i -t <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span> -- curl http://localhost/; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>web-0
web-1
</code></pre><!--
Even though you completely deleted the StatefulSet, and all of its Pods, the
Pods are recreated with their PersistentVolumes mounted, and `web-0` and
`web-1` will still serve their hostnames.

Finally delete the `web` StatefulSet and the `nginx` service.
-->
<p>即使你已经删除了 StatefulSet 和它的全部 Pod，这些 Pod 将会被重新创建并挂载它们的 PersistentVolumes，并且 <code>web-0</code> 和 <code>web-1</code> 将仍然使用它们的主机名提供服务。</p>
<p>最后删除 <code>nginx</code> service...</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete service nginx
</code></pre></div><pre><code>service &quot;nginx&quot; deleted
</code></pre><p>... 并且删除 <code>web</code> StatefulSet:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete statefulset web
</code></pre></div><pre><code>statefulset &quot;web&quot; deleted
</code></pre><!--
## Pod Management Policy

For some distributed systems, the StatefulSet ordering guarantees are
unnecessary and/or undesirable. These systems require only uniqueness and
identity. To address this, in Kubernetes 1.7, we introduced
`.spec.podManagementPolicy` to the StatefulSet API Object.

### OrderedReady Pod Management

`OrderedReady` pod management is the default for StatefulSets. It tells the
StatefulSet controller to respect the ordering guarantees demonstrated
above.

### Parallel Pod Management

`Parallel` pod management tells the StatefulSet controller to launch or
terminate all Pods in parallel, and not to wait for Pods to become Running
and Ready or completely terminated prior to launching or terminating another
Pod. This option only affects the behavior for scaling operations. Updates are not affected.
-->
<h2 id="pod-管理策略">Pod 管理策略</h2>
<p>对于某些分布式系统来说，StatefulSet 的顺序性保证是不必要和/或者不应该的。
这些系统仅仅要求唯一性和身份标志。为了解决这个问题，在 Kubernetes 1.7 中
我们针对 StatefulSet API 对象引入了 <code>.spec.podManagementPolicy</code>。
此选项仅影响扩缩操作的行为。更新不受影响。</p>
<h3 id="orderedready-pod-管理策略">OrderedReady Pod 管理策略</h3>
<p><code>OrderedReady</code> pod 管理策略是 StatefulSets 的默认选项。它告诉 StatefulSet 控制器遵循上文展示的顺序性保证。</p>
<h3 id="parallel-pod-管理策略">Parallel Pod 管理策略</h3>
<p><code>Parallel</code> pod 管理策略告诉 StatefulSet 控制器并行的终止所有 Pod，
在启动或终止另一个 Pod 前，不必等待这些 Pod 变成 Running 和 Ready 或者完全终止状态。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/web/web-parallel.yaml" download="application/web/web-parallel.yaml"><code>application/web/web-parallel.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-web-web-parallel-yaml')" title="Copy application/web/web-parallel.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-web-web-parallel-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;nginx&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podManagementPolicy</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Parallel&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/nginx-slim:0.8<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Download the example above, and save it to a file named `web-parallel.yaml`

This manifest is identical to the one you downloaded above except that the `.spec.podManagementPolicy`
of the `web` StatefulSet is set to `Parallel`.

In one terminal, watch the Pods in the StatefulSet.
-->
<p>下载上面的例子并保存为 <code>web-parallel.yaml</code>。</p>
<p>这份清单和你在上文下载的完全一样，只是 <code>web</code> StatefulSet 的 <code>.spec.podManagementPolicy</code> 设置成了 <code>Parallel</code>。</p>
<p>在一个终端窗口查看 StatefulSet 中的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get po -lapp<span style="color:#666">=</span>nginx -w
</code></pre></div><!--
In another terminal, create the StatefulSet and Service in the manifest:
-->
<p>在另一个终端窗口创建清单中的 StatefulSet 和 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f web-parallel.yaml
</code></pre></div><pre><code>service/nginx created
statefulset.apps/web created
</code></pre><!--
Examine the output of the `kubectl get` command that you executed in the first terminal.
-->
<p>查看你在第一个终端中运行的 <code>kubectl get</code> 命令的输出。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-1     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         10s
web-1     1/1       Running   0         10s
</code></pre><!--
The StatefulSet controller launched both `web-0` and `web-1` at the same time.

Keep the second terminal open, and, in another terminal window scale the
StatefulSet.
-->
<p>StatefulSet 控制器同时启动了 <code>web-0</code> 和 <code>web-1</code>。</p>
<p>保持第二个终端打开，并在另一个终端窗口中扩容 StatefulSet。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale statefulset/web --replicas<span style="color:#666">=</span><span style="color:#666">4</span>
</code></pre></div><pre><code>statefulset.apps/web scaled
</code></pre><!--
Examine the output of the terminal where the `kubectl get` command is running.
-->
<p>在 <code>kubectl get</code> 命令运行的终端里检查它的输出。</p>
<pre><code>web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         7s
web-3     0/1       ContainerCreating   0         7s
web-2     1/1       Running   0         10s
web-3     1/1       Running   0         26s
</code></pre><!--
The StatefulSet launched two new Pods, and it did not wait for
the first to become Running and Ready prior to launching the second.

## Cleaning up

You should have two terminals open, ready for you to run `kubectl` commands as
part of cleanup.
-->
<p>StatefulSet 启动了两个新的 Pod，而且在启动第二个之前并没有等待第一个变成 Running 和 Ready 状态。</p>
<h2 id="cleaning-up">Cleaning up</h2>
<p>您应该打开两个终端，准备在清理过程中运行 <code>kubectl</code> 命令。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete sts web
<span style="color:#080;font-style:italic"># sts is an abbreviation for statefulset</span>
</code></pre></div><!--
You can watch `kubectl get` to see those Pods being deleted.
-->
<p>你可以监测 <code>kubectl get</code> 来查看那些 Pod 被删除</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><pre><code>web-3     1/1       Terminating   0         9m
web-2     1/1       Terminating   0         9m
web-3     1/1       Terminating   0         9m
web-2     1/1       Terminating   0         9m
web-1     1/1       Terminating   0         44m
web-0     1/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-3     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-1     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-2     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-1     0/1       Terminating   0         44m
web-1     0/1       Terminating   0         44m
web-1     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-3     0/1       Terminating   0         9m
web-3     0/1       Terminating   0         9m
web-3     0/1       Terminating   0         9m
</code></pre><!--
The StatefulSet controller deletes all Pods concurrently, it does not wait for
a Pod's ordinal successor to terminate prior to deleting that Pod.

Close the terminal where the `kubectl get` command is running and delete the `nginx`
Service.
-->
<p>StatefulSet 控制器将并发的删除所有 Pod，在删除一个 Pod 前不会等待它的顺序后继者终止。</p>
<p>关闭 <code>kubectl get</code> 命令运行的终端并删除<code>nginx</code> Service。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete svc nginx
</code></pre></div><h2 id="cleaning-up-1">Cleaning up</h2>
<!--
You also need to delete the persistent storage media for the PersistentVolumes
used in this tutorial.


Follow the necessary steps, based on your environment, storage configuration,
and provisioning method, to ensure that all storage is reclaimed.
-->
<p>你需要删除本教程中用到的 PersistentVolumes 的持久化存储介质。基于你的环境、存储配置和提供方式，按照必须的步骤保证回收所有的存储。</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-97489f0aa8ac2df31a0d6b444a7bde62">7 - Services</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-5642e8c51749e4fe2e6a2ccc207f1fab">7.1 - 使用 Source IP</h1>
    
	<!-- overview -->
<p>Kubernetes 集群中运行的应用通过 Service 抽象来互相查找、通信和与外部世界沟通。本文介绍被发送到不同类型 Services 的数据包源 IP 的变化过程，你可以根据你的需求改变这些行为。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<h2 id="术语表">术语表</h2>
<p>本文使用了下列术语：</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Network_address_translation">NAT</a>: 网络地址转换</li>
<li><a href="https://en.wikipedia.org/wiki/Network_address_translation#SNAT">Source NAT</a>: 替换数据包的源 IP, 通常为节点的 IP</li>
<li><a href="https://en.wikipedia.org/wiki/Network_address_translation#DNAT">Destination NAT</a>: 替换数据包的目的 IP, 通常为 Pod 的 IP</li>
<li><a href="/zh/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">VIP</a>: 一个虚拟 IP, 例如分配给每个 Kubernetes Service 的 IP</li>
<li><a href="/zh/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">Kube-proxy</a>: 一个网络守护程序，在每个节点上协调 Service VIP 管理</li>
</ul>
<h2 id="准备工作">准备工作</h2>
<p>你必须拥有一个正常工作的 Kubernetes 1.5 集群来运行此文档中的示例。该示例使用一个简单的 nginx webserver，通过一个HTTP消息头返回它接收到请求的源IP。你可以像下面这样创建它：</p>
<pre><code class="language-console" data-lang="console">kubectl create deployment source-ip-app --image=k8s.gcr.io/echoserver:1.4
</code></pre><p>输出结果为</p>
<pre><code>deployment.apps/source-ip-app created
</code></pre><h2 id="objectives">Objectives</h2>
<ul>
<li>通过多种类型的 Services 暴露一个简单应用</li>
<li>理解每种 Service 类型如何处理源 IP NAT</li>
<li>理解保留源IP所涉及的折中</li>
</ul>
<!-- lessoncontent -->
<h2 id="type-clusterip-类型-services-的-source-ip">Type=ClusterIP 类型 Services 的 Source IP</h2>
<p>如果你的 kube-proxy 运行在 <a href="/zh/docs/user-guide/services/#proxy-mode-iptables">iptables 模式</a>下，从集群内部发送到 ClusterIP 的包永远不会进行源地址 NAT，这从 Kubernetes 1.2 开始是默认选项。Kube-proxy 通过一个 <code>proxyMode</code> endpoint 暴露它的模式。</p>
<pre><code class="language-console" data-lang="console">kubectl get nodes
</code></pre><p>输出结果与以下结果类似:</p>
<pre><code>NAME                           STATUS     ROLES    AGE     VERSION
kubernetes-node-6jst   Ready      &lt;none&gt;   2h      v1.13.0
kubernetes-node-cx31   Ready      &lt;none&gt;   2h      v1.13.0
kubernetes-node-jj1t   Ready      &lt;none&gt;   2h      v1.13.0
</code></pre><p>从其中一个节点中得到代理模式</p>
<pre><code class="language-console" data-lang="console">kubernetes-node-6jst $ curl localhost:10249/proxyMode
</code></pre><p>输出结果为：</p>
<pre><code>iptables
</code></pre><p>你可以通过在source IP应用上创建一个Service来测试源IP保留。</p>
<pre><code class="language-console" data-lang="console">kubectl expose deployment source-ip-app --name=clusterip --port=80 --target-port=8080
</code></pre><p>输出结果为：</p>
<pre><code>service/clusterip exposed
</code></pre><pre><code class="language-console" data-lang="console">kubectl get svc clusterip
</code></pre><p>输出结果与以下结果类似：</p>
<pre><code>NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
clusterip    ClusterIP   10.0.170.92   &lt;none&gt;        80/TCP    51s
</code></pre><p>从相同集群中的一个 pod 访问这个 <code>ClusterIP</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run busybox -it --image<span style="color:#666">=</span>busybox:1.28 --restart<span style="color:#666">=</span>Never --rm
</code></pre></div><p>输出结果与以下结果类似：</p>
<pre><code>Waiting for pod default/busybox to be running, status is Pending, pod ready: false
If you don't see a command prompt, try pressing enter.
</code></pre><p>然后你可以在 Pod 内运行命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 在终端内使用&#34;kubectl run&#34;执行</span>

ip addr
</code></pre></div><pre><code>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1460 qdisc noqueue
    link/ether 0a:58:0a:f4:03:08 brd ff:ff:ff:ff:ff:ff
    inet 10.244.3.8/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::188a:84ff:feb0:26a5/64 scope link
       valid_lft forever preferred_lft forever
</code></pre><p>然后使用 <code>wget</code> 去请求本地 Web 服务器</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 用名为 &#34;clusterip&#34; 的服务的 IPv4 地址替换 &#34;10.0.170.92&#34;</span>

wget -qO - 10.0.170.92
</code></pre></div><pre><code>CLIENT VALUES:
client_address=10.244.3.8
command=GET
...
</code></pre><p>无论客户端 pod 和 服务端 pod 是否在相同的节点上，client_address 始终是客户端 pod 的 IP 地址。</p>
<h2 id="type-nodeport-类型-services-的-source-ip">Type=NodePort 类型 Services 的 Source IP</h2>
<p>从 Kubernetes 1.5 开始，发送给类型为 <a href="/zh/docs/user-guide/services/#nodeport">Type=NodePort</a> Services 的数据包默认进行源地址 NAT。你可以通过创建一个 <code>NodePort</code> Service 来进行测试：</p>
<pre><code class="language-console" data-lang="console">kubectl expose deployment source-ip-app --name=nodeport --port=80 --target-port=8080 --type=NodePort
</code></pre><p>输出结果为：</p>
<pre><code>service/nodeport exposed
</code></pre><pre><code class="language-console" data-lang="console">NODEPORT=$(kubectl get -o jsonpath=&quot;{.spec.ports[0].nodePort}&quot; services nodeport)
NODES=$(kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==&quot;InternalIP&quot;)].address }')
</code></pre><p>如果你的集群运行在一个云服务上，你可能需要为上面报告的 <code>nodes:nodeport</code> 开启一条防火墙规则。
现在，你可以通过上面分配的节点端口从外部访问这个 Service。</p>
<pre><code class="language-console" data-lang="console">for node in $NODES; do curl -s $node:$NODEPORT | grep -i client_address; done
</code></pre><p>输出结果与以下结果类似：</p>
<pre><code>client_address=10.180.1.1
client_address=10.240.0.5
client_address=10.240.0.3
</code></pre><p>请注意，这些并不是正确的客户端 IP，它们是集群的内部 IP。这是所发生的事情：</p>
<ul>
<li>客户端发送数据包到 <code>node2:nodePort</code></li>
<li><code>node2</code> 使用它自己的 IP 地址替换数据包的源 IP 地址（SNAT）</li>
<li><code>node2</code> 使用 pod IP 地址替换数据包的目的 IP 地址</li>
<li>数据包被路由到 node 1，然后交给 endpoint</li>
<li>Pod 的回复被路由回 node2</li>
<li>Pod 的回复被发送回给客户端</li>
</ul>
<p>用图表示：</p>
<figure>
<div class="mermaid">
    
graph LR;
  client(client)-->node2[节点 2];
  node2-->client;
  node2-. SNAT .->node1[节点 1];
  node1-. SNAT .->node2;
  node1-->endpoint(端点);

  classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
  classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
  class node1,node2,endpoint k8s;
  class client plain;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<p>为了防止这种情况发生，Kubernetes 提供了一个特性来保留客户端的源 IP 地址<a href="/zh/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">(点击此处查看可用特性)</a>。设置 <code>service.spec.externalTrafficPolicy</code> 的值为 <code>Local</code>，请求就只会被代理到本地 endpoints 而不会被转发到其它节点。这样就保留了最初的源 IP 地址。如果没有本地 endpoints，发送到这个节点的数据包将会被丢弃。这样在应用到数据包的任何包处理规则下，你都能依赖这个正确的 source-ip 使数据包通过并到达 endpoint。</p>
<p>设置 <code>service.spec.externalTrafficPolicy</code> 字段如下：</p>
<pre><code class="language-console" data-lang="console">kubectl patch svc nodeport -p '{&quot;spec&quot;:{&quot;externalTrafficPolicy&quot;:&quot;Local&quot;}}'
</code></pre><p>输出结果为：</p>
<pre><code>service/nodeport patched
</code></pre><p>现在，重新运行测试：</p>
<pre><code class="language-console" data-lang="console">for node in $NODES; do curl --connect-timeout 1 -s $node:$NODEPORT | grep -i client_address; done
</code></pre><p>输出结果为：</p>
<pre><code>client_address=104.132.1.79
</code></pre><p>请注意，你只从 endpoint pod 运行的那个节点得到了一个回复，这个回复有<em>正确的</em>客户端 IP。</p>
<p>这是发生的事情：</p>
<ul>
<li>客户端发送数据包到 <code>node2:nodePort</code>，它没有任何 endpoints</li>
<li>数据包被丢弃</li>
<li>客户端发送数据包到 <code>node1:nodePort</code>，它<em>有</em>endpoints</li>
<li>node1 使用正确的源 IP 地址将数据包路由到 endpoint</li>
</ul>
<p>用图表示：</p>
<figure>
<div class="mermaid">
    
graph TD;
  client --> node1[节点 1];
  client(client) --x node2[节点 2];
  node1 --> endpoint(端点);
  endpoint --> node1;

  classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
  classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
  class node1,node2,endpoint k8s;
  class client plain;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<h2 id="type-loadbalancer-类型-services-的-source-ip">Type=LoadBalancer 类型 Services 的 Source IP</h2>
<p>从Kubernetes1.5开始，发送给类型为 <a href="/zh/docs/user-guide/services/#type-nodeport">Type=LoadBalancer</a> Services 的数据包默认进行源地址 NAT，这是因为所有处于 <code>Ready</code> 状态的可调度 Kubernetes 节点对于负载均衡的流量都是符合条件的。所以如果数据包到达一个没有 endpoint 的节点，系统将把这个包代理到<em>有</em> endpoint 的节点，并替换数据包的源 IP 为节点的 IP（如前面章节所述）。</p>
<p>你可以通过在一个 loadbalancer 上暴露这个 source-ip-app 来进行测试。</p>
<pre><code class="language-console" data-lang="console">kubectl expose deployment source-ip-app --name=loadbalancer --port=80 --target-port=8080 --type=LoadBalancer
</code></pre><p>输出结果为：</p>
<pre><code>service/loadbalancer exposed
</code></pre><p>打印Service的IPs：</p>
<pre><code class="language-console" data-lang="console">kubectl get svc loadbalancer
</code></pre><p>输出结果与以下结果类似：</p>
<pre><code>NAME           TYPE           CLUSTER-IP    EXTERNAL-IP       PORT(S)   AGE
loadbalancer   LoadBalancer   10.0.65.118   104.198.149.140   80/TCP    5m
</code></pre><pre><code class="language-console" data-lang="console">curl 104.198.149.140
</code></pre><p>输出结果与以下结果类似：</p>
<pre><code>CLIENT VALUES:
client_address=10.240.0.5
...
</code></pre><p>然而，如果你的集群运行在 Google Kubernetes Engine/GCE 上，可以通过设置 service.spec.externalTrafficPolicy 字段值为 Local ，故意导致健康检查失败来强制使没有 endpoints 的节点把自己从负载均衡流量的可选节点列表中删除。</p>
<p>用图表示：</p>
<p><img src="/images/docs/sourceip-externaltrafficpolicy.svg" alt="Source IP with externalTrafficPolicy"></p>
<p>你可以设置 annotation 来进行测试：</p>
<pre><code class="language-console" data-lang="console">kubectl patch svc loadbalancer -p '{&quot;spec&quot;:{&quot;externalTrafficPolicy&quot;:&quot;Local&quot;}}'
</code></pre><p>你应该能够立即看到 Kubernetes 分配的 <code>service.spec.healthCheckNodePort</code> 字段：</p>
<pre><code class="language-console" data-lang="console">kubectl get svc loadbalancer -o yaml | grep -i healthCheckNodePort
</code></pre><p>输出结果与以下结果类似：</p>
<pre><code>  healthCheckNodePort: 32122
</code></pre><p><code>service.spec.healthCheckNodePort</code> 字段指向每个节点在 <code>/healthz</code> 路径上提供的用于健康检查的端口。你可以这样测试：</p>
<pre><code class="language-console" data-lang="console">kubectl get pod -o wide -l run=source-ip-app
</code></pre><p>输出结果与以下结果类似：</p>
<pre><code>NAME                            READY     STATUS    RESTARTS   AGE       IP             NODE
source-ip-app-826191075-qehz4   1/1       Running   0          20h       10.180.1.136   kubernetes-node-6jst
</code></pre><p>使用 curl 命令发送请求到每个节点的 <code>/healthz</code> 路径。</p>
<pre><code class="language-console" data-lang="console">kubernetes-node-6jst $ curl localhost:32122/healthz
</code></pre><p>输出结果与以下结果类似：</p>
<pre><code>1 Service Endpoints found
</code></pre><pre><code class="language-console" data-lang="console">kubernetes-node-jj1t $ curl localhost:32122/healthz
</code></pre><p>输出结果与以下结果类似：</p>
<pre><code>No Service Endpoints Found
</code></pre><p>主节点运行的 service 控制器负责分配 cloud loadbalancer。在这样做的同时，它也会分配指向每个节点的 HTTP 健康检查的 port/path。等待大约 10 秒钟之后，没有 endpoints 的两个节点的健康检查会失败，然后 curl 负载均衡器的 ip：</p>
<pre><code class="language-console" data-lang="console">curl 104.198.149.140
</code></pre><p>输出结果与以下结果类似：</p>
<pre><code>CLIENT VALUES:
client_address=104.132.1.79
...
</code></pre><p><strong>跨平台支持</strong></p>
<p>从 Kubernetes 1.5 开始，通过类型为 Type=LoadBalancer 的 Services 进行源 IP 保存的支持仅在一部分 cloudproviders 中实现（GCP and Azure）。你的集群运行的 cloudprovider 可能以某些不同的方式满足 loadbalancer 的要求：</p>
<ol>
<li>
<p>使用一个代理终止客户端连接并打开一个到你的 nodes/endpoints 的新连接。在这种情况下，源 IP 地址将永远是云负载均衡器的地址而不是客户端的。</p>
</li>
<li>
<p>使用一个包转发器，因此从客户端发送到负载均衡器 VIP 的请求在拥有客户端源 IP 地址的节点终止，而不被中间代理。</p>
</li>
</ol>
<p>第一类负载均衡器必须使用一种它和后端之间约定的协议来和真实的客户端 IP 通信，例如 HTTP <a href="https://en.wikipedia.org/wiki/X-Forwarded-For">X-FORWARDED-FOR</a> 头，或者 <a href="https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">proxy 协议</a>。
第二类负载均衡器可以通过简单的在保存于 Service 的 <code>service.spec.healthCheckNodePort</code> 字段上创建一个 HTTP 健康检查点来使用上面描述的特性。</p>
<h2 id="cleaning-up">Cleaning up</h2>
<p>删除服务：</p>
<pre><code class="language-console" data-lang="console">$ kubectl delete svc -l app=source-ip-app
</code></pre><p>删除 Deployment、ReplicaSet 和 Pod：</p>
<pre><code class="language-console" data-lang="console">$ kubectl delete deployment source-ip-app
</code></pre><h2 id="what-s-next">What's next</h2>
<ul>
<li>进一步学习 <a href="/zh/docs/concepts/services-networking/connect-applications-service/">通过 services 连接应用</a></li>
</ul>

</div>



    
	
  

    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      
      <a class="text-white" href="/zh/docs/home/">主页</a>
      
      <a class="text-white" href="/zh/blog/">博客</a>
      
      <a class="text-white" href="/zh/training/">培训</a>
      
      <a class="text-white" href="/zh/partners/">合作伙伴</a>
      
      <a class="text-white" href="/zh/community/">社区</a>
      
      <a class="text-white" href="/zh/case-studies/">案例分析</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" href="https://discuss.kubernetes.io">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/kubernetesio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://youtube.com/kubernetescommunity">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/kubernetes/kubernetes">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://slack.k8s.io">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Contribute" aria-label="Contribute">
    <a class="text-white" target="_blank" href="https://git.k8s.io/community/contributors/guide">
      <i class="fas fa-edit"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" href="https://stackoverflow.com/questions/tagged/kubernetes">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2024 The Kubernetes Authors | Documentation Distributed under <a href="https://git.k8s.io/website/LICENSE" class="light-text">CC BY 4.0</a></small>
        <br/>
        <small class="text-white">Copyright &copy; 2024 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage" class="light-text">Trademark Usage page</a></small>
        <br/>
        <small class="text-white">ICP license: 京ICP备17074266号-3</small>
        
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>










<script src="/js/main.js"></script>






  </body>
</html>
