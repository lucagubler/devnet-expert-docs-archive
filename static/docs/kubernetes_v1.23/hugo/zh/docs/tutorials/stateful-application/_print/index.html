<!doctype html>
<html lang="zh" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>


<link rel="alternate" hreflang="en" href="http://localhost:1313/docs/tutorials/stateful-application/">
<link rel="alternate" hreflang="ko" href="http://localhost:1313/ko/docs/tutorials/stateful-application/">
<link rel="alternate" hreflang="ja" href="http://localhost:1313/ja/docs/tutorials/stateful-application/">
<link rel="alternate" hreflang="fr" href="http://localhost:1313/fr/docs/tutorials/stateful-application/">
<link rel="alternate" hreflang="es" href="http://localhost:1313/es/docs/tutorials/stateful-application/">
<link rel="alternate" hreflang="id" href="http://localhost:1313/id/docs/tutorials/stateful-application/">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.87.0" />
<link rel="canonical" type="text/html" href="http://localhost:1313/zh/docs/tutorials/stateful-application/">
<link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="apple-touch-icon" href="/images/kubernetes-192x192.png">
<title>有状态的应用 | Kubernetes</title><meta property="og:title" content="有状态的应用" />
<meta property="og:description" content="生产级别的容器编排系统" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/zh/docs/tutorials/stateful-application/" /><meta property="og:site_name" content="Kubernetes" />

<meta itemprop="name" content="有状态的应用">
<meta itemprop="description" content="生产级别的容器编排系统"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="有状态的应用"/>
<meta name="twitter:description" content="生产级别的容器编排系统"/>






<link href="/scss/main.css" rel="stylesheet">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "http://localhost:1313/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href="/css/feature-states.css">



<meta name="description" content="">
<meta property="og:description" content="">
<meta name="twitter:description" content="">
<meta property="og:url" content="http://localhost:1313/zh/docs/tutorials/stateful-application/">
<meta property="og:title" content="有状态的应用">
<meta name="twitter:title" content="有状态的应用">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">

<script src="/js/script.js"></script>


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/zh/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/zh/docs/" >文档</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/blog/" >Kubernetes 博客</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/training/" >培训</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/partners/" >合作伙伴</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/community/" >社区</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/case-studies/" >案例分析</a>
			</li>
			
			
			
			<li class="nav-item dropdown">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/releases">Release Information</a>
	
	<a class="dropdown-item" href="https://kubernetes.io/zh/docs/tutorials/stateful-application/">v1.23</a>
	
	<a class="dropdown-item" href="https://v1-22.docs.kubernetes.io/zh/docs/tutorials/stateful-application/">v1.22</a>
	
	<a class="dropdown-item" href="https://v1-21.docs.kubernetes.io/zh/docs/tutorials/stateful-application/">v1.21</a>
	
	<a class="dropdown-item" href="https://v1-20.docs.kubernetes.io/zh/docs/tutorials/stateful-application/">v1.20</a>
	
	<a class="dropdown-item" href="https://v1-19.docs.kubernetes.io/zh/docs/tutorials/stateful-application/">v1.19</a>
	
</div>
			</li>
			
			
			<li class="nav-item dropdown">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/tutorials/stateful-application/">English</a>
	
	<a class="dropdown-item" href="/ko/docs/tutorials/stateful-application/">한국어 Korean</a>
	
	<a class="dropdown-item" href="/ja/docs/tutorials/stateful-application/">日本語 Japanese</a>
	
	<a class="dropdown-item" href="/fr/docs/tutorials/stateful-application/">Français</a>
	
	<a class="dropdown-item" href="/es/docs/tutorials/stateful-application/">Español</a>
	
	<a class="dropdown-item" href="/id/docs/tutorials/stateful-application/">Bahasa Indonesia</a>
	
</div>

			</li>
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/zh/docs/tutorials/stateful-application/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">有状态的应用</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-27580b3f65f3c2da07fc0f83be69da75">示例：使用 Persistent Volumes 部署 WordPress 和 MySQL</a></li>


    
  
    
    
	
<li>2: <a href="#pg-bf0d8e08fddd6e0282709b9fef8b5f67">示例：使用 StatefulSet 部署 Cassandra</a></li>


    
  
    
    
	
<li>3: <a href="#pg-4bfac214b5eb9ebddaf1f3811901d327">运行 ZooKeeper，一个分布式协调系统</a></li>


    
  
    
    
	
<li>4: <a href="#pg-42e39658021b706bcc9478c8cc73c4a3">StatefulSet 基础</a></li>


    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-27580b3f65f3c2da07fc0f83be69da75">1 - 示例：使用 Persistent Volumes 部署 WordPress 和 MySQL</h1>
    
	<!--
title: "Example: Deploying WordPress and MySQL with Persistent Volumes"
reviewers:
- ahmetb
content_type: tutorial
weight: 20
card: 
  name: tutorials
  weight: 40
  title: "Stateful Example: Wordpress with Persistent Volumes"
-->
<!-- overview -->
<!--
This tutorial shows you how to deploy a WordPress site and a MySQL database using Minikube. Both applications use PersistentVolumes and PersistentVolumeClaims to store data.
-->
<p>本示例描述了如何通过 Minikube 在 Kubernetes 上安装 WordPress 和 MySQL。这两个应用都使用 PersistentVolumes 和 PersistentVolumeClaims 保存数据。</p>
<!--
 A [PersistentVolume](/docs/concepts/storage/persistent-volumes/)(PV)is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a [StorageClass](/docs/concepts/storage/storage-classes).  A [PersistentVolumeClaim](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)(PVC)is a request for storage by a user that can be fulfilled by a PV. PersistentVolumes and PersistentVolumeClaims are independent from Pod lifecycles and preserve data through restarting, rescheduling, and even deleting Pods.
 -->
<p><a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolume</a>（PV）是一块集群里由管理员手动提供，或 kubernetes 通过 <a href="/zh/docs/concepts/storage/storage-classes">StorageClass</a> 动态创建的存储。
<a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim</a>（PVC）是一个满足对 PV 存储需要的请求。PersistentVolumes 和 PersistentVolumeClaims 是独立于 Pod 生命周期而在 Pod 重启，重新调度甚至删除过程中保存数据。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
 This deployment is not suitable for production use cases, as it uses single instance WordPress and MySQL Pods. Consider using [WordPress Helm Chart](https://github.com/kubernetes/charts/tree/master/stable/wordpress) to deploy WordPress in production.
 -->
<p>这种部署并不适合生产场景，它使用单实例 WordPress 和 MySQL Pods。考虑使用 <a href="https://github.com/kubernetes/charts/tree/master/stable/wordpress">WordPress Helm Chart</a> 在生产场景中部署 WordPress。
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
 The files provided in this tutorial are using GA Deployment APIs and are specific to kubernetes version 1.9 and later. If you wish to use this tutorial with an earlier version of Kubernetes, please update the API version appropriately, or reference earlier versions of this tutorial.
-->
<p>本教程中提供的文件使用 GA Deployment API，并且特定于 kubernetes 1.9 或更高版本。如果您希望将本教程与 Kubernetes 的早期版本一起使用，请相应地更新 API 版本，或参考本教程的早期版本。
</div>
<h2 id="objectives">Objectives</h2>
<!--
* Create PersistentVolumeClaims and PersistentVolumes
* Create a `kustomization.yaml` with
  * a Secret generator
  * MySQL resource configs
  * WordPress resource configs
* Apply the kustomization directory by `kubectl apply -k ./`
* Clean up
-->
<ul>
<li>创建 PersistentVolumeClaims 和 PersistentVolumes</li>
<li>创建 <code>kustomization.yaml</code> 使用
<ul>
<li>Secret 生成器</li>
<li>MySQL 资源配置</li>
<li>WordPress 资源配置</li>
</ul>
</li>
<li>应用整个 kustomization 目录 <code>kubectl apply -k ./</code></li>
<li>清理</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
The example shown on this page works with `kubectl` 1.14 and above.

Download the following configuration files:

1. [mysql-deployment.yaml](/examples/application/wordpress/mysql-deployment.yaml)

1. [wordpress-deployment.yaml](/examples/application/wordpress/wordpress-deployment.yaml)
-->
<p>此例在<code>kubectl</code> 1.14 或者更高版本有效。</p>
<p>下载下面的配置文件：</p>
<ol>
<li>
<p><a href="/examples/application/wordpress/mysql-deployment.yaml">mysql-deployment.yaml</a></p>
</li>
<li>
<p><a href="/examples/application/wordpress/wordpress-deployment.yaml">wordpress-deployment.yaml</a></p>
</li>
</ol>
<!-- lessoncontent -->
<!--
## Create PersistentVolumeClaims and PersistentVolumes
-->
<h2 id="创建-persistentvolumeclaims-和-persistentvolumes">创建 PersistentVolumeClaims 和 PersistentVolumes</h2>
<!-- MySQL and Wordpress each require a PersistentVolume to store data.  Their PersistentVolumeClaims will be created at the deployment step.

Many cluster environments have a default StorageClass installed.  When a StorageClass is not specified in the PersistentVolumeClaim, the cluster's default StorageClass is used instead.

When a PersistentVolumeClaim is created, a PersistentVolume is dynamically provisioned based on the StorageClass configuration.
-->
<p>MySQL 和 Wordpress 都需要一个 PersistentVolume 来存储数据。他们的 PersistentVolumeClaims 将在部署步骤中创建。</p>
<p>许多群集环境都安装了默认的 StorageClass。如果在 PersistentVolumeClaim 中未指定 StorageClass，则使用群集的默认 StorageClass。</p>
<p>创建 PersistentVolumeClaim 时，将根据 StorageClass 配置动态设置 PersistentVolume。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
In local clusters, the default StorageClass uses the `hostPath` provisioner.  `hostPath` volumes are only suitable for development and testing. With `hostPath` volumes, your data lives in `/tmp` on the node the Pod is scheduled onto and does not move between nodes. If a Pod dies and gets scheduled to another node in the cluster, or the node is rebooted, the data is lost.
-->
<p>在本地群集中，默认的 StorageClass 使用<code>hostPath</code>供应器。 <code>hostPath</code>卷仅适用于开发和测试。使用 <code>hostPath</code> 卷，您的数据位于 Pod 调度到的节点上的<code>/tmp</code>中，并且不会在节点之间移动。如果 Pod 死亡并被调度到群集中的另一个节点，或者该节点重新启动，则数据将丢失。
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you are bringing up a cluster that needs to use the `hostPath` provisioner, the `--enable-hostpath-provisioner` flag must be set in the `controller-manager` component.
-->
<p>如果要建立需要使用<code>hostPath</code>设置程序的集群，则必须在 controller-manager 组件中设置<code>--enable-hostpath-provisioner</code>标志。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- If you have a Kubernetes cluster running on Google Kubernetes Engine, please follow [this guide](https://cloud.google.com/kubernetes-engine/docs/tutorials/persistent-disk). -->
<p>如果你已经有运行在 Google Kubernetes Engine 的集群，请参考 <a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/persistent-disk">this guide</a>。
</div>
<!--
## Create a kustomization.yaml
-->
<h2 id="创建-kustomization-yaml">创建 kustomization.yaml</h2>
<!--
### Add a Secret generator
-->
<h3 id="创建-secret-生成器">创建 Secret 生成器</h3>
<!--
A [Secret](/docs/concepts/configuration/secret/) is an object that stores a piece of sensitive data like a password or key. Since 1.14, `kubectl` supports the management of Kubernetes objects using a kustomization file. You can create a Secret by generators in `kustomization.yaml`.

Add a Secret generator in `kustomization.yaml` from the following command. You will need to replace `YOUR_PASSWORD` with the password you want to use.
-->
<p>A <a href="/zh/docs/concepts/configuration/secret/">Secret</a> 是存储诸如密码或密钥之类的敏感数据的对象。从 1.14 开始，<code>kubectl</code>支持使用 kustomization 文件管理 Kubernetes 对象。您可以通过<code>kustomization.yaml</code>中的生成器创建一个 Secret。</p>
<p>通过以下命令在<code>kustomization.yaml</code>中添加一个 Secret 生成器。您需要用您要使用的密码替换<code>YOUR_PASSWORD</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">secretGenerator:
</span><span style="color:#b44">- name: mysql-pass
</span><span style="color:#b44">  literals:
</span><span style="color:#b44">  - password=YOUR_PASSWORD
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
## Add resource configs for MySQL and WordPress
-->
<h2 id="补充-mysql-和-wordpress-的资源配置">补充 MySQL 和 WordPress 的资源配置</h2>
<!--
The following manifest describes a single-instance MySQL Deployment. The MySQL container mounts the PersistentVolume at /var/lib/mysql. The `MYSQL_ROOT_PASSWORD` environment variable sets the database password from the Secret.
-->
<p>以下 manifest 文件描述了单实例 MySQL 部署。MySQL 容器将 PersistentVolume 挂载在<code>/var/lib/mysql</code>。 <code>MYSQL_ROOT_PASSWORD</code>环境变量设置来自 Secret 的数据库密码。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/wordpress/mysql-deployment.yaml" download="application/wordpress/mysql-deployment.yaml"><code>application/wordpress/mysql-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-wordpress-mysql-deployment-yaml')" title="Copy application/wordpress/mysql-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-wordpress-mysql-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress-mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">3306</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>20Gi<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress-mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Recreate<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mysql:5.6<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MYSQL_ROOT_PASSWORD<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">secretKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-pass<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>password<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">3306</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-persistent-storage<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/mysql<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-persistent-storage<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>mysql-pv-claim<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
The following manifest describes a single-instance WordPress Deployment. The WordPress container mounts the
PersistentVolume at `/var/www/html` for website data files. The `WORDPRESS_DB_HOST` environment variable sets
the name of the MySQL Service defined above, and WordPress will access the database by Service. The
`WORDPRESS_DB_PASSWORD` environment variable sets the database password from the Secret kustomize generated.
-->
<p>以下 manifest 文件描述了单实例 WordPress 部署。WordPress 容器将网站数据文件位于<code>/var/www/html</code>的 PersistentVolume。<code>WORDPRESS_DB_HOST</code>环境变量集上面定义的 MySQL Service 的名称，WordPress 将通过 Service 访问数据库。<code>WORDPRESS_DB_PASSWORD</code>环境变量设置从 Secret kustomize 生成的数据库密码。


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/wordpress/wordpress-deployment.yaml" download="application/wordpress/wordpress-deployment.yaml"><code>application/wordpress/wordpress-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-wordpress-wordpress-deployment-yaml')" title="Copy application/wordpress/wordpress-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-wordpress-wordpress-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wp-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>20Gi<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Recreate<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>wordpress:4.8-apache<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>WORDPRESS_DB_HOST<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>wordpress-mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>WORDPRESS_DB_PASSWORD<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">secretKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-pass<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>password<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress-persistent-storage<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/www/html<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>wordpress-persistent-storage<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>wp-pv-claim<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>

</p>
<!--
1. Download the MySQL deployment configuration file.

      ```shell
      curl -LO https://k8s.io/examples/application/wordpress/mysql-deployment.yaml
      ```

2. Download the WordPress configuration file.

      ```shell
      curl -LO https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml
      ```

3. Add them to `kustomization.yaml` file.

      ```shell
      cat <<EOF >>./kustomization.yaml
      resources:
        - mysql-deployment.yaml
        - wordpress-deployment.yaml
      EOF
      ```
-->
<ol>
<li>
<p>下载 MySQL deployment 配置文件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -LO https://k8s.io/examples/application/wordpress/mysql-deployment.yaml
</code></pre></div></li>
<li>
<p>下载 WordPress 配置文件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -LO https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml
</code></pre></div></li>
<li>
<p>补充到 <code>kustomization.yaml</code> 文件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt;&gt;./kustomization.yaml
</span><span style="color:#b44">resources:
</span><span style="color:#b44">  - mysql-deployment.yaml
</span><span style="color:#b44">  - wordpress-deployment.yaml
</span><span style="color:#b44">EOF</span>
</code></pre></div></li>
</ol>
<!--
## Apply and Verify
-->
<h2 id="应用和验证">应用和验证</h2>
<!--
The `kustomization.yaml` contains all the resources for deploying a WordPress site and a
MySQL database. You can apply the directory by
```shell
kubectl apply -k ./
```

Now you can verify that all objects exist.

1. Verify that the Secret exists by running the following command:

      ```shell
      kubectl get secrets
      ```

      The response should be like this:

      ```shell
      NAME                    TYPE                                  DATA   AGE
      mysql-pass-c57bb4t7mf   Opaque                                1      9s
      ```

2. Verify that a PersistentVolume got dynamically provisioned.

      ```shell
      kubectl get pvc
      ```

      <div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> It can take up to a few minutes for the PVs to be provisioned and bound.
</div>

      The response should be like this:

      ```shell
      NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
      mysql-pv-claim   Bound     pvc-8cbd7b2e-4044-11e9-b2bb-42010a800002   20Gi       RWO            standard           77s
      wp-pv-claim      Bound     pvc-8cd0df54-4044-11e9-b2bb-42010a800002   20Gi       RWO            standard           77s
      ```

3. Verify that the Pod is running by running the following command:

      ```shell
      kubectl get pods
      ```

      <div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> It can take up to a few minutes for the Pod's Status to be <code>RUNNING</code>.
</div>

      The response should be like this:

      ```
      NAME                               READY     STATUS    RESTARTS   AGE
      wordpress-mysql-1894417608-x5dzt   1/1       Running   0          40s
      ```

4. Verify that the Service is running by running the following command:

      ```shell
      kubectl get services wordpress
      ```

      The response should be like this:

      ```
      NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
      wordpress   ClusterIP   10.0.0.89    <pending>     80:32406/TCP   4m
      ```

      <div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Minikube can only expose Services through <code>NodePort</code>. The EXTERNAL-IP is always pending.
</div>

5. Run the following command to get the IP Address for the WordPress Service:

      ```shell
      minikube service wordpress --url
      ```

      The response should be like this:

      ```
      http://1.2.3.4:32406
      ```

6. Copy the IP address, and load the page in your browser to view your site.

   You should see the WordPress set up page similar to the following screenshot.

   ![wordpress-init](https://raw.githubusercontent.com/kubernetes/examples/master/mysql-wordpress-pd/WordPress.png)
-->
<p><code>kustomization.yaml</code>包含用于部署 WordPress 网站的所有资源以及 MySQL 数据库。您可以通过以下方式应用目录</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -k ./
</code></pre></div><p>现在，您可以验证所有对象是否存在。</p>
<ol>
<li>
<p>通过运行以下命令验证 Secret 是否存在：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secrets
</code></pre></div><p>响应应如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                    TYPE                                  DATA   AGE
mysql-pass-c57bb4t7mf   Opaque                                <span style="color:#666">1</span>      9s
</code></pre></div></li>
<li>
<p>验证是否已动态配置 PersistentVolume：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pvc
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 设置和绑定 PV 可能要花费几分钟。
</div>
<p>响应应如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
mysql-pv-claim   Bound     pvc-8cbd7b2e-4044-11e9-b2bb-42010a800002   20Gi       RWO            standard           77s
wp-pv-claim      Bound     pvc-8cd0df54-4044-11e9-b2bb-42010a800002   20Gi       RWO            standard           77s
</code></pre></div></li>
<li>
<p>通过运行以下命令来验证 Pod 是否正在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 等待 Pod 状态变成<code>RUNNING</code>可能会花费几分钟。
</div>
<p>响应应如下所示：</p>
<pre><code>NAME                               READY     STATUS    RESTARTS   AGE
wordpress-mysql-1894417608-x5dzt   1/1       Running   0          40s
</code></pre></li>
<li>
<p>通过运行以下命令来验证 Service 是否正在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get services wordpress
</code></pre></div><p>响应应如下所示：</p>
<pre><code>NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
wordpress   ClusterIP   10.0.0.89    &lt;pending&gt;     80:32406/TCP   4m
</code></pre><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Minikube 只能通过 NodePort 公开服务。EXTERNAL-IP 始终处于挂起状态
</div>
</li>
<li>
<p>运行以下命令以获取 WordPress 服务的 IP 地址：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube service wordpress --url
</code></pre></div><p>响应应如下所示：</p>
<pre><code>http://1.2.3.4:32406
</code></pre></li>
<li>
<p>复制 IP 地址，然后将页面加载到浏览器中来查看您的站点。</p>
<p>您应该看到类似于以下屏幕截图的 WordPress 设置页面。</p>
<p><img src="https://raw.githubusercontent.com/kubernetes/examples/master/mysql-wordpress-pd/WordPress.png" alt="wordpress-init"></p>
</li>
</ol>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
Do not leave your WordPress installation on this page. If another user finds it, they can set up a website on your instance and use it to serve malicious content. <br/><br/>Either install WordPress by creating a username and password or delete your instance.
-->
<p>不要在此页面上保留 WordPress 安装。如果其他用户找到了它，他们可以在您的实例上建立一个网站并使用它来提供恶意内容。<br/><br/>通过创建用户名和密码来安装 WordPress 或删除您的实例。
</div>


<h2 id="cleaning-up">Cleaning up</h2>
<!--
1. Run the following command to delete your Secret, Deployments, Services and PersistentVolumeClaims:

      ```shell
      kubectl delete -k ./
      ```
-->
<ol>
<li>
<p>运行一下命令删除您的 Secret，Deployments，Services and PersistentVolumeClaims：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete -k ./
</code></pre></div></li>
</ol>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [Introspection and Debugging](/docs/tasks/debug/debug-application)
* Learn more about [Jobs](/docs/concepts/workloads/controllers/jobs-run-to-completion/)
* Learn more about [Port Forwarding](/docs/tasks/access-application-cluster/port-forward-access-application-cluster/)
* Learn how to [Get a Shell to a Container](/docs/tasks/debug/debug-application/get-shell-running-container/)
-->
<ul>
<li>了解更多关于 <a href="/zh/docs/tasks/debug/debug-application">Introspection and Debugging</a></li>
<li>了解更多关于 <a href="/zh/docs/concepts/workloads/controllers/jobs-run-to-completion/">Jobs</a></li>
<li>了解更多关于 <a href="/zh/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">Port Forwarding</a></li>
<li>了解如何 <a href="/zh/docs/tasks/debug/debug-application/get-shell-running-container/">Get a Shell to a Container</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bf0d8e08fddd6e0282709b9fef8b5f67">2 - 示例：使用 StatefulSet 部署 Cassandra</h1>
    
	<!--
title: "Example: Deploying Cassandra with a StatefulSet"
reviewers:
- ahmetb
content_type: tutorial
weight: 30
-->
<!-- overview -->
<!--
This tutorial shows you how to run [Apache Cassandra](https://cassandra.apache.org/) on Kubernetes.
Cassandra, a database, needs persistent storage to provide data durability (application _state_).
In this example, a custom Cassandra seed provider lets the database discover new Cassandra instances as they join the Cassandra cluster.
-->
<p>本教程描述拉如何在 Kubernetes 上运行 <a href="https://cassandra.apache.org/">Apache Cassandra</a>。
数据库 Cassandra 需要永久性存储提供数据持久性（应用“状态”）。
在此示例中，自定义 Cassandra seed provider 使数据库在加入 Cassandra
集群时发现新的 Cassandra 实例。</p>
<!--
*StatefulSets* make it easier to deploy stateful applications into your Kubernetes cluster.
For more information on the features used in this tutorial, see
[StatefulSet](/docs/concepts/workloads/controllers/statefulset/).
-->
<p>使用&quot;StatefulSets&quot;可以更轻松地将有状态的应用程序部署到你的 Kubernetes 集群中。
有关本教程中使用的功能的更多信息，
参阅 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Cassandra and Kubernetes both use the term _node_ to mean a member of a cluster. In this
tutorial, the Pods that belong to the StatefulSet are Cassandra nodes and are members
of the Cassandra cluster (called a _ring_). When those Pods run in your Kubernetes cluster,
the Kubernetes control plane schedules those Pods onto Kubernetes
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='Nodes'>Nodes</a>.
-->
<p>Cassandra 和 Kubernetes 都使用术语“节点（node）”来表示集群的成员。
在本教程中，属于 StatefulSet 的 Pod 是 Cassandra 节点，并且是 Cassandra 集群的成员（称为 “ring”）。
当这些 Pod 在你的 Kubernetes 集群中运行时，Kubernetes 控制平面会将这些 Pod 调度到 Kubernetes 的
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>上。</p>
<!--
When a Cassandra node starts, it uses a _seed list_ to bootstrap discovery of other
nodes in the ring.
This tutorial deploys a custom Cassandra seed provider that lets the database discover
new Cassandra Pods as they appear inside your Kubernetes cluster.
-->
<p>当 Cassandra 节点启动时，使用 <em>seed列表</em> 来引导发现 ring 中其他节点。
本教程部署了一个自定义的 Cassandra seed provider，使数据库可以发现新的 Cassandra Pod
出现在 Kubernetes 集群中。</p>

</div>
<h2 id="objectives">Objectives</h2>
<!--
* Create and validate a Cassandra headless <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>.
* Use a <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a> to create a Cassandra ring.
* Validate the StatefulSet.
* Modify the StatefulSet.
* Delete the StatefulSet and its <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>.
-->
<ul>
<li>创建并验证 Cassandra 无头（headless）<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>..</li>
<li>使用 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a> 创建一个 Cassandra ring。</li>
<li>验证 StatefulSet。</li>
<li>修改 StatefulSet。</li>
<li>删除 StatefulSet 及其 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>.</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!--
To complete this tutorial, you should already have a basic familiarity with
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>,
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a>, and
<a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSets'>StatefulSets</a>.
-->
<p>要完成本教程，你应该已经熟悉 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>，
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> 和
<a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>。</p>
<!--
### Additional Minikube setup instructions

<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p><a href="https://minikube.sigs.k8s.io/docs/">Minikube</a> defaults to 2048MB of memory and 2 CPU.
Running Minikube with the default resource configuration results in insufficient resource
errors during this tutorial. To avoid these errors, start Minikube with the following settings:
--&gt;</p>
<h3 id="额外的-minikube-设置说明">额外的 Minikube 设置说明</h3>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p><a href="https://minikube.sigs.k8s.io/docs/">Minikube</a>默认为 2048MB 内存和 2 个 CPU。
在本教程中，使用默认资源配置运行 Minikube 会导致资源不足的错误。为避免这些错误，请使用以下设置启动 Minikube：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube start --memory <span style="color:#666">5120</span> --cpus<span style="color:#666">=</span><span style="color:#666">4</span>
</code></pre></div>
</div>
<!-- lessoncontent -->
<!--
## Creating a headless Service for Cassandra {#creating-a-cassandra-headless-service}

In Kubernetes, a <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> describes a set of
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> that perform the same task.

The following Service is used for DNS lookups between Cassandra Pods and clients within your cluster:

Create a Service to track all Cassandra StatefulSet members from the `cassandra-service.yaml` file:
-->
<h2 id="creating-a-cassandra-headless-service">为 Cassandra 创建无头（headless） Services</h2>
<p>在 Kubernetes 中，一个 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>
描述了一组执行相同任务的 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>。</p>
<p>以下 Service 用于在 Cassandra Pod 和集群中的客户端之间进行 DNS 查找：</p>
<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/cassandra/cassandra-service.yaml" download="application/cassandra/cassandra-service.yaml"><code>application/cassandra/cassandra-service.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-cassandra-cassandra-service-yaml')" title="Copy application/cassandra/cassandra-service.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-cassandra-cassandra-service-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">9042</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>
<p>创建一个 Service 来跟踪 <code>cassandra-service.yaml</code> 文件中的所有 Cassandra StatefulSet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-service.yaml
</code></pre></div><!--
### Validating (optional) {#validating}

Get the Cassandra Service.
-->
<h3 id="validating">验证(可选)</h3>
<p>获取 Cassandra Service。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc cassandra
</code></pre></div><!-- 
The response is 
-->
<p>响应是：</p>
<pre><code>NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
cassandra   ClusterIP   None         &lt;none&gt;        9042/TCP   45s
</code></pre><!--
If you don't see a Service named `cassandra`, that means creation failed. Read
[Debug Services](/docs/tasks/debug/debug-application/debug-service/)
for help troubleshooting common issues.
-->
<p>如果没有看到名为 <code>cassandra</code> 的服务，则表示创建失败。
请阅读<a href="/zh/docs/tasks/debug/debug-application/debug-service/">调试服务</a>，以解决常见问题。</p>
<!--
## Using a StatefulSet to create a Cassandra ring

The StatefulSet manifest, included below, creates a Cassandra ring that consists of three Pods.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> This example uses the default provisioner for Minikube.
Please update the following StatefulSet for the cloud you are working with.
</div>
-->
<h2 id="使用-statefulset-创建-cassandra-ring">使用 StatefulSet 创建 Cassandra Ring</h2>
<p>下面包含的 StatefulSet 清单创建了一个由三个 Pod 组成的 Cassandra ring。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 本示例使用 Minikube 的默认配置程序。
请为正在使用的云更新以下 StatefulSet。
</div>
<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/cassandra/cassandra-statefulset.yaml" download="application/cassandra/cassandra-statefulset.yaml"><code>application/cassandra/cassandra-statefulset.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-cassandra-cassandra-statefulset-yaml')" title="Copy application/cassandra/cassandra-statefulset.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-cassandra-cassandra-statefulset-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">1800</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/cassandra:v13<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">7000</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>intra-node<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">7001</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>tls-intra-node<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">7199</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>jmx<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9042</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">capabilities</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">add</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- IPC_LOCK<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">lifecycle</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">preStop</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> 
</span><span style="color:#bbb">              </span>- /bin/sh<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- nodetool drain<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MAX_HEAP_SIZE<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>512M<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>HEAP_NEWSIZE<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>100M<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>CASSANDRA_SEEDS<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;cassandra-0.cassandra.default.svc.cluster.local&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>CASSANDRA_CLUSTER_NAME<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;K8Demo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>CASSANDRA_DC<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;DC1-K8Demo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>CASSANDRA_RACK<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Rack1-K8Demo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>POD_IP<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>status.podIP<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- /bin/bash<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- /ready-probe.sh<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># These volume mounts are persistent. They are like inline claims,</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># but not exactly because the names need to match exactly one of</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># the stateful pod volumes.</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra-data<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/cassandra_data<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># These are converted to volume claims by the controller</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># and mounted at the paths mentioned above.</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># do not use these in production until ssd GCEPersistentDisk or other ssd pd</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra-data<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>k8s.io/minikube-hostpath<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>pd-ssd<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>
<!--
Create the Cassandra StatefulSet from the `cassandra-statefulset.yaml` file:
-->
<p>使用 <code>cassandra-statefulset.yaml</code> 文件创建 Cassandra StatefulSet ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 如果你能未经修改地 apply cassandra-statefulset.yaml，请使用此命令</span>
kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml
</code></pre></div><!--
If you need to modify `cassandra-statefulset.yaml` to suit your cluster, download
https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml and then apply
that manifest, from the folder you saved the modified version into:
-->
<p>如果你为了适合你的集群需要修改 <code>cassandra-statefulset.yaml</code>，
下载 <a href="https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml">https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml</a>，
然后 apply 修改后的清单。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 如果使用本地的 cassandra-statefulset.yaml ，请使用此命令</span>
kubectl apply -f cassandra-statefulset.yaml
</code></pre></div><!--
## Validating the Cassandra StatefulSet

1. Get the Cassandra StatefulSet:
-->
<h2 id="验证-cassandra-statefulset">验证 Cassandra StatefulSet</h2>
<ol>
<li>
<p>获取 Cassandra StatefulSet:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get statefulset cassandra
</code></pre></div><!--
The response should be similar to:
-->
<p>响应应该与此类似：</p>
<pre><code>NAME        DESIRED   CURRENT   AGE
cassandra   3         0         13s
</code></pre><!--
The `StatefulSet` resource deploys Pods sequentially.
-->
<p><code>StatefulSet</code> 资源会按顺序部署 Pod。</p>
</li>
</ol>
<!--
1. Get the Pods to see the ordered creation status:
-->
<ol start="2">
<li>
<p>获取 Pod 查看已排序的创建状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l<span style="color:#666">=</span><span style="color:#b44">&#34;app=cassandra&#34;</span>
</code></pre></div><!--
The response should be similar to:
-->
<p>响应应该与此类似：</p>
<pre><code>NAME          READY     STATUS              RESTARTS   AGE
cassandra-0   1/1       Running             0          1m
cassandra-1   0/1       ContainerCreating   0          8s
</code></pre><!--
It can take several minutes for all three Pods to deploy. Once they are deployed, the same command
returns output similar to:
-->
<p>这三个 Pod 要花几分钟的时间才能部署。部署之后，相同的命令将返回类似于以下的输出：</p>
<pre><code>NAME          READY     STATUS    RESTARTS   AGE
cassandra-0   1/1       Running   0          10m
cassandra-1   1/1       Running   0          9m
cassandra-2   1/1       Running   0          8m
</code></pre></li>
</ol>
<!--
3. Run the Cassandra [nodetool](https://cwiki.apache.org/confluence/display/CASSANDRA2/NodeTool) inside the first Pod, to
   display the status of the ring.
-->
<ol start="3">
<li>
<p>运行第一个 Pod 中的 Cassandra <a href="https://cwiki.apache.org/confluence/display/CASSANDRA2/NodeTool">nodetool</a>，
以显示 ring 的状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it cassandra-0 -- nodetool status
</code></pre></div><!--
The response should be similar to:
-->
<p>响应应该与此类似：</p>
<pre><code>Datacenter: DC1-K8Demo
======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address     Load       Tokens       Owns (effective)  Host ID                               Rack
UN  172.17.0.5  83.57 KiB  32           74.0%             e2dd09e6-d9d3-477e-96c5-45094c08db0f  Rack1-K8Demo
UN  172.17.0.4  101.04 KiB  32           58.8%             f89d6835-3a42-4419-92b3-0e62cae1479c  Rack1-K8Demo
UN  172.17.0.6  84.74 KiB  32           67.1%             a6a1e8c2-3dc5-4417-b1a0-26507af2aaad  Rack1-K8Demo
</code></pre></li>
</ol>
<!--
## Modifying the Cassandra StatefulSet

Use `kubectl edit` to modify the size of a Cassandra StatefulSet.

1. Run the following command:
-->
<h2 id="修改-cassandra-statefulset">修改 Cassandra StatefulSet</h2>
<p>使用 <code>kubectl edit</code> 修改 Cassandra StatefulSet 的大小。</p>
<ol>
<li>
<p>运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit statefulset cassandra
</code></pre></div><!--
This command opens an editor in your terminal. The line you need to change is the `replicas` field.
The following sample is an excerpt of the StatefulSet file:
-->
<p>此命令你的终端中打开一个编辑器。需要更改的是 <code>replicas</code> 字段。下面是 StatefulSet 文件的片段示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># and an empty file will abort the edit. If an error occurs while saving this file will be</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># reopened with the relevant failures.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2016-08-13T18:40:58Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">generation</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cassandra<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;323&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>7a219483-6185-11e6-a910-42010a8a0fc0<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
1. Change the number of replicas to 4, and then save the manifest.

   The StatefulSet now scales to run with 4 Pods.

1. Get the Cassandra StatefulSet to verify your change:
-->
<ol start="2">
<li>
<p>将副本数（replicas）更改为 4，然后保存清单。</p>
<p>StatefulSet 现在可以扩展到运行 4 个 Pod。</p>
</li>
<li>
<p>获取 Cassandra StatefulSet 验证更改：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get statefulset cassandra
</code></pre></div><!--
The response should be similar to:
-->
<p>响应应该与此类似：</p>
<pre><code>NAME        DESIRED   CURRENT   AGE
cassandra   4         4         36m
</code></pre></li>
</ol>
<h2 id="cleaning-up">Cleaning up</h2>
<!--
Deleting or scaling a StatefulSet down does not delete the volumes associated with the StatefulSet.
This setting is for your safety because your data is more valuable than automatically purging all related StatefulSet resources.
-->
<p>删除或缩小 StatefulSet 不会删除与 StatefulSet 关联的卷。
这个设置是出于安全考虑，因为你的数据比自动清除所有相关的 StatefulSet 资源更有价值。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
Depending on the storage class and reclaim policy, deleting the *PersistentVolumeClaims* may cause the associated volumes
to also be deleted. Never assume you'll be able to access data if its volume claims are deleted.
-->
<p>根据存储类和回收策略，删除 <em>PersistentVolumeClaims</em> 可能导致关联的卷也被删除。
千万不要认为其容量声明被删除，你就能访问数据。
</div>
<!--
1. Run the following commands (chained together into a single command) to delete everything in the Cassandra StatefulSet:
-->
<ol>
<li>
<p>运行以下命令（连在一起成为一个单独的命令）删除 Cassandra StatefulSet 中的所有内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">grace</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get pod cassandra-0 -o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.spec.terminationGracePeriodSeconds}&#39;</span><span style="color:#a2f;font-weight:bold">)</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#666">&amp;&amp;</span> kubectl delete statefulset -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>cassandra <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#666">&amp;&amp;</span> <span style="color:#a2f">echo</span> <span style="color:#b44">&#34;Sleeping </span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">grace</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44"> seconds&#34;</span> 1&gt;&amp;<span style="color:#666">2</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#666">&amp;&amp;</span> sleep <span style="color:#b8860b">$grace</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#666">&amp;&amp;</span> kubectl delete persistentvolumeclaim -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>cassandra
</code></pre></div></li>
</ol>
<!--
1. Run the following command to delete the Service you set up for Cassandra:
-->
<ol start="2">
<li>
<p>运行以下命令，删除你为 Cassandra 设置的 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete service -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>cassandra
</code></pre></div></li>
</ol>
<!--
## Cassandra container environment variables

The Pods in this tutorial use the [`gcr.io/google-samples/cassandra:v13`](https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile)
image from Google's [container registry](https://cloud.google.com/container-registry/docs/).
The Docker image above is based on [debian-base](https://github.com/kubernetes/release/tree/master/images/build/debian-base)
and includes OpenJDK 8.

This image includes a standard Cassandra installation from the Apache Debian repo.
By using environment variables you can change values that are inserted into `cassandra.yaml`.
-->
<h2 id="cassandra-容器环境变量">Cassandra 容器环境变量</h2>
<p>本教程中的 Pod 使用来自 Google <a href="https://cloud.google.com/container-registry/docs/">容器镜像库</a>
的 <a href="https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile"><code>gcr.io/google-samples/cassandra:v13</code></a>
镜像。上面的 Docker 镜像基于 <a href="https://github.com/kubernetes/release/tree/master/images/build/debian-base">debian-base</a>，
并且包含 OpenJDK 8。</p>
<p>该映像包括来自 Apache Debian 存储库的标准 Cassandra 安装。
通过使用环境变量，您可以更改插入到 <code>cassandra.yaml</code> 中的值。</p>
<table>
<thead>
<tr>
<th>环境变量</th>
<th style="text-align:center">默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CASSANDRA_CLUSTER_NAME</code></td>
<td style="text-align:center"><code>'Test Cluster'</code></td>
</tr>
<tr>
<td><code>CASSANDRA_NUM_TOKENS</code></td>
<td style="text-align:center"><code>32</code></td>
</tr>
<tr>
<td><code>CASSANDRA_RPC_ADDRESS</code></td>
<td style="text-align:center"><code>0.0.0.0</code></td>
</tr>
</tbody>
</table>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn how to [Scale a StatefulSet](/docs/tasks/run-application/scale-stateful-set/).
* Learn more about the [*KubernetesSeedProvider*](https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java)
* See more custom [Seed Provider Configurations](https://git.k8s.io/examples/cassandra/java/README.md)
-->
<ul>
<li>了解如何<a href="/docs/tasks/run-application/scale-stateful-set/">扩缩 StatefulSet</a>。</li>
<li>了解有关 <a href="https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java"><em>KubernetesSeedProvider</em></a> 的更多信息</li>
<li>查看更多自定义 <a href="https://git.k8s.io/examples/cassandra/java/README.md">Seed Provider Configurations</a></li>
</ul>

</div>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4bfac214b5eb9ebddaf1f3811901d327">3 - 运行 ZooKeeper，一个分布式协调系统</h1>
    
	<!--
reviewers:
- bprashanth
- enisoc
- erictune
- foxish
- janetkuo
- kow3ns
- smarterclayton
title: Running ZooKeeper, A Distributed System Coordinator
content_type: tutorial
weight: 40
-->
<!-- overview -->
<!--
This tutorial demonstrates running [Apache Zookeeper](https://zookeeper.apache.org) on
Kubernetes using [StatefulSets](/docs/concepts/workloads/controllers/statefulset/),
[PodDisruptionBudgets](/docs/concepts/workloads/pods/disruptions/#pod-disruption-budget),
and [PodAntiAffinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity).
-->
<p>本教程展示了在 Kubernetes 上使用
<a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>，
<a href="/zh/docs/concepts/workloads/pods/disruptions/#pod-disruption-budget">PodDisruptionBudget</a> 和
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#%E4%BA%B2%E5%92%8C%E4%B8%8E%E5%8F%8D%E4%BA%B2%E5%92%8C">PodAntiAffinity</a>
特性运行 <a href="https://zookeeper.apache.org">Apache Zookeeper</a>。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
Before starting this tutorial, you should be familiar with the following
Kubernetes concepts.
-->
<p>在开始本教程前，你应该熟悉以下 Kubernetes 概念。</p>
<ul>
<li><a href="/zh/docs/concepts/workloads/pods/">Pods</a></li>
<li><a href="/zh/docs/concepts/services-networking/dns-pod-service/">集群 DNS</a></li>
<li><a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务（Headless Service）</a></li>
<li><a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/main/staging/persistent-volume-provisioning/">PersistentVolume 制备</a></li>
<li><a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a></li>
<li><a href="/zh/docs/concepts/workloads/pods/disruptions/#pod-disruption-budget">PodDisruptionBudget</a></li>
<li><a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#%E4%BA%B2%E5%92%8C%E4%B8%8E%E5%8F%8D%E4%BA%B2%E5%92%8C">PodAntiAffinity</a></li>
<li><a href="/zh/docs/reference/kubectl/kubectl/">kubectl CLI</a></li>
</ul>
<!--
You must have a cluster with at least four nodes, and each node requires at least 2 CPUs and 4 GiB of memory. In this tutorial you will cordon and drain the cluster's nodes. **This means that the cluster will terminate and evict all Pods on its nodes, and the nodes will temporarily become unschedulable.** You should use a dedicated cluster for this tutorial, or you should ensure that the disruption you cause will not interfere with other tenants.
-->
<p>你需要一个至少包含四个节点的集群，每个节点至少 2 CPUs 和 4 GiB 内存。
在本教程中你将会隔离（Cordon）和腾空（Drain ）集群的节点。
<strong>这意味着集群节点上所有的 Pods 将会被终止并移除。这些节点也会暂时变为不可调度</strong>。
在本教程中你应该使用一个独占的集群，或者保证你造成的干扰不会影响其它租户。</p>
<!--
This tutorial assumes that you have configured your cluster to dynamically provision
PersistentVolumes. If your cluster is not configured to do so, you
will have to manually provision three 20 GiB volumes before starting this
tutorial.
-->
<p>本教程假设你的集群配置为动态的提供 PersistentVolumes。
如果你的集群没有配置成这样，在开始本教程前，你需要手动准备三个 20 GiB 的卷。</p>
<h2 id="objectives">Objectives</h2>
<!--
After this tutorial, you will know the following.

- How to deploy a ZooKeeper ensemble using StatefulSet.
- How to consistently configure the ensemble.
- How to spread the deployment of ZooKeeper servers in the ensemble.
- How to use PodDisruptionBudgets to ensure service availability during planned maintenance.
-->
<p>在学习本教程后，你将熟悉下列内容。</p>
<ul>
<li>如何使用 StatefulSet 部署一个 ZooKeeper ensemble。</li>
<li>如何一致性配置 ensemble。</li>
<li>如何在 ensemble 中 分布 ZooKeeper 服务器的部署。</li>
<li>如何在计划维护中使用 PodDisruptionBudgets 确保服务可用性。</li>
</ul>
<!-- lessoncontent -->
<!-- 
### ZooKeeper

[Apache ZooKeeper](https://zookeeper.apache.org/doc/current/) is a
distributed, open-source coordination service for distributed applications.
ZooKeeper allows you to read, write, and observe updates to data. Data are
organized in a file system like hierarchy and replicated to all ZooKeeper
servers in the ensemble (a set of ZooKeeper servers). All operations on data
are atomic and sequentially consistent. ZooKeeper ensures this by using the
[Zab](https://pdfs.semanticscholar.org/b02c/6b00bd5dbdbd951fddb00b906c82fa80f0b3.pdf)
consensus protocol to replicate a state machine across all servers in the ensemble.
-->
<h3 id="zookeeper-basics">ZooKeeper  </h3>
<p><a href="https://zookeeper.apache.org/doc/current/">Apache ZooKeeper</a>
是一个分布式的开源协调服务，用于分布式系统。
ZooKeeper 允许你读取、写入数据和发现数据更新。
数据按层次结构组织在文件系统中，并复制到 ensemble（一个 ZooKeeper 服务器的集合）
中所有的 ZooKeeper 服务器。对数据的所有操作都是原子的和顺序一致的。
ZooKeeper 通过
<a href="https://pdfs.semanticscholar.org/b02c/6b00bd5dbdbd951fddb00b906c82fa80f0b3.pdf">Zab</a>
一致性协议在 ensemble 的所有服务器之间复制一个状态机来确保这个特性。</p>
<!--
The ensemble uses the Zab protocol to elect a leader, and the ensemble cannot write data until that election is complete. Once complete, the ensemble uses Zab to ensure that it replicates all writes to a quorum before it acknowledges and makes them visible to clients. Without respect to weighted quorums, a quorum is a majority component of the ensemble containing the current leader. For instance, if the ensemble has three servers, a component that contains the leader and one other server constitutes a quorum. If the ensemble can not achieve a quorum, the ensemble cannot write data.
-->
<p>Ensemble 使用 Zab 协议选举一个领导者，在选举出领导者前不能写入数据。
一旦选举出了领导者，ensemble 使用 Zab 保证所有写入被复制到一个 quorum，
然后这些写入操作才会被确认并对客户端可用。
如果没有遵照加权 quorums，一个 quorum 表示包含当前领导者的 ensemble 的多数成员。
例如，如果 ensemble 有 3 个服务器，一个包含领导者的成员和另一个服务器就组成了一个
quorum。
如果 ensemble 不能达成一个 quorum，数据将不能被写入。</p>
<!--
ZooKeeper servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on storage media. When a server crashes, it can recover its previous state by replaying the WAL. To prevent the WAL from growing without bound, ZooKeeper servers will periodically snapshot them in memory state to storage media. These snapshots can be loaded directly into memory, and all WAL entries that preceded the snapshot may be discarded.
-->
<p>ZooKeeper 在内存中保存它们的整个状态机，但是每个改变都被写入一个在存储介质上的
持久 WAL（Write Ahead Log）。
当一个服务器出现故障时，它能够通过回放 WAL 恢复之前的状态。
为了防止 WAL 无限制的增长，ZooKeeper 服务器会定期的将内存状态快照保存到存储介质。
这些快照能够直接加载到内存中，所有在这个快照之前的 WAL 条目都可以被安全的丢弃。</p>
<!--
## Creating a ZooKeeper Ensemble

The manifest below contains a
[Headless Service](/docs/concepts/services-networking/service/#headless-services),
a [Service](/docs/concepts/services-networking/service/),
a [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets),
and a [StatefulSet](/docs/concepts/workloads/controllers/statefulset/).
-->
<h2 id="创建一个-zookeeper-ensemble">创建一个 ZooKeeper Ensemble</h2>
<p>下面的清单包含一个
<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>，
一个 <a href="/zh/docs/concepts/services-networking/service/">Service</a>，
一个 <a href="/zh/docs/concepts/workloads/pods/disruptions/#specifying-a-poddisruptionbudget">PodDisruptionBudget</a>，
和一个 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/zookeeper/zookeeper.yaml" download="application/zookeeper/zookeeper.yaml"><code>application/zookeeper/zookeeper.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-zookeeper-zookeeper-yaml')" title="Copy application/zookeeper/zookeeper.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-zookeeper-zookeeper-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk-hs<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">2888</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>server<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">3888</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>leader-election<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk-cs<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">2181</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>client<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>policy/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PodDisruptionBudget<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk-pdb<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">maxUnavailable</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span>zk-hs<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">updateStrategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>RollingUpdate<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podManagementPolicy</span>:<span style="color:#bbb"> </span>OrderedReady<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zk<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">podAntiAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;app&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">                    </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">                    </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                    </span>- zk<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kubernetes-zookeeper<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0.5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">2181</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>client<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">2888</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>server<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">3888</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>leader-election<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#b44">&#34;start-zookeeper \
</span><span style="color:#b44">          --servers=3 \
</span><span style="color:#b44">          --data_dir=/var/lib/zookeeper/data \
</span><span style="color:#b44">          --data_log_dir=/var/lib/zookeeper/data/log \
</span><span style="color:#b44">          --conf_dir=/opt/zookeeper/conf \
</span><span style="color:#b44">          --client_port=2181 \
</span><span style="color:#b44">          --election_port=3888 \
</span><span style="color:#b44">          --server_port=2888 \
</span><span style="color:#b44">          --tick_time=2000 \
</span><span style="color:#b44">          --init_limit=10 \
</span><span style="color:#b44">          --sync_limit=5 \
</span><span style="color:#b44">          --heap=512M \
</span><span style="color:#b44">          --max_client_cnxns=60 \
</span><span style="color:#b44">          --snap_retain_count=3 \
</span><span style="color:#b44">          --purge_interval=12 \
</span><span style="color:#b44">          --max_session_timeout=40000 \
</span><span style="color:#b44">          --min_session_timeout=4000 \
</span><span style="color:#b44">          --log_level=INFO&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#b44">&#34;zookeeper-ready 2181&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#b44">&#34;zookeeper-ready 2181&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>datadir<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/zookeeper<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">runAsUser</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">fsGroup</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>datadir<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Open a terminal, and use the
[`kubectl apply`](/docs/reference/generated/kubectl/kubectl-commands/#apply) command to create the
manifest.
-->
<p>打开一个命令行终端，使用命令
<a href="/docs/reference/generated/kubectl/kubectl-commands/#apply"><code>kubectl apply</code></a>
创建这个清单。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/zookeeper/zookeeper.yaml
</code></pre></div><!--
This creates the `zk-hs` Headless Service, the `zk-cs` Service,
the `zk-pdb` PodDisruptionBudget, and the `zk` StatefulSet.
-->
<p>这个操作创建了 <code>zk-hs</code> 无头服务、<code>zk-cs</code> 服务、<code>zk-pdb</code> PodDisruptionBudget
和 <code>zk</code> StatefulSet。</p>
<pre><code>service/zk-hs created
service/zk-cs created
poddisruptionbudget.policy/zk-pdb created
statefulset.apps/zk created
</code></pre><!--
Use [`kubectl get`](/docs/reference/generated/kubectl/kubectl-commands/#get) to watch the
StatefulSet controller create the StatefulSet's Pods.
-->
<p>使用命令
<a href="/docs/reference/generated/kubectl/kubectl-commands/#get"><code>kubectl get</code></a>
查看 StatefulSet 控制器创建的 Pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
Once the `zk-2` Pod is Running and Ready, use `CTRL-C` to terminate kubectl.
-->
<p>一旦  <code>zk-2</code> Pod 变成 Running 和 Ready 状态，使用 <code>CRTL-C</code> 结束 kubectl。</p>
<pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      0/1       Pending   0          0s
zk-0      0/1       Pending   0         0s
zk-0      0/1       ContainerCreating   0         0s
zk-0      0/1       Running   0         19s
zk-0      1/1       Running   0         40s
zk-1      0/1       Pending   0         0s
zk-1      0/1       Pending   0         0s
zk-1      0/1       ContainerCreating   0         0s
zk-1      0/1       Running   0         18s
zk-1      1/1       Running   0         40s
zk-2      0/1       Pending   0         0s
zk-2      0/1       Pending   0         0s
zk-2      0/1       ContainerCreating   0         0s
zk-2      0/1       Running   0         19s
zk-2      1/1       Running   0         40s
</code></pre><!--
The StatefulSet controller creates three Pods, and each Pod has a container with
a [ZooKeeper](https://www-us.apache.org/dist/zookeeper/stable/) server.
-->
<p>StatefulSet 控制器创建 3 个 Pods，每个 Pod 包含一个
<a href="https://www-us.apache.org/dist/zookeeper/stable/">ZooKeeper</a> 服务容器。</p>
<!--
### Facilitating Leader Election

Because there is no terminating algorithm for electing a leader in an anonymous network, Zab requires explicit membership configuration to perform leader election. Each server in the ensemble needs to have a unique identifier, all servers need to know the global set of identifiers, and each identifier needs to be associated with a network address.

Use [`kubectl exec`](/docs/reference/generated/kubectl/kubectl-commands/#exec) to get the hostnames
of the Pods in the `zk` StatefulSet.
-->
<h3 id="facilitating-leader-election">促成 Leader 选举 </h3>
<p>由于在匿名网络中没有用于选举 leader 的终止算法，Zab 要求显式的进行成员关系配置，
以执行 leader 选举。Ensemble 中的每个服务器都需要具有一个独一无二的标识符，
所有的服务器均需要知道标识符的全集，并且每个标识符都需要和一个网络地址相关联。</p>
<p>使用命令
<a href="/docs/reference/generated/kubectl/kubectl-commands/#exec"><code>kubectl exec</code></a>
获取 <code>zk</code> StatefulSet 中 Pods 的主机名。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> zk-<span style="color:#b8860b">$i</span> -- hostname; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
The StatefulSet controller provides each Pod with a unique hostname based on its ordinal index. The hostnames take the form of `<statefulset name>-<ordinal index>`. Because the `replicas` field of the `zk` StatefulSet is set to `3`, the Set's controller creates three Pods with their hostnames set to `zk-0`, `zk-1`, and
`zk-2`.
-->
<p>StatefulSet 控制器基于每个 Pod 的序号索引为它们各自提供一个唯一的主机名。
主机名采用 <code>&lt;statefulset 名称&gt;-&lt;序数索引&gt;</code> 的形式。
由于 <code>zk</code> StatefulSet 的 <code>replicas</code> 字段设置为 3，这个集合的控制器将创建
3 个 Pods，主机名为：<code>zk-0</code>、<code>zk-1</code> 和 <code>zk-2</code>。</p>
<pre><code>zk-0
zk-1
zk-2
</code></pre><!--
The servers in a ZooKeeper ensemble use natural numbers as unique identifiers, and store each server's identifier in a file called `myid` in the server's data directory.

To examine the contents of the `myid` file for each server use the following command.
-->
<p>ZooKeeper ensemble 中的服务器使用自然数作为唯一标识符，
每个服务器的标识符都保存在服务器的数据目录中一个名为 <code>myid</code> 的文件里。</p>
<p>检查每个服务器的 <code>myid</code> 文件的内容。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> <span style="color:#a2f">echo</span> <span style="color:#b44">&#34;myid zk-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span>;kubectl <span style="color:#a2f">exec</span> zk-<span style="color:#b8860b">$i</span> -- cat /var/lib/zookeeper/data/myid; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
Because the identifiers are natural numbers and the ordinal indices are non-negative integers, you can generate an identifier by adding 1 to the ordinal.
-->
<p>由于标识符为自然数并且序号索引是非负整数，你可以在序号上加 1 来生成一个标识符。</p>
<pre><code>myid zk-0
1
myid zk-1
2
myid zk-2
3
</code></pre><!--
To get the Fully Qualified Domain Name (FQDN) of each Pod in the `zk` StatefulSet use the following command.
-->
<p>获取 <code>zk</code> StatefulSet 中每个 Pod 的全限定域名（Fully Qualified Domain Name，FQDN）。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> zk-<span style="color:#b8860b">$i</span> -- hostname -f; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
The `zk-hs` Service creates a domain for all of the Pods,
`zk-hs.default.svc.cluster.local`.
-->
<p><code>zk-hs</code> Service 为所有 Pods 创建了一个域：<code>zk-hs.default.svc.cluster.local</code>。</p>
<pre><code>zk-0.zk-hs.default.svc.cluster.local
zk-1.zk-hs.default.svc.cluster.local
zk-2.zk-hs.default.svc.cluster.local
</code></pre><!--
The A records in [Kubernetes DNS](/docs/concepts/services-networking/dns-pod-service/) resolve the FQDNs to the Pods' IP addresses. If Kubernetes reschedules the Pods, it will update the A records with the Pods' new IP addresses, but the A records names will not change.

ZooKeeper stores its application configuration in a file named `zoo.cfg`. Use `kubectl exec` to view the contents of the `zoo.cfg` file in the `zk-0` Pod.
-->
<p><a href="/zh/docs/concepts/services-networking/dns-pod-service/">Kubernetes DNS</a>
中的 A 记录将 FQDNs 解析成为 Pods 的 IP 地址。
如果 Pods 被调度，这个 A 记录将会使用 Pods 的新 IP 地址完成更新，
但 A 记录的名称不会改变。</p>
<p>ZooKeeper 在一个名为 <code>zoo.cfg</code> 的文件中保存它的应用配置。
使用 <code>kubectl exec</code> 在  <code>zk-0</code> Pod 中查看 <code>zoo.cfg</code> 文件的内容。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 -- cat /opt/zookeeper/conf/zoo.cfg
</code></pre></div><!--
In the `server.1`, `server.2`, and `server.3` properties at the bottom of
the file, the `1`, `2`, and `3` correspond to the identifiers in the
ZooKeeper servers' `myid` files. They are set to the FQDNs for the Pods in
the `zk` StatefulSet.
-->
<p>文件底部为 <code>server.1</code>、<code>server.2</code> 和 <code>server.3</code>，其中的 <code>1</code>、<code>2</code> 和 <code>3</code>
分别对应 ZooKeeper 服务器的 <code>myid</code> 文件中的标识符。
它们被设置为 <code>zk</code> StatefulSet 中的 Pods 的 FQDNs。</p>
<pre><code>clientPort=2181
dataDir=/var/lib/zookeeper/data
dataLogDir=/var/lib/zookeeper/log
tickTime=2000
initLimit=10
syncLimit=2000
maxClientCnxns=60
minSessionTimeout= 4000
maxSessionTimeout= 40000
autopurge.snapRetainCount=3
autopurge.purgeInterval=0
server.1=zk-0.zk-hs.default.svc.cluster.local:2888:3888
server.2=zk-1.zk-hs.default.svc.cluster.local:2888:3888
server.3=zk-2.zk-hs.default.svc.cluster.local:2888:3888
</code></pre><!--
### Achieving consensus

Consensus protocols require that the identifiers of each participant be unique. No two participants in the Zab protocol should claim the same unique identifier. This is necessary to allow the processes in the system to agree on which processes have committed which data. If two Pods are launched with the same ordinal, two ZooKeeper servers would both identify themselves as the same server.
-->
<h3 id="achieving-consensus">达成共识  </h3>
<p>一致性协议要求每个参与者的标识符唯一。
在 Zab 协议里任何两个参与者都不应该声明相同的唯一标识符。
对于让系统中的进程协商哪些进程已经提交了哪些数据而言，这是必须的。
如果有两个 Pods 使用相同的序号启动，这两个 ZooKeeper 服务器
会将自己识别为相同的服务器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      0/1       Pending   0          0s
zk-0      0/1       Pending   0         0s
zk-0      0/1       ContainerCreating   0         0s
zk-0      0/1       Running   0         19s
zk-0      1/1       Running   0         40s
zk-1      0/1       Pending   0         0s
zk-1      0/1       Pending   0         0s
zk-1      0/1       ContainerCreating   0         0s
zk-1      0/1       Running   0         18s
zk-1      1/1       Running   0         40s
zk-2      0/1       Pending   0         0s
zk-2      0/1       Pending   0         0s
zk-2      0/1       ContainerCreating   0         0s
zk-2      0/1       Running   0         19s
zk-2      1/1       Running   0         40s
</code></pre><!--
The A records for each Pod are entered when the Pod becomes Ready. Therefore,
the FQDNs of the ZooKeeper servers will resolve to a single endpoint, and that
endpoint will be the unique ZooKeeper server claiming the identity configured
in its `myid` file.
-->
<p>每个 Pod 的 A 记录仅在 Pod 变成 Ready状态时被录入。
因此，ZooKeeper 服务器的 FQDNs 只会解析到一个端点，而那个端点将会是
一个唯一的 ZooKeeper 服务器，这个服务器声明了配置在它的 <code>myid</code>
文件中的标识符。</p>
<pre><code>zk-0.zk-hs.default.svc.cluster.local
zk-1.zk-hs.default.svc.cluster.local
zk-2.zk-hs.default.svc.cluster.local
</code></pre><!--
This ensures that the `servers` properties in the ZooKeepers' `zoo.cfg` files
represents a correctly configured ensemble.
-->
<p>这保证了 ZooKeepers 的 <code>zoo.cfg</code> 文件中的 <code>servers</code> 属性代表了
一个正确配置的 ensemble。</p>
<pre><code>server.1=zk-0.zk-hs.default.svc.cluster.local:2888:3888
server.2=zk-1.zk-hs.default.svc.cluster.local:2888:3888
server.3=zk-2.zk-hs.default.svc.cluster.local:2888:3888
</code></pre><!--
When the servers use the Zab protocol to attempt to commit a value, they will either achieve consensus and commit the value (if leader election has succeeded and at least two of the Pods are Running and Ready), or they will fail to do so (if either of the conditions are not met). No state will arise where one server acknowledges a write on behalf of another.
-->
<p>当服务器使用 Zab 协议尝试提交一个值的时候，它们会达成一致并成功提交这个值
（如果领导者选举成功并且至少有两个 Pods 处于 Running 和 Ready状态），
或者将会失败（如果没有满足上述条件中的任意一条）。
当一个服务器承认另一个服务器的代写时不会有状态产生。</p>
<!--
### Sanity Testing the Ensemble

The most basic sanity test is to write data to one ZooKeeper server and
to read the data from another.

The command below executes the `zkCli.sh` script to write `world` to the path `/hello` on the `zk-0` Pod in the ensemble.
-->
<h3 id="ensemble-健康检查">Ensemble 健康检查</h3>
<p>最基本的健康检查是向一个 ZooKeeper 服务器写入一些数据，然后从
另一个服务器读取这些数据。</p>
<p>使用 <code>zkCli.sh</code> 脚本在 <code>zk-0</code> Pod 上写入 <code>world</code> 到路径 <code>/hello</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 zkCli.sh create /hello world
</code></pre></div><pre><code>WATCHER::

WatchedEvent state:SyncConnected type:None path:null
Created /hello
</code></pre><!--
To get the data from the `zk-1` Pod use the following command.
-->
<p>使用下面的命令从 <code>zk-1</code> Pod 获取数据。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-1 zkCli.sh get /hello
</code></pre></div><!--
The data that you created on `zk-0` is available on all the servers in the
ensemble.
-->
<p>你在 <code>zk-0</code> 上创建的数据在 ensemble 中所有的服务器上都是可用的。</p>
<pre><code>WATCHER::

WatchedEvent state:SyncConnected type:None path:null
world
cZxid = 0x100000002
ctime = Thu Dec 08 15:13:30 UTC 2016
mZxid = 0x100000002
mtime = Thu Dec 08 15:13:30 UTC 2016
pZxid = 0x100000002
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 5
numChildren = 0
</code></pre><!--
### Providing Durable Storage

As mentioned in the [ZooKeeper Basics](#zookeeper-basics) section,
ZooKeeper commits all entries to a durable WAL, and periodically writes snapshots
in memory state, to storage media. Using WALs to provide durability is a common
technique for applications that use consensus protocols to achieve a replicated
state machine.

Use the [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands/#delete) command to delete the
`zk` StatefulSet.
-->
<h3 id="提供持久存储">提供持久存储</h3>
<p>如同在 <a href="#zookeeper-basics">ZooKeeper</a> 一节所提到的，ZooKeeper 提交
所有的条目到一个持久 WAL，并周期性的将内存快照写入存储介质。
对于使用一致性协议实现一个复制状态机的应用来说，使用 WALs 提供持久化
是一种常用的技术，对于普通的存储应用也是如此。</p>
<p>使用 <a href="/docs/reference/generated/kubectl/kubectl-commands/#delete"><code>kubectl delete</code></a>
删除 <code>zk</code> StatefulSet。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete statefulset zk
</code></pre></div><pre><code>statefulset.apps &quot;zk&quot; deleted
</code></pre><!--
Watch the termination of the Pods in the StatefulSet.
-->
<p>观察 StatefulSet 中的 Pods 变为终止状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
When `zk-0` if fully terminated, use `CTRL-C` to terminate kubectl.
-->
<p>当 <code>zk-0</code> 完全终止时，使用 <code>CRTL-C</code> 结束 kubectl。</p>
<pre><code>zk-2      1/1       Terminating   0         9m
zk-0      1/1       Terminating   0         11m
zk-1      1/1       Terminating   0         10m
zk-2      0/1       Terminating   0         9m
zk-2      0/1       Terminating   0         9m
zk-2      0/1       Terminating   0         9m
zk-1      0/1       Terminating   0         10m
zk-1      0/1       Terminating   0         10m
zk-1      0/1       Terminating   0         10m
zk-0      0/1       Terminating   0         11m
zk-0      0/1       Terminating   0         11m
zk-0      0/1       Terminating   0         11m
</code></pre><!--
Reapply the manifest in `zookeeper.yaml`.
-->
<p>重新应用 <code>zookeeper.yaml</code> 中的清单。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/zookeeper/zookeeper.yaml
</code></pre></div><!--
This creates the `zk` StatefulSet object, but the other API objects in the manifest are not modified because they already exist.

Watch the StatefulSet controller recreate the StatefulSet's Pods.
-->
<p><code>zk</code> StatefulSet 将会被创建。由于清单中的其他 API 对象已经存在，所以它们不会被修改。</p>
<p>观察 StatefulSet 控制器重建 StatefulSet 的 Pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
Once the `zk-2` Pod is Running and Ready, use `CTRL-C` to terminate kubectl.
-->
<p>一旦 <code>zk-2</code> Pod 处于 Running 和 Ready 状态，使用 <code>CRTL-C</code> 停止 kubectl命令。</p>
<pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      0/1       Pending   0          0s
zk-0      0/1       Pending   0         0s
zk-0      0/1       ContainerCreating   0         0s
zk-0      0/1       Running   0         19s
zk-0      1/1       Running   0         40s
zk-1      0/1       Pending   0         0s
zk-1      0/1       Pending   0         0s
zk-1      0/1       ContainerCreating   0         0s
zk-1      0/1       Running   0         18s
zk-1      1/1       Running   0         40s
zk-2      0/1       Pending   0         0s
zk-2      0/1       Pending   0         0s
zk-2      0/1       ContainerCreating   0         0s
zk-2      0/1       Running   0         19s
zk-2      1/1       Running   0         40s
</code></pre><!--
Use the command below to get the value you entered during the [sanity test](#sanity-testing-the-ensemble),
from the `zk-2` Pod.
-->
<p>从 <code>zk-2</code> Pod 中获取你在<a href="#Ensemble-%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5">健康检查</a>中输入的值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-2 zkCli.sh get /hello
</code></pre></div><!--
Even though you terminated and recreated all of the Pods in the `zk` StatefulSet, the ensemble still serves the original value.
-->
<p>尽管 <code>zk</code> StatefulSet 中所有的 Pods 都已经被终止并重建过，ensemble
仍然使用原来的数值提供服务。</p>
<pre><code>WATCHER::

WatchedEvent state:SyncConnected type:None path:null
world
cZxid = 0x100000002
ctime = Thu Dec 08 15:13:30 UTC 2016
mZxid = 0x100000002
mtime = Thu Dec 08 15:13:30 UTC 2016
pZxid = 0x100000002
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 5
numChildren = 0
</code></pre><!--
The `volumeClaimTemplates` field of the `zk` StatefulSet's `spec` specifies a PersistentVolume provisioned for each Pod.
-->
<p><code>zk</code> StatefulSet 的 <code>spec</code> 中的 <code>volumeClaimTemplates</code> 字段标识了
将要为每个 Pod 准备的 PersistentVolume。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>datadir<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volume.alpha.kubernetes.io/storage-class</span>:<span style="color:#bbb"> </span>anything<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>20Gi<span style="color:#bbb">
</span></code></pre></div><!--
The `StatefulSet` controller generates a `PersistentVolumeClaim` for each Pod in
the `StatefulSet`.

Use the following command to get the `StatefulSet`'s `PersistentVolumeClaims`.
-->
<p><code>StatefulSet</code> 控制器为 <code>StatefulSet</code> 中的每个 Pod 生成一个 <code>PersistentVolumeClaim</code>。</p>
<p>获取 <code>StatefulSet</code> 的 <code>PersistentVolumeClaim</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pvc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
When the `StatefulSet` recreated its Pods, it remounts the Pods' PersistentVolumes.
-->
<p>当 <code>StatefulSet</code> 重新创建它的 Pods 时，Pods 的 PersistentVolumes 会被重新挂载。</p>
<pre><code>NAME           STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
datadir-zk-0   Bound     pvc-bed742cd-bcb1-11e6-994f-42010a800002   20Gi       RWO           1h
datadir-zk-1   Bound     pvc-bedd27d2-bcb1-11e6-994f-42010a800002   20Gi       RWO           1h
datadir-zk-2   Bound     pvc-bee0817e-bcb1-11e6-994f-42010a800002   20Gi       RWO           1h
</code></pre><!--
The `volumeMounts` section of the `StatefulSet`'s container `template` mounts the PersistentVolumes in the ZooKeeper servers' data directories.
-->
<p>StatefulSet 的容器 <code>template</code> 中的 <code>volumeMounts</code> 一节使得
PersistentVolumes 被挂载到 ZooKeeper 服务器的数据目录。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>datadir<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/zookeeper<span style="color:#bbb">
</span></code></pre></div><!--
When a Pod in the `zk` `StatefulSet` is (re)scheduled, it will always have the
same `PersistentVolume` mounted to the ZooKeeper server's data directory.
Even when the Pods are rescheduled, all the writes made to the ZooKeeper
servers' WALs, and all their snapshots, remain durable.
-->
<p>当 <code>zk</code> <code>StatefulSet</code> 中的一个 Pod 被（重新）调度时，它总是拥有相同的 PersistentVolume，
挂载到 ZooKeeper 服务器的数据目录。
即使在 Pods 被重新调度时，所有对 ZooKeeper 服务器的 WALs 的写入和它们的
全部快照都仍然是持久的。</p>
<!--
## Ensuring consistent configuration

As noted in the [Facilitating Leader Election](#facilitating-leader-election) and
[Achieving Consensus](#achieving-consensus) sections, the servers in a
ZooKeeper ensemble require consistent configuration to elect a leader
and form a quorum. They also require consistent configuration of the Zab protocol
in order for the protocol to work correctly over a network. In our example we
achieve consistent configuration by embedding the configuration directly into
the manifest.

Get the `zk` StatefulSet.
-->
<h2 id="确保一致性配置">确保一致性配置</h2>
<p>如同在<a href="#facilitating-leader-election">促成领导者选举</a> 和<a href="#achieving-consensus">达成一致</a>
小节中提到的，ZooKeeper ensemble 中的服务器需要一致性的配置来选举一个领导者并形成一个
quorum。它们还需要 Zab 协议的一致性配置来保证这个协议在网络中正确的工作。
在这次的示例中，我们通过直接将配置写入代码清单中来达到该目的。</p>
<p>获取 <code>zk</code> StatefulSet。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get sts zk -o yaml
</code></pre></div><pre><code>    ...
    command:
      - sh
      - -c
      - &quot;start-zookeeper \
        --servers=3 \
        --data_dir=/var/lib/zookeeper/data \
        --data_log_dir=/var/lib/zookeeper/data/log \
        --conf_dir=/opt/zookeeper/conf \
        --client_port=2181 \
        --election_port=3888 \
        --server_port=2888 \
        --tick_time=2000 \
        --init_limit=10 \
        --sync_limit=5 \
        --heap=512M \
        --max_client_cnxns=60 \
        --snap_retain_count=3 \
        --purge_interval=12 \
        --max_session_timeout=40000 \
        --min_session_timeout=4000 \
        --log_level=INFO&quot;
...
</code></pre><!--
The command used to start the ZooKeeper servers passed the configuration as command line parameter. You can also use environment variables to pass configuration to the ensemble.
-->
<p>用于启动 ZooKeeper 服务器的命令将这些配置作为命令行参数传给了 ensemble。
你也可以通过环境变量来传入这些配置。</p>
<!--
### Configuring Logging

One of the files generated by the `zkGenConfig.sh` script controls ZooKeeper's logging.
ZooKeeper uses [Log4j](https://logging.apache.org/log4j/2.x/), and, by default,
it uses a time and size based rolling file appender for its logging configuration.

Use the command below to get the logging configuration from one of Pods in the `zk` `StatefulSet`.
-->
<h3 id="configuring-logging">配置日志  </h3>
<p><code>zkGenConfig.sh</code> 脚本产生的一个文件控制了 ZooKeeper 的日志行为。
ZooKeeper 使用了 <a href="http://logging.apache.org/log4j/2.x/">Log4j</a> 并默认使用
基于文件大小和时间的滚动文件追加器作为日志配置。</p>
<p>从 <code>zk</code> StatefulSet 的一个 Pod 中获取日志配置。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 cat /usr/etc/zookeeper/log4j.properties
</code></pre></div><!--
The logging configuration below will cause the ZooKeeper process to write all
of its logs to the standard output file stream.
-->
<p>下面的日志配置会使 ZooKeeper 进程将其所有的日志写入标志输出文件流中。</p>
<pre><code>zookeeper.root.logger=CONSOLE
zookeeper.console.threshold=INFO
log4j.rootLogger=${zookeeper.root.logger}
log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
log4j.appender.CONSOLE.Threshold=${zookeeper.console.threshold}
log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n
</code></pre><!--
This is the simplest possible way to safely log inside the container.
Because the applications write logs to standard out, Kubernetes will handle log rotation for you.
Kubernetes also implements a sane retention policy that ensures application logs written to
standard out and standard error do not exhaust local storage media.

Use [`kubectl logs`](/docs/reference/generated/kubectl/kubectl-commands/#logs) to retrieve the last 20 log lines from one of the Pods.
-->
<p>这是在容器里安全记录日志的最简单的方法。
由于应用的日志被写入标准输出，Kubernetes 将会为你处理日志轮转。
Kubernetes 还实现了一个智能保存策略，保证写入标准输出和标准错误流
的应用日志不会耗尽本地存储媒介。</p>
<p>使用命令 <a href="/docs/reference/generated/kubectl/kubectl-commands/#logs"><code>kubectl logs</code></a>
从一个 Pod 中取回最后 20 行日志。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs zk-0 --tail <span style="color:#666">20</span>
</code></pre></div><!--
You can view application logs written to standard out or standard error using `kubectl logs` and from the Kubernetes Dashboard.
-->
<p>使用 <code>kubectl logs</code> 或者从 Kubernetes Dashboard 可以查看写入到标准输出和标准错误流中的应用日志。</p>
<pre><code>2016-12-06 19:34:16,236 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52740
2016-12-06 19:34:16,237 [myid:1] - INFO  [Thread-1136:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52740 (no session established for client)
2016-12-06 19:34:26,155 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52749
2016-12-06 19:34:26,155 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52749
2016-12-06 19:34:26,156 [myid:1] - INFO  [Thread-1137:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52749 (no session established for client)
2016-12-06 19:34:26,222 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52750
2016-12-06 19:34:26,222 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52750
2016-12-06 19:34:26,226 [myid:1] - INFO  [Thread-1138:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52750 (no session established for client)
2016-12-06 19:34:36,151 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52760
2016-12-06 19:34:36,152 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52760
2016-12-06 19:34:36,152 [myid:1] - INFO  [Thread-1139:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52760 (no session established for client)
2016-12-06 19:34:36,230 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52761
2016-12-06 19:34:36,231 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52761
2016-12-06 19:34:36,231 [myid:1] - INFO  [Thread-1140:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52761 (no session established for client)
2016-12-06 19:34:46,149 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52767
2016-12-06 19:34:46,149 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52767
2016-12-06 19:34:46,149 [myid:1] - INFO  [Thread-1141:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52767 (no session established for client)
2016-12-06 19:34:46,230 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52768
2016-12-06 19:34:46,230 [myid:1] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52768
2016-12-06 19:34:46,230 [myid:1] - INFO  [Thread-1142:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52768 (no session established for client)
</code></pre><!--
Kubernetes integrates with many logging solutions. You can choose a logging solution
that best fits your cluster and applications. For cluster-level logging and aggregation,
consider deploying a [sidecar container](/docs/concepts/cluster-administration/logging#sidecar-container-with-logging-agent) to rotate and ship your logs.
-->
<p>Kubernetes 支持与多种日志方案集成。你可以选择一个最适合你的集群和应用
的日志解决方案。对于集群级别的日志输出与整合，可以考虑部署一个
<a href="/zh/docs/concepts/cluster-administration/logging#sidecar-container-with-logging-agent">边车容器</a>
来轮转和提供日志数据。</p>
<!--
### Configuring a non-privileged user

The best practices to allow an application to run as a privileged
user inside of a container are a matter of debate. If your organization requires
that applications run as a non-privileged user you can use a
[SecurityContext](/docs/tasks/configure-pod-container/security-context/) to control the user that
the entry point runs as.

The `zk` `StatefulSet`'s Pod `template` contains a `SecurityContext`.
-->
<h3 id="配置非特权用户">配置非特权用户</h3>
<p>在容器中允许应用以特权用户运行这条最佳实践是值得商讨的。
如果你的组织要求应用以非特权用户运行，你可以使用
<a href="/zh/docs/tasks/configure-pod-container/security-context/">SecurityContext</a>
控制运行容器入口点所使用的用户。</p>
<p><code>zk</code> StatefulSet 的 Pod 的 <code>template</code> 包含了一个 <code>SecurityContext</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">runAsUser</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">fsGroup</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span></code></pre></div><!--
In the Pods' containers, UID 1000 corresponds to the zookeeper user and GID 1000
corresponds to the zookeeper group.

Get the ZooKeeper process information from the `zk-0` Pod.
-->
<p>在 Pods 的容器内部，UID 1000 对应用户 zookeeper，GID 1000 对应用户组 zookeeper。</p>
<p>从 <code>zk-0</code> Pod 获取 ZooKeeper 进程信息。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 -- ps -elf
</code></pre></div><!--
As the `runAsUser` field of the `securityContext` object is set to 1000,
instead of running as root, the ZooKeeper process runs as the zookeeper user.
-->
<p>由于 <code>securityContext</code> 对象的 <code>runAsUser</code> 字段被设置为 1000 而不是 root，
ZooKeeper 进程将以 zookeeper 用户运行。</p>
<pre><code>F S UID        PID  PPID  C PRI  NI ADDR SZ WCHAN  STIME TTY          TIME CMD
4 S zookeep+     1     0  0  80   0 -  1127 -      20:46 ?        00:00:00 sh -c zkGenConfig.sh &amp;&amp; zkServer.sh start-foreground
0 S zookeep+    27     1  0  80   0 - 1155556 -    20:46 ?        00:00:19 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.9.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper: -Xmx2G -Xms2G -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/bin/../etc/zookeeper/zoo.cfg
</code></pre><!--
By default, when the Pod's PersistentVolumes is mounted to the ZooKeeper server's data directory, it is only accessible by the root user. This configuration prevents the ZooKeeper process from writing to its WAL and storing its snapshots.

Use the command below to get the file permissions of the ZooKeeper data directory on the `zk-0` Pod.
-->
<p>默认情况下，当 Pod 的 PersistentVolume 被挂载到 ZooKeeper 服务器的数据目录时，
它只能被 root 用户访问。这个配置将阻止 ZooKeeper 进程写入它的 WAL 及保存快照。</p>
<p>在 <code>zk-0</code> Pod 上获取 ZooKeeper 数据目录的文件权限。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -ti zk-0 -- ls -ld /var/lib/zookeeper/data
</code></pre></div><!--
Because the `fsGroup` field of the `securityContext` object is set to 1000, the ownership of the Pods' PersistentVolumes is set to the zookeeper group, and the ZooKeeper process is able to read and write its data.
-->
<p>由于 <code>securityContext</code> 对象的 <code>fsGroup</code> 字段设置为 1000，Pods 的
PersistentVolumes 的所有权属于 zookeeper 用户组，因而 ZooKeeper
进程能够成功地读写数据。</p>
<pre><code>drwxr-sr-x 3 zookeeper zookeeper 4096 Dec  5 20:45 /var/lib/zookeeper/data
</code></pre><!--
## Managing the ZooKeeper Process

The [ZooKeeper documentation](https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_supervision)
mentions that "You will want to have a supervisory process that
manages each of your ZooKeeper server processes (JVM)." Utilizing a watchdog
(supervisory process) to restart failed processes in a distributed system is a
common pattern. When deploying an application in Kubernetes, rather than using
an external utility as a supervisory process, you should use Kubernetes as the
watchdog for your application.
-->
<h2 id="管理-zookeeper-进程">管理 ZooKeeper 进程</h2>
<p><a href="https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_supervision">ZooKeeper 文档</a>
指出“你将需要一个监管程序用于管理每个 ZooKeeper 服务进程（JVM）”。
在分布式系统中，使用一个看门狗（监管程序）来重启故障进程是一种常用的模式。</p>
<!--
### Updating the ensemble

The `zk` `StatefulSet` is configured to use the `RollingUpdate` update strategy.

You can use `kubectl patch` to update the number of `cpus` allocated to the servers.
-->
<h3 id="更新-ensemble">更新 Ensemble</h3>
<p><code>zk</code> <code>StatefulSet</code> 的更新策略被设置为了 <code>RollingUpdate</code>。</p>
<p>你可以使用 <code>kubectl patch</code> 更新分配给每个服务器的 <code>cpus</code> 的数量。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch sts zk --type<span style="color:#666">=</span><span style="color:#b44">&#39;json&#39;</span> -p<span style="color:#666">=</span><span style="color:#b44">&#39;[{&#34;op&#34;: &#34;replace&#34;, &#34;path&#34;: &#34;/spec/template/spec/containers/0/resources/requests/cpu&#34;, &#34;value&#34;:&#34;0.3&#34;}]&#39;</span>
</code></pre></div><pre><code>statefulset.apps/zk patched
</code></pre><!--
Use `kubectl rollout status` to watch the status of the update.
-->
<p>使用 <code>kubectl rollout status</code> 观测更新状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status sts/zk
</code></pre></div><pre><code>waiting for statefulset rolling update to complete 0 pods at revision zk-5db4499664...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
waiting for statefulset rolling update to complete 1 pods at revision zk-5db4499664...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
waiting for statefulset rolling update to complete 2 pods at revision zk-5db4499664...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
statefulset rolling update complete 3 pods at revision zk-5db4499664...
</code></pre><!--
This terminates the Pods, one at a time, in reverse ordinal order, and recreates them with the new configuration. This ensures that quorum is maintained during a rolling update.

Use the `kubectl rollout history` command to view a history or previous configurations.

The output is similar to this:
-->
<p>这项操作会逆序地依次终止每一个 Pod，并用新的配置重新创建。
这样做确保了在滚动更新的过程中 quorum 依旧保持工作。</p>
<p>使用 <code>kubectl rollout history</code> 命令查看历史或先前的配置。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout <span style="color:#a2f">history</span> sts/zk
</code></pre></div><p>输出类似于：</p>
<pre><code>statefulsets &quot;zk&quot;
REVISION
1
2
</code></pre><!--
Use the `kubectl rollout undo` command to roll back the modification.

The output is similar to this:
-->
<p>使用 <code>kubectl rollout undo</code> 命令撤销这次的改动。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout undo sts/zk
</code></pre></div><p>输出类似于：</p>
<pre><code>statefulset.apps/zk rolled back
</code></pre><!--
### Handling process failure

[Restart Policies](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) control how
Kubernetes handles process failures for the entry point of the container in a Pod.
For Pods in a `StatefulSet`, the only appropriate `RestartPolicy` is Always, and this
is the default value. For stateful applications you should **never** override
the default policy.

Use the following command to examine the process tree for the ZooKeeper server running in the `zk-0` Pod.
-->
<h3 id="处理进程故障">处理进程故障</h3>
<p><a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">重启策略</a>
控制 Kubernetes 如何处理一个 Pod 中容器入口点的进程故障。
对于 StatefulSet 中的 Pods 来说，Always 是唯一合适的 RestartPolicy，也是默认值。
你应该<strong>绝不</strong>覆盖有状态应用的默认策略。</p>
<p>检查 <code>zk-0</code> Pod 中运行的 ZooKeeper 服务器的进程树。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 -- ps -ef
</code></pre></div><!--
The command used as the container's entry point has PID 1, and
the ZooKeeper process, a child of the entry point, has PID 27.
-->
<p>作为容器入口点的命令的 PID 为 1，Zookeeper 进程是入口点的子进程，
PID 为 27。</p>
<pre><code>UID        PID  PPID  C STIME TTY          TIME CMD
zookeep+     1     0  0 15:03 ?        00:00:00 sh -c zkGenConfig.sh &amp;&amp; zkServer.sh start-foreground
zookeep+    27     1  0 15:03 ?        00:00:03 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.9.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper: -Xmx2G -Xms2G -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/bin/../etc/zookeeper/zoo.cfg
</code></pre><!--
In another terminal watch the Pods in the `zk` `StatefulSet` with the following command.
-->
<p>在一个终端观察 <code>zk</code> <code>StatefulSet</code> 中的 Pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
In another terminal, terminate the ZooKeeper process in Pod `zk-0` with the following command.
-->
<p>在另一个终端杀掉 Pod <code>zk-0</code> 中的 ZooKeeper 进程。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"> kubectl <span style="color:#a2f">exec</span> zk-0 -- pkill java
</code></pre></div><!--
The termination of the ZooKeeper process caused its parent process to terminate. Because the `RestartPolicy` of the container is Always, it restarted the parent process.
-->
<p>ZooKeeper 进程的终结导致了它父进程的终止。由于容器的 <code>RestartPolicy</code>
是 Always，父进程被重启。</p>
<pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      1/1       Running   0          21m
zk-1      1/1       Running   0          20m
zk-2      1/1       Running   0          19m
NAME      READY     STATUS    RESTARTS   AGE
zk-0      0/1       Error     0          29m
zk-0      0/1       Running   1         29m
zk-0      1/1       Running   1         29m
</code></pre><!--
If your application uses a script (such as `zkServer.sh`) to launch the process
that implements the application's business logic, the script must terminate with the
child process. This ensures that Kubernetes will restart the application's
container when the process implementing the application's business logic fails.
-->
<p>如果你的应用使用一个脚本（例如 <code>zkServer.sh</code>）来启动一个实现了应用业务逻辑的进程，
这个脚本必须和子进程一起结束。这保证了当实现应用业务逻辑的进程故障时，
Kubernetes 会重启这个应用的容器。</p>
<!--
### Testing for liveness

Configuring your application to restart failed processes is not enough to
keep a distributed system healthy. There are scenarios where
a system's processes can be both alive and unresponsive, or otherwise
unhealthy. You should use liveness probes to notify Kubernetes
that your application's processes are unhealthy and it should restart them.

The Pod `template` for the `zk` `StatefulSet` specifies a liveness probe.
-->
<h3 id="存活性测试">存活性测试</h3>
<p>你的应用配置为自动重启故障进程，但这对于保持一个分布式系统的健康来说是不够的。
许多场景下，一个系统进程可以是活动状态但不响应请求，或者是不健康状态。
你应该使用存活性探针来通知 Kubernetes 你的应用进程处于不健康状态，需要被重启。</p>
<p><code>zk</code> <code>StatefulSet</code> 的 Pod 的 <code>template</code> 一节指定了一个存活探针。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;zookeeper-ready 2181&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></code></pre></div><!--
The probe calls a bash script that uses the ZooKeeper `ruok` four letter
word to test the server's health.
-->
<p>这个探针调用一个简单的 Bash 脚本，使用 ZooKeeper 的四字缩写 <code>ruok</code>
来测试服务器的健康状态。</p>
<pre><code>OK=$(echo ruok | nc 127.0.0.1 $1)
if [ &quot;$OK&quot; == &quot;imok&quot; ]; then
    exit 0
else
    exit 1
fi
</code></pre><!--
In one terminal window, use the following command to watch the Pods in the `zk` StatefulSet.
-->
<p>在一个终端窗口中使用下面的命令观察 <code>zk</code> StatefulSet 中的 Pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
In another window, using the following command to delete the `zookeeper-ready` script from the file system of Pod `zk-0`.
-->
<p>在另一个窗口中，从 Pod <code>zk-0</code> 的文件系统中删除 <code>zookeeper-ready</code> 脚本。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 -- rm /opt/zookeeper/bin/zookeeper-ready
</code></pre></div><!--
When the liveness probe for the ZooKeeper process fails, Kubernetes will
automatically restart the process for you, ensuring that unhealthy processes in
the ensemble are restarted.
-->
<p>当 ZooKeeper 进程的存活探针探测失败时，Kubernetes 将会为你自动重启这个进程，
从而保证 ensemble 中不健康状态的进程都被重启。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      1/1       Running   0          1h
zk-1      1/1       Running   0          1h
zk-2      1/1       Running   0          1h
NAME      READY     STATUS    RESTARTS   AGE
zk-0      0/1       Running   0          1h
zk-0      0/1       Running   1         1h
zk-0      1/1       Running   1         1h
</code></pre><!--
### Testing for readiness

Readiness is not the same as liveness. If a process is alive, it is scheduled
and healthy. If a process is ready, it is able to process input. Liveness is
a necessary, but not sufficient, condition for readiness. There are cases,
particularly during initialization and termination, when a process can be
alive but not ready.
-->
<h3 id="就绪性测试">就绪性测试</h3>
<p>就绪不同于存活。如果一个进程是存活的，它是可调度和健康的。
如果一个进程是就绪的，它应该能够处理输入。存活是就绪的必要非充分条件。
在许多场景下，特别是初始化和终止过程中，一个进程可以是存活但没有就绪的。</p>
<!--
If you specify a readiness probe, Kubernetes will ensure that your application's
processes will not receive network traffic until their readiness checks pass.

For a ZooKeeper server, liveness implies readiness.  Therefore, the readiness
probe from the `zookeeper.yaml` manifest is identical to the liveness probe.
-->
<p>如果你指定了一个就绪探针，Kubernetes 将保证在就绪检查通过之前，
你的应用不会接收到网络流量。</p>
<p>对于一个 ZooKeeper 服务器来说，存活即就绪。
因此 <code>zookeeper.yaml</code> 清单中的就绪探针和存活探针完全相同。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;zookeeper-ready 2181&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></code></pre></div><!--
Even though the liveness and readiness probes are identical, it is important
to specify both. This ensures that only healthy servers in the ZooKeeper
ensemble receive network traffic.
-->
<p>虽然存活探针和就绪探针是相同的，但同时指定它们两者仍然重要。
这保证了 ZooKeeper ensemble 中只有健康的服务器能接收网络流量。</p>
<!--
## Tolerating node failure

ZooKeeper needs a quorum of servers to successfully commit mutations
to data. For a three server ensemble, two servers must be healthy for
writes to succeed. In quorum based systems, members are deployed across failure
domains to ensure availability. To avoid an outage, due to the loss of an
individual machine, best practices preclude co-locating multiple instances of the
application on the same machine.
-->
<h2 id="容忍节点故障">容忍节点故障</h2>
<p>ZooKeeper 需要一个 quorum 来提交数据变动。对于一个拥有 3 个服务器的 ensemble 来说，
必须有两个服务器是健康的，写入才能成功。
在基于 quorum 的系统里，成员被部署在多个故障域中以保证可用性。
为了防止由于某台机器断连引起服务中断，最佳实践是防止应用的多个实例在相同的机器上共存。</p>
<!--
By default, Kubernetes may co-locate Pods in a `StatefulSet` on the same node.
For the three server ensemble you created, if two servers are on the same node, and that node fails,
the clients of your ZooKeeper service will experience an outage until at least one of the Pods can be rescheduled.
-->
<p>默认情况下，Kubernetes 可以把 <code>StatefulSet</code> 的 Pods 部署在相同节点上。
对于你创建的 3 个服务器的 ensemble 来说，如果有两个服务器并存于
相同的节点上并且该节点发生故障时，ZooKeeper 服务将中断，
直至至少一个 Pods 被重新调度。</p>
<!--
You should always provision additional capacity to allow the processes of critical
systems to be rescheduled in the event of node failures. If you do so, then the
outage will only last until the Kubernetes scheduler reschedules one of the ZooKeeper
servers. However, if you want your service to tolerate node failures with no downtime,
you should set `podAntiAffinity`.

Use the command below to get the nodes for Pods in the `zk` `StatefulSet`.
-->
<p>你应该总是提供多余的容量以允许关键系统进程在节点故障时能够被重新调度。
如果你这样做了，服务故障就只会持续到 Kubernetes 调度器重新调度某个
ZooKeeper 服务器为止。
但是，如果希望你的服务在容忍节点故障时无停服时间，你应该设置 <code>podAntiAffinity</code>。</p>
<p>使用下面的命令获取 <code>zk</code> <code>StatefulSet</code> 中的 Pods 的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl get pod zk-<span style="color:#b8860b">$i</span> --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span>; <span style="color:#a2f">echo</span> <span style="color:#b44">&#34;&#34;</span>; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
All of the Pods in the `zk` `StatefulSet` are deployed on different nodes.
-->
<p><code>zk</code> <code>StatefulSet</code> 中所有的 Pods 都被部署在不同的节点。</p>
<pre><code>kubernetes-node-cxpk
kubernetes-node-a5aq
kubernetes-node-2g2d
</code></pre><!--
This is because the Pods in the `zk` `StatefulSet` have a `PodAntiAffinity` specified.
-->
<p>这是因为 <code>zk</code> <code>StatefulSet</code> 中的 Pods 指定了 <code>PodAntiAffinity</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podAntiAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;app&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span>- zk<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
The `requiredDuringSchedulingIgnoredDuringExecution` field tells the
Kubernetes Scheduler that it should never co-locate two Pods which have `app` label
as `zk` in the domain defined by the `topologyKey`. The `topologyKey`
`kubernetes.io/hostname` indicates that the domain is an individual node. Using
different rules, labels, and selectors, you can extend this technique to spread
your ensemble across physical, network, and power failure domains.
-->
<p><code>requiredDuringSchedulingIgnoredDuringExecution</code> 告诉 Kubernetes 调度器，
在以 <code>topologyKey</code> 指定的域中，绝对不要把带有键为 <code>app</code>、值为 <code>zk</code> 的标签
的两个 Pods 调度到相同的节点。<code>topologyKey</code> <code>kubernetes.io/hostname</code> 表示
这个域是一个单独的节点。
使用不同的规则、标签和选择算符，你能够通过这种技术把你的 ensemble 分布
在不同的物理、网络和电力故障域之间。</p>
<!--
## Surviving maintenance

**In this section you will cordon and drain nodes. If you are using this tutorial
on a shared cluster, be sure that this will not adversely affect other tenants.**

The previous section showed you how to spread your Pods across nodes to survive
unplanned node failures, but you also need to plan for temporary node failures
that occur due to planned maintenance.

Use this command to get the nodes in your cluster.
-->
<h2 id="节点维护期间保持应用可用">节点维护期间保持应用可用</h2>
<p><strong>在本节中你将会隔离（Cordon）和腾空（Drain）节点。
如果你是在一个共享的集群里使用本教程，请保证不会影响到其他租户。</strong></p>
<p>上一小节展示了如何在节点之间分散 Pods 以在计划外的节点故障时保证服务存活。
但是你也需要为计划内维护引起的临时节点故障做准备。</p>
<p>使用此命令获取你的集群中的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><!--
Use [`kubectl cordon`](/docs/reference/generated/kubectl/kubectl-commands/#cordon) to
cordon all but four of the nodes in your cluster.
-->
<p>使用 <a href="/docs/reference/generated/kubectl/kubectl-commands/#cordon"><code>kubectl cordon</code></a>
隔离你的集群中除 4 个节点以外的所有节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cordon &lt;node-name&gt;
</code></pre></div><!--
Use this command to get the `zk-pdb` `PodDisruptionBudget`.
-->
<p>使用下面的命令获取 <code>zk-pdb</code> <code>PodDisruptionBudget</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pdb zk-pdb
</code></pre></div><!--
The `max-unavailable` field indicates to Kubernetes that at most one Pod from
`zk` `StatefulSet` can be unavailable at any time.
-->
<p><code>max-unavailable</code> 字段指示 Kubernetes 在任何时候，<code>zk</code> <code>StatefulSet</code>
至多有一个 Pod 是不可用的。</p>
<pre><code>NAME      MIN-AVAILABLE   MAX-UNAVAILABLE   ALLOWED-DISRUPTIONS   AGE
zk-pdb    N/A             1                 1
</code></pre><!--
In one terminal, use this command to watch the Pods in the `zk` `StatefulSet`.
-->
<p>在一个终端中，使用下面的命令观察 <code>zk</code> <code>StatefulSet</code> 中的 Pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><!--
In another terminal, use this command to get the nodes that the Pods are currently scheduled on.
-->
<p>在另一个终端中，使用下面的命令获取 Pods 当前调度的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl get pod zk-<span style="color:#b8860b">$i</span> --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span>; <span style="color:#a2f">echo</span> <span style="color:#b44">&#34;&#34;</span>; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>kubernetes-node-pb41
kubernetes-node-ixsl
kubernetes-node-i4c4
</code></pre><!--
Use [`kubectl drain`](/docs/reference/generated/kubectl/kubectl-commands/#drain) to cordon and
drain the node on which the `zk-0` Pod is scheduled.

The output is similar to this:
-->
<p>使用 <a href="/docs/reference/generated/kubectl/kubectl-commands/#drain"><code>kubectl drain</code></a>
来隔离和腾空 <code>zk-0</code> Pod 调度所在的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain <span style="color:#a2f;font-weight:bold">$(</span>kubectl get pod zk-0 --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span><span style="color:#a2f;font-weight:bold">)</span> --ignore-daemonsets --force --delete-emptydir-data
</code></pre></div><p>输出类似于：</p>
<pre><code>node &quot;kubernetes-node-pb41&quot; cordoned

WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-pb41, kube-proxy-kubernetes-node-pb41; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-o5elz
pod &quot;zk-0&quot; deleted
node &quot;kubernetes-node-pb41&quot; drained
</code></pre><!--
As there are four nodes in your cluster, `kubectl drain`, succeeds and the
`zk-0` is rescheduled to another node.
-->
<p>由于你的集群中有 4 个节点, <code>kubectl drain</code> 执行成功，<code>zk-0</code> 被调度到其它节点。</p>
<pre><code>NAME      READY     STATUS    RESTARTS   AGE
zk-0      1/1       Running   2          1h
zk-1      1/1       Running   0          1h
zk-2      1/1       Running   0          1h
NAME      READY     STATUS        RESTARTS   AGE
zk-0      1/1       Terminating   2          2h
zk-0      0/1       Terminating   2         2h
zk-0      0/1       Terminating   2         2h
zk-0      0/1       Terminating   2         2h
zk-0      0/1       Pending   0         0s
zk-0      0/1       Pending   0         0s
zk-0      0/1       ContainerCreating   0         0s
zk-0      0/1       Running   0         51s
zk-0      1/1       Running   0         1m
</code></pre><!--
Keep watching the `StatefulSet`'s Pods in the first terminal and drain the node on which
`zk-1` is scheduled.

The output is similar to this:
-->
<p>在第一个终端中持续观察 <code>StatefulSet</code> 的 Pods 并腾空 <code>zk-1</code> 调度所在的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain <span style="color:#a2f;font-weight:bold">$(</span>kubectl get pod zk-1 --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span><span style="color:#a2f;font-weight:bold">)</span> --ignore-daemonsets --force --delete-emptydir-data
</code></pre></div><p>输出类似于：</p>
<pre><code>kubernetes-node-ixsl&quot; cordoned
WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-ixsl, kube-proxy-kubernetes-node-ixsl; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-voc74
pod &quot;zk-1&quot; deleted
node &quot;kubernetes-node-ixsl&quot; drained
</code></pre><!--
The `zk-1` Pod cannot be scheduled because the `zk` `StatefulSet` contains a `PodAntiAffinity` rule preventing
co-location of the Pods, and as only two nodes are schedulable, the Pod will remain in a Pending state.

The output is similar to this:
-->
<p><code>zk-1</code> Pod 不能被调度，这是因为 <code>zk</code> <code>StatefulSet</code> 包含了一个防止 Pods
共存的 <code>PodAntiAffinity</code> 规则，而且只有两个节点可用于调度，
这个 Pod 将保持在 Pending 状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><p>输出类似于：</p>
<pre><code>NAME      READY     STATUS              RESTARTS   AGE
zk-0      1/1       Running             2          1h
zk-1      1/1       Running             0          1h
zk-2      1/1       Running             0          1h
NAME      READY     STATUS              RESTARTS   AGE
zk-0      1/1       Terminating         2          2h
zk-0      0/1       Terminating         2          2h
zk-0      0/1       Terminating         2          2h
zk-0      0/1       Terminating         2          2h
zk-0      0/1       Pending             0          0s
zk-0      0/1       Pending             0          0s
zk-0      0/1       ContainerCreating   0          0s
zk-0      0/1       Running             0          51s
zk-0      1/1       Running             0          1m
zk-1      1/1       Terminating         0          2h
zk-1      0/1       Terminating         0          2h
zk-1      0/1       Terminating         0          2h
zk-1      0/1       Terminating         0          2h
zk-1      0/1       Pending             0          0s
zk-1      0/1       Pending             0          0s
</code></pre><!--
Continue to watch the Pods of the StatefulSet, and drain the node on which
`zk-2` is scheduled.

The output is similar to this:
-->
<p>继续观察 StatefulSet 中的 Pods 并腾空 <code>zk-2</code> 调度所在的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain <span style="color:#a2f;font-weight:bold">$(</span>kubectl get pod zk-2 --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span><span style="color:#a2f;font-weight:bold">)</span> --ignore-daemonsets --force --delete-emptydir-data
</code></pre></div><p>输出类似于：</p>
<pre><code>node &quot;kubernetes-node-i4c4&quot; cordoned

WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-i4c4, kube-proxy-kubernetes-node-i4c4; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog
WARNING: Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog; Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-i4c4, kube-proxy-kubernetes-node-i4c4
There are pending pods when an error occurred: Cannot evict pod as it would violate the pod's disruption budget.
pod/zk-2
</code></pre><!--
Use `CTRL-C` to terminate to kubectl.

You cannot drain the third node because evicting `zk-2` would violate `zk-budget`. However, the node will remain cordoned.

Use `zkCli.sh` to retrieve the value you entered during the sanity test from `zk-0`.
-->
<p>使用 <code>CTRL-C</code> 终止 kubectl。</p>
<p>你不能腾空第三个节点，因为驱逐 <code>zk-2</code> 将和 <code>zk-budget</code> 冲突。
然而这个节点仍然处于隔离状态（Cordoned）。</p>
<p>使用 <code>zkCli.sh</code> 从 <code>zk-0</code> 取回你的健康检查中输入的数值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> zk-0 zkCli.sh get /hello
</code></pre></div><!--
The service is still available because its `PodDisruptionBudget` is respected.
-->
<p>由于遵守了 <code>PodDisruptionBudget</code>，服务仍然可用。</p>
<pre><code>WatchedEvent state:SyncConnected type:None path:null
world
cZxid = 0x200000002
ctime = Wed Dec 07 00:08:59 UTC 2016
mZxid = 0x200000002
mtime = Wed Dec 07 00:08:59 UTC 2016
pZxid = 0x200000002
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 5
numChildren = 0
</code></pre><!--
Use [`kubectl uncordon`](/docs/reference/generated/kubectl/kubectl-commands/#uncordon) to uncordon the first node.

The output is similar to this:
-->
<p>使用 <a href="/docs/reference/generated/kubectl/kubectl-commands/#uncordon"><code>kubectl uncordon</code></a>
来取消对第一个节点的隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl uncordon kubernetes-node-pb41
</code></pre></div><p>输出类似于：</p>
<pre><code>node &quot;kubernetes-node-pb41&quot; uncordoned
</code></pre><!--
`zk-1` is rescheduled on this node. Wait until `zk-1` is Running and Ready.

The output is similar to this:
-->
<p><code>zk-1</code> 被重新调度到了这个节点。等待 <code>zk-1</code> 变为 Running 和 Ready 状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>zk
</code></pre></div><p>输出类似于：</p>
<pre><code>NAME      READY     STATUS             RESTARTS  AGE
zk-0      1/1       Running            2         1h
zk-1      1/1       Running            0         1h
zk-2      1/1       Running            0         1h
NAME      READY     STATUS             RESTARTS  AGE
zk-0      1/1       Terminating        2         2h
zk-0      0/1       Terminating        2         2h
zk-0      0/1       Terminating        2         2h
zk-0      0/1       Terminating        2         2h
zk-0      0/1       Pending            0         0s
zk-0      0/1       Pending            0         0s
zk-0      0/1       ContainerCreating  0         0s
zk-0      0/1       Running            0         51s
zk-0      1/1       Running            0         1m
zk-1      1/1       Terminating        0         2h
zk-1      0/1       Terminating        0         2h
zk-1      0/1       Terminating        0         2h
zk-1      0/1       Terminating        0         2h
zk-1      0/1       Pending            0         0s
zk-1      0/1       Pending            0         0s
zk-1      0/1       Pending            0         12m
zk-1      0/1       ContainerCreating  0         12m
zk-1      0/1       Running            0         13m
zk-1      1/1       Running            0         13m
</code></pre><!--
Attempt to drain the node on which `zk-2` is scheduled.
-->
<p>尝试腾空 <code>zk-2</code> 调度所在的节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain <span style="color:#a2f;font-weight:bold">$(</span>kubectl get pod zk-2 --template <span style="color:#666">{{</span>.spec.nodeName<span style="color:#666">}}</span><span style="color:#a2f;font-weight:bold">)</span> --ignore-daemonsets --force --delete-emptydir-data
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>node &quot;kubernetes-node-i4c4&quot; already cordoned
WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-i4c4, kube-proxy-kubernetes-node-i4c4; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog
pod &quot;heapster-v1.2.0-2604621511-wht1r&quot; deleted
pod &quot;zk-2&quot; deleted
node &quot;kubernetes-node-i4c4&quot; drained
</code></pre><!--
This time `kubectl drain` succeeds.

Uncordon the second node to allow `zk-2` to be rescheduled.

The output is similar to this:
-->
<p>这次 <code>kubectl drain</code> 执行成功。</p>
<p>取消第二个节点的隔离，以允许 <code>zk-2</code> 被重新调度。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl uncordon kubernetes-node-ixsl
</code></pre></div><p>输出类似于：</p>
<pre><code>node &quot;kubernetes-node-ixsl&quot; uncordoned
</code></pre><!--
You can use `kubectl drain` in conjunction with `PodDisruptionBudgets` to ensure that your services remain available during maintenance.
If drain is used to cordon nodes and evict pods prior to taking the node offline for maintenance,
services that express a disruption budget will have that budget respected.
You should always allocate additional capacity for critical services so that their Pods can be immediately rescheduled.
-->
<p>你可以同时使用 <code>kubectl drain</code> 和 <code>PodDisruptionBudgets</code> 来保证你的服务
在维护过程中仍然可用。如果使用了腾空操作来隔离节点并在节点离线之前驱逐了 pods，
那么设置了干扰预算的服务将会遵守该预算。
你应该总是为关键服务分配额外容量，这样它们的 Pods 就能够迅速的重新调度。</p>
<h2 id="cleaning-up">Cleaning up</h2>
<!--
- Use `kubectl uncordon` to uncordon all the nodes in your cluster.
- You must delete the persistent storage media for the PersistentVolumes used in this tutorial.
  Follow the necessary steps, based on your environment, storage configuration,
  and provisioning method, to ensure that all storage is reclaimed.
-->
<ul>
<li>使用 <code>kubectl uncordon</code> 解除你集群中所有节点的隔离。</li>
<li>你需要删除在本教程中使用的 PersistentVolumes 的持久存储媒介。
请遵循必须的步骤，基于你的环境、存储配置和制备方法，保证回收所有的存储。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-42e39658021b706bcc9478c8cc73c4a3">4 - StatefulSet 基础</h1>
    
	<!-- overview -->
<!--
This tutorial provides an introduction to managing applications with
[StatefulSets](/docs/concepts/workloads/controllers/statefulset/). It
demonstrates how to create, delete, scale, and update the Pods of StatefulSets.
-->
<p>本教程介绍了如何使用 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a> 来管理应用。
演示了如何创建、删除、扩容/缩容和更新 StatefulSets 的 Pods。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
Before you begin this tutorial, you should familiarize yourself with the
following Kubernetes concepts.
-->
<p>在开始本教程之前，你应该熟悉以下 Kubernetes 的概念：</p>
<ul>
<li><a href="/zh/docs/concepts/workloads/pods/">Pods</a></li>
<li><a href="/zh/docs/concepts/services-networking/dns-pod-service/">Cluster DNS</a></li>
<li><a href="/zh/docs/concepts/services-networking/service/#headless-services">Headless Services</a></li>
<li><a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/">PersistentVolume Provisioning</a></li>
<li><a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a></li>
<li><a href="/zh/docs/user-guide/kubectl/">kubectl CLI</a></li>
</ul>
<!--
This tutorial assumes that your cluster is configured to dynamically provision
PersistentVolumes. If your cluster is not configured to do so, you
will have to manually provision two 1 GiB volumes prior to starting this
tutorial.
-->
<p>本教程假设你的集群被配置为动态的提供 PersistentVolumes。如果没有这样配置，在开始本教程之前，你需要手动准备 2 个 1 GiB 的存储卷。</p>
<h2 id="objectives">Objectives</h2>
<!--
StatefulSets are intended to be used with stateful applications and distributed
systems. However, the administration of stateful applications and
distributed systems on Kubernetes is a broad, complex topic. In order to
demonstrate the basic features of a StatefulSet, and not to conflate the former
topic with the latter, you will deploy a simple web application using a StatefulSet.

After this tutorial, you will be familiar with the following.

* How to create a StatefulSet
* How a StatefulSet manages its Pods
* How to delete a StatefulSet
* How to scale a StatefulSet
* How to update a StatefulSet's Pods
-->
<p>StatefulSets 旨在与有状态的应用及分布式系统一起使用。然而在 Kubernetes 上管理有状态应用和分布式系统是一个宽泛而复杂的话题。
为了演示 StatefulSet 的基本特性，并且不使前后的主题混淆，你将会使用 StatefulSet 部署一个简单的 web 应用。</p>
<p>在阅读本教程后，你将熟悉以下内容：</p>
<ul>
<li>如何创建 StatefulSet</li>
<li>StatefulSet 怎样管理它的 Pods</li>
<li>如何删除 StatefulSet</li>
<li>如何对 StatefulSet 进行扩容/缩容</li>
<li>如何更新一个 StatefulSet 的 Pods</li>
</ul>
<!-- lessoncontent -->
<!--
## Creating a StatefulSet

Begin by creating a StatefulSet using the example below. It is similar to the
example presented in the
[StatefulSets](/docs/concepts/workloads/controllers/statefulset/) concept.
It creates a [headless Service](/docs/concepts/services-networking/service/#headless-services),
`nginx`, to publish the IP addresses of Pods in the StatefulSet, `web`.
-->
<h2 id="创建-statefulset">创建 StatefulSet</h2>
<p>作为开始，使用如下示例创建一个 StatefulSet。它和 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a> 概念中的示例相似。
它创建了一个 <a href="/zh/docs/concepts/services-networking/service/#headless-services">Headless Service</a> <code>nginx</code> 用来发布 StatefulSet <code>web</code> 中的 Pod 的 IP 地址。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/web/web.yaml" download="application/web/web.yaml"><code>application/web/web.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-web-web-yaml')" title="Copy application/web/web.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-web-web-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;nginx&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/nginx-slim:0.8<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Download the example above, and save it to a file named `web.yaml`

You will need to use two terminal windows. In the first terminal, use
[`kubectl get`](/docs/reference/generated/kubectl/kubectl-commands/#get) to watch the creation
of the StatefulSet's Pods.
-->
<p>下载上面的例子并保存为文件 <code>web.yaml</code>。</p>
<p>你需要使用两个终端窗口。 在第一个终端中，使用 <a href="/zh/docs/user-guide/kubectl/v1.23/#get"><code>kubectl get</code></a>  来查看 StatefulSet 的 Pods 的创建情况。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In the second terminal, use
[`kubectl apply`](/docs/reference/generated/kubectl/kubectl-commands/#apply) to create the
Headless Service and StatefulSet defined in `web.yaml`.
-->
<p>在另一个终端中，使用 <a href="/zh/docs/reference/generated/kubectl/kubectl-commands/#apply"><code>kubectl apply</code></a>来创建定义在 <code>web.yaml</code> 中的 Headless Service 和 StatefulSet。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f web.yaml
</code></pre></div><pre><code>service/nginx created
statefulset.apps/web created
</code></pre><!--
The command above creates two Pods, each running an
[NGINX](https://www.nginx.com) webserver. Get the `nginx` Service and the
`web` StatefulSet to verify that they were created successfully.
-->
<p>上面的命令创建了两个 Pod，每个都运行了一个 <a href="https://www.nginx.com">NGINX</a> web 服务器。
获取 <code>nginx</code> Service 和 <code>web</code> StatefulSet 来验证是否成功的创建了它们。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service nginx
</code></pre></div><pre><code>NAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     ClusterIP    None         &lt;none&gt;        80/TCP    12s
</code></pre><!--
...then get the `web` StatefulSet, to verify that both were created successfully:
-->
<p>...然后获取 <code>web</code> StatefulSet，以验证两者均已成功创建：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get statefulset web
</code></pre></div><pre><code>NAME      DESIRED   CURRENT   AGE
web       2         1         20s
</code></pre><!--

### Ordered Pod Creation

For a StatefulSet with N replicas, when Pods are being deployed, they are
created sequentially, in order from {0..N-1}. Examine the output of the
`kubectl get` command in the first terminal. Eventually, the output will
look like the example below.
-->
<h3 id="顺序创建-pod">顺序创建 Pod</h3>
<p>对于一个拥有 N 个副本的 StatefulSet，Pod 被部署时是按照 {0 …… N-1} 的序号顺序创建的。
在第一个终端中使用 <code>kubectl get</code> 检查输出。这个输出最终将看起来像下面的样子。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         19s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         18s
</code></pre><!--
Notice that the `web-1` Pod is not launched until the `web-0` Pod is
[Running and Ready](/docs/user-guide/pod-states).
-->
<p>请注意在 <code>web-0</code> Pod 处于 <a href="/zh/docs/user-guide/pod-states">Running和Ready</a> 状态后 <code>web-1</code> Pod 才会被启动。</p>
<!--
## Pods in a StatefulSet


Pods in a StatefulSet have a unique ordinal index and a stable network identity.

### Examining the Pod's Ordinal Index

Get the StatefulSet's Pods.
-->
<h2 id="statefulset-中的-pod">StatefulSet 中的 Pod</h2>
<p>StatefulSet 中的 Pod 拥有一个唯一的顺序索引和稳定的网络身份标识。</p>
<h3 id="检查-pod-的顺序索引">检查 Pod 的顺序索引</h3>
<p>获取 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          1m
web-1     1/1       Running   0          1m
</code></pre><!--
As mentioned in the [StatefulSets](/docs/concepts/workloads/controllers/statefulset/)
concept, the Pods in a StatefulSet have a sticky, unique identity. This identity
is based on a unique ordinal index that is assigned to each Pod by the
StatefulSet controller. The Pods' names take the form
`<statefulset name>-<ordinal index>`. Since the `web` StatefulSet has two
replicas, it creates two Pods, `web-0` and `web-1`.

### Using Stable Network Identities

Each Pod has a stable hostname based on its ordinal index. Use
[`kubectl exec`](/docs/reference/generated/kubectl/kubectl-commands/#exec) to execute the
`hostname` command in each Pod.
-->
<p>如同 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a> 概念中所提到的，
StatefulSet 中的 Pod 拥有一个具有黏性的、独一无二的身份标志。
这个标志基于 StatefulSet 控制器分配给每个 Pod 的唯一顺序索引。
Pod 的名称的形式为<code>&lt;statefulset name&gt;-&lt;ordinal index&gt;</code>。
<code>web</code>StatefulSet 拥有两个副本，所以它创建了两个 Pod：<code>web-0</code>和<code>web-1</code>。</p>
<h3 id="使用稳定的网络身份标识">使用稳定的网络身份标识</h3>
<p>每个 Pod 都拥有一个基于其顺序索引的稳定的主机名。使用<a href="/zh/docs/reference/generated/kubectl/kubectl-commands/#exec"><code>kubectl exec</code></a>在每个 Pod 中执行<code>hostname</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span> -- sh -c <span style="color:#b44">&#39;hostname&#39;</span>; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>web-0
web-1
</code></pre><!--
Use [`kubectl run`](/docs/reference/generated/kubectl/kubectl-commands/#run) to execute
a container that provides the `nslookup` command from the `dnsutils` package.
Using `nslookup` on the Pods' hostnames, you can examine their in-cluster DNS
addresses.
-->
<p>使用 <a href="/zh/docs/reference/generated/kubectl/kubectl-commands/#run"><code>kubectl run</code></a>
运行一个提供 <code>nslookup</code> 命令的容器，该命令来自于 <code>dnsutils</code> 包。
通过对 Pod 的主机名执行 <code>nslookup</code>，你可以检查他们在集群内部的 DNS 地址。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run -i --tty --image busybox:1.28 dns-test --restart<span style="color:#666">=</span>Never --rm
</code></pre></div><!--
which starts a new shell. In that new shell, run:
-->
<p>这将启动一个新的 shell。在新 shell 中，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># Run this in the dns-test container shell</span>
nslookup web-0.nginx
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.6

nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.6
</code></pre><!--
The CNAME of the headless service points to SRV records (one for each Pod that
is Running and Ready). The SRV records point to A record entries that
contain the Pods' IP addresses.

In one terminal, watch the StatefulSet's Pods.
-->
<p>headless service 的 CNAME 指向 SRV 记录（记录每个 Running 和 Ready 状态的 Pod）。
SRV 记录指向一个包含 Pod IP 地址的记录表项。</p>
<p>在一个终端中查看 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In a second terminal, use
[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands/#delete) to delete all
the Pods in the StatefulSet.
-->
<p>在另一个终端中使用 <a href="/zh/docs/reference/generated/kubectl/kubectl-commands/#delete"><code>kubectl delete</code></a> 删除 StatefulSet 中所有的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>pod &quot;web-0&quot; deleted
pod &quot;web-1&quot; deleted
</code></pre><!--
Wait for the StatefulSet to restart them, and for both Pods to transition to
Running and Ready.
-->
<p>等待 StatefulSet 重启它们，并且两个 Pod 都变成 Running 和 Ready 状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         34s
</code></pre><!--
Use `kubectl exec` and `kubectl run` to view the Pods hostnames and in-cluster
DNS entries.
-->
<p>使用 <code>kubectl exec</code> 和 <code>kubectl run</code> 查看 Pod 的主机名和集群内部的 DNS 表项。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> web-<span style="color:#b8860b">$i</span> -- sh -c <span style="color:#b44">&#39;hostname&#39;</span>; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>web-0
web-1
</code></pre><!--
then, run:
-->
<p>然后，运行：</p>
<pre><code>kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh
</code></pre><!--
which starts a new shell.  
In that new shell, run:
-->
<p>这将启动一个新的 shell。在新 shell 中，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># Run this in the dns-test container shell</span>
nslookup web-0.nginx
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.7

nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.8
</code></pre><!--
The Pods' ordinals, hostnames, SRV records, and A record names have not changed,
but the IP addresses associated with the Pods may have changed. In the cluster
used for this tutorial, they have. This is why it is important not to configure
other applications to connect to Pods in a StatefulSet by IP address.


If you need to find and connect to the active members of a StatefulSet, you
should query the CNAME of the Headless Service
(`nginx.default.svc.cluster.local`). The SRV records associated with the
CNAME will contain only the Pods in the StatefulSet that are Running and
Ready.

If your application already implements connection logic that tests for
liveness and readiness, you can use the SRV records of the Pods (
`web-0.nginx.default.svc.cluster.local`,
`web-1.nginx.default.svc.cluster.local`), as they are stable, and your
application will be able to discover the Pods' addresses when they transition
to Running and Ready.
-->
<p>Pod 的序号、主机名、SRV 条目和记录名称没有改变，但和 Pod 相关联的 IP 地址可能发生了改变。
在本教程中使用的集群中它们就改变了。这就是为什么不要在其他应用中使用 StatefulSet 中的 Pod 的 IP 地址进行连接，这点很重要。</p>
<p>如果你需要查找并连接一个 StatefulSet 的活动成员，你应该查询 Headless Service 的 CNAME。
和 CNAME 相关联的 SRV 记录只会包含 StatefulSet 中处于 Running 和 Ready 状态的 Pod。</p>
<p>如果你的应用已经实现了用于测试 liveness 和 readiness 的连接逻辑，你可以使用 Pod 的 SRV 记录（<code>web-0.nginx.default.svc.cluster.local</code>，
<code>web-1.nginx.default.svc.cluster.local</code>）。因为他们是稳定的，并且当你的 Pod 的状态变为 Running 和 Ready 时，你的应用就能够发现它们的地址。</p>
<!--
### Writing to Stable Storage

Get the PersistentVolumeClaims for `web-0` and `web-1`.
-->
<h3 id="写入稳定的存储">写入稳定的存储</h3>
<p>获取 <code>web-0</code> 和 <code>web-1</code> 的 PersistentVolumeClaims。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pvc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s
</code></pre><!--
The StatefulSet controller created two PersistentVolumeClaims that are
bound to two [PersistentVolumes](/docs/concepts/storage/persistent-volumes/). As the cluster used in this tutorial is configured to dynamically provision
PersistentVolumes, the PersistentVolumes were created and bound automatically.

The NGINX webservers, by default, will serve an index file at
`/usr/share/nginx/html/index.html`. The `volumeMounts` field in the
StatefulSets `spec` ensures that the `/usr/share/nginx/html` directory is
backed by a PersistentVolume.

Write the Pods' hostnames to their `index.html` files and verify that the NGINX
webservers serve the hostnames.
-->
<p>StatefulSet 控制器创建了两个 PersistentVolumeClaims，绑定到两个 <a href="/zh/docs/concepts/storage/volumes/">PersistentVolumes</a>。由于本教程使用的集群配置为动态提供 PersistentVolume，所有的 PersistentVolume 都是自动创建和绑定的。</p>
<p>NGINX web 服务器默认会加载位于 <code>/usr/share/nginx/html/index.html</code> 的 index 文件。
StatefulSets <code>spec</code> 中的 <code>volumeMounts</code> 字段保证了 <code>/usr/share/nginx/html</code> 文件夹由一个 PersistentVolume 支持。</p>
<p>将 Pod 的主机名写入它们的<code>index.html</code>文件并验证 NGINX web 服务器使用该主机名提供服务。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span> -- sh -c <span style="color:#b44">&#39;echo &#34;$(hostname)&#34; &gt; /usr/share/nginx/html/index.html&#39;</span>; <span style="color:#a2f;font-weight:bold">done</span>

<span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> -i -t <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span> -- curl http://localhost/; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>web-0
web-1
</code></pre><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you instead see **403 Forbidden** responses for the above curl command,
you will need to fix the permissions of the directory mounted by the `volumeMounts`
(due to a [bug when using hostPath volumes](https://github.com/kubernetes/kubernetes/issues/2630)),
by running:
-->
<p>请注意，如果你看见上面的 curl 命令返回了 <strong>403 Forbidden</strong> 的响应，你需要像这样修复使用 <code>volumeMounts</code>
（原因归咎于<a href="https://github.com/kubernetes/kubernetes/issues/2630">使用 hostPath 卷时存在的缺陷</a>）
挂载的目录的权限
运行：</p>
<p><code>for i in 0 1; do kubectl exec web-$i -- chmod 755 /usr/share/nginx/html; done</code></p>
<!--
before retrying the `curl` command above.
-->
<p>在你重新尝试上面的 <code>curl</code> 命令之前。</p>

</div>
<!--
In one terminal, watch the StatefulSet's Pods.
-->
<p>在一个终端查看 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In a second terminal, delete all of the StatefulSet's Pods.
-->
<p>在另一个终端删除 StatefulSet 所有的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>pod &quot;web-0&quot; deleted
pod &quot;web-1&quot; deleted
</code></pre><!--
Examine the output of the `kubectl get` command in the first terminal, and wait
for all of the Pods to transition to Running and Ready.
-->
<p>在第一个终端里检查 <code>kubectl get</code> 命令的输出，等待所有 Pod 变成 Running 和 Ready 状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         34s
</code></pre><!--
Verify the web servers continue to serve their hostnames.
-->
<p>验证所有 web 服务器在继续使用它们的主机名提供服务。</p>
<pre><code>for i in 0 1; do kubectl exec -i -t &quot;web-$i&quot; -- curl http://localhost/; done
</code></pre><pre><code>web-0
web-1
</code></pre><!--
Even though `web-0` and `web-1` were rescheduled, they continue to serve their
hostnames because the PersistentVolumes associated with their
PersistentVolumeClaims are remounted to their `volumeMounts`. No matter what
node `web-0`and `web-1` are scheduled on, their PersistentVolumes will be
mounted to the appropriate mount points.

## Scaling a StatefulSet
Scaling a StatefulSet refers to increasing or decreasing the number of replicas.
This is accomplished by updating the `replicas` field. You can use either
[`kubectl scale`](/docs/reference/generated/kubectl/kubectl-commands/#scale) or
[`kubectl patch`](/docs/reference/generated/kubectl/kubectl-commands/#patch) to scale a StatefulSet.

### Scaling Up

In one terminal window, watch the Pods in the StatefulSet.
-->
<p>虽然 <code>web-0</code> 和 <code>web-1</code> 被重新调度了，但它们仍然继续监听各自的主机名，因为和它们的 PersistentVolumeClaim 相关联的 PersistentVolume 被重新挂载到了各自的 <code>volumeMount</code> 上。
不管 <code>web-0</code> 和 <code>web-1</code> 被调度到了哪个节点上，它们的 PersistentVolumes 将会被挂载到合适的挂载点上。</p>
<h2 id="扩容-缩容-statefulset">扩容/缩容 StatefulSet</h2>
<p>扩容/缩容 StatefulSet 指增加或减少它的副本数。这通过更新 <code>replicas</code> 字段完成。
你可以使用<a href="/zh/docs/user-guide/kubectl/v1.23/#scale"><code>kubectl scale</code></a>
或者<a href="/zh/docs/user-guide/kubectl/v1.23/#patch"><code>kubectl patch</code></a>来扩容/缩容一个 StatefulSet。</p>
<h3 id="扩容">扩容</h3>
<p>在一个终端窗口观察 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In another terminal window, use `kubectl scale` to scale the number of replicas
to 5.-->
<p>在另一个终端窗口使用 <code>kubectl scale</code> 扩展副本数为 5。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale sts web --replicas<span style="color:#666">=</span><span style="color:#666">5</span>
</code></pre></div><pre><code>statefulset.apps/web scaled
</code></pre><!--
Examine the output of the `kubectl get` command in the first terminal, and wait
for the three additional Pods to transition to Running and Ready.
-->
<p>在第一个 终端中检查 <code>kubectl get</code> 命令的输出，等待增加的 3 个 Pod 的状态变为 Running 和 Ready。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2h
web-1     1/1       Running   0          2h
NAME      READY     STATUS    RESTARTS   AGE
web-2     0/1       Pending   0          0s
web-2     0/1       Pending   0         0s
web-2     0/1       ContainerCreating   0         0s
web-2     1/1       Running   0         19s
web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         0s
web-3     0/1       ContainerCreating   0         0s
web-3     1/1       Running   0         18s
web-4     0/1       Pending   0         0s
web-4     0/1       Pending   0         0s
web-4     0/1       ContainerCreating   0         0s
web-4     1/1       Running   0         19s
</code></pre><!--
The StatefulSet controller scaled the number of replicas. As with
[StatefulSet creation](#ordered-pod-creation), the StatefulSet controller
created each Pod sequentially with respect to its ordinal index, and it
waited for each Pod's predecessor to be Running and Ready before launching the
subsequent Pod.

### Scaling Down

In one terminal, watch the StatefulSet's Pods.
-->
<p>StatefulSet 控制器扩展了副本的数量。
如同<a href="#%E9%A1%BA%E5%BA%8F%E5%88%9B%E5%BB%BApod">创建 StatefulSet</a> 所述，StatefulSet 按序号索引顺序的创建每个 Pod，并且会等待前一个 Pod 变为 Running 和 Ready 才会启动下一个 Pod。</p>
<h3 id="缩容">缩容</h3>
<p>在一个终端观察 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In another terminal, use `kubectl patch` to scale the StatefulSet back down to
three replicas.
-->
<p>在另一个终端使用 <code>kubectl patch</code> 将 StatefulSet 缩容回三个副本。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch sts web -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;replicas&#34;:3}}&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
Wait for `web-4` and `web-3` to transition to Terminating.
-->
<p>等待 <code>web-4</code> 和 <code>web-3</code> 状态变为 Terminating。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          3h
web-1     1/1       Running             0          3h
web-2     1/1       Running             0          55s
web-3     1/1       Running             0          36s
web-4     0/1       ContainerCreating   0          18s
NAME      READY     STATUS    RESTARTS   AGE
web-4     1/1       Running   0          19s
web-4     1/1       Terminating   0         24s
web-4     1/1       Terminating   0         24s
web-3     1/1       Terminating   0         42s
web-3     1/1       Terminating   0         42s
</code></pre><!--
### Ordered Pod Termination

The controller deleted one Pod at a time, in reverse order with respect to its
ordinal index, and it waited for each to be completely shutdown before
deleting the next.

Get the StatefulSet's PersistentVolumeClaims.
-->
<h3 id="顺序终止-pod">顺序终止 Pod</h3>
<p>控制器会按照与 Pod 序号索引相反的顺序每次删除一个 Pod。在删除下一个 Pod 前会等待上一个被完全关闭。</p>
<p>获取 StatefulSet 的 PersistentVolumeClaims。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pvc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-2   Bound     pvc-e1125b27-b508-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-3   Bound     pvc-e1176df6-b508-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-4   Bound     pvc-e11bb5f8-b508-11e6-932f-42010a800002   1Gi        RWO           13h

</code></pre><!--
There are still five PersistentVolumeClaims and five PersistentVolumes.
When exploring a Pod's [stable storage](#writing-to-stable-storage), we saw that the PersistentVolumes mounted to the Pods of a StatefulSet are not deleted when the StatefulSet's Pods are deleted. This is still true when Pod deletion is caused by scaling the StatefulSet down.

## Updating StatefulSets

In Kubernetes 1.7 and later, the StatefulSet controller supports automated updates.  The
strategy used is determined by the `spec.updateStrategy` field of the
StatefulSet API Object. This feature can be used to upgrade the container
images, resource requests and/or limits, labels, and annotations of the Pods in a
StatefulSet. There are two valid update strategies, `RollingUpdate` and
`OnDelete`.

`RollingUpdate` update strategy is the default for StatefulSets.
-->
<p>五个 PersistentVolumeClaims 和五个 PersistentVolumes 仍然存在。
查看 Pod 的 <a href="#stable-storage">稳定存储</a>，我们发现当删除 StatefulSet 的 Pod 时，挂载到 StatefulSet 的 Pod 的 PersistentVolumes 不会被删除。
当这种删除行为是由 StatefulSet 缩容引起时也是一样的。</p>
<h2 id="更新-statefulset">更新 StatefulSet</h2>
<p>Kubernetes 1.7 版本的 StatefulSet 控制器支持自动更新。
更新策略由 StatefulSet API Object 的<code>spec.updateStrategy</code> 字段决定。这个特性能够用来更新一个 StatefulSet 中的 Pod 的 container images，resource requests，以及 limits，labels 和 annotations。
<code>RollingUpdate</code>滚动更新是 StatefulSets 默认策略。</p>
<!--
The `RollingUpdate` update strategy will update all Pods in a StatefulSet, in
reverse ordinal order, while respecting the StatefulSet guarantees.

Patch the `web` StatefulSet to apply the `RollingUpdate` update strategy.
-->
<h3 id="rolling-update-策略">Rolling Update 策略</h3>
<p><code>RollingUpdate</code> 更新策略会更新一个 StatefulSet 中所有的 Pod，采用与序号索引相反的顺序并遵循 StatefulSet 的保证。</p>
<p>Patch <code>web</code> StatefulSet 来执行 <code>RollingUpdate</code> 更新策略。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;updateStrategy&#34;:{&#34;type&#34;:&#34;RollingUpdate&#34;}}}&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
In one terminal window, patch the `web` StatefulSet to change the container
image again.
-->
<p>在一个终端窗口中 patch <code>web</code> StatefulSet 来再次的改变容器镜像。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web --type<span style="color:#666">=</span><span style="color:#b44">&#39;json&#39;</span> -p<span style="color:#666">=</span><span style="color:#b44">&#39;[{&#34;op&#34;: &#34;replace&#34;, &#34;path&#34;: &#34;/spec/template/spec/containers/0/image&#34;, &#34;value&#34;:&#34;gcr.io/google_containers/nginx-slim:0.8&#34;}]&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
In another terminal, watch the Pods in the StatefulSet.
-->
<p>在另一个终端监控 StatefulSet 中的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get po -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          7m
web-1     1/1       Running   0          7m
web-2     1/1       Running   0          8m
web-2     1/1       Terminating   0         8m
web-2     1/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Pending   0         0s
web-2     0/1       Pending   0         0s
web-2     0/1       ContainerCreating   0         0s
web-2     1/1       Running   0         19s
web-1     1/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         6s
web-0     1/1       Terminating   0         7m
web-0     1/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Pending   0         0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         10s
</code></pre><!--
The Pods in the StatefulSet are updated in reverse ordinal order. The
StatefulSet controller terminates each Pod, and waits for it to transition to Running and
Ready prior to updating the next Pod. Note that, even though the StatefulSet
controller will not proceed to update the next Pod until its ordinal successor
is Running and Ready, it will restore any Pod that fails during the update to
its current version. Pods that have already received the update will be
restored to the updated version, and Pods that have not yet received the
update will be restored to the previous version. In this way, the controller
attempts to continue to keep the application healthy and the update consistent
in the presence of intermittent failures.

Get the Pods to view their container images.
-->
<p>StatefulSet 里的 Pod 采用和序号相反的顺序更新。在更新下一个 Pod 前，StatefulSet 控制器终止每个 Pod 并等待它们变成 Running 和 Ready。
请注意，虽然在顺序后继者变成 Running 和 Ready 之前 StatefulSet 控制器不会更新下一个 Pod，但它仍然会重建任何在更新过程中发生故障的 Pod，使用的是它们当前的版本。
已经接收到更新请求的 Pod 将会被恢复为更新的版本，没有收到请求的 Pod 则会被恢复为之前的版本。
像这样，控制器尝试继续使应用保持健康并在出现间歇性故障时保持更新的一致性。</p>
<p>获取 Pod 来查看他们的容器镜像。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> p in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl get pod <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$p</span><span style="color:#b44">&#34;</span> --template <span style="color:#b44">&#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}&#39;</span>; echo; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>k8s.gcr.io/nginx-slim:0.8
k8s.gcr.io/nginx-slim:0.8
k8s.gcr.io/nginx-slim:0.8

</code></pre><!--
All the Pods in the StatefulSet are now running the previous container image.

**Tip** You can also use `kubectl rollout status sts/<name>` to view
the status of a rolling update.

#### Staging an Update
You can stage an update to a StatefulSet by using the `partition` parameter of
the `RollingUpdate` update strategy. A staged update will keep all of the Pods
in the StatefulSet at the current version while allowing mutations to the
StatefulSet's `.spec.template`.

Patch the `web` StatefulSet to add a partition to the `updateStrategy` field.
-->
<p>StatefulSet 中的所有 Pod 现在都在运行之前的容器镜像。</p>
<p><strong>小窍门</strong>：你还可以使用 <code>kubectl rollout status sts/&lt;name&gt;</code> 来查看 rolling update 的状态。</p>
<h4 id="分段更新">分段更新</h4>
<p>你可以使用 <code>RollingUpdate</code> 更新策略的 <code>partition</code> 参数来分段更新一个 StatefulSet。
分段的更新将会使 StatefulSet 中的其余所有 Pod 保持当前版本的同时仅允许改变 StatefulSet 的  <code>.spec.template</code>。</p>
<p>Patch <code>web</code> StatefulSet 来对 <code>updateStrategy</code> 字段添加一个分区。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;updateStrategy&#34;:{&#34;type&#34;:&#34;RollingUpdate&#34;,&#34;rollingUpdate&#34;:{&#34;partition&#34;:3}}}}&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
Patch the StatefulSet again to change the container's image.
-->
<p>再次 Patch StatefulSet 来改变容器镜像。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web --type<span style="color:#666">=</span><span style="color:#b44">&#39;json&#39;</span> -p<span style="color:#666">=</span><span style="color:#b44">&#39;[{&#34;op&#34;: &#34;replace&#34;, &#34;path&#34;: &#34;/spec/template/spec/containers/0/image&#34;, &#34;value&#34;:&#34;k8s.gcr.io/nginx-slim:0.7&#34;}]&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
Delete a Pod in the StatefulSet.
-->
<p>删除 StatefulSet 中的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod web-2
</code></pre></div><pre><code>pod &quot;web-2&quot; deleted
</code></pre><!--
Wait for the Pod to be Running and Ready.
-->
<p>等待 Pod 变成 Running 和 Ready。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s
</code></pre><!--
Get the Pod's container.
-->
<p>获取 Pod 的容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod web-2 --template <span style="color:#b44">&#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}&#39;</span>
</code></pre></div><pre><code>k8s.gcr.io/nginx-slim:0.8
</code></pre><!--
Notice that, even though the update strategy is `RollingUpdate` the StatefulSet
controller restored the Pod with its original container. This is because the
ordinal of the Pod is less than the `partition` specified by the
`updateStrategy`.

#### Rolling Out a Canary
You can roll out a canary to test a modification by decrementing the `partition`
you specified [above](#staging-an-update).

Patch the StatefulSet to decrement the partition.
-->
<p>请注意，虽然更新策略是 <code>RollingUpdate</code>，StatefulSet 控制器还是会使用原始的容器恢复 Pod。
这是因为 Pod 的序号比 <code>updateStrategy</code> 指定的 <code>partition</code> 更小。</p>
<h4 id="灰度发布">灰度发布</h4>
<p>你可以通过减少 <a href="#%E5%88%86%E6%AE%B5%E6%9B%B4%E6%96%B0">上文</a>指定的 <code>partition</code> 来进行灰度发布，以此来测试你的程序的改动。</p>
<p>通过 patch 命令修改 StatefulSet 来减少分区。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;updateStrategy&#34;:{&#34;type&#34;:&#34;RollingUpdate&#34;,&#34;rollingUpdate&#34;:{&#34;partition&#34;:2}}}}&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
Wait for `web-2` to be Running and Ready.
-->
<p>等待 <code>web-2</code> 变成 Running 和 Ready。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s
</code></pre><!--
Get the Pod's container.
-->
<p>获取 Pod 的容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod web-2 --template <span style="color:#b44">&#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}&#39;</span>
</code></pre></div><pre><code>k8s.gcr.io/nginx-slim:0.7

</code></pre><!--
When you changed the `partition`, the StatefulSet controller automatically
updated the `web-2` Pod because the Pod's ordinal was greater than or equal to
the `partition`.

Delete the `web-1` Pod.
-->
<p>当你改变 <code>partition</code> 时，StatefulSet 会自动的更新 <code>web-2</code> Pod，这是因为 Pod 的序号大于或等于 <code>partition</code>。</p>
<p>删除 <code>web-1</code> Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod web-1
</code></pre></div><pre><code>pod &quot;web-1&quot; deleted
</code></pre><!--
Wait for the `web-1` Pod to be Running and Ready.
-->
<p>等待 <code>web-1</code> 变成 Running 和 Ready。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>NAME      READY     STATUS        RESTARTS   AGE
web-0     1/1       Running       0          6m
web-1     0/1       Terminating   0          6m
web-2     1/1       Running       0          2m
web-1     0/1       Terminating   0         6m
web-1     0/1       Terminating   0         6m
web-1     0/1       Terminating   0         6m
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         18s
</code></pre><!--
Get the `web-1` Pods container.
-->
<p>获取 <code>web-1</code> Pod 的容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod web-1 --template <span style="color:#b44">&#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}&#39;</span>
</code></pre></div><pre><code>k8s.gcr.io/nginx-slim:0.8
</code></pre><!--
`web-1` was restored to its original configuration because the Pod's ordinal
was less than the partition. When a partition is specified, all Pods with an
ordinal that is greater than or equal to the partition will be updated when the
StatefulSet's `.spec.template` is updated. If a Pod that has an ordinal less
than the partition is deleted or otherwise terminated, it will be restored to
its original configuration.

#### Phased Roll Outs
You can perform a phased roll out (e.g. a linear, geometric, or exponential
roll out) using a partitioned rolling update in a similar manner to how you
rolled out a [canary](#rolling-out-a-canary). To perform a phased roll out, set
the `partition` to the ordinal at which you want the controller to pause the
update.

The partition is currently set to `2`. Set the partition to `0`.
-->
<p><code>web-1</code> 被按照原来的配置恢复，因为 Pod 的序号小于分区。当指定了分区时，如果更新了 StatefulSet 的 <code>.spec.template</code>，则所有序号大于或等于分区的 Pod 都将被更新。
如果一个序号小于分区的 Pod 被删除或者终止，它将被按照原来的配置恢复。</p>
<h4 id="分阶段的发布">分阶段的发布</h4>
<p>你可以使用类似<a href="#%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83">灰度发布</a>的方法执行一次分阶段的发布（例如一次线性的、等比的或者指数形式的发布）。
要执行一次分阶段的发布，你需要设置 <code>partition</code> 为希望控制器暂停更新的序号。</p>
<p>分区当前为<code>2</code>。请将分区设置为<code>0</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulset web -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;updateStrategy&#34;:{&#34;type&#34;:&#34;RollingUpdate&#34;,&#34;rollingUpdate&#34;:{&#34;partition&#34;:0}}}}&#39;</span>
</code></pre></div><pre><code>statefulset.apps/web patched
</code></pre><!--
Wait for all of the Pods in the StatefulSet to become Running and Ready.
-->
<p>等待 StatefulSet 中的所有 Pod 变成 Running 和 Ready。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          3m
web-1     0/1       ContainerCreating   0          11s
web-2     1/1       Running             0          2m
web-1     1/1       Running   0         18s
web-0     1/1       Terminating   0         3m
web-0     1/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Pending   0         0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         3s
</code></pre><!--
Get the Pod's containers.
-->
<p>获取 Pod 的容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> p in <span style="color:#666">0</span> <span style="color:#666">1</span> 2; <span style="color:#a2f;font-weight:bold">do</span> kubectl get pod <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$p</span><span style="color:#b44">&#34;</span> --template <span style="color:#b44">&#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}&#39;</span>; echo; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>k8s.gcr.io/nginx-slim:0.7
k8s.gcr.io/nginx-slim:0.7
k8s.gcr.io/nginx-slim:0.7
</code></pre><!--
By moving the `partition` to `0`, you allowed the StatefulSet controller to
continue the update process.

### On Delete

The `OnDelete` update strategy implements the legacy (1.6 and prior) behavior,
When you select this update strategy, the StatefulSet controller will not
automatically update Pods when a modification is made to the StatefulSet's
`.spec.template` field. This strategy can be selected by setting the
`.spec.template.updateStrategy.type` to `OnDelete`.


## Deleting StatefulSets

StatefulSet supports both Non-Cascading and Cascading deletion. In a
Non-Cascading Delete, the StatefulSet's Pods are not deleted when the StatefulSet is deleted. In a Cascading Delete, both the StatefulSet and its Pods are
deleted.

### Non-Cascading Delete

In one terminal window, watch the Pods in the StatefulSet.
-->
<p>将 <code>partition</code> 改变为 <code>0</code> 以允许 StatefulSet 控制器继续更新过程。</p>
<h3 id="on-delete-策略">On Delete 策略</h3>
<p><code>OnDelete</code> 更新策略实现了传统（1.7 之前）行为，它也是默认的更新策略。
当你选择这个更新策略并修改 StatefulSet 的 <code>.spec.template</code> 字段时，StatefulSet 控制器将不会自动的更新 Pod。</p>
<h2 id="删除-statefulset">删除 StatefulSet</h2>
<p>StatefulSet 同时支持级联和非级联删除。使用非级联方式删除 StatefulSet 时，StatefulSet 的 Pod 不会被删除。使用级联删除时，StatefulSet 和它的 Pod 都会被删除。</p>
<h3 id="非级联删除">非级联删除</h3>
<p>在一个终端窗口查看 StatefulSet 中的 Pod。</p>
<pre><code>kubectl get pods -w -l app=nginx
</code></pre><!--
Use [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands/#delete) to delete the
StatefulSet. Make sure to supply the `--cascade=orphan` parameter to the
command. This parameter tells Kubernetes to only delete the StatefulSet, and to
not delete any of its Pods.
-->
<p>使用 <a href="/zh/docs/reference/generated/kubectl/kubectl-commands/#delete"><code>kubectl delete</code></a> 删除 StatefulSet。
请确保提供了 <code>--cascade=orphan</code> 参数给命令。这个参数告诉 Kubernetes 只删除 StatefulSet 而不要删除它的任何 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete statefulset web --cascade<span style="color:#666">=</span>orphan
</code></pre></div><pre><code>statefulset.apps &quot;web&quot; deleted
</code></pre><!--
Get the Pods to examine their status.
-->
<p>获取 Pod 来检查他们的状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          6m
web-1     1/1       Running   0          7m
web-2     1/1       Running   0          5m
</code></pre><!--
Even though `web` has been deleted, all of the Pods are still Running and Ready.
Delete `web-0`.
-->
<p>虽然 <code>web</code>  已经被删除了，但所有 Pod 仍然处于 Running 和 Ready 状态。
删除 <code>web-0</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod web-0
</code></pre></div><pre><code>pod &quot;web-0&quot; deleted
</code></pre><!--
Get the StatefulSet's Pods.
-->
<p>获取 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-1     1/1       Running   0          10m
web-2     1/1       Running   0          7m
</code></pre><!--
As the `web` StatefulSet has been deleted, `web-0` has not been relaunched.

In one terminal, watch the StatefulSet's Pods.
-->
<p>由于 <code>web</code> StatefulSet 已经被删除，<code>web-0</code>没有被重新启动。</p>
<p>在一个终端监控 StatefulSet 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In a second terminal, recreate the StatefulSet. Note that, unless
you deleted the `nginx` Service ( which you should not have ), you will see
an error indicating that the Service already exists.
-->
<p>在另一个终端里重新创建 StatefulSet。请注意，除非你删除了 <code>nginx</code> Service （你不应该这样做），你将会看到一个错误，提示 Service 已经存在。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f web.yaml
</code></pre></div><pre><code>statefulset.apps/web created
service/nginx unchanged
</code></pre><!--
Ignore the error. It only indicates that an attempt was made to create the nginx
Headless Service even though that Service already exists.

Examine the output of the `kubectl get` command running in the first terminal.
-->
<p>请忽略这个错误。它仅表示 kubernetes 进行了一次创建 nginx Headless Service 的尝试，尽管那个 Service 已经存在。</p>
<p>在第一个终端中运行并检查 <code>kubectl get</code> 命令的输出。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-1     1/1       Running   0          16m
web-2     1/1       Running   0          2m
NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         18s
web-2     1/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m
</code></pre><!--
When the `web` StatefulSet was recreated, it first relaunched `web-0`.
Since `web-1` was already Running and Ready, when `web-0` transitioned to
Running and Ready, it adopted this Pod. Since you recreated the StatefulSet
with `replicas` equal to 2, once `web-0` had been recreated, and once
`web-1` had been determined to already be Running and Ready, `web-2` was
terminated.

Let's take another look at the contents of the `index.html` file served by the
Pods' webservers:
-->
<p>当重新创建 <code>web</code> StatefulSet 时，<code>web-0</code> 被第一个重新启动。
由于 <code>web-1</code> 已经处于 Running 和 Ready 状态，当 <code>web-0</code> 变成 Running 和 Ready 时，
StatefulSet 会接收这个 Pod。由于你重新创建的 StatefulSet 的 <code>replicas</code> 等于 2，
一旦 <code>web-0</code> 被重新创建并且 <code>web-1</code> 被认为已经处于 Running 和 Ready 状态时，<code>web-2</code> 将会被终止。</p>
<p>让我们再看看被 Pod 的 web 服务器加载的 <code>index.html</code> 的内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> -i -t <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span> -- curl http://localhost/; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>web-0
web-1
</code></pre><!--
Even though you deleted both the StatefulSet and the `web-0` Pod, it still
serves the hostname originally entered into its `index.html` file. This is
because the StatefulSet never deletes the PersistentVolumes associated with a
Pod. When you recreated the StatefulSet and it relaunched `web-0`, its original
PersistentVolume was remounted.

### Cascading Delete

In one terminal window, watch the Pods in the StatefulSet.
-->
<p>尽管你同时删除了 StatefulSet 和 <code>web-0</code> Pod，但它仍然使用最初写入 <code>index.html</code> 文件的主机名进行服务。
这是因为 StatefulSet 永远不会删除和一个 Pod 相关联的 PersistentVolumes。
当你重建这个 StatefulSet 并且重新启动了 <code>web-0</code> 时，它原本的 PersistentVolume 会被重新挂载。</p>
<h3 id="级联删除">级联删除</h3>
<p>在一个终端窗口观察 StatefulSet 里的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
In another terminal, delete the StatefulSet again. This time, omit the
`--cascade=orphan` parameter.
-->
<p>在另一个窗口中再次删除这个 StatefulSet。这次省略 <code>--cascade=orphan</code> 参数。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete statefulset web
</code></pre></div><pre><code>statefulset.apps &quot;web&quot; deleted
</code></pre><!--
Examine the output of the `kubectl get` command running in the first terminal,
and wait for all of the Pods to transition to Terminating.
-->
<p>在第一个终端检查 <code>kubectl get</code> 命令的输出，并等待所有的 Pod 变成 Terminating 状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -w -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          11m
web-1     1/1       Running   0          27m
NAME      READY     STATUS        RESTARTS   AGE
web-0     1/1       Terminating   0          12m
web-1     1/1       Terminating   0         29m
web-0     0/1       Terminating   0         12m
web-0     0/1       Terminating   0         12m
web-0     0/1       Terminating   0         12m
web-1     0/1       Terminating   0         29m
web-1     0/1       Terminating   0         29m
web-1     0/1       Terminating   0         29m

</code></pre><!--
As you saw in the [Scaling Down](#scaling-down) section, the Pods
are terminated one at a time, with respect to the reverse order of their ordinal
indices. Before terminating a Pod, the StatefulSet controller waits for
the Pod's successor to be completely terminated.

Note that, while a cascading delete will delete the StatefulSet and its Pods,
it will not delete the Headless Service associated with the StatefulSet. You
must delete the `nginx` Service manually.
-->
<p>如同你在<a href="#ordered-pod-termination">缩容</a>一节看到的，Pod 按照和他们序号索引相反的顺序每次终止一个。
在终止一个 Pod 前，StatefulSet 控制器会等待 Pod 后继者被完全终止。</p>
<p>请注意，虽然级联删除会删除 StatefulSet 和它的 Pod，但它并不会删除和 StatefulSet 关联的 Headless Service。你必须手动删除<code>nginx</code> Service。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete service nginx
</code></pre></div><pre><code>service &quot;nginx&quot; deleted
</code></pre><!--
Recreate the StatefulSet and Headless Service one more time.
-->
<p>再一次重新创建 StatefulSet 和 Headless Service。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f web.yaml
</code></pre></div><pre><code>service/nginx created
statefulset.apps/web created
</code></pre><!--
When all of the StatefulSet's Pods transition to Running and Ready, retrieve
the contents of their `index.html` files.
-->
<p>当 StatefulSet 所有的 Pod 变成 Running 和 Ready 时，获取它们的 <code>index.html</code> 文件的内容。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">0</span> 1; <span style="color:#a2f;font-weight:bold">do</span> kubectl <span style="color:#a2f">exec</span> -i -t <span style="color:#b44">&#34;web-</span><span style="color:#b8860b">$i</span><span style="color:#b44">&#34;</span> -- curl http://localhost/; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre><code>web-0
web-1
</code></pre><!--
Even though you completely deleted the StatefulSet, and all of its Pods, the
Pods are recreated with their PersistentVolumes mounted, and `web-0` and
`web-1` will still serve their hostnames.

Finally delete the `web` StatefulSet and the `nginx` service.
-->
<p>即使你已经删除了 StatefulSet 和它的全部 Pod，这些 Pod 将会被重新创建并挂载它们的 PersistentVolumes，并且 <code>web-0</code> 和 <code>web-1</code> 将仍然使用它们的主机名提供服务。</p>
<p>最后删除 <code>nginx</code> service...</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete service nginx
</code></pre></div><pre><code>service &quot;nginx&quot; deleted
</code></pre><p>... 并且删除 <code>web</code> StatefulSet:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete statefulset web
</code></pre></div><pre><code>statefulset &quot;web&quot; deleted
</code></pre><!--
## Pod Management Policy

For some distributed systems, the StatefulSet ordering guarantees are
unnecessary and/or undesirable. These systems require only uniqueness and
identity. To address this, in Kubernetes 1.7, we introduced
`.spec.podManagementPolicy` to the StatefulSet API Object.

### OrderedReady Pod Management

`OrderedReady` pod management is the default for StatefulSets. It tells the
StatefulSet controller to respect the ordering guarantees demonstrated
above.

### Parallel Pod Management

`Parallel` pod management tells the StatefulSet controller to launch or
terminate all Pods in parallel, and not to wait for Pods to become Running
and Ready or completely terminated prior to launching or terminating another
Pod. This option only affects the behavior for scaling operations. Updates are not affected.
-->
<h2 id="pod-管理策略">Pod 管理策略</h2>
<p>对于某些分布式系统来说，StatefulSet 的顺序性保证是不必要和/或者不应该的。
这些系统仅仅要求唯一性和身份标志。为了解决这个问题，在 Kubernetes 1.7 中
我们针对 StatefulSet API 对象引入了 <code>.spec.podManagementPolicy</code>。
此选项仅影响扩缩操作的行为。更新不受影响。</p>
<h3 id="orderedready-pod-管理策略">OrderedReady Pod 管理策略</h3>
<p><code>OrderedReady</code> pod 管理策略是 StatefulSets 的默认选项。它告诉 StatefulSet 控制器遵循上文展示的顺序性保证。</p>
<h3 id="parallel-pod-管理策略">Parallel Pod 管理策略</h3>
<p><code>Parallel</code> pod 管理策略告诉 StatefulSet 控制器并行的终止所有 Pod，
在启动或终止另一个 Pod 前，不必等待这些 Pod 变成 Running 和 Ready 或者完全终止状态。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/web/web-parallel.yaml" download="application/web/web-parallel.yaml"><code>application/web/web-parallel.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-web-web-parallel-yaml')" title="Copy application/web/web-parallel.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-web-web-parallel-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;nginx&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podManagementPolicy</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Parallel&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/nginx-slim:0.8<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Download the example above, and save it to a file named `web-parallel.yaml`

This manifest is identical to the one you downloaded above except that the `.spec.podManagementPolicy`
of the `web` StatefulSet is set to `Parallel`.

In one terminal, watch the Pods in the StatefulSet.
-->
<p>下载上面的例子并保存为 <code>web-parallel.yaml</code>。</p>
<p>这份清单和你在上文下载的完全一样，只是 <code>web</code> StatefulSet 的 <code>.spec.podManagementPolicy</code> 设置成了 <code>Parallel</code>。</p>
<p>在一个终端窗口查看 StatefulSet 中的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get po -lapp<span style="color:#666">=</span>nginx -w
</code></pre></div><!--
In another terminal, create the StatefulSet and Service in the manifest:
-->
<p>在另一个终端窗口创建清单中的 StatefulSet 和 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f web-parallel.yaml
</code></pre></div><pre><code>service/nginx created
statefulset.apps/web created
</code></pre><!--
Examine the output of the `kubectl get` command that you executed in the first terminal.
-->
<p>查看你在第一个终端中运行的 <code>kubectl get</code> 命令的输出。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-1     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         10s
web-1     1/1       Running   0         10s
</code></pre><!--
The StatefulSet controller launched both `web-0` and `web-1` at the same time.

Keep the second terminal open, and, in another terminal window scale the
StatefulSet.
-->
<p>StatefulSet 控制器同时启动了 <code>web-0</code> 和 <code>web-1</code>。</p>
<p>保持第二个终端打开，并在另一个终端窗口中扩容 StatefulSet。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale statefulset/web --replicas<span style="color:#666">=</span><span style="color:#666">4</span>
</code></pre></div><pre><code>statefulset.apps/web scaled
</code></pre><!--
Examine the output of the terminal where the `kubectl get` command is running.
-->
<p>在 <code>kubectl get</code> 命令运行的终端里检查它的输出。</p>
<pre><code>web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         7s
web-3     0/1       ContainerCreating   0         7s
web-2     1/1       Running   0         10s
web-3     1/1       Running   0         26s
</code></pre><!--
The StatefulSet launched two new Pods, and it did not wait for
the first to become Running and Ready prior to launching the second.

## Cleaning up

You should have two terminals open, ready for you to run `kubectl` commands as
part of cleanup.
-->
<p>StatefulSet 启动了两个新的 Pod，而且在启动第二个之前并没有等待第一个变成 Running 和 Ready 状态。</p>
<h2 id="cleaning-up">Cleaning up</h2>
<p>您应该打开两个终端，准备在清理过程中运行 <code>kubectl</code> 命令。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete sts web
<span style="color:#080;font-style:italic"># sts is an abbreviation for statefulset</span>
</code></pre></div><!--
You can watch `kubectl get` to see those Pods being deleted.
-->
<p>你可以监测 <code>kubectl get</code> 来查看那些 Pod 被删除</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -w
</code></pre></div><pre><code>web-3     1/1       Terminating   0         9m
web-2     1/1       Terminating   0         9m
web-3     1/1       Terminating   0         9m
web-2     1/1       Terminating   0         9m
web-1     1/1       Terminating   0         44m
web-0     1/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-3     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-1     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-2     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-1     0/1       Terminating   0         44m
web-1     0/1       Terminating   0         44m
web-1     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-3     0/1       Terminating   0         9m
web-3     0/1       Terminating   0         9m
web-3     0/1       Terminating   0         9m
</code></pre><!--
The StatefulSet controller deletes all Pods concurrently, it does not wait for
a Pod's ordinal successor to terminate prior to deleting that Pod.

Close the terminal where the `kubectl get` command is running and delete the `nginx`
Service.
-->
<p>StatefulSet 控制器将并发的删除所有 Pod，在删除一个 Pod 前不会等待它的顺序后继者终止。</p>
<p>关闭 <code>kubectl get</code> 命令运行的终端并删除<code>nginx</code> Service。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete svc nginx
</code></pre></div><h2 id="cleaning-up-1">Cleaning up</h2>
<!--
You also need to delete the persistent storage media for the PersistentVolumes
used in this tutorial.


Follow the necessary steps, based on your environment, storage configuration,
and provisioning method, to ensure that all storage is reclaimed.
-->
<p>你需要删除本教程中用到的 PersistentVolumes 的持久化存储介质。基于你的环境、存储配置和提供方式，按照必须的步骤保证回收所有的存储。</p>

</div>



    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      
      <a class="text-white" href="/zh/docs/home/">主页</a>
      
      <a class="text-white" href="/zh/blog/">博客</a>
      
      <a class="text-white" href="/zh/training/">培训</a>
      
      <a class="text-white" href="/zh/partners/">合作伙伴</a>
      
      <a class="text-white" href="/zh/community/">社区</a>
      
      <a class="text-white" href="/zh/case-studies/">案例分析</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" href="https://discuss.kubernetes.io">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/kubernetesio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://youtube.com/kubernetescommunity">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/kubernetes/kubernetes">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://slack.k8s.io">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Contribute" aria-label="Contribute">
    <a class="text-white" target="_blank" href="https://git.k8s.io/community/contributors/guide">
      <i class="fas fa-edit"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" href="https://stackoverflow.com/questions/tagged/kubernetes">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2024 The Kubernetes Authors | Documentation Distributed under <a href="https://git.k8s.io/website/LICENSE" class="light-text">CC BY 4.0</a></small>
        <br/>
        <small class="text-white">Copyright &copy; 2024 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage" class="light-text">Trademark Usage page</a></small>
        <br/>
        <small class="text-white">ICP license: 京ICP备17074266号-3</small>
        
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>










<script src="/js/main.js"></script>






  </body>
</html>
