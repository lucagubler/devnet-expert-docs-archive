<!doctype html>
<html lang="zh" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>


<link rel="alternate" hreflang="en" href="http://localhost:1313/docs/tasks/administer-cluster/">
<link rel="alternate" hreflang="ko" href="http://localhost:1313/ko/docs/tasks/administer-cluster/">
<link rel="alternate" hreflang="ja" href="http://localhost:1313/ja/docs/tasks/administer-cluster/">
<link rel="alternate" hreflang="fr" href="http://localhost:1313/fr/docs/tasks/administer-cluster/">
<link rel="alternate" hreflang="de" href="http://localhost:1313/de/docs/tasks/administer-cluster/">
<link rel="alternate" hreflang="es" href="http://localhost:1313/es/docs/tasks/administer-cluster/">
<link rel="alternate" hreflang="id" href="http://localhost:1313/id/docs/tasks/administer-cluster/">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.87.0" />
<link rel="canonical" type="text/html" href="http://localhost:1313/zh/docs/tasks/administer-cluster/">
<link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="apple-touch-icon" href="/images/kubernetes-192x192.png">
<title>管理集群 | Kubernetes</title><meta property="og:title" content="管理集群" />
<meta property="og:description" content="了解管理集群的常见任务。" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/zh/docs/tasks/administer-cluster/" /><meta property="og:site_name" content="Kubernetes" />

<meta itemprop="name" content="管理集群">
<meta itemprop="description" content="了解管理集群的常见任务。"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="管理集群"/>
<meta name="twitter:description" content="了解管理集群的常见任务。"/>






<link href="/scss/main.css" rel="stylesheet">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "http://localhost:1313/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href="/css/feature-states.css">



<meta name="description" content="了解管理集群的常见任务。">
<meta property="og:description" content="了解管理集群的常见任务。">
<meta name="twitter:description" content="了解管理集群的常见任务。">
<meta property="og:url" content="http://localhost:1313/zh/docs/tasks/administer-cluster/">
<meta property="og:title" content="管理集群">
<meta name="twitter:title" content="管理集群">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">

<script src="/js/script.js"></script>


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/zh/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/zh/docs/" >文档</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/blog/" >Kubernetes 博客</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/training/" >培训</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/partners/" >合作伙伴</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/community/" >社区</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/case-studies/" >案例分析</a>
			</li>
			
			
			
			<li class="nav-item dropdown">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/releases">Release Information</a>
	
	<a class="dropdown-item" href="https://kubernetes.io/zh/docs/tasks/administer-cluster/">v1.23</a>
	
	<a class="dropdown-item" href="https://v1-22.docs.kubernetes.io/zh/docs/tasks/administer-cluster/">v1.22</a>
	
	<a class="dropdown-item" href="https://v1-21.docs.kubernetes.io/zh/docs/tasks/administer-cluster/">v1.21</a>
	
	<a class="dropdown-item" href="https://v1-20.docs.kubernetes.io/zh/docs/tasks/administer-cluster/">v1.20</a>
	
	<a class="dropdown-item" href="https://v1-19.docs.kubernetes.io/zh/docs/tasks/administer-cluster/">v1.19</a>
	
</div>
			</li>
			
			
			<li class="nav-item dropdown">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/tasks/administer-cluster/">English</a>
	
	<a class="dropdown-item" href="/ko/docs/tasks/administer-cluster/">한국어 Korean</a>
	
	<a class="dropdown-item" href="/ja/docs/tasks/administer-cluster/">日本語 Japanese</a>
	
	<a class="dropdown-item" href="/fr/docs/tasks/administer-cluster/">Français</a>
	
	<a class="dropdown-item" href="/de/docs/tasks/administer-cluster/">Deutsch</a>
	
	<a class="dropdown-item" href="/es/docs/tasks/administer-cluster/">Español</a>
	
	<a class="dropdown-item" href="/id/docs/tasks/administer-cluster/">Bahasa Indonesia</a>
	
</div>

			</li>
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/zh/docs/tasks/administer-cluster/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">管理集群</h1>
<div class="lead">了解管理集群的常见任务。</div>




    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-adb6c52e773f4d890595e14a9251f59b">从 dockershim 迁移</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>1.1: <a href="#pg-b8acce0768c2f92cdb8eaa31e8072353">将节点上的容器运行时从 Docker Engine 改为 containerd</a></li>


    
  
    
    
	
<li>1.2: <a href="#pg-d79db9ed1698f75ec5f2228987290e49">查明节点上所使用的容器运行时</a></li>


    
  
    
    
	
<li>1.3: <a href="#pg-58702e4818c09c9b3d574349c1a71cb3">检查弃用 Dockershim 对你的影响</a></li>


    
  
    
    
	
<li>1.4: <a href="#pg-eb3e279a6c5e1224e744080a52ee3f28">从 dockershim 迁移遥测和安全代理</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2: <a href="#pg-8e16d69617b175d61e2e7a6e1642c9d6">用 kubeadm 进行管理</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1: <a href="#pg-f62fba1de4084f3be070785757c8079c">使用 kubeadm 进行证书管理</a></li>


    
  
    
    
	
<li>2.2: <a href="#pg-6134c5061298affa145ddb801b5c29da">配置 cgroup 驱动</a></li>


    
  
    
    
	
<li>2.3: <a href="#pg-98530eb3653d28fef34bff4543364aa7">重新配置 kubeadm 集群</a></li>


    
  
    
    
	
<li>2.4: <a href="#pg-2e173356df5179cab9eec90a606f0aa4">升级 kubeadm 集群</a></li>


    
  
    
    
	
<li>2.5: <a href="#pg-9133578f1e75663bb031e5a377ca896d">添加 Windows 节点</a></li>


    
  
    
    
	
<li>2.6: <a href="#pg-e805c7d8d4ad6195cb82dbbc843bfc29">升级 Windows 节点</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3: <a href="#pg-47be5dd51f686017f1766e6ec7aa6f41">管理内存，CPU 和 API 资源</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.1: <a href="#pg-337620c76587e4aeb32009cb23be46de">为命名空间配置默认的内存请求和限制</a></li>


    
  
    
    
	
<li>3.2: <a href="#pg-320af95e480962c538ebef7ae205845c">为命名空间配置默认的 CPU 请求和限制</a></li>


    
  
    
    
	
<li>3.3: <a href="#pg-adb489b1ab985c9215657b0d4c6ae92b">配置命名空间的最小和最大内存约束</a></li>


    
  
    
    
	
<li>3.4: <a href="#pg-a87cbd1f9379dac7a48ae320da68a9ad">为命名空间配置 CPU 最小和最大约束</a></li>


    
  
    
    
	
<li>3.5: <a href="#pg-fe3283559a3df299aae3ee00ecea2fad">为命名空间配置内存和 CPU 配额</a></li>


    
  
    
    
	
<li>3.6: <a href="#pg-40e30a9209e0c9f4153707e43243e9d7">配置命名空间下 Pod 配额</a></li>


    
  

    </ul>
    
  
    
    
	
<li>4: <a href="#pg-7743f043c43f7b12e8654e2227dbc658">证书</a></li>


    
  
    
    
	
<li>5: <a href="#pg-8c31aafd38fad5b0de0bd191758d6f93">安装网络策略驱动</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.1: <a href="#pg-b4418905b0c14630e4e9cb1368241534">使用 Antrea 提供 NetworkPolicy</a></li>


    
  
    
    
	
<li>5.2: <a href="#pg-1239a77618c6278373832a142cd85519">使用 Calico 提供 NetworkPolicy</a></li>


    
  
    
    
	
<li>5.3: <a href="#pg-95039241255a31df196beaa405b68eba">使用 Cilium 提供 NetworkPolicy</a></li>


    
  
    
    
	
<li>5.4: <a href="#pg-505a0a6a7e6eff361bbb3be81c84b2e0">使用 kube-router 提供 NetworkPolicy</a></li>


    
  
    
    
	
<li>5.5: <a href="#pg-2842eac98aa0e229a5c6755c4c83d2a7">使用 Romana 提供 NetworkPolicy</a></li>


    
  
    
    
	
<li>5.6: <a href="#pg-ac075c3fdfd0d41aa753cc70e42be064">使用 Weave Net 提供 NetworkPolicy</a></li>


    
  

    </ul>
    
  
    
    
	
<li>6: <a href="#pg-b45f024608e1b367cdacb1fd9d77278a">IP Masquerade Agent 用户指南</a></li>


    
  
    
    
	
<li>7: <a href="#pg-ce4cd28c8feb9faa783e79b48af37961">Kubernetes 云管理控制器</a></li>


    
  
    
    
	
<li>8: <a href="#pg-c4d0832845adc92b7ccd54aed63fc932">为 Kubernetes 运行 etcd 集群</a></li>


    
  
    
    
	
<li>9: <a href="#pg-b64a1d2bb3f4ed9f7021134e09a75c36">为系统守护进程预留计算资源</a></li>


    
  
    
    
	
<li>10: <a href="#pg-a8f6511197efcd7d0db80ade49620f9d">为节点发布扩展资源</a></li>


    
  
    
    
	
<li>11: <a href="#pg-f6f3b8f9789fda4286bf410b8e108f69">以非root用户身份运行 Kubernetes 节点组件</a></li>


    
  
    
    
	
<li>12: <a href="#pg-e1afcdac8d5e8458274b3c481c5ebcda">使用 CoreDNS 进行服务发现</a></li>


    
  
    
    
	
<li>13: <a href="#pg-669c88964b4a9eb2b040057266e4b60d">使用 KMS 驱动进行数据加密</a></li>


    
  
    
    
	
<li>14: <a href="#pg-e77685d5b88d2db5c7631a27b9472eea">使用 Kubernetes API 访问集群</a></li>


    
  
    
    
	
<li>15: <a href="#pg-778055e4a4415ca195169b42cd42ddf9">使用 NUMA 感知的内存管理器</a></li>


    
  
    
    
	
<li>16: <a href="#pg-12001be83d15fcd7f3242313a55777df">保护集群</a></li>


    
  
    
    
	
<li>17: <a href="#pg-4a02bcca41439e16655f43fa37c81da4">关键插件 Pod 的调度保证</a></li>


    
  
    
    
	
<li>18: <a href="#pg-fe6b50655c29ab0b7c1ee549ff64c138">升级集群</a></li>


    
  
    
    
	
<li>19: <a href="#pg-56de8c25b1486599777034111645b803">名字空间演练</a></li>


    
  
    
    
	
<li>20: <a href="#pg-09cc2cf3e0f23a3996e6cb31dc4d867c">启用/禁用 Kubernetes API</a></li>


    
  
    
    
	
<li>21: <a href="#pg-9ceed97f912df7289ed8872e290cfbad">在 Kubernetes 集群中使用 NodeLocal DNSCache</a></li>


    
  
    
    
	
<li>22: <a href="#pg-fe5ad73163d38596340536ec03a205f0">在 Kubernetes 集群中使用 sysctl</a></li>


    
  
    
    
	
<li>23: <a href="#pg-eec61e72c300dbfbf7302400ca966432">在运行中的集群上重新配置节点的 kubelet</a></li>


    
  
    
    
	
<li>24: <a href="#pg-4e9de5bc3973e5d2bb8f09ff940c3319">在集群中使用级联删除</a></li>


    
  
    
    
	
<li>25: <a href="#pg-a3790dfb57271d13517e549dffa805b9">声明网络策略</a></li>


    
  
    
    
	
<li>26: <a href="#pg-b35b8ddb9bbc15620ce9636f4346c05c">安全地清空一个节点</a></li>


    
  
    
    
	
<li>27: <a href="#pg-9585dc0efb0450fd68728e7511754717">开发云控制器管理器</a></li>


    
  
    
    
	
<li>28: <a href="#pg-00733cc3747eb3f5fe1c9e0439262967">开启服务拓扑</a></li>


    
  
    
    
	
<li>29: <a href="#pg-7127e6b7344b315b30b1ce8c4d8bfc55">控制节点上的 CPU 管理策略</a></li>


    
  
    
    
	
<li>30: <a href="#pg-8060aed5bf1172fa62199a4c306a4cd1">控制节点上的拓扑管理策略</a></li>


    
  
    
    
	
<li>31: <a href="#pg-2bffd7f3571cdd609bd97fb2e1bdb2fe">改变默认 StorageClass</a></li>


    
  
    
    
	
<li>32: <a href="#pg-fbc9136f53eccd6eb8c80f4bbea3b8f4">更改 PersistentVolume 的回收策略</a></li>


    
  
    
    
	
<li>33: <a href="#pg-966cd1cc69c69410d8698b3ac74abce2">自动扩缩集群 DNS 服务</a></li>


    
  
    
    
	
<li>34: <a href="#pg-3d0cd7d2f13d4759094f281504cf57b8">自定义 DNS 服务</a></li>


    
  
    
    
	
<li>35: <a href="#pg-8bcf4aeb5bbb6d6969a146e5ab97557b">调试 DNS 问题</a></li>


    
  
    
    
	
<li>36: <a href="#pg-a24171610b6ea75a142cb9c8c7882390">迁移多副本的控制面以使用云控制器管理器</a></li>


    
  
    
    
	
<li>37: <a href="#pg-1e966f5d0540bbee0876f9d0d08d54dc">通过名字空间共享集群</a></li>


    
  
    
    
	
<li>38: <a href="#pg-f58763cc9447491b6c40f939a02d441d">通过配置文件设置 Kubelet 参数</a></li>


    
  
    
    
	
<li>39: <a href="#pg-5e59f5575dce11fdaed640afdbeedfc1">配置 API 对象配额</a></li>


    
  
    
    
	
<li>40: <a href="#pg-a02f35804917d7a269c38d7e2c475005">限制存储消耗</a></li>


    
  
    
    
	
<li>41: <a href="#pg-6b4e7ca6586f448c8533a120c29bdd25">静态加密 Secret 数据</a></li>


    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-adb6c52e773f4d890595e14a9251f59b">1 - 从 dockershim 迁移</h1>
    
	<!-- 
title: "Migrating from dockershim"
weight: 10
content_type: task 
-->
<!-- overview -->
<!-- 
This section presents information you need to know when migrating from
dockershim to other container runtimes.
-->
<p>本节提供从 dockershim 迁移到其他容器运行时的必备知识。</p>
<!-- 
Since the announcement of [dockershim deprecation](/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation)
in Kubernetes 1.20, there were questions on how this will affect various workloads and Kubernetes
installations. Our [Dockershim Removal FAQ](/blog/2022/02/17/dockershim-faq/) is there to help you
to understand the problem better.
-->
<p>自从 Kubernetes 1.20 宣布
<a href="/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation">弃用 dockershim</a>，
各类疑问随之而来：这对各类工作负载和 Kubernetes 部署会产生什么影响。
我们的<a href="/blog/2022/02/17/dockershim-faq/">弃用  Dockershim 常见问题</a>可以帮助你更好地理解这个问题。</p>
<!-- It is recommended to migrate from dockershim to alternative container runtimes.
Check out [container runtimes](/docs/setup/production-environment/container-runtimes/)
section to know your options. Make sure to
[report issues](https://github.com/kubernetes/kubernetes/issues) you encountered
with the migration. So the issue can be fixed in a timely manner and your cluster would be
ready for dockershim removal.
-->
<p>建议从 dockershim 迁移到其他替代的容器运行时。
请参阅<a href="/zh/docs/setup/production-environment/container-runtimes/">容器运行时</a>
一节以了解可用的备选项。
当在迁移过程中遇到麻烦，请<a href="https://github.com/kubernetes/kubernetes/issues">上报问题</a>。
那么问题就可以及时修复，你的集群也可以进入移除 dockershim 前的就绪状态。</p>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b8acce0768c2f92cdb8eaa31e8072353">1.1 - 将节点上的容器运行时从 Docker Engine 改为 containerd</h1>
    
	<!--
title: "Changing the Container Runtime on a Node from Docker Engine to containerd"
weight: 8
content_type: task 
-->
<!--
This task outlines the steps needed to update your container runtime to containerd from Docker. It
is applicable for cluster operators running Kubernetes 1.23 or earlier. Also  this covers an
example scenario for migrating from dockershim to containerd and alternative container runtimes
can be picked from this [page](/docs/setup/production-environment/container-runtimes/).
-->
<p>本任务给出将容器运行时从 Docker 改为 containerd 所需的步骤。
此任务适用于运行 1.23 或更早版本 Kubernetes 的集群操作人员。
同时，此任务也涉及从 dockershim 迁移到 containerd 的示例场景，
以及可以从<a href="/zh/docs/setup/production-environment/container-runtimes/">此页面</a>
获得的其他容器运行时列表。</p>
<h2 id="before-you-begin">Before you begin</h2>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<!--
Install containerd. For more information see
[containerd's installation documentation](https://containerd.io/docs/getting-started/)
and for specific prerequisite follow
[the containerd guide](/docs/setup/production-environment/container-runtimes/#containerd).
-->
<p>安装 containerd。进一步的信息可参见
<a href="https://containerd.io/docs/getting-started/">containerd 的安装文档</a>。
关于一些特定的环境准备工作，请遵循 <a href="/zh/docs/setup/production-environment/container-runtimes/#containerd">containerd 指南</a>。</p>
<!--
## Drain the node 

```shell
kubectl drain <node-to-drain> --ignore-daemonsets
```

Replace `<node-to-drain>` with the name of your node you are draining.
-->
<h2 id="drain-the-node">腾空节点   </h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</code></pre></div><p>将 <code>&lt;node-to-drain&gt;</code> 替换为你所要腾空的节点的名称</p>
<!--
## Stop the Docker daemon
-->
<h2 id="stop-the-docker-daemon">停止 Docker 守护进程  </h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">systemctl stop kubelet
systemctl disable docker.service --now
</code></pre></div><!--
## Install Containerd

Follow the [guide](/docs/setup/production-environment/container-runtimes/#containerd)
for detailed steps to install containerd.
-->
<h2 id="install-containerd">安装 Containerd   </h2>
<p>遵循此<a href="/zh/docs/setup/production-environment/container-runtimes/#containerd">指南</a>
了解安装 containerd 的详细步骤。</p>
<ul class="nav nav-tabs" id="tab-cri-containerd-installation" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tab-cri-containerd-installation-0" role="tab" aria-controls="tab-cri-containerd-installation-0" aria-selected="true">Linux</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-cri-containerd-installation-1" role="tab" aria-controls="tab-cri-containerd-installation-1">Windows (PowerShell)</a></li></ul>
<div class="tab-content" id="tab-cri-containerd-installation"><div id="tab-cri-containerd-installation-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tab-cri-containerd-installation-0">

<p><!--
1. Install the `containerd.io` package from the official Docker repositories. 
   Instructions for setting up the Docker repository for your respective Linux distribution and
   installing the `containerd.io` package can be found at 
   [Install Docker Engine](https://docs.docker.com/engine/install/#server).
-->
<ol>
<li>从官方的 Docker 仓库安装 <code>containerd.io</code> 包。关于为你所使用的 Linux 发行版来设置
Docker 仓库，以及安装 <code>containerd.io</code> 包的详细说明，可参见
<a href="https://docs.docker.com/engine/install/#server">Install Docker Engine</a>。</li>
</ol>
<!--
1. Configure containerd:
-->
<ol start="2">
<li>
<p>配置 containerd：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
</code></pre></div></li>
</ol>
<!--
1. Restart containerd:
-->
<ol start="3">
<li>
<p>重启 containerd：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl restart containerd
</code></pre></div></li>
</ol>
</div>
  <div id="tab-cri-containerd-installation-1" class="tab-pane" role="tabpanel" aria-labelledby="tab-cri-containerd-installation-1">

<p><!--
Start a Powershell session, set `$Version` to the desired version (ex: `$Version="1.4.3"`), and
then run the following commands:
-->
<p>启动一个 Powershell 会话，将 <code>$Version</code> 设置为期望的版本（例如：<code>$Version=&quot;1.4.3&quot;</code>），
之后运行下面的命令：</p>
<!--
1. Download containerd:
-->
<ol>
<li>
<p>下载 containerd：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">curl.exe -L https<span style="">:</span>//github.com/containerd/containerd/releases/download/v<span style="color:#b8860b">$Version</span>/containerd-<span style="color:#b8860b">$Version</span>-windows-amd64.tar.gz -o <span style="color:#a2f">containerd-windows</span>-amd64.tar.gz
tar.exe xvf .\<span style="color:#a2f">containerd-windows</span>-amd64.tar.gz
</code></pre></div></li>
</ol>
<!--
2. Extract and configure:
-->
<ol start="2">
<li>
<p>解压缩并执行配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">Copy-Item</span> -Path <span style="color:#b44">&#34;.\bin\&#34;</span> -Destination <span style="color:#b44">&#34;$Env:ProgramFiles\containerd&#34;</span> -Recurse -Force
<span style="color:#a2f">cd </span><span style="color:#b8860b">$Env:ProgramFiles</span>\containerd\
.\containerd.exe config <span style="color:#a2f;font-weight:bold">default</span> | <span style="color:#a2f">Out-File</span> config.toml -Encoding ascii

<span style="color:#080;font-style:italic"># 请审查配置信息。取决于你的安装环境，你可能需要调整：</span>
<span style="color:#080;font-style:italic"># - the sandbox_image （Kubernetes pause 镜像）</span>
<span style="color:#080;font-style:italic"># - cni bin_dir 和 conf_dir 的位置</span>
<span style="color:#a2f">Get-Content</span> config.toml

<span style="color:#080;font-style:italic"># （可选步骤，但强烈建议执行）将 containerd 排除在 Windows Defender 扫描之外</span>
<span style="color:#a2f">Add-MpPreference</span> -ExclusionProcess <span style="color:#b44">&#34;$Env:ProgramFiles\containerd\containerd.exe&#34;</span>
</code></pre></div></li>
</ol>
<!--
3. Start containerd:
-->
<ol start="3">
<li>
<p>启动 containerd：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">.\containerd.exe --register-service
<span style="color:#a2f">Start-Service</span> containerd
</code></pre></div></li>
</ol>
</div></div>

<!--
## Configure the kubelet to use containerd as its container runtime

Edit the file `/var/lib/kubelet/kubeadm-flags.env` and add the containerd runtime to the flags.
`--container-runtime=remote` and
`--container-runtime-endpoint=unix:///run/containerd/containerd.sock"`.
-->
<h2 id="配置-kubelet-使用-containerd-作为其容器运行时">配置 kubelet 使用 containerd 作为其容器运行时</h2>
<p>编辑文件 <code>/var/lib/kubelet/kubeadm-flags.env</code>，将 containerd 运行时添加到标志中：
<code>--container-runtime=remote</code> 和 <code>--container-runtime-endpoint=unix:///run/containerd/containerd.sock&quot;</code>。</p>
<!--
For users using kubeadm should consider the following:

Users using kubeadm should be aware that the `kubeadm` tool stores the CRI socket for each host as
an annotation in the Node object for that host. To change it you can execute the following command
on a machine that has the kubeadm `/etc/kubernetes/admin.conf` file.
-->
<p>对于使用 kubeadm 的用户，可以考虑下面的问题：</p>
<p><code>kubeadm</code> 工具将每个主机的 CRI 套接字保存在该主机对应的 Node 对象的注解中。
使用 <code>kubeadm</code> 的用户应该知道，<code>kubeadm</code> 工具将每个主机的 CRI 套接字保存在该主机对应的 Node 对象的注解中。
要更改这一注解信息，你可以在一台包含 kubeadm <code>/etc/kubernetes/admin.conf</code> 文件的机器上执行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit no &lt;node-name&gt;
</code></pre></div><!--
This will start a text editor where you can edit the Node object.

To choose a text editor you can set the `KUBE_EDITOR` environment variable.

- Change the value of `kubeadm.alpha.kubernetes.io/cri-socket` from `/var/run/dockershim.sock`
  to the CRI socket path of your choice (for example `unix:///run/containerd/containerd.sock`).
   
  Note that new CRI socket paths must be prefixed with `unix://` ideally.

- Save the changes in the text editor, which will update the Node object.
-->
<p>这一命令会打开一个文本编辑器，供你在其中编辑 Node 对象。
要选择不同的文本编辑器，你可以设置 <code>KUBE_EDITOR</code> 环境变量。</p>
<ul>
<li>
<p>更改 <code>kubeadm.alpha.kubernetes.io/cri-socket</code> 值，将其从
<code>/var/run/dockershim.sock</code> 改为你所选择的 CRI 套接字路径
（例如：<code>unix:///run/containerd/containerd.sock</code>）。</p>
<p>注意新的 CRI 套接字路径必须带有 <code>unix://</code> 前缀。</p>
</li>
<li>
<p>保存文本编辑器中所作的修改，这会更新 Node 对象。</p>
</li>
</ul>
<!--
## Restart the kubelet
-->
<h2 id="restart-the-kubelet">重启 kubelet   </h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">systemctl start kubelet
</code></pre></div><!--
## Verify that the node is healthy

Run `kubectl get nodes -o wide` and containerd appears as the runtime for the node we just changed.

## Remove Docker Engine
-->
<h2 id="verify-that-the-node-is-healthy">验证节点处于健康状态  </h2>
<p>运行 <code>kubectl get nodes -o wide</code>，containerd 会显示为我们所更改的节点上的运行时。</p>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<!--
Finally if everything goes well, remove Docker.
-->
<p>最后，在一切顺利时删除 Docker。</p>
<ul class="nav nav-tabs" id="tab-remove-docker-enigine" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tab-remove-docker-enigine-0" role="tab" aria-controls="tab-remove-docker-enigine-0" aria-selected="true">CentOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-remove-docker-enigine-1" role="tab" aria-controls="tab-remove-docker-enigine-1">Debian</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-remove-docker-enigine-2" role="tab" aria-controls="tab-remove-docker-enigine-2">Fedora</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-remove-docker-enigine-3" role="tab" aria-controls="tab-remove-docker-enigine-3">Ubuntu</a></li></ul>
<div class="tab-content" id="tab-remove-docker-enigine"><div id="tab-remove-docker-enigine-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tab-remove-docker-enigine-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo yum remove docker-ce docker-ce-cli
</code></pre></div></div>
  <div id="tab-remove-docker-enigine-1" class="tab-pane" role="tabpanel" aria-labelledby="tab-remove-docker-enigine-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo apt-get purge docker-ce docker-ce-cli
</code></pre></div></div>
  <div id="tab-remove-docker-enigine-2" class="tab-pane" role="tabpanel" aria-labelledby="tab-remove-docker-enigine-2">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo dnf remove docker-ce docker-ce-cli
</code></pre></div></div>
  <div id="tab-remove-docker-enigine-3" class="tab-pane" role="tabpanel" aria-labelledby="tab-remove-docker-enigine-3">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo apt-get purge docker-ce docker-ce-cli
</code></pre></div></div></div>


</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d79db9ed1698f75ec5f2228987290e49">1.2 - 查明节点上所使用的容器运行时</h1>
    
	<!--
title: Find Out What Container Runtime is Used on a Node
content_type: task
reviewers:
- SergeyKanzhelev
weight: 10
-->
<!-- overview -->
<!--
This page outlines steps to find out what [container runtime](/docs/setup/production-environment/container-runtimes/)
the nodes in your cluster use.
-->
<p>本页面描述查明集群中节点所使用的<a href="/zh/docs/setup/production-environment/container-runtimes/">容器运行时</a>
的步骤。</p>
<!--
Depending on the way you run your cluster, the container runtime for the nodes may
have been pre-configured or you need to configure it. If you're using a managed
Kubernetes service, there might be vendor-specific ways to check what container runtime is
configured for the nodes. The method described on this page should work whenever
the execution of `kubectl` is allowed.
-->
<p>取决于你运行集群的方式，节点所使用的容器运行时可能是事先配置好的，
也可能需要你来配置。如果你在使用托管的 Kubernetes 服务，
可能存在特定于厂商的方法来检查节点上配置的容器运行时。
本页描述的方法应该在能够执行 <code>kubectl</code> 的场合下都可以工作。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
Install and configure `kubectl`. See [Install Tools](/docs/tasks/tools/#kubectl) section for details.
-->
<p>安装并配置 <code>kubectl</code>。参见<a href="/zh/docs/tasks/tools/#kubectl">安装工具</a> 节了解详情。</p>
<!--
## Find out the container runtime used on a Node

Use `kubectl` to fetch and show node information:
-->
<h2 id="查明节点所使用的容器运行时">查明节点所使用的容器运行时</h2>
<p>使用 <code>kubectl</code> 来读取并显示节点信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes -o wide
</code></pre></div><!--
The output is similar to the following. The column `CONTAINER-RUNTIME` outputs
the runtime and its version.
-->
<p>输出如下面所示。<code>CONTAINER-RUNTIME</code> 列给出容器运行时及其版本。</p>
<pre><code class="language-none" data-lang="none"># For dockershim
NAME         STATUS   VERSION    CONTAINER-RUNTIME
node-1       Ready    v1.16.15   docker://19.3.1
node-2       Ready    v1.16.15   docker://19.3.1
node-3       Ready    v1.16.15   docker://19.3.1
</code></pre><pre><code class="language-none" data-lang="none"># For containerd
NAME         STATUS   VERSION   CONTAINER-RUNTIME
node-1       Ready    v1.19.6   containerd://1.4.1
node-2       Ready    v1.19.6   containerd://1.4.1
node-3       Ready    v1.19.6   containerd://1.4.1
</code></pre><!--
Find out more information about container runtimes
on [Container Runtimes](/docs/setup/production-environment/container-runtimes/) page.
-->
<p>你可以在<a href="/zh/docs/setup/production-environment/container-runtimes/">容器运行时</a>
页面找到与容器运行时相关的更多信息。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-58702e4818c09c9b3d574349c1a71cb3">1.3 - 检查弃用 Dockershim 对你的影响</h1>
    
	<!-- 
title: Check whether Dockershim deprecation affects you
content_type: task 
reviewers:
- SergeyKanzhelev
weight: 20
-->
<!-- overview -->
<!-- 
The `dockershim` component of Kubernetes allows to use Docker as a Kubernetes's
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>.
Kubernetes' built-in `dockershim` component was
[deprecated](/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation)
in release v1.20.
-->
<p>Kubernetes 的 <code>dockershim</code> 组件使得你可以把 Docker 用作 Kubernetes 的
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>。
在 Kubernetes v1.20 版本中，内建组件 <code>dockershim</code> 被<a href="/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation">弃用</a>。</p>
<!-- 
This page explains how your cluster could be using Docker as a container runtime,
provides details on the role that `dockershim` plays when in use, and shows steps
you can take to check whether any workloads could be affected by `dockershim` deprecation.
-->
<p>本页讲解你的集群把 Docker 用作容器运行时的运作机制，
并提供使用 <code>dockershim</code> 时，它所扮演角色的详细信息，
继而展示了一组操作，可用来检查弃用 <code>dockershim</code> 对你的工作负载是否有影响。</p>
<!-- 
## Finding if your app has a dependencies on Docker {#find-docker-dependencies} 
-->
<h2 id="find-docker-dependencies">检查你的应用是否依赖于 Docker</h2>
<!-- 
If you are using Docker for building your application containers, you can still
run these containers on any container runtime. This use of Docker does not count
as a dependency on Docker as a container runtime.
-->
<p>即使你是通过 Docker 创建的应用容器，也不妨碍你在其他任何容器运行时上运行这些容器。
这种使用 Docker 的方式并不构成对 Docker 作为一个容器运行时的依赖。</p>
<!-- 
When alternative container runtime is used, executing Docker commands may either
not work or yield unexpected output. This is how you can find whether you have a
dependency on Docker:
-->
<p>当用了别的容器运行时之后，Docker 命令可能不工作，或者产生意外的输出。
下面是判定你是否依赖于 Docker 的方法。</p>
<!--
1. Make sure no privileged Pods execute Docker commands (like `docker ps`),
   restart the Docker service (commands such as `systemctl restart docker.service`),
   or modify Docker-specific files such as `/etc/docker/daemon.json`.
1. Check for any private registries or image mirror settings in the Docker
   configuration file (like `/etc/docker/daemon.json`). Those typically need to
   be reconfigured for another container runtime.
1. Check that scripts and apps running on nodes outside of your Kubernetes
   infrastructure do not execute Docker commands. It might be:
   - SSH to nodes to troubleshoot;
   - Node startup scripts;
   - Monitoring and security agents installed on nodes directly.
1. Third-party tools that perform above mentioned privileged operations. See
   [Migrating telemetry and security agents from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents)
   for more information.
1. Make sure there is no indirect dependencies on dockershim behavior.
   This is an edge case and unlikely to affect your application. Some tooling may be configured
   to react to Docker-specific behaviors, for example, raise alert on specific metrics or search for
   a specific log message as part of troubleshooting instructions.
   If you have such tooling configured, test the behavior on test
   cluster before migration.
-->
<ol>
<li>确认没有特权 Pod 执行 Docker 命令（如 <code>docker ps</code>）、重新启动 Docker
服务（如 <code>systemctl restart docker.service</code>）或修改 Docker 配置文件
<code>/etc/docker/daemon.json</code>。</li>
<li>检查 Docker 配置文件（如 <code>/etc/docker/daemon.json</code>）中容器镜像仓库的镜像（mirror）站点设置。
这些配置通常需要针对不同容器运行时来重新设置。</li>
<li>检查确保在 Kubernetes 基础设施之外的节点上运行的脚本和应用程序没有执行 Docker 命令。
可能的情况如：
<ul>
<li>SSH 到节点排查故障；</li>
<li>节点启动脚本；</li>
<li>直接安装在节点上的监控和安全代理。</li>
</ul>
</li>
<li>检查执行上述特权操作的第三方工具。详细操作请参考
<a href="/zh/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents">从 dockershim 迁移遥测和安全代理</a>。</li>
<li>确认没有对 dockershim 行为的间接依赖。这是一种极端情况，不太可能影响你的应用。
一些工具很可能被配置为使用了 Docker 特性，比如，基于特定指标发警报，
或者在故障排查指令的一个环节中搜索特定的日志信息。
如果你有此类配置的工具，需要在迁移之前，在测试集群上测试这类行为。</li>
</ol>
<!-- 
## Dependency on Docker explained {#role-of-dockershim}  
-->
<h2 id="role-of-dockershim">Docker 依赖详解</h2>
<!-- 
A [container runtime](/docs/concepts/containers/#container-runtimes) is software that can
execute the containers that make up a Kubernetes pod. Kubernetes is responsible for orchestration
and scheduling of Pods; on each node, the <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>
uses the container runtime interface as an abstraction so that you can use any compatible
container runtime.
 -->
<p><a href="/zh/docs/concepts/containers/#container-runtimes">容器运行时</a>是一个软件，用来运行组成 Kubernetes Pod 的容器。
Kubernetes 负责编排和调度 Pod；在每一个节点上，<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>
使用抽象的容器运行时接口，所以你可以任意选用兼容的容器运行时。</p>
<!-- 
In its earliest releases, Kubernetes offered compatibility with one container runtime: Docker.
Later in the Kubernetes project's history, cluster operators wanted to adopt additional container runtimes.
The CRI was designed to allow this kind of flexibility - and the kubelet began supporting CRI. However,
because Docker existed before the CRI specification was invented, the Kubernetes project created an
adapter component, `dockershim`. The dockershim adapter allows the kubelet to interact with Docker as
if Docker were a CRI compatible runtime.
 -->
<p>在早期版本中，Kubernetes 提供的兼容性支持一个容器运行时：Docker。
在 Kubernetes 发展历史中，集群运营人员希望采用更多的容器运行时。
于是 CRI 被设计出来满足这类灵活性需要 - 而 kubelet 亦开始支持 CRI。
然而，因为 Docker 在 CRI 规范创建之前就已经存在，Kubernetes 就创建了一个适配器组件 <code>dockershim</code>。
dockershim 适配器允许 kubelet 与 Docker 交互，就好像 Docker 是一个 CRI 兼容的运行时一样。</p>
<!-- 
You can read about it in [Kubernetes Containerd integration goes GA](/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/) blog post.
 -->
<p>你可以阅读博文
<a href="/zh/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/">Kubernetes 正式支持集成 Containerd</a>。</p>
<!-- Dockershim vs. CRI with Containerd -->
<p><img src="/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/cri-containerd.png" alt="Dockershim 和 Containerd CRI 的实现对比图"></p>
<!-- 
Switching to Containerd as a container runtime eliminates the middleman. All the
same containers can be run by container runtimes like Containerd as before. But
now, since containers schedule directly with the container runtime, they are not visible to Docker.
So any Docker tooling or fancy UI you might have used
before to check on these containers is no longer available.
 -->
<p>切换到容器运行时 Containerd 可以消除掉中间环节。
所有相同的容器都可由 Containerd 这类容器运行时来运行。
但是现在，由于直接用容器运行时调度容器，它们对 Docker 是不可见的。
因此，你以前用来检查这些容器的 Docker 工具或漂亮的 UI 都不再可用。</p>
<!-- 
You cannot get container information using `docker ps` or `docker inspect`
commands. As you cannot list containers, you cannot get logs, stop containers,
or execute something inside container using `docker exec`.
 -->
<p>你不能再使用 <code>docker ps</code> 或 <code>docker inspect</code> 命令来获取容器信息。
由于你不能列出容器，因此你不能获取日志、停止容器，甚至不能通过 <code>docker exec</code> 在容器中执行命令。</p>
<!-- 
If you're running workloads via Kubernetes, the best way to stop a container is through
the Kubernetes API rather than directly through the container runtime (this advice applies
for all container runtimes, not only Docker).
 -->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果你在用 Kubernetes 运行工作负载，最好通过 Kubernetes API 停止容器，
而不是通过容器运行时来停止它们
（此建议适用于所有容器运行时，不仅仅是针对 Docker）。
</div>
<!-- 
You can still pull images or build them using `docker build` command. But images
built or pulled by Docker would not be visible to container runtime and
Kubernetes. They needed to be pushed to some registry to allow them to be used
by Kubernetes.
 -->
<p>你仍然可以下载镜像，或者用 <code>docker build</code> 命令创建它们。
但用 Docker 创建、下载的镜像，对于容器运行时和 Kubernetes，均不可见。
为了在 Kubernetes 中使用，需要把镜像推送（push）到某镜像仓库。</p>
<!-- ## What's next

- Read [Migrating from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/) to understand your next steps
- Read the [dockershim deprecation FAQ](/blog/2020/12/02/dockershim-faq/) article for more information. 
-->
<h2 id="what-s-next">What's next</h2>
<ul>
<li>阅读<a href="/zh/docs/tasks/administer-cluster/migrating-from-dockershim/">从 dockershim 迁移</a>以了解你的下一步工作</li>
<li>阅读<a href="/zh/blog/2020/12/02/dockershim-faq/">dockershim 弃用常见问题解答</a>文章了解更多信息。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-eb3e279a6c5e1224e744080a52ee3f28">1.4 - 从 dockershim 迁移遥测和安全代理</h1>
    
	<!-- 
title: Migrating telemetry and security agents from dockershim
content_type: task 
reviewers:
- SergeyKanzhelev
weight: 70
-->
<!-- overview -->
<!-- 
With Kubernetes 1.20 dockershim was deprecated. From the
[Dockershim Deprecation FAQ](/blog/2020/12/02/dockershim-faq/)
you might already know that most apps do not have a direct dependency on runtime hosting
containers. However, there are still a lot of telemetry and security agents
that has a dependency on docker to collect containers metadata, logs and
metrics. This document aggregates information on how to detect tese
dependencies and links on how to migrate these agents to use generic tools or
alternative runtimes.
-->
<p>在 Kubernetes 1.20 版本中，dockershim 被弃用。
在博文<a href="/zh/blog/2020/12/02/dockershim-faq/">弃用 Dockershim 常见问题</a>中，
你大概已经了解到，大多数应用并没有直接通过运行时来托管容器。
但是，仍然有大量的遥测和安全代理依赖 docker 来收集容器元数据、日志和指标。
本文汇总了一些信息和链接：信息用于阐述如何探查这些依赖，链接用于解释如何迁移这些代理去使用通用的工具或其他容器运行。</p>
<!-- 
## Telemetry and security agents 
-->
<h2 id="telemetry-and-security-agents">遥测和安全代理</h2>
<!-- 
There are a few ways agents may run on Kubernetes cluster. Agents may run on
nodes directly or as DaemonSets.
-->
<p>为了让代理运行在 Kubernetes 集群中，我们有几种办法。
代理既可以直接在节点上运行，也可以作为守护进程运行。</p>
<!-- 
### Why do telemetry agents rely on Docker?
-->
<h3 id="why-do-telemetry-agents-relyon-docker">为什么遥测代理依赖于 Docker？</h3>
<!-- 
Historically, Kubernetes was built on top of Docker. Kubernetes is managing
networking and scheduling, Docker was placing and operating containers on a
node. So you can get scheduling-related metadata like a pod name from Kubernetes
and containers state information from Docker. Over time more runtimes were
created to manage containers. Also there are projects and Kubernetes features
that generalize container status information extraction across many runtimes.
-->
<p>因为历史原因，Kubernetes 建立在 Docker 之上。
Kubernetes 管理网络和调度，Docker 则在具体的节点上定位并操作容器。
所以，你可以从 Kubernetes 取得调度相关的元数据，比如 Pod 名称；从 Docker 取得容器状态信息。
后来，人们开发了更多的运行时来管理容器。
同时一些项目和 Kubernetes 特性也不断涌现，支持跨多个运行时收集容器状态信息。</p>
<!-- 
Some agents are tied specifically to the Docker tool. The agents may run
commands like [`docker ps`](https://docs.docker.com/engine/reference/commandline/ps/)
or [`docker top`](https://docs.docker.com/engine/reference/commandline/top/) to list
containers and processes or [docker logs](https://docs.docker.com/engine/reference/commandline/logs/)
to subscribe on docker logs. With the deprecating of Docker as a container runtime,
these commands will not work any longer.
-->
<p>一些代理和 Docker 工具紧密绑定。此类代理可以这样运行命令，比如用
<a href="https://docs.docker.com/engine/reference/commandline/ps/"><code>docker ps</code></a>
或 <a href="https://docs.docker.com/engine/reference/commandline/top/"><code>docker top</code></a>
这类命令来列出容器和进程，用
<a href="https://docs.docker.com/engine/reference/commandline/logs/">docker logs</a>
订阅 Docker 的日志。
但随着 Docker 作为容器运行时被弃用，这些命令将不再工作。</p>
<!-- 
### Identify DaemonSets that depend on Docker {#identify-docker-dependency }
-->
<h3 id="identify-docker-dependency">识别依赖于 Docker 的 DaemonSet</h3>
<!-- 
If a pod wants to make calls to the `dockerd` running on the node, the pod must either:

- mount the filesystem containing the Docker daemon's privileged socket, as a
  <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='volume'>volume</a>; or
- mount the specific path of the Docker daemon's privileged socket directly, also as a volume.
-->
<p>如果某 Pod 想调用运行在节点上的 <code>dockerd</code>，该 Pod 必须满足以下两个条件之一：</p>
<ul>
<li>将包含 Docker 守护进程特权套接字的文件系统挂载为一个<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>；或</li>
<li>直接以卷的形式挂载 Docker 守护进程特权套接字的特定路径。</li>
</ul>
<!-- 
For example: on COS images, Docker exposes its Unix domain socket at
`/var/run/docker.sock` This means that the pod spec will include a
`hostPath` volume mount of `/var/run/docker.sock`.
-->
<p>举例来说：在 COS 镜像中，Docker 通过 <code>/var/run/docker.sock</code> 开放其 Unix 域套接字。
这意味着 Pod 的规约中需要包含 <code>hostPath</code> 卷以挂载 <code>/var/run/docker.sock</code>。</p>
<!-- 
Here's a sample shell script to find Pods that have a mount directly mapping the
Docker socket. This script outputs the namespace and name of the pod. You can
remove the grep `/var/run/docker.sock` to review other mounts.
-->
<p>下面是一个 shell 示例脚本，用于查找包含直接映射 Docker 套接字的挂载点的 Pod。
你也可以删掉 grep <code>/var/run/docker.sock</code> 这一代码片段以查看其它挂载信息。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get pods --all-namespaces <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>-o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{range .items[*]}{&#34;\n&#34;}{.metadata.namespace}{&#34;:\t&#34;}{.metadata.name}{&#34;:\t&#34;}{range .spec.volumes[*]}{.hostPath.path}{&#34;, &#34;}{end}{end}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>| sort <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>| grep <span style="color:#b44">&#39;/var/run/docker.sock&#39;</span>
</code></pre></div><!-- 
There are alternative ways for a pod to access Docker on the host. For instance, the parent
directory `/var/run` may be mounted instead of the full path (like in [this
example](https://gist.github.com/itaysk/7bc3e56d69c4d72a549286d98fd557dd)).
The script above only detects the most common uses.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 对于 Pod 来说，访问宿主机上的 Docker 还有其他方式。
例如，可以挂载 <code>/var/run</code> 的父目录而非其完整路径
（就像<a href="https://gist.github.com/itaysk/7bc3e56d69c4d72a549286d98fd557dd">这个例子</a>）。
上述脚本只检测最常见的使用方式。
</div>
<!-- 
### Detecting Docker dependency from node agents
-->
<h3 id="detecting-docker-dependency-from-node-agents">检测节点代理对 Docker 的依赖性</h3>
<!-- 
In case your cluster nodes are customized and install additional security and
telemetry agents on the node, make sure to check with the vendor of the agent whether it has dependency on Docker.
-->
<p>在你的集群节点被定制、且在各个节点上均安装了额外的安全和遥测代理的场景下，
一定要和代理的供应商确认：该代理是否依赖于 Docker。</p>
<!-- 
### Telemetry and security agent vendors
-->
<h3 id="telemetry-and-security-agent-vendors">遥测和安全代理的供应商</h3>
<!-- 
We keep the work in progress version of migration instructions for various telemetry and security agent vendors
in [Google doc](https://docs.google.com/document/d/1ZFi4uKit63ga5sxEiZblfb-c23lFhvy6RXVPikS8wf0/edit#).
Please contact the vendor to get up to date instructions for migrating from dockershim.
-->
<p>我们通过
<a href="https://docs.google.com/document/d/1ZFi4uKit63ga5sxEiZblfb-c23lFhvy6RXVPikS8wf0/edit#">谷歌文档</a>
提供了为各类遥测和安全代理供应商准备的持续更新的迁移指导。
请与供应商联系，获取从 dockershim 迁移的最新说明。</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8e16d69617b175d61e2e7a6e1642c9d6">2 - 用 kubeadm 进行管理</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-f62fba1de4084f3be070785757c8079c">2.1 - 使用 kubeadm 进行证书管理</h1>
    
	<!--
reviewers:
- sig-cluster-lifecycle
title: Certificate Management with kubeadm
content_type: task
weight: 10
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.15 [stable]</code>
</div>


<!-- 
Client certificates generated by [kubeadm](/docs/reference/setup-tools/kubeadm/) expire after 1 year. This page explains how to manage certificate renewals with kubeadm. 
-->
<p>由 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm</a> 生成的客户端证书在 1 年后到期。
本页说明如何使用 kubeadm 管理证书续订。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
You should be familiar with [PKI certificates and requirements in Kubernetes](/docs/setup/best-practices/certificates/).
-->
<p>你应该熟悉 <a href="/zh/docs/setup/best-practices/certificates/">Kubernetes 中的 PKI 证书和要求</a>。</p>
<!-- steps -->
<!--
## Using custom certificates {#custom-certificates}

By default, kubeadm generates all the certificates needed for a cluster to run.
You can override this behavior by providing your own certificates.
-->
<h2 id="custom-certificates">使用自定义的证书</h2>
<p>默认情况下, kubeadm 会生成运行一个集群所需的全部证书。
你可以通过提供你自己的证书来改变这个行为策略。</p>
<!--
To do so, you must place them in whatever directory is specified by the
`--cert-dir` flag or the `CertificatesDir`field of kubeadm's `ClusterConfiguration` . By default this
is `/etc/kubernetes/pki`.
-->
<p>如果要这样做, 你必须将证书文件放置在通过 <code>--cert-dir</code> 命令行参数或者 kubeadm 配置中的
<code>CertificatesDir</code> 配置项指明的目录中。默认的值是 <code>/etc/kubernetes/pki</code>。</p>
<!--
If a given certificate and private key pair exists before running `kubeadm init`,
kubeadm does not overwrite them. This means you can, for example, copy an existing
CA into `/etc/kubernetes/pki/ca.crt` and `/etc/kubernetes/pki/ca.key`,
and kubeadm will use this CA for signing the rest of the certificates.
-->
<p>如果在运行 <code>kubeadm init</code> 之前存在给定的证书和私钥对，kubeadm 将不会重写它们。
例如，这意味着您可以将现有的 CA 复制到 <code>/etc/kubernetes/pki/ca.crt</code> 和
<code>/etc/kubernetes/pki/ca.key</code> 中，而 kubeadm 将使用此 CA 对其余证书进行签名。</p>
<!--
## External CA mode {#external-ca-mode}

It is also possible to provide only the `ca.crt` file and not the
`ca.key` file (this is only available for the root CA file, not other cert pairs).
If all other certificates and kubeconfig files are in place, kubeadm recognizes
this condition and activates the "External CA" mode. kubeadm will proceed without the CA key on disk.
-->
<h2 id="external-ca-mode">外部 CA 模式</h2>
<p>只提供了 <code>ca.crt</code> 文件但是不提供 <code>ca.key</code> 文件也是可以的
（这只对 CA 根证书可用，其它证书不可用）。
如果所有的其它证书和 kubeconfig 文件已就绪，kubeadm 检测到满足以上条件就会激活
&quot;外部 CA&quot; 模式。kubeadm 将会在没有 CA 密钥文件的情况下继续执行。</p>
<!--
Instead, run the controller-manager standalone with `--controllers=csrsigner` and
point to the CA certificate and key.
-->
<p>否则, kubeadm 将独立运行 controller-manager，附加一个
<code>--controllers=csrsigner</code> 的参数，并且指明 CA 证书和密钥。</p>
<!--
[PKI certificates and requirements](/docs/setup/best-practices/certificates/) includes guidance on
setting up a cluster to use an external CA.
-->
<p><a href="/zh/docs/setup/best-practices/certificates/">PKI 证书和要求</a>包括集群使用外部 CA 的设置指南。</p>
<!-- 
## Check certificate expiration 

You can use the `check-expiration` subcommand to check when certificates expire:
-->
<h2 id="检查证书是否过期">检查证书是否过期</h2>
<p>你可以使用 <code>check-expiration</code> 子命令来检查证书何时过期</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm certs check-expiration
</code></pre></div><!-- 
The output is similar to this: 
-->
<p>输出类似于以下内容：</p>
<pre><code>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Dec 30, 2020 23:36 UTC   364d                                    no
apiserver                  Dec 30, 2020 23:36 UTC   364d            ca                      no
apiserver-etcd-client      Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
apiserver-kubelet-client   Dec 30, 2020 23:36 UTC   364d            ca                      no
controller-manager.conf    Dec 30, 2020 23:36 UTC   364d                                    no
etcd-healthcheck-client    Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
etcd-peer                  Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
etcd-server                Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
front-proxy-client         Dec 30, 2020 23:36 UTC   364d            front-proxy-ca          no
scheduler.conf             Dec 30, 2020 23:36 UTC   364d                                    no

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Dec 28, 2029 23:36 UTC   9y              no
etcd-ca                 Dec 28, 2029 23:36 UTC   9y              no
front-proxy-ca          Dec 28, 2029 23:36 UTC   9y              no
</code></pre><!-- 
The command shows expiration/residual time for the client certificates in the `/etc/kubernetes/pki` folder and for the client certificate embedded in the KUBECONFIG files used by kubeadm (`admin.conf`, `controller-manager.conf` and `scheduler.conf`). 
-->
<p>该命令显示 <code>/etc/kubernetes/pki</code> 文件夹中的客户端证书以及
kubeadm（<code>admin.conf</code>, <code>controller-manager.conf</code> 和 <code>scheduler.conf</code>）
使用的 KUBECONFIG 文件中嵌入的客户端证书的到期时间/剩余时间。</p>
<!-- 
Additionally, kubeadm informs the user if the certificate is externally managed; in this case, the user should take care of managing certificate renewal manually/using other tools. 
-->
<p>另外， kubeadm 会通知用户证书是否由外部管理；
在这种情况下，用户应该小心的手动/使用其他工具来管理证书更新。</p>
<!--
`kubeadm` cannot manage certificates signed by an external CA.
 -->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <code>kubeadm</code> 不能管理由外部 CA 签名的证书
</div>


<!-- 
`kubelet.conf` is not included in the list above because kubeadm configures kubelet
for [automatic certificate renewal](/docs/tasks/tls/certificate-rotation/)
with rotatable certificates under `/var/lib/kubelet/pki`.
To repair an expired kubelet client certificate see
[Kubelet client certificate rotation fails](/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#kubelet-client-cert).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 上面的列表中没有包含 <code>kubelet.conf</code>，因为 kubeadm 将 kubelet 配置为
<a href="/docs/tasks/tls/certificate-rotation/">自动更新证书</a>。
轮换的证书位于目录 <code>/var/lib/kubelet/pki</code>。
要修复过期的 kubelet 客户端证书，请参阅
<a href="/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#kubelet-client-cert">kubelet 客户端证书轮换失败</a>。
</div>
<!--
On nodes created with `kubeadm init`, prior to kubeadm version 1.17, there is a
[bug](https://github.com/kubernetes/kubeadm/issues/1753) where you manually have to modify the contents of `kubelet.conf`. After `kubeadm init` finishes, you should update `kubelet.conf` to point to the
rotated kubelet client certificates, by replacing `client-certificate-data` and `client-key-data` with:
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <p>在通过 <code>kubeadm init</code> 创建的节点上，在 kubeadm 1.17 版本之前有一个
<a href="https://github.com/kubernetes/kubeadm/issues/1753">缺陷</a>，该缺陷
使得你必须手动修改 <code>kubelet.conf</code> 文件的内容。
<code>kubeadm init</code> 操作结束之后，你必须更新 <code>kubelet.conf</code> 文件
将 <code>client-certificate-data</code> 和 <code>client-key-data</code> 改为如下所示的内容
以便使用轮换后的 kubelet 客户端证书：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">client-certificate</span>:<span style="color:#bbb"> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">client-key</span>:<span style="color:#bbb"> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style="color:#bbb">
</span></code></pre></div>
</div>


<!-- 
## Automatic certificate renewal

`kubeadm` renews all the certificates during control plane [upgrade](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15/). 
-->
<h2 id="自动更新证书">自动更新证书</h2>
<p><code>kubeadm</code> 会在控制面
<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">升级</a>
的时候更新所有证书。</p>
<!-- 
This feature is designed for addressing the simplest use cases; 
if you don't have specific requirements on certificate renewal and perform Kubernetes version upgrades regularly (less than 1 year in between each upgrade), kubeadm will take care of keeping your cluster up to date and reasonably secure. 
-->
<p>这个功能旨在解决最简单的用例；如果你对此类证书的更新没有特殊要求，
并且定期执行 Kubernetes 版本升级（每次升级之间的间隔时间少于 1 年），
则 kubeadm 将确保你的集群保持最新状态并保持合理的安全性。</p>
<!-- 
It is a best practice to upgrade your cluster frequently in order to stay secure.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 最佳的做法是经常升级集群以确保安全。
</div>
<!-- 
If you have more complex requirements for certificate renewal, you can opt out from the default behavior by passing `--certificate-renewal=false` to `kubeadm upgrade apply` or to `kubeadm upgrade node`. 
-->
<p>如果你对证书更新有更复杂的需求，则可通过将 <code>--certificate-renewal=false</code> 传递给
<code>kubeadm upgrade apply</code> 或者 <code>kubeadm upgrade node</code>，从而选择不采用默认行为。</p>
<!--
Prior to kubeadm version 1.17 there is a [bug](https://github.com/kubernetes/kubeadm/issues/1818)
where the default value for `--certificate-renewal` is `false` for the `kubeadm upgrade node`
command. In that case, you should explicitly set `--certificate-renewal=true`.
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> kubeadm 在 1.17 版本之前有一个<a href="https://github.com/kubernetes/kubeadm/issues/1818">缺陷</a>，
该缺陷导致 <code>kubeadm update node</code> 执行时 <code>--certificate-renewal</code> 的默认值被设置为 <code>false</code>。
在这种情况下，你需要显式地设置 <code>--certificate-renewal=true</code>。
</div>


<!-- 
## Manual certificate renewal 

You can renew your certificates manually at any time with the `kubeadm certs renew` command. 
-->
<h2 id="手动更新证书">手动更新证书</h2>
<p>你能随时通过 <code>kubeadm certs renew</code> 命令手动更新你的证书。</p>
<!-- 
This command performs the renewal using CA (or front-proxy-CA) certificate and key stored in `/etc/kubernetes/pki`.

After running the command you should restart the control plane Pods. This is required since
dynamic certificate reload is currently not supported for all components and certificates.
[Static Pods](/docs/tasks/configure-pod-container/static-pod/) are managed by the local kubelet
and not by the API Server, thus kubectl cannot be used to delete and restart them.
To restart a static Pod you can temporarily remove its manifest file from `/etc/kubernetes/manifests/`
and wait for 20 seconds (see the `fileCheckFrequency` value in [KubeletConfiguration struct](/docs/reference/config-api/kubelet-config.v1beta1/).
The kubelet will terminate the Pod if it's no longer in the manifest directory.
You can then move the file back and after another `fileCheckFrequency` period, the kubelet will recreate
the Pod and the certificate renewal for the component can complete.
-->
<p>此命令用 CA （或者 front-proxy-CA ）证书和存储在 <code>/etc/kubernetes/pki</code> 中的密钥执行更新。</p>
<p>执行完此命令之后你需要重启控制面 Pods。因为动态证书重载目前还不被所有组件和证书支持，所有这项操作是必须的。
<a href="/zh/docs/tasks/configure-pod-container/static-pod/">静态 Pods</a> 是被本地 kubelet 而不是 API Server 管理，
所以 kubectl 不能用来删除或重启他们。
要重启静态 Pod 你可以临时将清单文件从 <code>/etc/kubernetes/manifests/</code> 移除并等待 20 秒
（参考 <a href="/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration 结构</a> 中的<code>fileCheckFrequency</code> 值）。
如果 Pod 不在清单目录里，kubelet将会终止它。
在另一个 <code>fileCheckFrequency</code> 周期之后你可以将文件移回去，为了组件可以完成 kubelet 将重新创建 Pod 和证书更新。</p>
<!-- 
If you are running an HA cluster, this command needs to be executed on all the control-plane nodes. 
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 如果你运行了一个 HA 集群，这个命令需要在所有控制面板节点上执行。
</div>


<!-- 
` certs renew` uses the existing certificates as the authoritative source for attributes (Common Name, Organization, SAN, etc.) instead of the kubeadm-config ConfigMap. It is strongly recommended to keep them both in sync.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <code>certs renew</code> 使用现有的证书作为属性 (Common Name、Organization、SAN 等) 的权威来源，
而不是 kubeadm-config ConfigMap 。强烈建议使它们保持同步。
</div>
<!--
`kubeadm certs renew` provides the following options:
-->
<p><code>kubeadm certs renew</code>提供以下选项：</p>
<!--
The Kubernetes certificates normally reach their expiration date after one year.
-->
<p>Kubernetes 证书通常在一年后到期。</p>
<!-- 

- `--csr-only` can be used to renew certificats with an external CA by generating certificate signing requests (without actually renewing certificates in place); see next paragraph for more information. 
- It's also possible to renew a single certificate instead of all.
 -->
<ul>
<li><code>--csr-only</code> 可用于经过一个外部 CA 生成的证书签名请求来更新证书（无需实际替换更新证书）；
更多信息请参见下节。</li>
<li>可以更新单个证书而不是全部证书。</li>
</ul>
<!--
## Renew certificates with the Kubernetes certificates API

This section provide more details about how to execute manual certificate renewal using the Kubernetes certificates API.
-->
<h2 id="用-kubernetes-证书-api-更新证书">用 Kubernetes 证书 API 更新证书</h2>
<p>本节提供有关如何使用 Kubernetes 证书 API 执行手动证书更新的更多详细信息。</p>
<!-- 
These are advanced topics for users who need to integrate their organization's certificate infrastructure into a kubeadm-built cluster. If the default kubeadm configuration satisfies your needs, you should let kubeadm manage certificates instead. 
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 这些是针对需要将其组织的证书基础结构集成到 kubeadm 构建的集群中的用户的高级主题。
如果默认的 kubeadm 配置满足了你的需求，则应让 kubeadm 管理证书。
</div>

<!--
### Set up a signer

The Kubernetes Certificate Authority does not work out of the box.
You can configure an external signer such as [cert-manager](https://cert-manager.io/docs/configuration/ca/), or you can use the build-in signer.
The built-in signer is part of [`kube-controller-manager`](/docs/reference/command-line-tools-reference/kube-controller-manager/).
To activate the build-in signer, you must pass the `--cluster-signing-cert-file` and `--cluster-signing-key-file` flags.
-->
<h3 id="设置一个签名者-signer">设置一个签名者（Signer）</h3>
<p>Kubernetes 证书颁发机构不是开箱即用。
你可以配置外部签名者，例如
<a href="https://cert-manager.io/docs/configuration/ca/">cert-manager</a>，
也可以使用内置签名者。
内置签名者是
<a href="/zh/docs/reference/command-line-tools-reference/kube-controller-manager/"><code>kube-controller-manager</code></a>
的一部分。
要激活内置签名者，请传递 <code>--cluster-signing-cert-file</code> 和 <code>--cluster-signing-key-file</code> 参数。</p>
<!--
If you're creating a new cluster, you can use a kubeadm [configuration file](https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta3): 
-->
<p>如果你正在创建一个新的集群，你可以使用 kubeadm 的
<a href="/docs/reference/config-api/kubeadm-config.v1beta3/">配置文件</a>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">controllerManager</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">extraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster-signing-cert-file</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki/ca.crt<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster-signing-key-file</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki/ca.key<span style="color:#bbb">
</span></code></pre></div><!-- 
### Create certificate signing requests (CSR)
-->
<h3 id="创建证书签名请求-csr">创建证书签名请求 (CSR)</h3>
<!--
See [Create CertificateSigningRequest](/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatesigningrequest) for creating CSRs with the Kubernetes API.
-->
<p>有关使用 Kubernetes API 创建 CSR 的信息，
请参见<a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatesigningrequest">创建 CertificateSigningRequest</a>。</p>
<!--
## Renew certificates with external CA

This section provides more details about how to execute manual certificate renewal using an external CA.
-->
<h2 id="通过外部-ca-更新证书">通过外部 CA 更新证书</h2>
<p>本节提供有关如何使用外部 CA 执行手动更新证书的更多详细信息。</p>
<!--
To better integrate with external CAs, kubeadm can also produce certificate signing requests (CSRs).
A CSR represents a request to a CA for a signed certificate for a client.
In kubeadm terms, any certificate that would normally be signed by an on-disk CA can be produced as a CSR instead. A CA, however, cannot be produced as a CSR.
-->
<p>为了更好的与外部 CA 集成，kubeadm 还可以生成证书签名请求（CSR）。
CSR 表示向 CA 请求客户的签名证书。
在 kubeadm 术语中，通常由磁盘 CA 签名的任何证书都可以作为 CSR 生成。但是，CA 不能作为 CSR 生成。</p>
<!-- 
### Create certificate signing requests (CSR) 

You can create certificate signing requests with `kubeadm certs renew --csr-only`.

Both the CSR and the accompanying private key are given in the output.
You can pass in a directory with `--csr-dir` to output the CSRs to the specified location.
If `--csr-dir` is not specified, the default certificate directory (`/etc/kubernetes/pki`) is used.
-->
<h3 id="创建证书签名请求-csr-1">创建证书签名请求 (CSR)</h3>
<p>你可以通过 <code>kubeadm certs renew --csr-only</code> 命令创建证书签名请求。</p>
<p>CSR 和随附的私钥都在输出中给出。
你可以传入一个带有 <code>--csr-dir</code> 的目录，将 CRS 输出到指定位置。
如果未指定 <code>--csr-dir</code> ，则使用默认证书目录（<code>/etc/kubernetes/pki</code>）。</p>
<!--
Certificates can be renewed with `kubeadm certs renew --csr-only`.
As with `kubeadm init`, an output directory can be specified with the `--csr-dir` flag.
-->
<p>证书可以通过 <code>kubeadm certs renew --csr-only</code> 来续订。
和 <code>kubeadm init</code> 一样，可以使用 <code>--csr-dir</code> 标志指定一个输出目录。</p>
<p>CSR 签署证书后，必须将证书和私钥复制到 PKI 目录（默认情况下为 <code>/etc/kubernetes/pki</code>）。</p>
<!--
A CSR contains a certificate's name, domains, and IPs, but it does not specify usages.
It is the responsibility of the CA to specify [the correct cert usages](/docs/setup/best-practices/certificates/#all-certificates)
when issuing a certificate.
-->
<p>CSR 中包含一个证书的名字，域和 IP，但是未指定用法。
颁发证书时，CA 有责任指定<a href="/zh/docs/setup/best-practices/certificates/#all-certificates">正确的证书用法</a></p>
<!-- 
* In `openssl` this is done with the
  [`openssl ca` command](https://superuser.com/questions/738612/openssl-ca-keyusage-extension).
* In `cfssl` you specify
  [usages in the config file](https://github.com/cloudflare/cfssl/blob/master/doc/cmd/cfssl.txt#L170).
-->
<ul>
<li>在 <code>openssl</code> 中，这是通过
<a href="https://superuser.com/questions/738612/openssl-ca-keyusage-extension"><code>openssl ca</code> 命令</a>
来完成的。</li>
<li>在 <code>cfssl</code> 中，这是通过
<a href="https://github.com/cloudflare/cfssl/blob/master/doc/cmd/cfssl.txt#L170">在配置文件中指定用法</a>
来完成的。</li>
</ul>
<!-- 
After a certificate is signed using your preferred method, the certificate and the private key must be copied to the PKI directory (by default `/etc/kubernetes/pki`). 
-->
<p>使用首选方法对证书签名后，必须将证书和私钥复制到 PKI 目录（默认为 <code>/etc/kubernetes/pki</code> ）。</p>
<!--
## Certificate authority (CA) rotation {#certificate-authority-rotation}

Kubeadm does not support rotation or replacement of CA certificates out of the box.

For more information about manual rotation or replacement of CA, see [manual rotation of CA certificates](/docs/tasks/tls/manual-rotation-of-ca-certificates/).
-->
<h2 id="certificate-authority-rotation">证书机构（CA）轮换    </h2>
<p>kubeadm 并不直接支持对 CA 证书的轮换或者替换。</p>
<p>关于手动轮换或者置换 CA 的更多信息，可参阅
<a href="/zh/docs/tasks/tls/manual-rotation-of-ca-certificates/">手动轮换 CA 证书</a>。</p>
<!--
## Enabling signed kubelet serving certificates {#kubelet-serving-certs}

By default the kubelet serving certificate deployed by kubeadm is self-signed.
This means a connection from external services like the
[metrics-server](https://github.com/kubernetes-sigs/metrics-server) to a
kubelet cannot be secured with TLS.

To configure the kubelets in a new kubeadm cluster to obtain properly signed serving
certificates you must pass the following minimal configuration to `kubeadm init`:
-->
<h2 id="kubelet-serving-certs">启用已签名的 kubelet 服务证书  </h2>
<p>默认情况下，kubeadm 所部署的 kubelet 服务证书是自签名（Self-Signed））。
这意味着从 <a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a>
这类外部服务发起向 kubelet 的链接时无法使用 TLS 来完成保护。</p>
<p>要在新的 kubeadm 集群中配置 kubelet 以使用被正确签名的服务证书，
你必须向 <code>kubeadm init</code> 传递如下最小配置数据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">serverTLSBootstrap</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div><!--
If you have already created the cluster you must adapt it by doing the following:
 - Find and edit the `kubelet-config-1.23` ConfigMap in the `kube-system` namespace.
In that ConfigMap, the `kubelet` key has a
[KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration)
document as its value. Edit the KubeletConfiguration document to set `serverTLSBootstrap: true`.
- On each node, add the `serverTLSBootstrap: true` field in `/var/lib/kubelet/config.yaml`
and restart the kubelet with `systemctl restart kubelet`
-->
<p>如果你已经创建了集群，你必须通过执行下面的操作来完成适配：</p>
<ul>
<li>找到 <code>kube-system</code> 名字空间中名为 <code>kubelet-config-1.23</code>
的 ConfigMap 并编辑之。
在该 ConfigMap 中，<code>kubelet</code> 键下面有一个
<a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration">KubeletConfiguration</a>
文档作为其取值。编辑该 KubeletConfiguration 文档以设置
<code>serverTLSBootstrap: true</code>。</li>
<li>在每个节点上，在 <code>/var/lib/kubelet/config.yaml</code> 文件中添加
<code>serverTLSBootstrap: true</code> 字段，并使用 <code>systemctl restart kubelet</code>
来重启 kubelet。</li>
</ul>
<!--
The field `serverTLSBootstrap: true` will enable the bootstrap of kubelet serving
certificates by requesting them from the `certificates.k8s.io` API. One known limitation
is that the CSRs (Certificate Signing Requests) for these certificates cannot be automatically
approved by the default signer in the kube-controller-manager -
[`kubernetes.io/kubelet-serving`](/docs/reference/access-authn-authz/certificate-signing-requests/#kubernetes-signers).
This will require action from the user or a third party controller.

These CSRs can be viewed using:
-->
<p>字段 <code>serverTLSBootstrap</code> 将允许启动引导 kubelet 的服务证书，方式
是从 <code>certificates.k8s.io</code> API 处读取。这种方式的一种局限在于这些
证书的 CSR（证书签名请求）不能被 kube-controller-manager 中默认的
签名组件
<a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/#kubernetes-signers"><code>kubernetes.io/kubelet-serving</code></a>
批准。需要用户或者第三方控制器来执行此操作。</p>
<p>可以使用下面的命令来查看 CSR：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get csr
</code></pre></div><pre><code class="language-none" data-lang="none">NAME        AGE     SIGNERNAME                        REQUESTOR                      CONDITION
csr-9wvgt   112s    kubernetes.io/kubelet-serving     system:node:worker-1           Pending
csr-lz97v   1m58s   kubernetes.io/kubelet-serving     system:node:control-plane-1    Pending
</code></pre><!--
To approve them you can do the following:
-->
<p>你可以执行下面的操作来批准这些请求：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl certificate approve &lt;CSR-名称&gt;
</code></pre></div><!--
By default, these serving certificate will expire after one year. Kubeadm sets the
`KubeletConfiguration` field `rotateCertificates` to `true`, which means that close
to expiration a new set of CSRs for the serving certificates will be created and must
be approved to complete the rotation. To understand more see
[Certificate Rotation](/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#certificate-rotation).
-->
<p>默认情况下，这些服务证书上会在一年后过期。
kubeadm 将 <code>KubeletConfiguration</code> 的 <code>rotateCertificates</code> 字段设置为
<code>true</code>；这意味着证书快要过期时，会生成一组针对服务证书的新的 CSR，而
这些 CSR 也要被批准才能完成证书轮换。
要进一步了解这里的细节，可参阅
<a href="/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#certificate-rotation">证书轮换</a>
文档。</p>
<!--
If you are looking for a solution for automatic approval of these CSRs it is recommended
that you contact your cloud provider and ask if they have a CSR signer that verifies
the node identity with an out of band mechanism.
-->
<p>如果你在寻找一种能够自动批准这些 CSR 的解决方案，建议你与你的云提供商
联系，询问他们是否有 CSR 签名组件，用来以带外（out-of-band）的方式检查
节点的标识符。</p>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<!--
Third party custom controllers can be used:
- [kubelet-rubber-stamp](https://github.com/kontena/kubelet-rubber-stamp)

Such a controller is not a secure mechanism unless it not only verifies the CommonName
in the CSR but also verifies the requested IPs and domain names. This would prevent
a malicious actor that has access to a kubelet client certificate to create
CSRs requesting serving certificates for any IP or domain name.
-->
<p>也可以使用第三方定制的控制器：</p>
<ul>
<li><a href="https://github.com/kontena/kubelet-rubber-stamp">kubelet-rubber-stamp</a></li>
</ul>
<p>除非既能够验证 CSR 中的 CommonName，也能检查请求的 IP 和域名，
这类控制器还算不得安全的机制。
只有完成彻底的检查，才有可能避免有恶意的、能够访问 kubelet 客户端证书的第三方
为任何 IP 或域名请求服务证书。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-6134c5061298affa145ddb801b5c29da">2.2 - 配置 cgroup 驱动</h1>
    
	<!-- 
---
title: Configuring a cgroup driver
content_type: task
weight: 10
---
-->
<!-- overview -->
<!-- 
This page explains how to configure the kubelet cgroup driver to match the container
runtime cgroup driver for kubeadm clusters.
-->
<p>本页阐述如何配置 kubelet 的 cgroup 驱动以匹配 kubeadm 集群中的容器运行时的 cgroup 驱动。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!-- 
You should be familiar with the Kubernetes
[container runtime requirements](/docs/setup/production-environment/container-runtimes).
-->
<p>你应该熟悉 Kubernetes 的<a href="/zh/docs/setup/production-environment/container-runtimes">容器运行时需求</a>。</p>
<!-- steps -->
<!-- 
## Configuring the container runtime cgroup driver
-->
<h2 id="configuring-the-container-runtime-cgroup-driver">配置容器运行时 cgroup 驱动</h2>
<!-- 
The [Container runtimes](/docs/setup/production-environment/container-runtimes) page
explains that the `systemd` driver is recommended for kubeadm based setups instead
of the `cgroupfs` driver, because kubeadm manages the kubelet as a systemd service.
-->
<p><a href="/zh/docs/setup/production-environment/container-runtimes">容器运行时</a>页面提到：
由于 kubeadm 把 kubelet 视为一个系统服务来管理，所以对基于 kubeadm 的安装，
我们推荐使用 <code>systemd</code> 驱动，不推荐 <code>cgroupfs</code> 驱动。</p>
<!-- 
The page also provides details on how to setup a number of different container runtimes with the
`systemd` driver by default.
-->
<p>此页还详述了如何安装若干不同的容器运行时，并将 <code>systemd</code> 设为其默认驱动。</p>
<!-- 
## Configuring the kubelet cgroup driver
-->
<h2 id="配置-kubelet-的-cgroup-驱动">配置 kubelet 的 cgroup 驱动</h2>
<!-- 
kubeadm allows you to pass a `KubeletConfiguration` structure during `kubeadm init`.
This `KubeletConfiguration` can include the `cgroupDriver` field which controls the cgroup
driver of the kubelet.
-->
<p>kubeadm 支持在执行 <code>kubeadm init</code> 时，传递一个 <code>KubeletConfiguration</code> 结构体。
<code>KubeletConfiguration</code> 包含 <code>cgroupDriver</code> 字段，可用于控制 kubelet 的 cgroup 驱动。</p>
<!-- 
In v1.22, if the user is not setting the `cgroupDriver` field under `KubeletConfiguration`,
`kubeadm init` will default it to `systemd`.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在版本 1.22 中，如果用户没有在 <code>KubeletConfiguration</code> 中设置 <code>cgroupDriver</code> 字段，
<code>kubeadm init</code> 会将它设置为默认值 <code>systemd</code>。
</div>
<!-- 
A minimal example of configuring the field explicitly:
-->
<p>这是一个最小化的示例，其中显式的配置了此字段：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># kubeadm-config.yaml</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kubernetesVersion</span>:<span style="color:#bbb"> </span>v1.21.0<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">cgroupDriver</span>:<span style="color:#bbb"> </span>systemd<span style="color:#bbb">
</span></code></pre></div><!-- 
Such a configuration file can then be passed to the kubeadm command:
-->
<p>这样一个配置文件就可以传递给 kubeadm 命令了：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm init --config kubeadm-config.yaml
</code></pre></div><!-- 
Kubeadm uses the same `KubeletConfiguration` for all nodes in the cluster.
The `KubeletConfiguration` is stored in a [ConfigMap](/docs/concepts/configuration/configmap)
object under the `kube-system` namespace.

Executing the sub commands `init`, `join` and `upgrade` would result in kubeadm
writing the `KubeletConfiguration` as a file under `/var/lib/kubelet/config.yaml`
and passing it to the local node kubelet.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>Kubeadm 对集群所有的节点，使用相同的 <code>KubeletConfiguration</code>。
<code>KubeletConfiguration</code> 存放于 <code>kube-system</code> 命名空间下的某个
<a href="/zh/docs/concepts/configuration/configmap">ConfigMap</a> 对象中。</p>
<p>执行 <code>init</code>、<code>join</code> 和 <code>upgrade</code> 等子命令会促使 kubeadm
将 <code>KubeletConfiguration</code> 写入到文件 <code>/var/lib/kubelet/config.yaml</code> 中，
继而把它传递给本地节点的 kubelet。</p>

</div>
<!-- 
## Using the `cgroupfs` driver
-->
<h1 id="使用-cgroupfs-驱动">使用 <code>cgroupfs</code> 驱动</h1>
<!-- 
As this guide explains using the `cgroupfs` driver with kubeadm is not recommended.

To continue using `cgroupfs` and to prevent `kubeadm upgrade` from modifying the
`KubeletConfiguration` cgroup driver on existing setups, you must be explicit
about its value. This applies to a case where you do not wish future versions
of kubeadm to apply the `systemd` driver by default.
-->
<p>正如本指南阐述的：不推荐与 kubeadm 一起使用 <code>cgroupfs</code> 驱动。</p>
<p>如仍需使用 <code>cgroupfs</code>，
且要防止 <code>kubeadm upgrade</code> 修改现有系统中 <code>KubeletConfiguration</code> 的 cgroup 驱动，
你必须显式声明它的值。
此方法应对的场景为：在将来某个版本的 kubeadm 中，你不想使用默认的 <code>systemd</code> 驱动。</p>
<!-- 
See the below section on "Modify the kubelet ConfigMap" for details on
how to be explicit about the value.

If you wish to configure a container runtime to use the `cgroupfs` driver,
you must refer to the documentation of the container runtime of your choice.
-->
<p>参阅以下章节“修改 kubelet 的 ConfigMap”，了解显式设置该值的方法。</p>
<p>如果你希望配置容器运行时来使用 <code>cgroupfs</code> 驱动，
则必须参考所选容器运行时的文档。</p>
<!-- 
## Migrating to the `systemd` driver
-->
<h2 id="迁移到-systemd-驱动">迁移到 <code>systemd</code> 驱动</h2>
<!-- 
To change the cgroup driver of an existing kubeadm cluster to `systemd` in-place,
a similar procedure to a kubelet upgrade is required. This must include both
steps outlined below.
-->
<p>要将现有 kubeadm 集群的 cgroup 驱动就地升级为 <code>systemd</code>，
需要执行一个与 kubelet 升级类似的过程。
该过程必须包含下面两个步骤：</p>
<!-- 
Alternatively, it is possible to replace the old nodes in the cluster with new ones
that use the `systemd` driver. This requires executing only the first step below
before joining the new nodes and ensuring the workloads can safely move to the new
nodes before deleting the old nodes.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 还有一种方法，可以用已配置了 <code>systemd</code> 的新节点替换掉集群中的老节点。
按这种方法，在加入新节点、确保工作负载可以安全迁移到新节点、及至删除旧节点这一系列操作之前，
只需执行以下第一个步骤。
</div>
<!-- 
### Modify the kubelet ConfigMap
-->
<h3 id="修改-kubelet-的-configmap">修改 kubelet 的 ConfigMap</h3>
<!-- 
- Find the kubelet ConfigMap name using `kubectl get cm -n kube-system | grep kubelet-config`.
- Call `kubectl edit cm kubelet-config-x.yy -n kube-system` (replace `x.yy` with
the Kubernetes version).
- Either modify the existing `cgroupDriver` value or add a new field that looks like this:
-->
<ul>
<li>
<p>用命令 <code>kubectl get cm -n kube-system | grep kubelet-config</code> 找到 kubelet 的 ConfigMap 名称。</p>
</li>
<li>
<p>运行 <code>kubectl edit cm kubelet-config-x.yy -n kube-system</code> （把 <code>x.yy</code> 替换为 Kubernetes 版本）。</p>
</li>
<li>
<p>修改现有 <code>cgroupDriver</code> 的值，或者新增如下式样的字段：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">cgroupDriver</span>:<span style="color:#bbb"> </span>systemd<span style="color:#bbb">
</span></code></pre></div><!-- 
This field must be present under the `kubelet:` section of the ConfigMap.
-->
<p>该字段必须出现在 ConfigMap 的 <code>kubelet:</code> 小节下。</p>
</li>
</ul>
<!-- 
### Update the cgroup driver on all nodes
-->
<h3 id="更新所有节点的-cgroup-驱动">更新所有节点的 cgroup 驱动</h3>
<!-- 
For each node in the cluster:

- [Drain the node](/docs/tasks/administer-cluster/safely-drain-node) using `kubectl drain <node-name> --ignore-daemonsets`
- Stop the kubelet using `systemctl stop kubelet`
- Stop the container runtime
- Modify the container runtime cgroup driver to `systemd`
- Set `cgroupDriver: systemd` in `/var/lib/kubelet/config.yaml`
- Start the container runtime
- Start the kubelet using `systemctl start kubelet`
- [Uncordon the node](/docs/tasks/administer-cluster/safely-drain-node) using `kubectl uncordon <node-name>`
-->
<p>对于集群中的每一个节点：</p>
<ul>
<li>执行命令 <code>kubectl drain &lt;node-name&gt; --ignore-daemonsets</code>，以
<a href="/zh/docs/tasks/administer-cluster/safely-drain-node">腾空节点</a></li>
<li>执行命令 <code>systemctl stop kubelet</code>，以停止 kubelet</li>
<li>停止容器运行时</li>
<li>修改容器运行时 cgroup 驱动为 <code>systemd</code></li>
<li>在文件 <code>/var/lib/kubelet/config.yaml</code> 中添加设置 <code>cgroupDriver: systemd</code></li>
<li>启动容器运行时</li>
<li>执行命令 <code>systemctl start kubelet</code>，以启动 kubelet</li>
<li>执行命令 <code>kubectl uncordon &lt;node-name&gt;</code>，以
<a href="/zh/docs/tasks/administer-cluster/safely-drain-node">取消节点隔离</a></li>
</ul>
<!-- 
Execute these steps on nodes one at a time to ensure workloads
have sufficient time to schedule on different nodes.

Once the process is complete ensure that all nodes and workloads are healthy.
-->
<p>在节点上依次执行上述步骤，确保工作负载有充足的时间被调度到其他节点。</p>
<p>流程完成后，确认所有节点和工作负载均健康如常。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-98530eb3653d28fef34bff4543364aa7">2.3 - 重新配置 kubeadm 集群</h1>
    
	<!--
reviewers:
- sig-cluster-lifecycle
title: Reconfiguring a kubeadm cluster
content_type: task
weight: 10
-->
<!-- overview -->
<!--
kubeadm does not support automated ways of reconfiguring components that
were deployed on managed nodes. One way of automating this would be
by using a custom [operator](/docs/concepts/extend-kubernetes/operator/).
-->
<p>kubeadm 不支持自动重新配置部署在托管节点上的组件的方式。
一种自动化的方法是使用自定义的
<a href="/zh/docs/concepts/extend-kubernetes/operator/">operator</a>。</p>
<!--
To modify the components configuration you must manually edit associated cluster
objects and files on disk.

This guide shows the correct sequence of steps that need to be performed
to achieve kubeadm cluster reconfiguration.
-->
<p>要修改组件配置，你必须手动编辑磁盘上关联的集群对象和文件。
本指南展示了实现 kubeadm 集群重新配置所需执行的正确步骤顺序。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
- You need a cluster that was deployed using kubeadm
- Have administrator credentials (`/etc/kubernetes/admin.conf`) and network connectivity
to a running kube-apiserver in the cluster from a host that has kubectl installed
- Have a text editor installed on all hosts
-->
<ul>
<li>你需要一个使用 kubeadm 部署的集群</li>
<li>拥有管理员凭据（<code>/etc/kubernetes/admin.conf</code>）
和从安装了 kubectl 的主机到集群中正在运行的 kube-apiserver 的网络连接</li>
<li>在所有主机上安装文本编辑器</li>
</ul>
<!-- steps -->
<!--
## Reconfiguring the cluster
kubeadm writes a set of cluster wide component configuration options in
ConfigMaps and other objects. These objects must be manually edited. The command `kubectl edit`
can be used for that.
-->
<h2 id="重新配置集群">重新配置集群</h2>
<p>kubeadm 在 ConfigMap 和其他对象中写入了一组集群范围的组件配置选项。
这些对象必须手动编辑，可以使用命令 <code>kubectl edit</code>。</p>
<!--
The `kubectl edit` command will open a text editor where you can edit and save the object directly.

You can use the environment variables `KUBECONFIG` and `KUBE_EDITOR` to specify the location of
the kubectl consumed kubeconfig file and preferred text editor.

For example:
-->
<p><code>kubectl edit</code> 命令将打开一个文本编辑器，你可以在其中直接编辑和保存对象。
你可以使用环境变量 <code>KUBECONFIG</code> 和 <code>KUBE_EDITOR</code> 来指定 kubectl
使用的 kubeconfig 文件和首选文本编辑器的位置。</p>
<p>例如：</p>
<pre><code>KUBECONFIG=/etc/kubernetes/admin.conf KUBE_EDITOR=nano kubectl edit &lt;parameters&gt;
</code></pre><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Upon saving any changes to these cluster objects, components running on nodes may not be
automatically updated. The steps below instruct you on how to perform that manually.
-->
<p>保存对这些集群对象的任何更改后，节点上运行的组件可能不会自动更新。
以下步骤将指导你如何手动执行该操作。
</div>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
Component configuration in ConfigMaps is stored as unstructured data (YAML string).
This means that validation will not be performed upon updating the contents of a ConfigMap.
You have to be careful to follow the documented API format for a particular
component configuration and avoid introducing typos and YAML indentation mistakes.
-->
<p>ConfigMaps 中的组件配置存储为非结构化数据（YAML 字符串）。 这意味着在更新
ConfigMap 的内容时不会执行验证。 你必须小心遵循特定组件配置的文档化 API 格式，
并避免引入拼写错误和 YAML 缩进错误。
</div>


<!--
### Applying cluster configuration changes

#### Updating the `ClusterConfiguration`

During cluster creation and upgrade, kubeadm writes its
[`ClusterConfiguration`](/docs/reference/config-api/kubeadm-config.v1beta3/)
in a ConfigMap called `kubeadm-config` in the `kube-system` namespace.

To change a particular option in the `ClusterConfiguration` you can edit the ConfigMap with this command:

The configuration is located under the `data.ClusterConfiguration` key.
-->
<h3 id="应用集群配置更改">应用集群配置更改</h3>
<h4 id="更新-clusterconfiguration">更新 <code>ClusterConfiguration</code></h4>
<p>在集群创建和升级期间，kubeadm 将其
<a href="/zh/docs/reference/config-api/kubeadm-config.v1beta3/"><code>ClusterConfiguration</code></a>
写入 <code>kube-system</code> 命名空间中名为 <code>kubeadm-config</code> 的 ConfigMap。</p>
<p>要更改 <code>ClusterConfiguration</code> 中的特定选项，你可以使用以下命令编辑 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit cm -n kube-system kubeadm-config
</code></pre></div><p>配置位于 <code>data.ClusterConfiguration</code> 键下。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The `ClusterConfiguration` includes a variety of options that affect the configuration of individual
components such as kube-apiserver, kube-scheduler, kube-controller-manager, CoreDNS, etcd and kube-proxy.
Changes to the configuration must be reflected on node components manually.
-->
<p><code>ClusterConfiguration</code> 包括各种影响单个组件配置的选项， 例如
kube-apiserver、kube-scheduler、kube-controller-manager、
CoreDNS、etcd 和 kube-proxy。 对配置的更改必须手动反映在节点组件上。
</div>
<!--
#### Reflecting `ClusterConfiguration` changes on control plane nodes

kubeadm manages the control plane components as static Pod manifests located in
the directory `/etc/kubernetes/manifests`.
Any changes to the `ClusterConfiguration` under the `apiServer`, `controllerManager`, `scheduler` or `etcd`
keys must be reflected in the associated files in the manifests directory on a control plane node.
-->
<h4 id="在控制平面节点上反映-clusterconfiguration-更改">在控制平面节点上反映 <code>ClusterConfiguration</code> 更改</h4>
<p>kubeadm 将控制平面组件作为位于 <code>/etc/kubernetes/manifests</code>
目录中的静态 Pod 清单进行管理。
对 <code>apiServer</code>、<code>controllerManager</code>、<code>scheduler</code> 或 <code>etcd</code>键下的
<code>ClusterConfiguration</code> 的任何更改都必须反映在控制平面节点上清单目录中的关联文件中。</p>
<!--
Such changes may include:
- `extraArgs` - requires updating the list of flags passed to a component container
- `extraMounts` - requires updated the volume mounts for a component container
- `*SANs` - requires writing new certificates with updated Subject Alternative Names.

Before proceeding with these changes, make sure you have backed up the directory `/etc/kubernetes/`.
-->
<p>此类更改可能包括:</p>
<ul>
<li><code>extraArgs</code> - 需要更新传递给组件容器的标志列表</li>
<li><code>extraMounts</code> - 需要更新组件容器的卷挂载</li>
<li><code>*SANs</code> - 需要使用更新的主题备用名称编写新证书</li>
</ul>
<p>在继续进行这些更改之前，请确保你已备份目录 <code>/etc/kubernetes/</code>。</p>
<!--
To write new certificates you can use:

To write new manifest files in `/etc/kubernetes/manifests` you can use:
-->
<p>要编写新证书，你可以使用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm init phase certs &lt;component-name&gt; --config &lt;config-file&gt;
</code></pre></div><p>要在 <code>/etc/kubernetes/manifests</code> 中编写新的清单文件，你可以使用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm init phase control-plane &lt;component-name&gt; --config &lt;config-file&gt;
</code></pre></div><!--
The `<config-file>` contents must match the updated `ClusterConfiguration`.
The `<component-name>` value must be the name of the component.
-->
<p><code>&lt;config-file&gt;</code> 内容必须与更新后的 <code>ClusterConfiguration</code> 匹配。
<code>&lt;component-name&gt;</code> 值必须是组件的名称。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Updating a file in `/etc/kubernetes/manifests` will tell the kubelet to restart the static Pod for the corresponding component.
Try doing these changes one node at a time to leave the cluster without downtime.
-->
<p>更新 <code>/etc/kubernetes/manifests</code> 中的文件将告诉 kubelet 重新启动相应组件的静态 Pod。
尝试一次对一个节点进行这些更改，以在不停机的情况下离开集群。
</div>
<!--
### Applying kubelet configuration changes

#### Updating the `KubeletConfiguration`

During cluster creation and upgrade, kubeadm writes its
[`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/)
in a ConfigMap called `kubelet-config` in the `kube-system` namespace.

You can edit the ConfigMap with this command:

The configuration is located under the `data.kubelet` key.
-->
<h3 id="应用-kubelet-配置更改">应用 kubelet 配置更改</h3>
<h4 id="更新-kubeletconfiguration">更新 <code>KubeletConfiguration</code></h4>
<p>在集群创建和升级期间，kubeadm 将其
<a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>
写入 <code>kube-system</code> 命名空间中名为 <code>kubelet-config</code> 的 ConfigMap。
你可以使用以下命令编辑 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit cm -n kube-system kubelet-config
</code></pre></div><p>配置位于 <code>data.kubelet</code> 键下。</p>
<!--
#### Reflecting the kubelet changes

To reflect the change on kubeadm nodes you must do the following:
- Log in to a kubeadm node
- Run `kubeadm upgrade node phase kubelet-config` to download the latest `kubelet-config`
ConfigMap contents into the local file `/var/lib/kubelet/config.conf`
- Edit the file `/var/lib/kubelet/kubeadm-flags.env` to apply additional configuration with
flags
- Restart the kubelet service with `systemctl restart kubelet`
-->
<h4 id="反映-kubelet-的更改">反映 kubelet 的更改</h4>
<p>要反映 kubeadm 节点上的更改，你必须执行以下操作：</p>
<ul>
<li>登录到 kubeadm 节点</li>
<li>运行 <code>kubeadm upgrade node phase kubelet-config</code> 下载最新的
<code>kubelet-config</code> ConfigMap 内容到本地文件 <code>/var/lib/kubelet/config.conf</code></li>
<li>编辑文件 <code>/var/lib/kubelet/kubeadm-flags.env</code> 以使用标志来应用额外的配置</li>
<li>使用 <code>systemctl restart kubelet</code> 重启 kubelet 服务</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Do these changes one node at a time to allow workloads to be rescheduled properly.
-->
<p>一次执行一个节点的这些更改，以允许正确地重新安排工作负载。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
During `kubeadm upgrade`, kubeadm downloads the `KubeletConfiguration` from the
`kubelet-config` ConfigMap and overwrite the contents of `/var/lib/kubelet/config.conf`.
This means that node local configuration must be applied either by flags in
`/var/lib/kubelet/kubeadm-flags.env` or by manually updating the contents of
`/var/lib/kubelet/config.conf` after `kubeadm upgrade`, and then restarting the kubelet.
-->
<p>在 <code>kubeadm upgrade</code> 期间，kubeadm 从 <code>kubelet-config</code> ConfigMap
下载 <code>KubeletConfiguration</code> 并覆盖 <code>/var/lib/kubelet/config.conf</code> 的内容。
这意味着节点本地配置必须通过<code>/var/lib/kubelet/kubeadm-flags.env</code>中的标志或在
kubeadm upgrade<code> 后手动更新</code>/var/lib/kubelet/config.conf`的内容来应用，然后重新启动 kubelet。
</div>
<!--
### Applying kube-proxy configuration changes

#### Updating the `KubeProxyConfiguration`

During cluster creation and upgrade, kubeadm writes its
[`KubeProxyConfiguration`](/docs/reference/config-api/kube-proxy-config.v1alpha1/)
in a ConfigMap in the `kube-system` namespace called `kube-proxy`.

This ConfigMap is used by the `kube-proxy` DaemonSet in the `kube-system` namespace.

To change a particular option in the `KubeProxyConfiguration`, you can edit the ConfigMap with this command:

The configuration is located under the `data.config.conf` key.
-->
<h3 id="应用-kube-proxy-配置更改">应用 kube-proxy 配置更改</h3>
<h4 id="更新-kubeproxyconfiguration">更新 <code>KubeProxyConfiguration</code></h4>
<p>在集群创建和升级期间，kubeadm 将其写入
<a href="/zh/docs/reference/config-api/kube-proxy-config.v1alpha1/"><code>KubeProxyConfiguration</code></a>
在名为 <code>kube-proxy</code> 的 <code>kube-system</code> 命名空间中的 ConfigMap 中。</p>
<p>此 ConfigMap 由 <code>kube-system</code> 命名空间中的 <code>kube-proxy</code> DaemonSet 使用。</p>
<p>要更改 <code>KubeProxyConfiguration</code> 中的特定选项，你可以使用以下命令编辑 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit cm -n kube-system kube-proxy
</code></pre></div><p>配置位于 <code>data.config.conf</code> 键下。</p>
<!--
#### Reflecting the kube-proxy changes

Once the `kube-proxy` ConfigMap is updated, you can restart all kube-proxy Pods:

Obtain the Pod names:

Delete a Pod with:

New Pods that use the updated ConfigMap will be created.
-->
<h4 id="反映-kube-proxy-的更改">反映 kube-proxy 的更改</h4>
<p>更新 <code>kube-proxy</code> ConfigMap 后，你可以重新启动所有 kube-proxy Pod：</p>
<p>获取 Pod 名称：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get po -n kube-system | grep kube-proxy
</code></pre></div><p>使用以下命令删除 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete po -n kube-system &lt;pod-name&gt;
</code></pre></div><p>将创建使用更新的 ConfigMap 的新 Pod。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Because kubeadm deploys kube-proxy as a DaemonSet, node specific configuration is unsupported.
-->
<p>由于 kubeadm 将 kube-proxy 部署为 DaemonSet，因此不支持特定于节点的配置。
</div>
<!--
### Applying CoreDNS configuration changes

#### Updating the CoreDNS Deployment and Service

kubeadm deploys CoreDNS as a Deployment called `coredns` and with a Service `kube-dns`,
both in the `kube-system` namespace.

To update any of the CoreDNS settings, you can edit the Deployment and
Service objects:
-->
<h3 id="应用-coredns-配置更改">应用 CoreDNS 配置更改</h3>
<h4 id="更新-coredns-的-deployment-和-service">更新 CoreDNS 的 Deployment 和 Service</h4>
<p>kubeadm 将 CoreDNS 部署为名为 <code>coredns</code> 的 Deployment，并使用 Service <code>kube-dns</code>，
两者都在 <code>kube-system</code> 命名空间中。</p>
<p>要更新任何 CoreDNS 设置，你可以编辑 Deployment 和 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit deployment -n kube-system coredns
kubectl edit service -n kube-system kube-dns
</code></pre></div><!--
#### Reflecting the CoreDNS changes

Once the CoreDNS changes are applied you can delete the CoreDNS Pods:

Obtain the Pod names:

Delete a Pod with:
-->
<h4 id="反映-coredns-的更改">反映 CoreDNS 的更改</h4>
<p>应用 CoreDNS 更改后，你可以删除 CoreDNS Pod。</p>
<p>获取 Pod 名称：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get po -n kube-system | grep coredns
</code></pre></div><p>使用以下命令删除 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete po -n kube-system &lt;pod-name&gt;
</code></pre></div><!--
New Pods with the updated CoreDNS configuration will be created.
-->
<p>将创建具有更新的 CoreDNS 配置的新 Pod。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
kubeadm does not allow CoreDNS configuration during cluster creation and upgrade.
This means that if you execute `kubeadm upgrade apply`, your changes to the CoreDNS
-->
<p>kubeadm 不允许在集群创建和升级期间配置 CoreDNS。
这意味着如果执行了 <code>kubeadm upgrade apply</code>，你对
CoreDNS 对象的更改将丢失并且必须重新应用。
</div>
<!--
## Persisting the reconfiguration

During the execution of `kubeadm upgrade` on a managed node, kubeadm might overwrite configuration
that was applied after the cluster was created (reconfiguration).
-->
<h2 id="持久化重新配置">持久化重新配置</h2>
<p>在受管节点上执行 <code>kubeadm upgrade</code> 期间，kubeadm
可能会覆盖在创建集群（重新配置）后应用的配置。</p>
<!--
### Persisting Node object reconfiguration

kubeadm writes Labels, Taints, CRI socket and other information on the Node object for a particular
Kubernetes node. To change any of the contents of this Node object you can use:
-->
<h3 id="持久化-node-对象重新配置">持久化 Node 对象重新配置</h3>
<p>kubeadm 在特定 Kubernetes 节点的 Node 对象上写入标签、污点、CRI
套接字和其他信息。要更改此 Node 对象的任何内容，你可以使用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit no &lt;node-name&gt;
</code></pre></div><!--
During `kubeadm upgrade` the contents of such a Node might get overwritten.
If you would like to persist your modifications to the Node object after upgrade,
you can prepare a [kubectl patch](/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/)
and apply it to the Node object:
-->
<p>在 <code>kubeadm upgrade</code> 期间，此类节点的内容可能会被覆盖。
如果你想在升级后保留对 Node 对象的修改，你可以准备一个
<a href="/zh/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">kubectl patch</a>
并将其应用到 Node 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch no &lt;node-name&gt; --patch-file &lt;patch-file&gt;
</code></pre></div><!--
#### Persisting control plane component reconfiguration

The main source of control plane configuration is the `ClusterConfiguration`
object stored in the cluster. To extend the static Pod manifests configuration,
[patches](/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#patches) can be used.

These patch files must remain as files on the control plane nodes to ensure that
they can be used by the `kubeadm upgrade ... --patches <directory>`.

If reconfiguration is done to the `ClusterConfiguration` and static Pod manifests on disk,
the set of node specific patches must be updated accordingly.
-->
<h4 id="持久化控制平面组件重新配置">持久化控制平面组件重新配置</h4>
<p>控制平面配置的主要来源是存储在集群中的 <code>ClusterConfiguration</code> 对象。
要扩展静态 Pod 清单配置，可以使用
<a href="/zh/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#patches">patches</a>。</p>
<p>这些补丁文件必须作为文件保留在控制平面节点上，以确保它们可以被
<code>kubeadm upgrade ... --patches &lt;directory&gt;</code> 使用。</p>
<p>如果对 <code>ClusterConfiguration</code> 和磁盘上的静态 Pod 清单进行了重新配置，则必须相应地更新节点特定补丁集。</p>
<!--
#### Persisting kubelet reconfiguration

Any changes to the `KubeletConfiguration` stored in `/var/lib/kubelet/config.conf` will be overwritten on
`kubeadm upgrade` by downloading the contents of the cluster wide `kubelet-config` ConfigMap.
To persist kubelet node specific configuration either the file `/var/lib/kubelet/config.conf`
has to be updated manually post-upgrade or the file `/var/lib/kubelet/kubeadm-flags.env` can include flags.
The kubelet flags override the associated `KubeletConfiguration` options, but note that
some of the flags are deprecated.

A kubelet restart will be required after changing `/var/lib/kubelet/config.conf` or
`/var/lib/kubelet/kubeadm-flags.env`.
-->
<h4 id="持久化-kubelet-重新配置">持久化 kubelet 重新配置</h4>
<p>对存储在 <code>/var/lib/kubelet/config.conf</code> 中的 <code>KubeletConfiguration</code>
所做的任何更改都将在 <code>kubeadm upgrade</code> 时因为下载集群范围内的 <code>kubelet-config</code>
ConfigMap 的内容而被覆盖。
要持久保存 kubelet 节点特定的配置，文件<code>/var/lib/kubelet/config.conf</code>
必须在升级后手动更新，或者文件<code>/var/lib/kubelet/kubeadm-flags.env</code> 可以包含标志。
kubelet 标志会覆盖相关的 <code>KubeletConfiguration</code> 选项，但请注意，有些标志已被弃用。</p>
<p>更改 <code>/var/lib/kubelet/config.conf</code> 或 <code>/var/lib/kubelet/kubeadm-flags.env</code>
后需要重启 kubelet。</p>
<p>What's next</p>
<!--
- [Upgrading kubeadm clusters](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade)
- [Customizing components with the kubeadm API](/docs/setup/production-environment/tools/kubeadm/control-plane-flags)
- [Certificate management with kubeadm](/docs/tasks/administer-cluster/kubeadm/kubeadm-certs)
-->
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade">升级 kubeadm 集群</a></li>
<li><a href="/zh/docs/setup/production-environment/tools/kubeadm/control-plane-flags">使用 kubeadm API 自定义组件</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-certs">使用 kubeadm 管理证书</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2e173356df5179cab9eec90a606f0aa4">2.4 - 升级 kubeadm 集群</h1>
    
	<!--
reviewers:
- sig-cluster-lifecycle
title: Upgrading kubeadm clusters
content_type: task
weight: 20
min-kubernetes-server-version: 1.18
-->
<!-- overview -->
<!--
This page explains how to upgrade a Kubernetes cluster created with kubeadm from version
1.22.x to version 1.23.x, and from version
1.23.x to 1.23.y (where `y > x`). Skipping MINOR versions
when upgrading is unsupported. For more details, please visit [Version Skew Policy](https://kubernetes.io/releases/version-skew-policy/).
-->
<p>本页介绍如何将 <code>kubeadm</code> 创建的 Kubernetes 集群从 1.22.x 版本
升级到 1.23.x 版本以及从 1.23.x
升级到 1.23.y（其中 <code>y &gt; x</code>）。略过次版本号的升级是
不被支持的。更多详情请访问<a href="https://kubernetes.io/releases/version-skew-policy/">版本倾斜政策</a>。</p>
<!--
To see information about upgrading clusters created using older versions of kubeadm,
please refer to following pages instead:
-->
<p>要查看 kubeadm 创建的有关旧版本集群升级的信息，请参考以下页面：</p>
<!--
- [Upgrading a kubeadm cluster from 1.21 to 1.22](https://v1-22.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
- [Upgrading a kubeadm cluster from 1.20 to 1.21](https://v1-21.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
- [Upgrading a kubeadm cluster from 1.19 to 1.20](https://v1-20.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
- [Upgrading a kubeadm cluster from 1.18 to 1.19](https://v1-19.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
-->
<ul>
<li><a href="https://v1-22.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">将 kubeadm 集群从 1.21 升级到 1.22</a></li>
<li><a href="https://v1-21.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">将 kubeadm 集群从 1.20 升级到 1.21</a></li>
<li><a href="https://v1-20.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">将 kubeadm 集群从 1.19 升级到 1.20</a></li>
<li><a href="https://v1-19.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">将 kubeadm 集群从 1.18 升级到 1.19</a></li>
</ul>
<!--
The upgrade workflow at high level is the following:

1. Upgrade a primary control plane node.
1. Upgrade additional control plane nodes.
1. Upgrade worker nodes.
-->
<p>升级工作的基本流程如下：</p>
<ol>
<li>升级主控制平面节点</li>
<li>升级其他控制平面节点</li>
<li>升级工作节点</li>
</ol>
<h2 id="before-you-begin">Before you begin</h2>
<!--
- Make sure you read the [release notes](https://git.k8s.io/kubernetes/CHANGELOG/CHANGELOG-1.23.md
) carefully.
- The cluster should use a static control plane and etcd pods or external etcd.
- Make sure to back up any important components, such as app-level state stored in a database.
  `kubeadm upgrade` does not touch your workloads, only components internal to Kubernetes, but backups are always a best practice.
-->
<ul>
<li>务必仔细认真阅读<a href="https://git.k8s.io/kubernetes/CHANGELOG/CHANGELOG-1.23.md
">发行说明</a>。</li>
<li>集群应使用静态的控制平面和 etcd Pod 或者外部 etcd。</li>
<li>务必备份所有重要组件，例如存储在数据库中应用层面的状态。
<code>kubeadm upgrade</code> 不会影响你的工作负载，只会涉及 Kubernetes 内部的组件，但备份终究是好的。</li>
<li><a href="https://serverfault.com/questions/684771/best-way-to-disable-swap-in-linux">必须禁用交换分区</a>。</li>
</ul>
<!--
### Additional information

- The instructions below outline when to drain each node during the upgrade process.
If you are performing a **minor** version upgrade for any kubelet, you **must**
first drain the node (or nodes) that you are upgrading. In the case of control plane nodes,
they could be running CoreDNS Pods or other critical workloads. For more information see
[Draining nodes](/docs/tasks/administer-cluster/safely-drain-node/).
- All containers are restarted after upgrade, because the container spec hash value is changed.
-->
<h3 id="附加信息">附加信息</h3>
<ul>
<li>下述说明了在升级过程中何时腾空每个节点。如果你正在对任何 kubelet 进行小版本升级，
你需要先腾空待升级的节点（或多个节点）。对于控制面节点，其上可能运行着 CoreDNS Pods
或者其它非常重要的负载。更多信息见<a href="/zh/docs/tasks/administer-cluster/safely-drain-node/">腾空节点</a>。</li>
<li>升级后，因为容器规约的哈希值已更改，所有容器都会被重新启动。</li>
</ul>
<!--
- To verify that the kubelet service has successfully restarted after the kubelet has been upgraded,
you can execute `systemctl status kubelet`  or view the service logs with `journalctl -xeu kubelet`.
- Usage of the `--config` flag of `kubeadm upgrade` with
[kubeadm configuration API types](/docs/reference/config-api/kubeadm-config.v1beta3)
with the purpose of reconfiguring the cluster is not recommended and can have unexpected results. Follow the steps in
[Reconfiguring a kubeadm cluster](/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure) instead.
-->
<ul>
<li>要验证 kubelet 服务在升级后是否成功重启，可以执行 <code>systemctl status kubelet</code>
或 <code>journalctl -xeu kubelet</code> 查看服务日志。</li>
<li>不建议使用 <code>kubeadm upgrade</code> 的 `--config 参数和 <a href="/zh/docs/reference/config-api/kubeadm-config.v1beta3">kubeadm 配置 API 类型</a>
来重新配置集群，这样会产生意想不到的结果。请按照<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure">重新配置 kubeadm 集群</a>
中的步骤来进行。</li>
</ul>
<!-- steps -->
<!--
## Determine which version to upgrade to

Find the latest patch release for Kubernetes 1.23 using the OS package manager:
-->
<h2 id="确定要升级到哪个版本">确定要升级到哪个版本</h2>
<p>使用操作系统的包管理器找到最新的补丁版本 Kubernetes 1.23：</p>
<ul class="nav nav-tabs" id="k8s-install-versions" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-versions-0" role="tab" aria-controls="k8s-install-versions-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-versions-1" role="tab" aria-controls="k8s-install-versions-1">CentOS、RHEL 或 Fedora</a></li></ul>
<div class="tab-content" id="k8s-install-versions"><div id="k8s-install-versions-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-versions-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">apt update
apt-cache madison kubeadm
<span style="color:#080;font-style:italic"># 在列表中查找最新的 1.23 版本</span>
<span style="color:#080;font-style:italic"># 它看起来应该是 1.23.x-00，其中 x 是最新的补丁版本</span>
</code></pre></div></div>
  <div id="k8s-install-versions-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-versions-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">yum list --showduplicates kubeadm --disableexcludes<span style="color:#666">=</span>kubernetes
<span style="color:#080;font-style:italic"># 在列表中查找最新的 1.23 版本</span>
<span style="color:#080;font-style:italic"># 它看起来应该是 1.23.x-0，其中 x 是最新的补丁版本</span>
</code></pre></div></div></div>

<!--
## Upgrade the control plane node

The upgrade procedure on control plane nodes should be executed one node at a time.
Pick a control plane node that you wish to upgrade first. It must have the `/etc/kubernetes/admin.conf` file.

### Call "kubeadm upgrade"
-->
<h2 id="升级控制平面节点">升级控制平面节点</h2>
<p>控制面节点上的升级过程应该每次处理一个节点。
首先选择一个要先行升级的控制面节点。该节点上必须拥有
<code>/etc/kubernetes/admin.conf</code> 文件。</p>
<h3 id="执行-kubeadm-upgrade">执行 &quot;kubeadm upgrade&quot;</h3>
<!--
**Upgrade the first control plane node**
-->
<p><strong>升级第一个控制面节点</strong></p>
<!--
- Upgrade kubeadm:
-->
<ul>
<li>升级 kubeadm：</li>
</ul>
<p><ul class="nav nav-tabs" id="k8s-install-kubeadm-first-cp" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-kubeadm-first-cp-0" role="tab" aria-controls="k8s-install-kubeadm-first-cp-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-kubeadm-first-cp-1" role="tab" aria-controls="k8s-install-kubeadm-first-cp-1">CentOS、RHEL 或 Fedora</a></li></ul>
<div class="tab-content" id="k8s-install-kubeadm-first-cp"><div id="k8s-install-kubeadm-first-cp-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-kubeadm-first-cp-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 用最新的补丁版本号替换 1.23.x-00 中的 x</span>
apt-mark unhold kubeadm <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-get update <span style="color:#666">&amp;&amp;</span> apt-get install -y <span style="color:#b8860b">kubeadm</span><span style="color:#666">=</span>1.23.x-00 <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-mark hold kubeadm
-

</code></pre></div></div>
  <div id="k8s-install-kubeadm-first-cp-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-kubeadm-first-cp-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 用最新的补丁版本号替换 1.23.x-0 中的 x</span>
yum install -y kubeadm-1.23.x-0 --disableexcludes<span style="color:#666">=</span>kubernetes
</code></pre></div></div></div>

<br /></p>
<!--
- Verify that the download works and has the expected version:
-->
<ul>
<li>
<p>验证下载操作正常，并且 kubeadm 版本正确：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm version
</code></pre></div></li>
</ul>
<!--
- Verify the upgrade plan:
-->
<ul>
<li>
<p>验证升级计划：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm upgrade plan
</code></pre></div><!--
This command checks that your cluster can be upgraded, and fetches the versions you can upgrade to.
It also shows a table with the component config version states.
-->
<p>此命令检查你的集群是否可被升级，并取回你要升级的目标版本。
命令也会显示一个包含组件配置版本状态的表格。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
  `kubeadm upgrade` also automatically renews the certificates that it manages on this node.
  To opt-out of certificate renewal the flag `--certificate-renewal=false` can be used.
  For more information see the [certificate management guide](/docs/tasks/administer-cluster/kubeadm/kubeadm-certs).
  -->
<p><code>kubeadm upgrade</code> 也会自动对 kubeadm 在节点上所管理的证书执行续约操作。
如果需要略过证书续约操作，可以使用标志 <code>--certificate-renewal=false</code>。
更多的信息，可参阅<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-certs">证书管理指南</a>。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
  If `kubeadm upgrade plan` shows any component configs that require manual upgrade, users must provide
  a config file with replacement configs to `kubeadm upgrade apply` via the `--config` command line flag.
  Failing to do so will cause `kubeadm upgrade apply` to exit with an error and not perform an upgrade.
  -->
<p>如果 <code>kubeadm upgrade plan</code> 给出任何需要手动升级的组件配置，用户必须
通过 <code>--config</code> 命令行标志向 <code>kubeadm upgrade apply</code> 命令提供替代的配置文件。
如果不这样做，<code>kubeadm upgrade apply</code> 会出错并退出，不再执行升级操作。
</div>
</li>
</ul>
<!--
- Choose a version to upgrade to, and run the appropriate command. For example:

  ```shell
  # replace x with the patch version you picked for this upgrade
  sudo kubeadm upgrade apply v1.23.x
  ```
-->
<p>选择要升级到的目标版本，运行合适的命令。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 x 替换为你为此次升级所选择的补丁版本号</span>
sudo kubeadm upgrade apply v1.23.x
</code></pre></div>  <!--
  Once the command finishes you should see:
  -->
<p>一旦该命令结束，你应该会看到：</p>
<pre><code class="language-console" data-lang="console">[upgrade/successful] SUCCESS! Your cluster was upgraded to &quot;v1.23.x&quot;. Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
</code></pre><!--
- Manually upgrade your CNI provider plugin.

  Your Container Network Interface (CNI) provider may have its own upgrade instructions to follow.
  Check the [addons](/docs/concepts/cluster-administration/addons/) page to
  find your CNI provider and see whether additional upgrade steps are required.

  This step is not required on additional control plane nodes if the CNI provider runs as a DaemonSet.
-->
<ul>
<li>
<p>手动升级你的 CNI 驱动插件。</p>
<p>你的容器网络接口（CNI）驱动应该提供了程序自身的升级说明。
参阅<a href="/zh/docs/concepts/cluster-administration/addons/">插件</a>页面查找你的 CNI 驱动，
并查看是否需要其他升级步骤。</p>
<p>如果 CNI 驱动作为 DaemonSet 运行，则在其他控制平面节点上不需要此步骤。</p>
</li>
</ul>
<!--
**For the other control plane nodes**
-->
<p><strong>对于其它控制面节点</strong></p>
<!--
Same as the first control plane node but use:
-->
<p>与第一个控制面节点相同，但是使用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo kubeadm upgrade node
</code></pre></div><!--
instead of:
-->
<p>而不是：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo kubeadm upgrade apply
</code></pre></div><!--
Also calling `kubeadm upgrade plan` and upgrading the CNI provider plugin is no longer needed.
-->
<p>此外，不需要执行 <code>kubeadm upgrade plan</code> 和更新 CNI 驱动插件的操作。</p>
<!--
### Drain the node

-  Prepare the node for maintenance by marking it unschedulable and evicting the workloads:

    ```shell
    # replace <node-to-drain> with the name of your node you are draining
    kubectl drain <node-to-drain> --ignore-daemonsets
    ```
-->
<h3 id="腾空节点">腾空节点</h3>
<ul>
<li>
<p>通过将节点标记为不可调度并腾空节点为节点作升级准备：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;node-to-drain&gt; 替换为你要腾空的控制面节点名称</span>
kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</code></pre></div></li>
</ul>
<!--
### Upgrade kubelet and kubectl

-  Upgrade the kubelet and kubectl:
-->
<h3 id="升级-kubelet-和-kubectl">升级 kubelet 和 kubectl</h3>
<ul>
<li>
<p>升级 kubelet 和 kubectl：</p>
<p><ul class="nav nav-tabs" id="k8s-install-kubelet" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-kubelet-0" role="tab" aria-controls="k8s-install-kubelet-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-kubelet-1" role="tab" aria-controls="k8s-install-kubelet-1">CentOS、RHEL 或 Fedora</a></li></ul>
<div class="tab-content" id="k8s-install-kubelet"><div id="k8s-install-kubelet-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-kubelet-0">

<p><pre><code>```shell
# 用最新的补丁版本替换 1.23.x-00 中的 x
apt-mark unhold kubelet kubectl &amp;&amp; \
apt-get update &amp;&amp; apt-get install -y kubelet=1.23.x-00 kubectl=1.23.x-00 &amp;&amp; \
apt-mark hold kubelet kubectl
```
</code></pre>
</div>
  <div id="k8s-install-kubelet-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-kubelet-1">

<p><pre><code>```shell
# 用最新的补丁版本号替换 1.23.x-00 中的 x
yum install -y kubelet-1.23.x-0 kubectl-1.23.x-0 --disableexcludes=kubernetes
```
</code></pre>
</div></div>

<br /></p>
</li>
</ul>
<!--
- Restart the kubelet
-->
<ul>
<li>
<p>重启 kubelet</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre></div></li>
</ul>
<!--
### Uncordon the node

- Bring the node back online by marking it schedulable:

  ```shell
  # replace <node-to-drain> with the name of your node
  kubectl uncordon <node-to-drain>

-->
<h3 id="解除节点的保护">解除节点的保护</h3>
<ul>
<li>
<p>通过将节点标记为可调度，让其重新上线：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;node-to-drain&gt; 替换为你的节点名称</span>
kubectl uncordon &lt;node-to-drain&gt;
</code></pre></div></li>
</ul>
<!--
## Upgrade worker nodes

The upgrade procedure on worker nodes should be executed one node at a time or few nodes at a time,
without compromising the minimum required capacity for running your workloads.
-->
<h2 id="升级工作节点">升级工作节点</h2>
<p>工作节点上的升级过程应该一次执行一个节点，或者一次执行几个节点，
以不影响运行工作负载所需的最小容量。</p>
<!--
### Upgrade kubeadm
-->
<h3 id="升级-kubeadm">升级 kubeadm</h3>
<!--
- Upgrade kubeadm:
-->
<ul>
<li>
<p>升级 kubeadm：</p>
<ul class="nav nav-tabs" id="k8s-install-kubeadm-worker-nodes" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-kubeadm-worker-nodes-0" role="tab" aria-controls="k8s-install-kubeadm-worker-nodes-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-kubeadm-worker-nodes-1" role="tab" aria-controls="k8s-install-kubeadm-worker-nodes-1">CentOS、RHEL 或 Fedora</a></li></ul>
<div class="tab-content" id="k8s-install-kubeadm-worker-nodes"><div id="k8s-install-kubeadm-worker-nodes-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-kubeadm-worker-nodes-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 1.23.x-00 中的 x 替换为最新的补丁版本号</span>
apt-mark unhold kubeadm <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-get update <span style="color:#666">&amp;&amp;</span> apt-get install -y <span style="color:#b8860b">kubeadm</span><span style="color:#666">=</span>1.23.x-00 <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-mark hold kubeadm
</code></pre></div></div>
  <div id="k8s-install-kubeadm-worker-nodes-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-kubeadm-worker-nodes-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 用最新的补丁版本替换 1.23.x-00 中的 x</span>
yum install -y kubeadm-1.23.x-0 --disableexcludes<span style="color:#666">=</span>kubernetes
</code></pre></div></div></div>

</li>
</ul>
<!--
### Call "kubeadm upgrade"

-  For worker nodes this upgrades the local kubelet configuration:
-->
<h3 id="执行-kubeadm-upgrade-1">执行 &quot;kubeadm upgrade&quot;</h3>
<ul>
<li>
<p>对于工作节点，下面的命令会升级本地的 kubelet 配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo kubeadm upgrade node
</code></pre></div></li>
</ul>
<!--
### Drain the node

- Prepare the node for maintenance by marking it unschedulable and evicting the workloads:

  ```shell
  # replace <node-to-drain> with the name of your node you are draining
  kubectl drain <node-to-drain> --ignore-daemonsets
  ```
-->
<h3 id="腾空节点-1">腾空节点</h3>
<ul>
<li>
<p>将节点标记为不可调度并驱逐所有负载，准备节点的维护：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;node-to-drain&gt; 替换为你正在腾空的节点的名称</span>
kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</code></pre></div></li>
</ul>
<!--
### Upgrade kubelet and kubectl
-->
<h3 id="升级-kubelet-和-kubectl-1">升级 kubelet 和 kubectl</h3>
<!--
-  Upgrade the kubelet and kubectl:
-->
<ul>
<li>
<p>升级 kubelet 和 kubectl：</p>
<ul class="nav nav-tabs" id="k8s-kubelet-and-kubectl" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-kubelet-and-kubectl-0" role="tab" aria-controls="k8s-kubelet-and-kubectl-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-kubelet-and-kubectl-1" role="tab" aria-controls="k8s-kubelet-and-kubectl-1">CentOS, RHEL or Fedora</a></li></ul>
<div class="tab-content" id="k8s-kubelet-and-kubectl"><div id="k8s-kubelet-and-kubectl-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-kubelet-and-kubectl-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 1.23.x-00 中的 x 替换为最新的补丁版本</span>
apt-mark unhold kubelet kubectl <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-get update <span style="color:#666">&amp;&amp;</span> apt-get install -y <span style="color:#b8860b">kubelet</span><span style="color:#666">=</span>1.23.x-00 <span style="color:#b8860b">kubectl</span><span style="color:#666">=</span>1.23.x-00 <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-mark hold kubelet kubectl
</code></pre></div></div>
  <div id="k8s-kubelet-and-kubectl-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-kubelet-and-kubectl-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 1.23.x-0 x 替换为最新的补丁版本</span>
yum install -y kubelet-1.23.x-0 kubectl-1.23.x-0 --disableexcludes<span style="color:#666">=</span>kubernetes
</code></pre></div></div></div>

</li>
</ul>
<!--
- Restart the kubelet

    ```shell
    sudo systemctl daemon-reload
    sudo systemctl restart kubelet
    ```
-->
<ul>
<li>
<p>重启 kubelet</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre></div></li>
</ul>
<!--
### Uncordon the node
-->
<h3 id="取消对节点的保护">取消对节点的保护</h3>
<!--
-  Bring the node back online by marking it schedulable:

    ```shell
    # replace <node-to-drain> with the name of your node
    kubectl uncordon <node-to-drain>
    ```
-->
<ul>
<li>
<p>通过将节点标记为可调度，让节点重新上线:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;node-to-drain&gt; 替换为当前节点的名称</span>
kubectl uncordon &lt;node-to-drain&gt;
</code></pre></div></li>
</ul>
<!--
## Verify the status of the cluster

After the kubelet is upgraded on all nodes verify that all nodes are available again by running the following command
from anywhere kubectl can access the cluster:

```shell
kubectl get nodes
```
-->
<h2 id="验证集群的状态">验证集群的状态</h2>
<p>在所有节点上升级 kubelet 后，通过从 kubectl 可以访问集群的任何位置运行以下命令，
验证所有节点是否再次可用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><!--
The `STATUS` column should show `Ready` for all your nodes, and the version number should be updated.
-->
<p><code>STATUS</code> 应显示所有节点为 <code>Ready</code> 状态，并且版本号已经被更新。</p>
<!--
## Recovering from a failure state

If `kubeadm upgrade` fails and does not roll back, for example because of an unexpected shutdown during execution, you can run `kubeadm upgrade` again.
This command is idempotent and eventually makes sure that the actual state is the desired state you declare.

To recover from a bad state, you can also run `kubeadm upgrade--force` without changing the version that your cluster is running.
-->
<h2 id="从故障状态恢复">从故障状态恢复</h2>
<p>如果 <code>kubeadm upgrade</code> 失败并且没有回滚，例如由于执行期间节点意外关闭，
你可以再次运行 <code>kubeadm upgrade</code>。
此命令是幂等的，并最终确保实际状态是你声明的期望状态。
要从故障状态恢复，你还可以运行 <code>kubeadm upgrade --force</code> 而无需更改集群正在运行的版本。</p>
<!--
During upgrade kubeadm writes the following backup folders under `/etc/kubernetes/tmp`:
- `kubeadm-backup-etcd-<date>-<time>`
- `kubeadm-backup-manifests-<date>-<time>`

`kubeadm-backup-etcd` contains a backup of the local etcd member data for this control-plane Node.
In case of an etcd upgrade failure and if the automatic rollback does not work, the contents of this folder
can be manually restored in `/var/lib/etcd`. In case external etcd is used this backup folder will be empty.

`kubeadm-backup-manifests` contains a backup of the static Pod manifest files for this control-plane Node.
In case of a upgrade failure and if the automatic rollback does not work, the contents of this folder can be
manually restored in `/etc/kubernetes/manifests`. If for some reason there is no difference between a pre-upgrade
and post-upgrade manifest file for a certain component, a backup file for it will not be written.
-->
<p>在升级期间，kubeadm 向 <code>/etc/kubernetes/tmp</code> 目录下的如下备份文件夹写入数据：</p>
<ul>
<li><code>kubeadm-backup-etcd-&lt;date&gt;-&lt;time&gt;</code></li>
<li><code>kubeadm-backup-manifests-&lt;date&gt;-&lt;time&gt;</code></li>
</ul>
<p><code>kubeadm-backup-etcd</code> 包含当前控制面节点本地 etcd 成员数据的备份。
如果 etcd 升级失败并且自动回滚也无法修复，则可以将此文件夹中的内容复制到
<code>/var/lib/etcd</code> 进行手工修复。如果使用的是外部的 etcd，则此备份文件夹为空。</p>
<p><code>kubeadm-backup-manifests</code> 包含当前控制面节点的静态 Pod 清单文件的备份版本。
如果升级失败并且无法自动回滚，则此文件夹中的内容可以复制到
<code>/etc/kubernetes/manifests</code> 目录实现手工恢复。
如果由于某些原因，在升级前后某个组件的清单未发生变化，则 kubeadm 也不会为之
生成备份版本。</p>
<!--
## How it works

`kubeadm upgrade apply` does the following:

- Checks that your cluster is in an upgradeable state:
  - The API server is reachable
  - All nodes are in the `Ready` state
  - The control plane is healthy
- Enforces the version skew policies.
- Makes sure the control plane images are available or available to pull to the machine.
- Generates replacements and/or uses user supplied overwrites if component configs require version upgrades.
- Upgrades the control plane components or rollbacks if any of them fails to come up.
- Applies the new `CoreDNS` and `kube-proxy` manifests and makes sure that all necessary RBAC rules are created.
- Creates new certificate and key files of the API server and backs up old files if they're about to expire in 180 days.
-->
<h2 id="how-it-works">工作原理  </h2>
<p><code>kubeadm upgrade apply</code> 做了以下工作：</p>
<ul>
<li>检查你的集群是否处于可升级状态:
<ul>
<li>API 服务器是可访问的</li>
<li>所有节点处于 <code>Ready</code> 状态</li>
<li>控制面是健康的</li>
</ul>
</li>
<li>强制执行版本偏差策略。</li>
<li>确保控制面的镜像是可用的或可拉取到服务器上。</li>
<li>如果组件配置要求版本升级，则生成替代配置与/或使用用户提供的覆盖版本配置。</li>
<li>升级控制面组件或回滚（如果其中任何一个组件无法启动）。</li>
<li>应用新的 <code>CoreDNS</code> 和 <code>kube-proxy</code> 清单，并强制创建所有必需的 RBAC 规则。</li>
<li>如果旧文件在 180 天后过期，将创建 API 服务器的新证书和密钥文件并备份旧文件。</li>
</ul>
<!--
`kubeadm upgrade node` does the following on additional control plane nodes:

- Fetches the kubeadm `ClusterConfiguration` from the cluster.
- Optionally backups the kube-apiserver certificate.
- Upgrades the static Pod manifests for the control plane components.
- Upgrades the kubelet configuration for this node.
-->
<p><code>kubeadm upgrade node</code> 在其他控制平节点上执行以下操作：</p>
<ul>
<li>从集群中获取 kubeadm <code>ClusterConfiguration</code>。</li>
<li>（可选操作）备份 kube-apiserver 证书。</li>
<li>升级控制平面组件的静态 Pod 清单。</li>
<li>为本节点升级 kubelet 配置</li>
</ul>
<!--
`kubeadm upgrade node` does the following on worker nodes:

- Fetches the kubeadm `ClusterConfiguration` from the cluster.
- Upgrades the kubelet configuration for this node.
-->
<p><code>kubeadm upgrade node</code> 在工作节点上完成以下工作：</p>
<ul>
<li>从集群取回 kubeadm <code>ClusterConfiguration</code>。</li>
<li>为本节点升级 kubelet 配置。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9133578f1e75663bb031e5a377ca896d">2.5 - 添加 Windows 节点</h1>
    
	<!--
reviewers:
- michmike
- patricklang
title: Adding Windows nodes
min-kubernetes-server-version: 1.17
content_type: tutorial
weight: 30
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<!--
You can use Kubernetes to run a mixture of Linux and Windows nodes, so you can mix Pods that run on Linux on with Pods that run on Windows. This page shows how to register Windows nodes to your cluster.
-->
<p>你可以使用 Kubernetes 来混合运行 Linux 和 Windows 节点，这样你就可以
混合使用运行于 Linux 上的 Pod 和运行于 Windows 上的 Pod。
本页面展示如何将 Windows 节点注册到你的集群。</p>
<h2 id="before-you-begin">Before you begin</h2>


Your Kubernetes server must be at or later than version 1.17.
 To check the version, enter <code>kubectl version</code>.

<!--
* Obtain a [Windows Server 2019 license](https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing)
(or higher) in order to configure the Windows node that hosts Windows containers.
If you are using VXLAN/Overlay networking you must have also have [KB4489899](https://support.microsoft.com/help/4489899) installed.

* A Linux-based Kubernetes kubeadm cluster in which you have access to the control plane (see [Creating a single control-plane cluster with kubeadm](/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)).
-->
<ul>
<li>
<p>获取 <a href="https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing">Windows Server 2019 或更高版本的授权</a>
以便配置托管 Windows 容器的 Windows 节点。
如果你在使用 VXLAN/覆盖（Overlay）联网设施，则你还必须安装 <a href="https://support.microsoft.com/help/4489899">KB4489899</a>。</p>
</li>
<li>
<p>一个利用 kubeadm 创建的基于 Linux 的 Kubernetes 集群；你能访问该集群的控制面
（参见<a href="/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">使用 kubeadm 创建一个单控制面的集群</a>)。</p>
</li>
</ul>
<h2 id="objectives">Objectives</h2>
<!--
* Register a Windows node to the cluster
* Configure networking so Pods and Services on Linux and Windows can communicate with each other
-->
<ul>
<li>将一个 Windows 节点注册到集群上</li>
<li>配置网络，以便 Linux 和 Windows 上的 Pod 和 Service 之间能够相互通信。</li>
</ul>
<!-- lessoncontent -->
<!--
## Getting Started: Adding a Windows Node to Your Cluster

### Networking Configuration

Once you have a Linux-based Kubernetes control-plane node you are ready to choose a networking solution. This guide illustrates using Flannel in VXLAN mode for simplicity.
-->
<h2 id="开始行动-向你的集群添加一个-windows-节点">开始行动：向你的集群添加一个 Windows 节点</h2>
<h3 id="networking-configuration">联网配置  </h3>
<p>一旦你有了一个基于 Linux 的 Kubernetes 控制面节点，你就可以为其选择联网方案。
出于简单考虑，本指南展示如何使用 VXLAN 模式的 Flannel。</p>
<!--
#### Configuring Flannel

1. Prepare Kubernetes control plane for Flannel

    Some minor preparation is recommended on the Kubernetes control plane in our cluster. It is recommended to enable bridged IPv4 traffic to iptables chains when using Flannel. The following command must be run on all Linux nodes:

    ```bash
    sudo sysctl net.bridge.bridge-nf-call-iptables=1
    ```
-->
<h4 id="configuring-flannel">配置 Flannel </h4>
<ol>
<li>
<p>为 Flannel 准备 Kubernetes 的控制面</p>
<p>在我们的集群中，建议对 Kubernetes 的控制面进行少许准备处理。
建议在使用 Flannel 时为 iptables 链启用桥接方式的 IPv4 流处理，
必须在所有 Linux 节点上执行如下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo sysctl net.bridge.bridge-nf-call-iptables<span style="color:#666">=</span><span style="color:#666">1</span>
</code></pre></div></li>
</ol>
<!--
1. Download & configure Flannel for Linux

    Download the most recent Flannel manifest:

    ```bash
    wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
    ```

    Modify the `net-conf.json` section of the flannel manifest in order to set the VNI to 4096 and the Port to 4789. It should look as follows:

    ```json
    net-conf.json: |
        {
          "Network": "10.244.0.0/16",
          "Backend": {
            "Type": "vxlan",
            "VNI": 4096,
            "Port": 4789
          }
        }
    ```

    The VNI must be set to 4096 and port 4789 for Flannel on Linux to interoperate with Flannel on Windows. See the [VXLAN documentation](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan).
    for an explanation of these fields.

    To use L2Bridge/Host-gateway mode instead change the value of `Type` to `"host-gw"` and omit `VNI` and `Port`.
-->
<ol start="2">
<li>
<p>下载并配置 Linux 版本的 Flannel</p>
<p>下载最新的 Flannel 清单文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre></div><p>修改 Flannel 清单中的 <code>net-conf.json</code> 部分，将 VNI 设置为 4096，并将 Port 设置为 4789。
结果看起来像下面这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="">net-conf.json:</span> <span style="">|</span>
    {
      <span style="color:#008000;font-weight:bold">&#34;Network&#34;</span>: <span style="color:#b44">&#34;10.244.0.0/16&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;Backend&#34;</span>: {
         <span style="color:#008000;font-weight:bold">&#34;Type&#34;</span>: <span style="color:#b44">&#34;vxlan&#34;</span>,
         <span style="color:#008000;font-weight:bold">&#34;VNI&#34;</span>: <span style="color:#666">4096</span>,
         <span style="color:#008000;font-weight:bold">&#34;Port&#34;</span>: <span style="color:#666">4789</span>
    }
}
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在 Linux 节点上 VNI 必须设置为 4096，端口必须设置为 4789，这样才能令其与 Windows 上的
Flannel 互操作。关于这些字段的详细说明，请参见
<a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan">VXLAN 文档</a>。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如要使用 L2Bridge/Host-gateway 模式，则可将 <code>Type</code> 值设置为
<code>&quot;host-gw&quot;</code>，并忽略 <code>VNI</code> 和 <code>Port</code> 的设置。
</div>
</li>
</ol>
<!--
1. Apply the Flannel manifest and validate

    Let's apply the Flannel configuration:

    ```bash
    kubectl apply -f kube-flannel.yml
    ```

    After a few minutes, you should see all the pods as running if the Flannel pod network was deployed.

    ```bash
    kubectl get pods -n kube-system
    ```

    The output should include the Linux flannel DaemonSet as running:

    ```
    NAMESPACE     NAME                                      READY        STATUS    RESTARTS   AGE
    ...
    kube-system   kube-flannel-ds-54954                     1/1          Running   0          1m
    ```
-->
<ol start="3">
<li>
<p>应用 Flannel 清单并验证</p>
<p>首先应用 Flannel 配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f kube-flannel.yml
</code></pre></div><p>几分钟之后，如果 Flannel Pod 网络被正确部署，你应该会看到所有 Pods 都处于运行中状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get pods -n kube-system
</code></pre></div><p>输出中应该包含处于运行中状态的 Linux Flannel DaemonSet：</p>
<pre><code>NAMESPACE     NAME                                      READY        STATUS    RESTARTS   AGE
...
kube-system   kube-flannel-ds-54954                     1/1          Running   0          1m
</code></pre></li>
</ol>
<!--
1. Add Windows Flannel and kube-proxy DaemonSets

    Now you can add Windows-compatible versions of Flannel and kube-proxy. In order
    to ensure that you get a compatible version of kube-proxy, you'll need to substitute
    the tag of the image. The following example shows usage for Kubernetes v1.23.0,
    but you should adjust the version for your own deployment.

    ```bash
    curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed 's/VERSION/v1.23.0/g' | kubectl apply -f -
    kubectl apply -f https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml
    ```

    If you're using host-gateway use https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-host-gw.yml instead

If you're using a different interface rather than Ethernet (i.e. "Ethernet0 2") on the Windows nodes, you have to modify the line:

```powershell
wins cli process run --path /k/flannel/setup.exe --args "--mode=overlay --interface=Ethernet"
```

in the `flannel-host-gw.yml` or `flannel-overlay.yml` file and specify your interface accordingly.

```bash
# Example
curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml | sed 's/Ethernet/Ethernet0 2/g' | kubectl apply -f -
```
-->    
<ol start="4">
<li>
<p>添加 Windows Flannel 和 kube-proxy DaemonSet</p>
<p>现在你可以添加 Windows 兼容版本的 Flannel 和 kube-proxy。为了确保你能获得兼容
版本的 kube-proxy，你需要替换镜像中的标签。
下面的例子中展示的是针对 Kubernetes v1.23.0 版本的用法，
不过你应该根据你自己的集群部署调整其中的版本号。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed <span style="color:#b44">&#39;s/VERSION/v1.23.0/g&#39;</span> | kubectl apply -f -
kubectl apply -f https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果你在使用 host-gateway 模式，则应该使用
<a href="https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-host-gw.yml">https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-host-gw.yml</a>
这一清单。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>如果你在 Windows 节点上使用的不是以太网（即，&quot;Ethernet0 2&quot;）接口，你需要
修改 <code>flannel-host-gw.yml</code> 或 <code>flannel-overlay.yml</code> 文件中的下面这行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">wins <span style="color:#a2f">cli </span><span style="color:#a2f;font-weight:bold">process</span> run --path /k/flannel/setup.exe --args <span style="color:#b44">&#34;--mode=overlay --interface=Ethernet&#34;</span>
</code></pre></div><p>在其中根据情况设置要使用的网络接口。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#080;font-style:italic"># Example</span>
curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml | sed <span style="color:#b44">&#39;s/Ethernet/Ethernet0 2/g&#39;</span> | kubectl apply -f -
</code></pre></div>
</div>
</li>
</ol>
<!--
### Joining a Windows worker node
-->
<h3 id="joining-a-windows-worker-node">加入 Windows 工作节点  </h3>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
All code snippets in Windows sections are to be run in a PowerShell environment
with elevated permissions (Administrator) on the Windows worker node.
-->
<p>Windows 节的所有代码片段都需要在 PowerShell 环境中执行，并且要求在
Windows 工作节点上具有提升的权限（Administrator）。
</div>
<ul class="nav nav-tabs" id="tab-windows-kubeadm-runtime-installation" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tab-windows-kubeadm-runtime-installation-0" role="tab" aria-controls="tab-windows-kubeadm-runtime-installation-0" aria-selected="true">Docker EE</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-windows-kubeadm-runtime-installation-1" role="tab" aria-controls="tab-windows-kubeadm-runtime-installation-1">CRI-containerD</a></li></ul>
<div class="tab-content" id="tab-windows-kubeadm-runtime-installation"><div id="tab-windows-kubeadm-runtime-installation-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tab-windows-kubeadm-runtime-installation-0">

<p><!--
#### Install Docker EE

Install the `Containers` feature
-->
<h4 id="安装-docker-ee">安装 Docker EE</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">Install-WindowsFeature</span> -Name containers
</code></pre></div><!--
Install Docker
Instructions to do so are available at [Install Docker Engine - Enterprise on Windows Servers](https://docs.microsoft.com/en-us/virtualization/windowscontainers/quick-start/set-up-environment?tabs=Windows-Server#install-docker).
-->
<p>安装 Docker
操作指南在 <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/quick-start/set-up-environment?tabs=Windows-Server#install-docker">Install Docker Engine - Enterprise on Windows Servers</a>。</p>
<!--
#### Install wins, kubelet, and kubeadm.
-->
<h4 id="安装-wins-kubelet-和-kubeadm">安装 wins、kubelet 和 kubeadm</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-PowerShell" data-lang="PowerShell">curl.exe -LO https<span style="">:</span>//raw.githubusercontent.com/<span style="color:#a2f">kubernetes-sigs</span>/<span style="color:#a2f">sig-windows</span>-tools/master/kubeadm/scripts/PrepareNode.ps1
.\PrepareNode.ps1 -KubernetesVersion v1.23.0
</code></pre></div><!--
#### Run `kubeadm` to join the node

    Use the command that was given to you when you ran `kubeadm init` on a control plane host.
    If you no longer have this command, or the token has expired, you can run `kubeadm token create -print-join-command`
    (on a control plane host) to generate a new token and join command.
-->
<h4 id="运行-kubeadm-添加节点">运行 <code>kubeadm</code> 添加节点</h4>
<p>当你在控制面主机上运行 <code>kubeadm init</code> 时，输出了一个命令。现在运行这个命令。
如果你找不到这个命令，或者命令中对应的令牌已经过期，你可以（在一个控制面主机上）运行
<code>kubeadm token create --print-join-command</code> 来生成新的令牌和 join 命令。</p>
</div>
  <div id="tab-windows-kubeadm-runtime-installation-1" class="tab-pane" role="tabpanel" aria-labelledby="tab-windows-kubeadm-runtime-installation-1">

<p><!--
#### Install containerD
-->
<h4 id="安装-containerd">安装 containerD</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">curl.exe -LO https<span style="">:</span>//github.com/<span style="color:#a2f">kubernetes-sigs</span>/<span style="color:#a2f">sig-windows</span>-tools/releases/latest/download/<span style="color:#a2f">Install-Containerd</span>.ps1
.\<span style="color:#a2f">Install-Containerd</span>.ps1
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
To install a specific version of containerD specify the version with -ContainerDVersion.
-->
<p>要安装特定版本的 containerD，使用参数 -ContainerDVersion指定版本。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#080;font-style:italic"># Example</span>
.\<span style="color:#a2f">Install-Containerd</span>.ps1 -ContainerDVersion 1.4.1
</code></pre></div>
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you're using a different interface rather than Ethernet (i.e. "Ethernet0 2") on the Windows nodes, specify the name with `-netAdapterName`.
-->
<p>如果你在 Windows 节点上使用了与 Ethernet 不同的接口（例如 &quot;Ethernet0 2&quot;），使用参数 <code>-netAdapterName</code> 指定名称。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#080;font-style:italic"># Example</span>
.\<span style="color:#a2f">Install-Containerd</span>.ps1 -netAdapterName <span style="color:#b44">&#34;Ethernet0 2&#34;</span>
</code></pre></div>
</div>
<!--
#### Install wins, kubelet, and kubeadm
-->
<h4 id="安装-wins-kubelet-和-kubeadm">安装 wins，kubelet 和 kubeadm</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-PowerShell" data-lang="PowerShell">curl.exe -LO https<span style="">:</span>//raw.githubusercontent.com/<span style="color:#a2f">kubernetes-sigs</span>/<span style="color:#a2f">sig-windows</span>-tools/master/kubeadm/scripts/PrepareNode.ps1
.\PrepareNode.ps1 -KubernetesVersion v1.23.0 -ContainerRuntime containerD
</code></pre></div><!--
#### Run `kubeadm` to join the node

Use the command that was given to you when you ran `kubeadm init` on a control plane host.
If you no longer have this command, or the token has expired, you can run `kubeadm token create --print-join-command`
(on a control plane host) to generate a new token and join command.
-->
<h4 id="运行-kubeadm-添加节点">运行 <code>kubeadm</code> 添加节点</h4>
<p>使用当你在控制面主机上运行 <code>kubeadm init</code> 时得到的命令。
如果你找不到这个命令，或者命令中对应的令牌已经过期，你可以（在一个控制面主机上）运行
<code>kubeadm token create --print-join-command</code> 来生成新的令牌和 join 命令。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> If using <strong>CRI-containerD</strong> add <code>--cri-socket &quot;npipe:////./pipe/containerd-containerd&quot;</code> to the kubeadm call
</div>
</div></div>

<!--
#### Verifying your installation

You should now be able to view the Windows node in your cluster by running:
-->
<h4 id="verifying-your-installation">检查你的安装  </h4>
<p>你现在应该能够通过运行下面的命令来查看集群中的 Windows 节点了：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get nodes -o wide
</code></pre></div><!--
If your new node is in the `NotReady` state it is likely because the flannel image is still downloading.
You can check the progress as before by checking on the flannel pods in the `kube-system` namespace:
-->
<p>如果你的新节点处于 <code>NotReady</code> 状态，很可能的原因是系统仍在下载 Flannel 镜像。
你可以像之前一样，通过检查 <code>kube-system</code> 名字空间中的 Flannel Pods 来了解
安装进度。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl -n kube-system get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>flannel
</code></pre></div><!--
Once the flannel Pod is running, your node should enter the `Ready` state and then be available to handle workloads.
-->
<p>一旦 Flannel Pod 运行起来，你的节点就应该能进入 <code>Ready</code> 状态并可
用来处理负载。</p>
<h2 id="what-s-next">What's next</h2>
<!--
- [Upgrading Windows kubeadm nodes](/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes)
-->
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes">升级 kubeadm 安装的 Windows 节点</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e805c7d8d4ad6195cb82dbbc843bfc29">2.6 - 升级 Windows 节点</h1>
    
	<!--
title: Upgrading Windows nodes
min-kubernetes-server-version: 1.17
content_type: task
weight: 40
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<!--
This page explains how to upgrade a Windows node [created with kubeadm](/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes).
-->
<p>本页解释如何升级<a href="/zh/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes">用 kubeadm 创建的</a>
Windows 节点。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

Your Kubernetes server must be at or later than version 1.17.
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
* Familiarize yourself with [the process for upgrading the rest of your kubeadm
cluster](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade). You will want to
upgrade the control plane nodes before upgrading your Windows nodes.
-->
<ul>
<li>熟悉<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade">更新 kubeadm 集群中的其余组件</a>。
在升级你的 Windows 节点之前你会想要升级控制面节点。</li>
</ul>
<!-- steps -->
<!--
## Upgrading worker nodes

### Upgrade kubeadm
-->
<h2 id="upgrading-worker-nodes">升级工作节点  </h2>
<h3 id="upgrade-kubeadm">升级 kubeadm   </h3>
<!--
1.  From the Windows node, upgrade kubeadm:

    ```powershell
    # replace v1.23.0 with your desired version
    curl.exe -Lo C:\k\kubeadm.exe https://dl.k8s.io/v1.23.0/bin/windows/amd64/kubeadm.exe
    ```
-->
<ol>
<li>
<p>在 Windows 节点上升级 kubeadm：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#080;font-style:italic"># 将 v1.23.0 替换为你希望的版本</span>
curl.exe -Lo C:\k\kubeadm.exe https<span style="">:</span>//dl.k8s.io/<span style="color:#a2f">/bin/windows/amd64/kubeadm.exe
</code></pre></div></li>
</ol>
<!--
### Drain the node

1.  From a machine with access to the Kubernetes API,
    prepare the node for maintenance by marking it unschedulable and evicting the workloads:

    ```shell
    # replace <node-to-drain> with the name of your node you are draining
    kubectl drain <node-to-drain> -ignore-daemonsets
    ```

    You should see output similar to this:

    ```
    node/ip-172-31-85-18 cordoned
    node/ip-172-31-85-18 drained
    ```
-->
<h3 id="drain-the-node">腾空节点  </h3>
<ol>
<li>
<p>在一个能访问到 Kubernetes API 的机器上，将 Windows 节点标记为不可调度并
驱逐其上的所有负载，以便准备节点维护操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;要腾空的节点&gt; 替换为你要腾空的节点的名称</span>
kubectl drain &lt;要腾空的节点&gt; -ignore-daemonsets
</code></pre></div><p>你应该会看到类似下面的输出：</p>
<pre><code>node/ip-172-31-85-18 cordoned
node/ip-172-31-85-18 drained
</code></pre></li>
</ol>
<!--
### Upgrade the kubelet configuration

1.  From the Windows node, call the following command to sync new kubelet configuration:

    ```powershell
    kubeadm upgrade node
    ```
-->
<h3 id="upgrade-the-kubelet-configuration">升级 kubelet 配置  </h3>
<ol>
<li>
<p>在 Windows 节点上，执行下面的命令来同步新的 kubelet 配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">kubeadm upgrade node
</code></pre></div></li>
</ol>
<!--
### Upgrade kubelet

1.  From the Windows node, upgrade and restart the kubelet:

    ```powershell
    stop-service kubelet
    curl.exe -Lo C:\k\kubelet.exe https://dl.k8s.io/v1.23.0/bin/windows/amd64/kubelet.exe
    restart-service kubelet
    ```
-->
<h3 id="upgrade-kubelet">升级 kubelet  </h3>
<ol>
<li>
<p>在 Windows 节点上升级并重启 kubelet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">stop-service</span> kubelet
curl.exe -Lo C:\k\kubelet.exe https<span style="">:</span>//dl.k8s.io/<span style="color:#a2f">/bin/windows/amd64/kubelet.exe
<span style="color:#a2f">restart-service</span> kubelet
</code></pre></div></li>
</ol>
<!--
### Uncordon the node

1.  From a machine with access to the Kubernetes API,
bring the node back online by marking it schedulable:

    ```shell
    # replace <node-to-drain> with the name of your node
    kubectl uncordon <node-to-drain>
    ```
-->
<h3 id="uncordon-the-node">对节点执行 uncordon 操作  </h3>
<ol>
<li>
<p>从一台能够访问到 Kubernetes API 的机器上，通过将节点标记为可调度，使之
重新上线：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;要腾空的节点&gt; 替换为你的节点名称</span>
kubectl uncordon &lt;要腾空的节点&gt;
</code></pre></div></li>
</ol>
<!--
### Upgrade kube-proxy

1. From a machine with access to the Kubernetes API, run the following,
again replacing v1.23.0 with your desired version:

    ```shell
    curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed 's/VERSION/v1.23.0/g' | kubectl apply -f -
    ```
-->
<h3 id="upgrade-kube-proxy">升级 kube-proxy  </h3>
<ol>
<li>
<p>在一台可访问 Kubernetes API 的机器上和，将 v1.23.0 替换成你
期望的版本后再次执行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed <span style="color:#b44">&#39;s/VERSION/v1.23.0/g&#39;</span> | kubectl apply -f -
</code></pre></div></li>
</ol>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-47be5dd51f686017f1766e6ec7aa6f41">3 - 管理内存，CPU 和 API 资源</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-337620c76587e4aeb32009cb23be46de">3.1 - 为命名空间配置默认的内存请求和限制</h1>
    
	<!--
title: Configure Default Memory Requests and Limits for a Namespace
content_type: task
weight: 10
-->
<!-- overview -->
<!--
This page shows how to configure default memory requests and limits for a namespace.
If a Container is created in a namespace that has a default memory limit, and the Container
does not specify its own memory limit, then the Container is assigned the default memory limit.
Kubernetes assigns a default memory request under certain conditions that are explained later in this topic.
-->
<p>本文介绍怎样给命名空间配置默认的内存请求和限制。
如果在一个有默认内存限制的命名空间创建容器，该容器没有声明自己的内存限制时，
将会被指定默认内存限制。
Kubernetes 还为某些情况指定了默认的内存请求，本章后面会进行介绍。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
Each node in your cluster must have at least 2 GiB of memory.
-->
<p>你的集群中的每个节点必须至少有 2 GiB 的内存。</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便本练习中所建的资源与集群的其余资源相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace default-mem-example
</code></pre></div><!--
## Create a LimitRange and a Pod

Here's the configuration file for a LimitRange object. The configuration specifies
a default memory request and a default memory limit.
-->
<h2 id="创建-limitrange-和-pod">创建 LimitRange 和 Pod</h2>
<p>这里给出了一个限制范围对象的配置文件。该配置声明了一个默认的内存请求和一个默认的内存限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-defaults.yaml" download="admin/resource/memory-defaults.yaml"><code>admin/resource/memory-defaults.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-defaults-yaml')" title="Copy admin/resource/memory-defaults.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-defaults-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mem-limit-range<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">default</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>512Mi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">defaultRequest</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>256Mi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the LimitRange in the default-mem-example namespace:
-->
<p>在 default-mem-example 命名空间创建限制范围：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
Now if a Container is created in the default-mem-example namespace, and the
Container does not specify its own values for memory request and memory limit,
the Container is given a default memory request of 256 MiB and a default
memory limit of 512 MiB.

Here's the configuration file for a Pod that has one Container. The Container
does not specify a memory request and limit.
-->
<p>现在，如果在 default-mem-example 命名空间创建容器，并且该容器没有声明自己的内存请求和限制值，
它将被指定默认的内存请求 256 MiB 和默认的内存限制 512 MiB。</p>
<p>下面是具有一个容器的 Pod 的配置文件。
容器未指定内存请求和限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-defaults-pod.yaml" download="admin/resource/memory-defaults-pod.yaml"><code>admin/resource/memory-defaults-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-defaults-pod-yaml')" title="Copy admin/resource/memory-defaults-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-defaults-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod.
-->
<p>创建 Pod</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod.yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 的详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod default-mem-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
The output shows that the Pod's Container has a memory request of 256 MiB and
a memory limit of 512 MiB. These are the default values specified by the LimitRange.
-->
<p>输出内容显示该 Pod 的容器有 256 MiB 的内存请求和 512 MiB 的内存限制。
这些都是 LimitRange 设置的默认值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">containers:
- image: nginx
  imagePullPolicy: Always
  name: default-mem-demo-ctr
  resources:
    limits:
      memory: 512Mi
    requests:
      memory: 256Mi
</code></pre></div><!--
Delete your Pod:
-->
<p>删除你的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod default-mem-demo --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
## What if you specify a Container's limit, but not its request?

Here's the configuration file for a Pod that has one Container. The Container
specifies a memory limit, but not a request:
-->
<h2 id="声明容器的限制而不声明它的请求会怎么样">声明容器的限制而不声明它的请求会怎么样？</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。该容器声明了内存限制，而没有声明内存请求：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-defaults-pod-2.yaml" download="admin/resource/memory-defaults-pod-2.yaml"><code>admin/resource/memory-defaults-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-defaults-pod-2-yaml')" title="Copy admin/resource/memory-defaults-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-defaults-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1Gi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-2.yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 的详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod default-mem-demo-2 --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
The output shows that the Container's memory request is set to match its memory limit.
Notice that the Container was not assigned the default memory request value of 256Mi.
-->
<p>输出结果显示容器的内存请求被设置为它的内存限制相同的值。注意该容器没有被指定默认的内存请求值 256MiB。</p>
<pre><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><!--
## What if you specify a Container's request, but not its limit?
-->
<h2 id="声明容器的内存请求而不声明内存限制会怎么样">声明容器的内存请求而不声明内存限制会怎么样？</h2>
<!--
Here's the configuration file for a Pod that has one Container. The Container
specifies a memory request, but not a limit:
-->
<p>这里给出了一个包含一个容器的 Pod 的配置文件。该容器声明了内存请求，但没有内存限制：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-defaults-pod-3.yaml" download="admin/resource/memory-defaults-pod-3.yaml"><code>admin/resource/memory-defaults-pod-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-defaults-pod-3-yaml')" title="Copy admin/resource/memory-defaults-pod-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-defaults-pod-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo-3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo-3-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;128Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-3.yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
View the Pod's specification:
-->
<p>查看 Pod 声明：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod default-mem-demo-3 --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
The output shows that the Container's memory request is set to the value specified in the
Container's configuration file. The Container's memory limit is set to 512Mi, which is the
default memory limit for the namespace.
-->
<p>输出结果显示该容器的内存请求被设置为了容器配置文件中声明的数值。
容器的内存限制被设置为 512MiB，即命名空间的默认内存限制。</p>
<pre><code>resources:
  limits:
    memory: 512Mi
  requests:
    memory: 128Mi
</code></pre><!--
## Motivation for default memory limits and requests

If your namespace has a resource quota,
it is helpful to have a default value in place for memory limit.
Here are two of the restrictions that a resource quota imposes on a namespace:
-->
<h2 id="设置默认内存限制和请求的动机">设置默认内存限制和请求的动机</h2>
<p>如果你的命名空间有资源配额，那么默认内存限制是很有帮助的。
下面是一个例子，通过资源配额为命名空间设置两项约束：</p>
<!--
* Every Container that runs in the namespace must have its own memory limit.
* The total amount of memory used by all Containers in the namespace must not exceed a specified limit.
-->
<ul>
<li>运行在命名空间中的每个容器必须有自己的内存限制。</li>
<li>命名空间中所有容器的内存使用量之和不能超过声明的限制值。</li>
</ul>
<!--
If a Container does not specify its own memory limit, it is given the default limit, and then
it can be allowed to run in a namespace that is restricted by a quota.
-->
<p>如果一个容器没有声明自己的内存限制，会被指定默认限制，然后它才会被允许在限定了配额的命名空间中运行。</p>
<!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace default-mem-example
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
### For cluster administrators

* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)

* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)

* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)

* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)

* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)

* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">为命名空间配置默认的 CPU 请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置最小和最大内存限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置最小和最大 CPU 限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)

* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)

* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-320af95e480962c538ebef7ae205845c">3.2 - 为命名空间配置默认的 CPU 请求和限制</h1>
    
	<!--
title: Configure Default CPU Requests and Limits for a Namespace
content_type: task
weight: 20
-->
<!-- overview -->
<!--
This page shows how to configure default CPU requests and limits for a namespace.
A Kubernetes cluster can be divided into namespaces. If a Container is created in a namespace
that has a default CPU limit, and the Container does not specify its own CPU limit, then
the Container is assigned the default CPU limit. Kubernetes assigns a default CPU request
under certain conditions that are explained later in this topic.
-->
<p>本章介绍怎样为命名空间配置默认的 CPU 请求和限制。
一个 Kubernetes 集群可被划分为多个命名空间。如果在配置了 CPU 限制的命名空间创建容器，
并且该容器没有声明自己的 CPU 限制，那么这个容器会被指定默认的 CPU 限制。
Kubernetes 在一些特定情况还会指定 CPU 请求，本文后续章节将会对其进行解释。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便本练习中创建的资源和集群的其余部分相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace default-cpu-example
</code></pre></div><!--
## Create a LimitRange and a Pod

Here's the configuration file for a LimitRange object. The configuration specifies
a default CPU request and a default CPU limit.
-->
<h2 id="创建-limitrange-和-pod">创建 LimitRange 和 Pod</h2>
<p>这里给出了 LimitRange 对象的配置文件。该配置声明了一个默认的 CPU 请求和一个默认的 CPU 限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-defaults.yaml" download="admin/resource/cpu-defaults.yaml"><code>admin/resource/cpu-defaults.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-defaults-yaml')" title="Copy admin/resource/cpu-defaults.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-defaults-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-limit-range<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">default</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">defaultRequest</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">0.5</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the LimitRange in the default-cpu-example namespace:
-->
<p>在命名空间 default-cpu-example 中创建 LimitRange 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</code></pre></div><!--
Now if a Container is created in the default-cpu-example namespace, and the
Container does not specify its own values for CPU request and CPU limit,
the Container is given a default CPU request of 0.5 and a default
CPU limit of 1.

Here's the configuration file for a Pod that has one Container. The Container
does not specify a CPU request and limit.
-->
<p>现在如果在 default-cpu-example 命名空间创建一个容器，该容器没有声明自己的 CPU 请求和限制时，
将会给它指定默认的 CPU 请求0.5和默认的 CPU 限制值1.</p>
<p>这里给出了包含一个容器的 Pod 的配置文件。该容器没有声明 CPU 请求和限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-defaults-pod.yaml" download="admin/resource/cpu-defaults-pod.yaml"><code>admin/resource/cpu-defaults-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-defaults-pod-yaml')" title="Copy admin/resource/cpu-defaults-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-defaults-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod.
-->
<p>创建 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</code></pre></div><!--
View the Pod's specification:
-->
<p>查看该 Pod 的声明：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod default-cpu-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-cpu-example
</code></pre></div><!--
The output shows that the Pod's Container has a CPU request of 500 millicpus and
a CPU limit of 1 cpu. These are the default values specified by the LimitRange.
-->
<p>输出显示该 Pod 的容器有一个500 millicpus的 CPU 请求和一个1 cpu的 CPU 限制。这些是 LimitRange 声明的默认值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">containers:
- image: nginx
  imagePullPolicy: Always
  name: default-cpu-demo-ctr
  resources:
    limits:
      cpu: <span style="color:#b44">&#34;1&#34;</span>
    requests:
      cpu: 500m
</code></pre></div><!--
## What if you specify a Container's limit, but not its request?

Here's the configuration file for a Pod that has one Container. The Container
specifies a CPU limit, but not a request:
-->
<h2 id="你只声明容器的限制-而不声明请求会怎么样">你只声明容器的限制，而不声明请求会怎么样？</h2>
<p>这是包含一个容器的 Pod 的配置文件。该容器声明了 CPU 限制，而没有声明 CPU 请求。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-defaults-pod-2.yaml" download="admin/resource/cpu-defaults-pod-2.yaml"><code>admin/resource/cpu-defaults-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-defaults-pod-2-yaml')" title="Copy admin/resource/cpu-defaults-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-defaults-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-2.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</code></pre></div><!--
View the Pod specification:
-->
<p>查看 Pod 的声明：</p>
<pre><code>kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example
</code></pre><!--
The output shows that the Container's CPU request is set to match its CPU limit.
Notice that the Container was not assigned the default CPU request value of 0.5 cpu.
-->
<p>输出显示该容器的 CPU 请求和 CPU 限制设置相同。注意该容器没有被指定默认的 CPU 请求值0.5 cpu。</p>
<pre><code>resources:
  limits:
    cpu: &quot;1&quot;
  requests:
    cpu: &quot;1&quot;
</code></pre><!--
## What if you specify a Container's request, but not its limit?

Here's the configuration file for a Pod that has one Container. The Container
specifies a CPU request, but not a limit:
-->
<h2 id="你只声明容器的请求-而不声明它的限制会怎么样">你只声明容器的请求，而不声明它的限制会怎么样？</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。该容器声明了 CPU 请求，而没有声明 CPU 限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-defaults-pod-3.yaml" download="admin/resource/cpu-defaults-pod-3.yaml"><code>admin/resource/cpu-defaults-pod-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-defaults-pod-3-yaml')" title="Copy admin/resource/cpu-defaults-pod-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-defaults-pod-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-3-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0.75&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-3.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</code></pre></div><!--
View the Pod specification:
-->
<p>查看 Pod 的规约：</p>
<pre><code>kubectl get pod default-cpu-demo-3 --output=yaml --namespace=default-cpu-example
</code></pre><!--
The output shows that the Container's CPU request is set to the value specified in the
Container's configuration file. The Container's CPU limit is set to 1 cpu, which is the
default CPU limit for the namespace.
-->
<p>结果显示该容器的 CPU 请求被设置为容器配置文件中声明的数值。
容器的CPU限制被设置为 1 CPU，即该命名空间的默认 CPU 限制值。</p>
<pre><code>resources:
  limits:
    cpu: &quot;1&quot;
  requests:
    cpu: 750m
</code></pre><!--
## Motivation for default CPU limits and requests

If your namespace has a
[resource quota](/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/),
it is helpful to have a default value in place for CPU limit.
Here are two of the restrictions that a resource quota imposes on a namespace:

* Every Container that runs in the namespace must have its own CPU limit.
* The total amount of CPU used by all Containers in the namespace must not exceed a specified limit.

If a Container does not specify its own CPU limit, it is given the default limit, and then
it can be allowed to run in a namespace that is restricted by a quota.
-->
<h2 id="默认-cpu-限制和请求的动机">默认 CPU 限制和请求的动机</h2>
<p>如果你的命名空间有一个
<a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">资源配额</a>，
那么有一个默认的 CPU 限制是有帮助的。这里有资源配额强加给命名空间的两条限制：</p>
<ul>
<li>命名空间中运行的每个容器必须有自己的 CPU 限制。</li>
<li>命名空间中所有容器使用的 CPU 总和不能超过一个声明值。</li>
</ul>
<p>如果容器没有声明自己的 CPU 限制，将会给它一个默认限制，这样它就能被允许运行在一个有配额限制的命名空间中。</p>
<!--
## Clean up

Delete your namespace:

```shell
kubectl delete namespace default-cpu-example
```
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace constraints-cpu-example
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)
* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)
* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)
* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-adb489b1ab985c9215657b0d4c6ae92b">3.3 - 配置命名空间的最小和最大内存约束</h1>
    
	<!--
title: Configure Minimum and Maximum Memory Constraints for a Namespace
content_type: task
weight: 30
-->
<!-- overview -->
<!--
This page shows how to set minimum and maximum values for memory used by Containers
running in a namespace. You specify minimum and maximum memory values in a
[LimitRange](/docs/reference/generated/kubernetes-api/v1.23/#limitrange-v1-core)
object. If a Pod does not meet the constraints imposed by the LimitRange,
it cannot be created in the namespace.
-->
<p>本页介绍如何设置在命名空间中运行的容器使用的内存的最小值和最大值。 你可以在
<a href="/docs/reference/generated/kubernetes-api/v1.23/#limitrange-v1-core">LimitRange</a>
对象中指定最小和最大内存值。如果 Pod 不满足 LimitRange 施加的约束，则无法在命名空间中创建它。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
Each node in your cluster must have at least 1 GiB of memory.
-->
<p>集群中每个节点必须至少要有 1 GiB 的内存。</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便在此练习中创建的资源与群集的其余资源隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace constraints-mem-example
</code></pre></div><!--
## Create a LimitRange and a Pod

Here's the configuration file for a LimitRange:
-->
<h2 id="创建-limitrange-和-pod">创建 LimitRange 和 Pod</h2>
<p>下面是 LimitRange 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-constraints.yaml" download="admin/resource/memory-constraints.yaml"><code>admin/resource/memory-constraints.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-constraints-yaml')" title="Copy admin/resource/memory-constraints.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-constraints-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mem-min-max-demo-lr<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">max</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">min</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>500Mi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the LimitRange:
-->
<p>创建 LimitRange:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
View detailed information about the LimitRange:
-->
<p>查看 LimitRange 的详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get limitrange mem-min-max-demo-lr --namespace<span style="color:#666">=</span>constraints-mem-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows the minimum and maximum memory constraints as expected. But
notice that even though you didn't specify default values in the configuration
file for the LimitRange, they were created automatically.
-->
<p>输出显示预期的最小和最大内存约束。 但请注意，即使你没有在 LimitRange 的配置文件中指定默认值，也会自动创建它们。</p>
<pre><code>  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container
</code></pre><!--
Now whenever a Container is created in the constraints-mem-example namespace, Kubernetes
performs these steps:

* If the Container does not specify its own memory request and limit, assign the default
memory request and limit to the Container.

* Verify that the Container has a memory request that is greater than or equal to 500 MiB.

* Verify that the Container has a memory limit that is less than or equal to 1 GiB.

Here's the configuration file for a Pod that has one Container. The Container manifest
specifies a memory request of 600 MiB and a memory limit of 800 MiB. These satisfy the
minimum and maximum memory constraints imposed by the LimitRange.
-->
<p>现在，只要在 constraints-mem-example 命名空间中创建容器，Kubernetes 就会执行下面的步骤：</p>
<ul>
<li>
<p>如果 Container 未指定自己的内存请求和限制，将为它指定默认的内存请求和限制。</p>
</li>
<li>
<p>验证 Container 的内存请求是否大于或等于500 MiB。</p>
</li>
<li>
<p>验证 Container 的内存限制是否小于或等于1 GiB。</p>
</li>
</ul>
<p>这里给出了包含一个 Container 的 Pod 配置文件。Container 声明了 600 MiB 的内存请求和
800 MiB 的内存限制， 这些满足了 LimitRange 施加的最小和最大内存约束。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-constraints-pod.yaml" download="admin/resource/memory-constraints-pod.yaml"><code>admin/resource/memory-constraints-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-constraints-pod-yaml')" title="Copy admin/resource/memory-constraints-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-constraints-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;600Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>确认下 Pod 中的容器在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod constraints-mem-demo --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod constraints-mem-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
The output shows that the Container has a memory request of 600 MiB and a memory limit
of 800 MiB. These satisfy the constraints imposed by the LimitRange.
-->
<p>输出结果显示容器的内存请求为600 MiB，内存限制为800 MiB。这些满足了 LimitRange 设定的限制范围。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>800Mi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>600Mi<span style="color:#bbb">
</span></code></pre></div><!--
Delete your Pod:
-->
<p>删除你创建的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod constraints-mem-demo --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
## Attempt to create a Pod that exceeds the maximum memory constraint

Here's the configuration file for a Pod that has one Container. The Container specifies a
memory request of 800 MiB and a memory limit of 1.5 GiB.
-->
<h2 id="尝试创建一个超过最大内存限制的-pod">尝试创建一个超过最大内存限制的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。容器声明了800 MiB 的内存请求和1.5 GiB 的内存限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-constraints-pod-2.yaml" download="admin/resource/memory-constraints-pod-2.yaml"><code>admin/resource/memory-constraints-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-constraints-pod-2-yaml')" title="Copy admin/resource/memory-constraints-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-constraints-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1.5Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Attempt to create the Pod:
-->
<p>尝试创建 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-2.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
The output shows that the Pod does not get created, because the Container specifies a memory limit that is
too large:
-->
<p>输出结果显示 Pod 没有创建成功，因为容器声明的内存限制太大了：</p>
<pre><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/memory-constraints-pod-2.yaml&quot;:
pods &quot;constraints-mem-demo-2&quot; is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi.
</code></pre><!--
## Attempt to create a Pod that does not meet the minimum memory request

Here's the configuration file for a Pod that has one Container. The Container specifies a
memory request of 100 MiB and a memory limit of 800 MiB.
-->
<h2 id="尝试创建一个不满足最小内存请求的-pod">尝试创建一个不满足最小内存请求的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。容器声明了100 MiB 的内存请求和800 MiB 的内存限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-constraints-pod-3.yaml" download="admin/resource/memory-constraints-pod-3.yaml"><code>admin/resource/memory-constraints-pod-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-constraints-pod-3-yaml')" title="Copy admin/resource/memory-constraints-pod-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-constraints-pod-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-3-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Attempt to create the Pod:
-->
<p>尝试创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-3.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
The output shows that the Pod does not get created, because the Container specifies a memory
request that is too small:
-->
<p>输出结果显示 Pod 没有创建成功，因为容器声明的内存请求太小了：</p>
<pre><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/memory-constraints-pod-3.yaml&quot;:
pods &quot;constraints-mem-demo-3&quot; is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi.
</code></pre><!--
## Create a Pod that does not specify any memory request or limit

Here's the configuration file for a Pod that has one Container. The Container does not
specify a memory request, and it does not specify a memory limit.
-->
<h2 id="创建一个没有声明内存请求和限制的-pod">创建一个没有声明内存请求和限制的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。容器没有声明内存请求，也没有声明内存限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-constraints-pod-4.yaml" download="admin/resource/memory-constraints-pod-4.yaml"><code>admin/resource/memory-constraints-pod-4.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-constraints-pod-4-yaml')" title="Copy admin/resource/memory-constraints-pod-4.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-constraints-pod-4-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-4<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-4-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-4.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 详情：</p>
<pre><code>kubectl get pod constraints-mem-demo-4 --namespace=constraints-mem-example --output=yaml
</code></pre><!--
The output shows that the Pod's Container has a memory request of 1 GiB and a memory limit of 1 GiB.
How did the Container get those values?
-->
<p>输出结果显示 Pod 的内存请求为1 GiB，内存限制为1 GiB。容器怎样获得哪些数值呢？</p>
<pre><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><!--
Because your Container did not specify its own memory request and limit, it was given the
[default memory request and limit](/docs/tasks/administer-cluster/memory-default-namespace/)
from the LimitRange.
-->
<p>因为你的容器没有声明自己的内存请求和限制，它从 LimitRange 那里获得了
<a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">默认的内存请求和限制</a>。</p>
<!--
At this point, your Container might be running or it might not be running. Recall that a prerequisite
for this task is that your Nodes have at least 1 GiB of memory. If each of your Nodes has only
1 GiB of memory, then there is not enough allocatable memory on any Node to accommodate a memory
request of 1 GiB. If you happen to be using Nodes with 2 GiB of memory, then you probably have
enough space to accommodate the 1 GiB request.

Delete your Pod:
-->
<p>此时，你的容器可能运行起来也可能没有运行起来。
回想一下我们本次任务的先决条件是你的每个节点都至少有1 GiB 的内存。
如果你的每个节点都只有1 GiB 的内存，那将没有一个节点拥有足够的可分配内存来满足1 GiB 的内存请求。</p>
<p>删除你的 Pod：</p>
<pre><code>kubectl delete pod constraints-mem-demo-4 --namespace=constraints-mem-example
</code></pre><!--
## Enforcement of minimum and maximum memory constraints

The maximum and minimum memory constraints imposed on a namespace by a LimitRange are enforced only
when a Pod is created or updated. If you change the LimitRange, it does not affect
Pods that were created previously.
-->
<h2 id="强制执行内存最小和最大限制">强制执行内存最小和最大限制</h2>
<p>LimitRange 为命名空间设定的最小和最大内存限制只有在 Pod 创建和更新时才会强制执行。
如果你更新 LimitRange，它不会影响此前创建的 Pod。</p>
<!--
## Motivation for minimum and maximum memory constraints
-->
<h2 id="设置内存最小和最大限制的动因">设置内存最小和最大限制的动因</h2>
<!--
As a cluster administrator, you might want to impose restrictions on the amount of memory that Pods can use.
For example:

* Each Node in a cluster has 2 GB of memory. You do not want to accept any Pod that requests
  more than 2 GB of memory, because no Node in the cluster can support the request.

* A cluster is shared by your production and development departments.
  You want to allow production workloads to consume up to 8 GB of memory, but
  you want development workloads to be limited to 512 MB. You create separate namespaces
  for production and development, and you apply memory constraints to each namespace.
-->
<p>作为集群管理员，你可能想规定 Pod 可以使用的内存总量限制。例如：</p>
<ul>
<li>集群的每个节点有 2 GB 内存。你不想接受任何请求超过 2 GB 的 Pod，因为集群中没有节点可以满足。</li>
<li>集群由生产部门和开发部门共享。你希望允许产品部门的负载最多耗用 8 GB 内存，
但是开发部门的负载最多可使用 512 MiB。
这时，你可以为产品部门和开发部门分别创建名字空间，并为各个名字空间设置内存约束。</li>
</ul>
<!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace constraints-mem-example
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)
* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)
* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)
* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a87cbd1f9379dac7a48ae320da68a9ad">3.4 - 为命名空间配置 CPU 最小和最大约束</h1>
    
	<!--
title: Configure Minimum and Maximum CPU Constraints for a Namespace
content_type: task
weight: 40
-->
<!-- overview -->
<!--
This page shows how to set minimum and maximum values for the CPU resources used by Containers
and Pods in a namespace. You specify minimum and maximum CPU values in a
[LimitRange](/docs/reference/generated/kubernetes-api/v1.23/#limitrange-v1-core)
object. If a Pod does not meet the constraints imposed by the LimitRange, it cannot be created
in the namespace.
-->
<p>本页介绍如何为命名空间中容器和 Pod 使用的 CPU 资源设置最小和最大值。
你可以通过
<a href="/docs/reference/generated/kubernetes-api/v1.23/#limitrange-v1-core">LimitRange</a>
对象声明 CPU 的最小和最大值. 如果 Pod 不能满足 LimitRange 的限制，它就不能在命名空间中创建。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
Your cluster must have at least 1 CPU available for use to run the task examples.
-->
<p>你的集群中每个节点至少要有 1 个 CPU 可用才能运行本任务示例。</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便本练习中创建的资源和集群的其余资源相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace constraints-cpu-example
</code></pre></div><!--
## Create a LimitRange and a Pod

Here's the configuration file for a LimitRange:
-->
<h2 id="创建-limitrange-和-pod">创建 LimitRange 和 Pod</h2>
<p>这里给出了 LimitRange 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-constraints.yaml" download="admin/resource/cpu-constraints.yaml"><code>admin/resource/cpu-constraints.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-constraints-yaml')" title="Copy admin/resource/cpu-constraints.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-constraints-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-min-max-demo-lr<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">max</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">min</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the LimitRange:
-->
<p>创建 LimitRange:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
View detailed information about the LimitRange:
-->
<p>查看 LimitRange 详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get limitrange cpu-min-max-demo-lr --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
The output shows the minimum and maximum CPU constraints as expected. But
notice that even though you didn't specify default values in the configuration
file for the LimitRange, they were created automatically.
-->
<p>输出结果显示 CPU 的最小和最大限制符合预期。但需要注意的是，尽管你在 LimitRange
的配置文件中你没有声明默认值，默认值也会被自动创建。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">default</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">defaultRequest</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">max</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">min</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>200m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></code></pre></div><!--
Now whenever a Container is created in the constraints-cpu-example namespace, Kubernetes
performs these steps:

* If the Container does not specify its own CPU request and limit, assign the default
CPU request and limit to the Container.

* Verify that the Container specifies a CPU request that is greater than or equal to 200 millicpu.

* Verify that the Container specifies a CPU limit that is less than or equal to 800 millicpu.
-->
<p>现在不管什么时候在 constraints-cpu-example 命名空间中创建容器，Kubernetes 都会执行下面这些步骤：</p>
<ul>
<li>
<p>如果容器没有声明自己的 CPU 请求和限制，将为容器指定默认 CPU 请求和限制。</p>
</li>
<li>
<p>核查容器声明的 CPU 请求确保其大于或者等于 200 millicpu。</p>
</li>
<li>
<p>核查容器声明的 CPU 限制确保其小于或者等于 800 millicpu。</p>
</li>
</ul>
<!--
When creating a `LimitRange` object, you can specify limits on huge-pages
or GPUs as well. However, when both `default` and `defaultRequest` are specified
on these resources, the two values must be the same.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 当创建 LimitRange 对象时，你也可以声明大页面和 GPU 的限制。
当这些资源同时声明了 'default' 和 'defaultRequest' 参数时，两个参数值必须相同。
</div>
<!--
Here's the configuration file for a Pod that has one Container. The Container manifest
specifies a CPU request of 500 millicpu and a CPU limit of 800 millicpu. These satisfy the
minimum and maximum CPU constraints imposed by the LimitRange.
-->
<p>这里给出了包含一个容器的 Pod 的配置文件。
该容器声明了 500 millicpu 的 CPU 请求和 800 millicpu 的 CPU 限制。
这些参数满足了 LimitRange 对象规定的 CPU 最小和最大限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-constraints-pod.yaml" download="admin/resource/cpu-constraints-pod.yaml"><code>admin/resource/cpu-constraints-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-constraints-pod-yaml')" title="Copy admin/resource/cpu-constraints-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-constraints-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>确认一下 Pod 中的容器在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod constraints-cpu-demo --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 的详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod constraints-cpu-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
The output shows that the Container has a CPU request of 500 millicpu and CPU limit
of 800 millicpu. These satisfy the constraints imposed by the LimitRange.
-->
<p>输出结果表明容器的 CPU 请求为 500 millicpu，CPU 限制为 800 millicpu。
这些参数满足 LimitRange 规定的限制范围。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>500m<span style="color:#bbb">
</span></code></pre></div><!--
## Delete the Pod
-->
<h2 id="删除-pod">删除 Pod</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod constraints-cpu-demo --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
## Attempt to create a Pod that exceeds the maximum CPU constraint

Here's the configuration file for a Pod that has one Container. The Container specifies a
CPU request of 500 millicpu and a cpu limit of 1.5 cpu.
-->
<h2 id="尝试创建一个超过最大-cpu-限制的-pod">尝试创建一个超过最大 CPU 限制的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。容器声明了 500 millicpu 的 CPU
请求和 1.5 CPU 的 CPU 限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-constraints-pod-2.yaml" download="admin/resource/cpu-constraints-pod-2.yaml"><code>admin/resource/cpu-constraints-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-constraints-pod-2-yaml')" title="Copy admin/resource/cpu-constraints-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-constraints-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1.5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Attempt to create the Pod:
-->
<p>尝试创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-2.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
The output shows that the Pod does not get created, because the Container specifies a CPU limit that is
too large:
-->
<p>输出结果表明 Pod 没有创建成功，因为容器声明的 CPU 限制太大了：</p>
<pre><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/cpu-constraints-pod-2.yaml&quot;:
pods &quot;constraints-cpu-demo-2&quot; is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m.
</code></pre><!--
## Attempt to create a Pod that does not meet the minimum CPU request

Here's the configuration file for a Pod that has one Container. The Container specifies a
CPU request of 100 millicpu and a CPU limit of 800 millicpu.
-->
<h2 id="尝试创建一个不满足最小-cpu-请求的-pod">尝试创建一个不满足最小 CPU 请求的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。该容器声明了100 millicpu的 CPU 请求和800 millicpu的 CPU 限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-constraints-pod-3.yaml" download="admin/resource/cpu-constraints-pod-3.yaml"><code>admin/resource/cpu-constraints-pod-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-constraints-pod-3-yaml')" title="Copy admin/resource/cpu-constraints-pod-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-constraints-pod-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-3-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Attempt to create the Pod:
-->
<p>尝试创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-3.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
The output shows that the Pod does not get created, because the Container specifies a CPU
request that is too small:
-->
<p>输出结果显示 Pod 没有创建成功，因为容器声明的 CPU 请求太小了：</p>
<pre><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/cpu-constraints-pod-3.yaml&quot;:
pods &quot;constraints-cpu-demo-4&quot; is forbidden: minimum cpu usage per Container is 200m, but request is 100m.
</code></pre><!--
## Create a Pod that does not specify any CPU request or limit

Here's the configuration file for a Pod that has one Container. The Container does not
specify a CPU request, and it does not specify a CPU limit.
-->
<h2 id="创建一个没有声明-cpu-请求和-cpu-限制的-pod">创建一个没有声明 CPU 请求和 CPU 限制的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。该容器没有设定 CPU 请求和 CPU 限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-constraints-pod-4.yaml" download="admin/resource/cpu-constraints-pod-4.yaml"><code>admin/resource/cpu-constraints-pod-4.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-constraints-pod-4-yaml')" title="Copy admin/resource/cpu-constraints-pod-4.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-constraints-pod-4-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-4<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-4-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>vish/stress<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-4.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 的详情：</p>
<pre><code>kubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml
</code></pre><!--
The output shows that the Pod's Container has a CPU request of 800 millicpu and a CPU limit of 800 millicpu.
How did the Container get those values?
-->
<p>输出结果显示 Pod 的容器有个 800 millicpu 的 CPU 请求和 800 millicpu 的 CPU 限制。
容器是怎样得到那些值的呢？</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span></code></pre></div><!--
Because your Container did not specify its own CPU request and limit, it was given the
[default CPU request and limit](/docs/tasks/administer-cluster/cpu-default-namespace/)
from the LimitRange.
-->
<p>因为你的 Container 没有声明自己的 CPU 请求和限制，LimitRange 给它指定了
<a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">默认的 CPU 请求和限制</a></p>
<!--
At this point, your Container might be running or it might not be running. Recall that a prerequisite
for this task is that your Nodes have at least 1 CPU. If each of your Nodes has only
1 CPU, then there might not be enough allocatable CPU on any Node to accommodate a request
of 800 millicpu. If you happen to be using Nodes with 2 CPU, then you probably have
enough CPU to accommodate the 800 millicpu request.

Delete your Pod:
-->
<p>此时，你的容器可能运行也可能没有运行。
回想一下，本任务的先决条件是你的节点要有 1 个 CPU。
如果你的每个节点仅有 1 个 CPU，那么可能没有任何一个节点可以满足 800 millicpu 的 CPU 请求。
如果你在用的节点恰好有两个 CPU，那么你才可能有足够的 CPU 来满足 800 millicpu 的请求。</p>
<pre><code>kubectl delete pod constraints-cpu-demo-4 --namespace=constraints-cpu-example
</code></pre><!--
## Enforcement of minimum and maximum CPU constraints

The maximum and minimum CPU constraints imposed on a namespace by a LimitRange are enforced only
when a Pod is created or updated. If you change the LimitRange, it does not affect
Pods that were created previously.
-->
<h2 id="cpu-最小和最大限制的强制执行">CPU 最小和最大限制的强制执行</h2>
<p>只有当 Pod 创建或者更新时，LimitRange 为命名空间规定的 CPU 最小和最大限制才会被强制执行。
如果你对 LimitRange 进行修改，那不会影响此前创建的 Pod。</p>
<!--
## Motivation for minimum and maximum CPU constraints

As a cluster administrator, you might want to impose restrictions on the CPU resources that Pods can use.
For example:
-->
<h2 id="最小和最大-cpu-限制范围的动机">最小和最大 CPU 限制范围的动机</h2>
<p>作为集群管理员，你可能想设定 Pod 可以使用的 CPU 资源限制。例如：</p>
<!--
* Each Node in a cluster has 2 CPU. You do not want to accept any Pod that requests
more than 2 CPU, because no Node in the cluster can support the request.

* A cluster is shared by your production and development departments.
You want to allow production workloads to consume up to 3 CPU, but you want development workloads to be limited
to 1 CPU. You create separate namespaces for production and development, and you apply CPU constraints to
each namespace.
-->
<ul>
<li>集群中的每个节点有两个 CPU。你不想接受任何请求超过 2 个 CPU 的 Pod，因为集群中没有节点可以支持这种请求。</li>
<li>你的生产和开发部门共享一个集群。你想允许生产工作负载消耗 3 个 CPU，
而开发部门工作负载的消耗限制为 1 个 CPU。
你可以为生产和开发创建不同的命名空间，并且为每个命名空间都应用 CPU 限制。</li>
</ul>
<!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace constraints-cpu-example
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)

* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)

* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)

* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)

* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)

* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员参考">集群管理员参考：</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考：</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fe3283559a3df299aae3ee00ecea2fad">3.5 - 为命名空间配置内存和 CPU 配额</h1>
    
	<!--
title: Configure Memory and CPU Quotas for a Namespace
content_type: task
weight: 50
-->
<!-- overview -->
<!--
This page shows how to set quotas for the total amount memory and CPU that
can be used by all Containers running in a namespace. You specify quotas in a
[ResourceQuota](/docs/reference/generated/kubernetes-api/v1.23/#resourcequota-v1-core)
object.
-->
<p>本文介绍怎样为命名空间设置容器可用的内存和 CPU 总量。你可以通过
<a href="/docs/reference/generated/kubernetes-api/v1.23/#resourcequota-v1-core">ResourceQuota</a>
对象设置配额.</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
Each node in your cluster must have at least 1 GiB of memory.
-->
<p>集群中每个节点至少有 1 GiB 的内存。</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便本练习中创建的资源和集群的其余部分相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace quota-mem-cpu-example
</code></pre></div><!--
## Create a ResourceQuota

Here is the configuration file for a ResourceQuota object:
-->
<h2 id="创建-resourcequota">创建 ResourceQuota</h2>
<p>这里给出一个 ResourceQuota 对象的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-mem-cpu.yaml" download="admin/resource/quota-mem-cpu.yaml"><code>admin/resource/quota-mem-cpu.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-mem-cpu-yaml')" title="Copy admin/resource/quota-mem-cpu.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-mem-cpu-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mem-cpu-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests.cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests.memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">limits.cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">limits.memory</span>:<span style="color:#bbb"> </span>2Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the ResourceQuota:
-->
<p>创建 ResourceQuota</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu.yaml --namespace<span style="color:#666">=</span>quota-mem-cpu-example
</code></pre></div><!--
View detailed information about the ResourceQuota:
-->
<p>查看 ResourceQuota 详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get resourcequota mem-cpu-demo --namespace<span style="color:#666">=</span>quota-mem-cpu-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The ResourceQuota places these requirements on the quota-mem-cpu-example namespace:

* Every Container must have a memory request, memory limit, cpu request, and cpu limit.
* The memory request total for all Containers must not exceed 1 GiB.
* The memory limit total for all Containers must not exceed 2 GiB.
* The CPU request total for all Containers must not exceed 1 cpu.
* The CPU limit total for all Containers must not exceed 2 cpu.
-->
<p>ResourceQuota 在 quota-mem-cpu-example 命名空间中设置了如下要求：</p>
<ul>
<li>每个容器必须有内存请求和限制，以及 CPU 请求和限制。</li>
<li>所有容器的内存请求总和不能超过1 GiB。</li>
<li>所有容器的内存限制总和不能超过2 GiB。</li>
<li>所有容器的 CPU 请求总和不能超过1 cpu。</li>
<li>所有容器的 CPU 限制总和不能超过2 cpu。</li>
</ul>
<!--
## Create a Pod

Here is the configuration file for a Pod:
-->
<h2 id="创建-pod">创建 Pod</h2>
<p>这里给出 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-mem-cpu-pod.yaml" download="admin/resource/quota-mem-cpu-pod.yaml"><code>admin/resource/quota-mem-cpu-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-mem-cpu-pod-yaml')" title="Copy admin/resource/quota-mem-cpu-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-mem-cpu-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;600Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;400m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod.yaml --namespace<span style="color:#666">=</span>quota-mem-cpu-example
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>检查下 Pod 中的容器在运行：</p>
<pre><code>kubectl get pod quota-mem-cpu-demo --namespace=quota-mem-cpu-example
</code></pre><!--
Once again, view detailed information about the ResourceQuota:
-->
<p>再查看 ResourceQuota 的详情：</p>
<pre><code>kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml
</code></pre><!--
The output shows the quota along with how much of the quota has been used.
You can see that the memory and CPU requests and limits for your Pod do not
exceed the quota.
-->
<p>输出结果显示了配额以及有多少配额已经被使用。你可以看到 Pod 的内存和 CPU 请求值及限制值没有超过配额。</p>
<pre><code>status:
  hard:
    limits.cpu: &quot;2&quot;
    limits.memory: 2Gi
    requests.cpu: &quot;1&quot;
    requests.memory: 1Gi
  used:
    limits.cpu: 800m
    limits.memory: 800Mi
    requests.cpu: 400m
    requests.memory: 600Mi
</code></pre><!--
## Attempt to create a second Pod

Here is the configuration file for a second Pod:
-->
<h2 id="尝试创建第二个-pod">尝试创建第二个 Pod</h2>
<p>这里给出了第二个 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-mem-cpu-pod-2.yaml" download="admin/resource/quota-mem-cpu-pod-2.yaml"><code>admin/resource/quota-mem-cpu-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-mem-cpu-pod-2-yaml')" title="Copy admin/resource/quota-mem-cpu-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-mem-cpu-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;700Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;400m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the Pod has a memory request of 700 MiB.
Notice that the sum of the used memory request and this new memory
request exceeds the memory request quota. 600 MiB + 700 MiB > 1 GiB.

Attempt to create the Pod:
-->
<p>配置文件中，你可以看到 Pod 的内存请求为 700 MiB。
请注意新的内存请求与已经使用的内存请求只和超过了内存请求的配额。
600 MiB + 700 MiB &gt; 1 GiB。</p>
<p>尝试创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod-2.yaml --namespace<span style="color:#666">=</span>quota-mem-cpu-example
</code></pre></div><!--
The second Pod does not get created. The output shows that creating the second Pod
would cause the memory request total to exceed the memory request quota.
-->
<p>第二个 Pod 不能被创建成功。输出结果显示创建第二个 Pod 会导致内存请求总量超过内存请求配额。</p>
<pre><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/quota-mem-cpu-pod-2.yaml&quot;:
pods &quot;quota-mem-cpu-demo-2&quot; is forbidden: exceeded quota: mem-cpu-demo,
requested: requests.memory=700Mi,used: requests.memory=600Mi, limited: requests.memory=1Gi
</code></pre><!--
## Discussion

As you have seen in this exercise, you can use a ResourceQuota to restrict
the memory request total for all Containers running in a namespace.
You can also restrict the totals for memory limit, cpu request, and cpu limit.

If you want to restrict individual Containers, instead of totals for all Containers, use a
[LimitRange](/docs/tasks/administer-cluster/memory-constraint-namespace/).
-->
<h2 id="讨论">讨论</h2>
<p>如你在本练习中所见，你可以用 ResourceQuota 限制命名空间中所有容器的内存请求总量。
同样你也可以限制内存限制总量、CPU 请求总量、CPU 限制总量。</p>
<p>如果你想对单个容器而不是所有容器进行限制，就请使用
<a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">LimitRange</a>。</p>
<!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace quota-mem-cpu-example
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)
* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)
* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)
* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)
* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配CPU资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-40e30a9209e0c9f4153707e43243e9d7">3.6 - 配置命名空间下 Pod 配额</h1>
    
	<!-- overview -->
<!--
This page shows how to set a quota for the total number of Pods that can run
in a namespace. You specify quotas in a
[ResourceQuota](/docs/reference/generated/kubernetes-api/v1.23/#resourcequota-v1-core)
object.
-->
<p>本文主要描述如何配置一个命名空间下可运行的 Pod 个数配额。
你可以使用
<a href="/docs/reference/generated/kubernetes-api/v1.23/#resourcequota-v1-core">ResourceQuota</a>
对象来配置配额。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建一个命名空间">创建一个命名空间</h2>
<p>首先创建一个命名空间，这样可以将本次操作中创建的资源与集群其他资源隔离开来。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace quota-pod-example
</code></pre></div><!--
## Create a ResourceQuota

Here is the configuration file for a ResourceQuota object:
-->
<h2 id="创建-resourcequota">创建 ResourceQuota</h2>
<p>下面是一个 ResourceQuota 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-pod.yaml" download="admin/resource/quota-pod.yaml"><code>admin/resource/quota-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-pod-yaml')" title="Copy admin/resource/quota-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 创建 ResourceQuota: -->
<p>创建这个 ResourceQuota：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace<span style="color:#666">=</span>quota-pod-example
</code></pre></div><!--
View detailed information about the ResourceQuota:
-->
<p>查看资源配额的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get resourcequota pod-demo --namespace<span style="color:#666">=</span>quota-pod-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows that the namespace has a quota of two Pods, and that currently there are
no Pods; that is, none of the quota is used.
-->
<p>从输出的信息我们可以看到，该命名空间下 Pod 的配额是 2 个，目前创建的 Pod 数为 0，
配额使用率为 0。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">used</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
Here is the configuration file for a Deployment:
-->
<p>下面是一个 Deployment 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-pod-deployment.yaml" download="admin/resource/quota-pod-deployment.yaml"><code>admin/resource/quota-pod-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-pod-deployment-yaml')" title="Copy admin/resource/quota-pod-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-pod-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod-quota-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">purpose</span>:<span style="color:#bbb"> </span>quota-demo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">purpose</span>:<span style="color:#bbb"> </span>quota-demo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod-quota-demo<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, `replicas: 3` tells Kubernetes to attempt to create three Pods, all running the same application.

Create the Deployment:
-->
<p>在配置文件中，<code>replicas: 3</code> 告诉 Kubernetes 尝试创建三个 Pods，且运行相同的应用。</p>
<p>创建这个 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace<span style="color:#666">=</span>quota-pod-example
</code></pre></div><!--
View detailed information about the Deployment:
-->
<p>查看 Deployment 的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment pod-quota-demo --namespace<span style="color:#666">=</span>quota-pod-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows that even though the Deployment specifies three replicas, only two
Pods were created because of the quota.
-->
<p>从输出的信息我们可以看到，尽管尝试创建三个 Pod，但是由于配额的限制，只有两个 Pod 能被成功创建。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">availableReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">lastUpdateTime</span>:<span style="color:#bbb"> </span>2017-07-07T20:57:05Z<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">message: &#39;unable to create pods</span>:<span style="color:#bbb"> </span>pods &#34;pod-quota-demo-1650323038-&#34; is forbidden:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">exceeded quota: pod-demo, requested: pods=1, used: pods=2, limited</span>:<span style="color:#bbb"> </span>pods=2&#39;<span style="color:#bbb">
</span></code></pre></div><!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace quota-pod-example
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/)
* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/)
* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/)
* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)
* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理人员参考">集群管理人员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认的内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">为命名空间配置默认的的 CPU 请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存的最小值和最大值约束</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 的最小值和最大值约束</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象的设置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发人员参考">应用开发人员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">给容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">配置 Pod 的服务质量</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7743f043c43f7b12e8654e2227dbc658">4 - 证书</h1>
    
	<!-- 
---
title: Certificates
content_type: task
weight: 20
---
-->
<!-- overview -->
<!-- 
When using client certificate authentication, you can generate certificates
manually through `easyrsa`, `openssl` or `cfssl`.
-->
<p>在使用客户端证书认证的场景下，你可以通过 <code>easyrsa</code>、<code>openssl</code> 或 <code>cfssl</code> 等工具以手工方式生成证书。</p>
<!-- body -->
<h3 id="easyrsa">easyrsa</h3>
<!-- 
**easyrsa** can manually generate certificates for your cluster.
-->
<p><strong>easyrsa</strong> 支持以手工方式为你的集群生成证书。</p>
<!-- 
1.  Download, unpack, and initialize the patched version of easyrsa3.
-->
<ol>
<li>
<p>下载、解压、初始化打过补丁的 easyrsa3。</p>
<pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz
tar xzf easy-rsa.tar.gz
cd easy-rsa-master/easyrsa3
./easyrsa init-pki
</code></pre>
<!-- 
1.  Generate a new certificate authority (CA). `--batch` sets automatic mode;
`--req-cn` specifies the Common Name (CN) for the CA's new root certificate.
-->
</li>
<li>
<p>生成新的证书颁发机构（CA）。参数 <code>--batch</code> 用于设置自动模式；
参数 <code>--req-cn</code> 用于设置新的根证书的通用名称（CN）。</p>
<pre><code>./easyrsa --batch &quot;--req-cn=${MASTER_IP}@`date +%s`&quot; build-ca nopass
</code></pre>
<!-- 
1.  Generate server certificate and key.
The argument `--subject-alt-name` sets the possible IPs and DNS names the API server will
be accessed with. The `MASTER_CLUSTER_IP` is usually the first IP from the service CIDR
that is specified as the `--service-cluster-ip-range` argument for both the API server and
the controller manager component. The argument `--days` is used to set the number of days
after which the certificate expires.
The sample below also assumes that you are using `cluster.local` as the default
DNS domain name.
-->
</li>
<li>
<p>生成服务器证书和秘钥。
参数 <code>--subject-alt-name</code> 设置 API 服务器的 IP 和 DNS 名称。
<code>MASTER_CLUSTER_IP</code> 用于 API 服务器和控制管理器，通常取 CIDR 的第一个 IP，由 <code>--service-cluster-ip-range</code> 的参数提供。
参数 <code>--days</code> 用于设置证书的过期时间。
下面的示例假定你的默认 DNS 域名为 <code>cluster.local</code>。</p>
<pre><code>./easyrsa --subject-alt-name=&quot;IP:${MASTER_IP},&quot;\
&quot;IP:${MASTER_CLUSTER_IP},&quot;\
&quot;DNS:kubernetes,&quot;\
&quot;DNS:kubernetes.default,&quot;\
&quot;DNS:kubernetes.default.svc,&quot;\
&quot;DNS:kubernetes.default.svc.cluster,&quot;\
&quot;DNS:kubernetes.default.svc.cluster.local&quot; \
--days=10000 \
build-server-full server nopass
</code></pre>
<!-- 
1.  Copy `pki/ca.crt`, `pki/issued/server.crt`, and `pki/private/server.key` to your directory.
1.  Fill in and add the following parameters into the API server start parameters:
-->
</li>
<li>
<p>拷贝文件 <code>pki/ca.crt</code>、<code>pki/issued/server.crt</code> 和 <code>pki/private/server.key</code> 到你的目录中。</p>
</li>
<li>
<p>在 API 服务器的启动参数中添加以下参数：</p>
<pre><code>--client-ca-file=/yourdirectory/ca.crt
--tls-cert-file=/yourdirectory/server.crt
--tls-private-key-file=/yourdirectory/server.key
</code></pre>
</li>
</ol>
<h3 id="openssl">openssl</h3>
<!-- 
**openssl** can manually generate certificates for your cluster.
-->
<p><strong>openssl</strong> 支持以手工方式为你的集群生成证书。</p>
<!-- 
1.  Generate a ca.key with 2048bit:
-->
<ol>
<li>
<p>生成一个 2048 位的 ca.key 文件</p>
<pre><code>openssl genrsa -out ca.key 2048
</code></pre>
<!-- 
1.  According to the ca.key generate a ca.crt (use -days to set the certificate effective time):
-->
</li>
<li>
<p>在 ca.key 文件的基础上，生成 ca.crt 文件（用参数 -days 设置证书有效期）</p>
<pre><code>openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=${MASTER_IP}&quot; -days 10000 -out ca.crt
</code></pre>
<!-- 
1.  Generate a server.key with 2048bit:
-->
</li>
<li>
<p>生成一个 2048 位的 server.key 文件：</p>
<pre><code>openssl genrsa -out server.key 2048
</code></pre>
<!-- 
1.  Create a config file for generating a Certificate Signing Request (CSR).
Be sure to substitute the values marked with angle brackets (e.g. `<MASTER_IP>`)
with real values before saving this to a file (e.g. `csr.conf`).
Note that the value for `MASTER_CLUSTER_IP` is the service cluster IP for the
API server as described in previous subsection.
The sample below also assumes that you are using `cluster.local` as the default
DNS domain name.
-->
</li>
<li>
<p>创建一个用于生成证书签名请求（CSR）的配置文件。
保存文件（例如：<code>csr.conf</code>）前，记得用真实值替换掉尖括号中的值（例如：<code>&lt;MASTER_IP&gt;</code>）。
注意：<code>MASTER_CLUSTER_IP</code> 就像前一小节所述，它的值是 API 服务器的服务集群 IP。
下面的例子假定你的默认 DNS 域名为 <code>cluster.local</code>。</p>
<pre><code>[ req ]
default_bits = 2048
prompt = no
default_md = sha256
req_extensions = req_ext
distinguished_name = dn

[ dn ]
C = &lt;country&gt;
ST = &lt;state&gt;
L = &lt;city&gt;
O = &lt;organization&gt;
OU = &lt;organization unit&gt;
CN = &lt;MASTER_IP&gt;

[ req_ext ]
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster
DNS.5 = kubernetes.default.svc.cluster.local
IP.1 = &lt;MASTER_IP&gt;
IP.2 = &lt;MASTER_CLUSTER_IP&gt;

[ v3_ext ]
authorityKeyIdentifier=keyid,issuer:always
basicConstraints=CA:FALSE
keyUsage=keyEncipherment,dataEncipherment
extendedKeyUsage=serverAuth,clientAuth
subjectAltName=@alt_names
</code></pre>
<!-- 
1.  Generate the certificate signing request based on the config file:
-->
</li>
<li>
<p>基于上面的配置文件生成证书签名请求：</p>
<pre><code>openssl req -new -key server.key -out server.csr -config csr.conf
</code></pre>
<!-- 
1.  Generate the server certificate using the ca.key, ca.crt and server.csr:
-->
</li>
<li>
<p>基于 ca.key、ca.crt 和 server.csr 等三个文件生成服务端证书：</p>
<pre><code>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \
-CAcreateserial -out server.crt -days 10000 \
-extensions v3_ext -extfile csr.conf
</code></pre>
<!--
1.  View the certificate signing request:
-->
</li>
<li>
<p>查看证书签名请求：</p>
<pre><code>openssl req  -noout -text -in ./server.csr
</code></pre>
<!-- 
1.  View the certificate:
-->
</li>
<li>
<p>查看证书：</p>
<pre><code>openssl x509  -noout -text -in ./server.crt
</code></pre>
</li>
</ol>
<!-- 
Finally, add the same parameters into the API server start parameters.
-->
<p>最后，为 API 服务器添加相同的启动参数。</p>
<h3 id="cfssl">cfssl</h3>
<!-- 
**cfssl** is another tool for certificate generation.
-->
<p><strong>cfssl</strong> 是另一个用于生成证书的工具。</p>
<!-- 
1.  Download, unpack and prepare the command line tools as shown below.
    Note that you may need to adapt the sample commands based on the hardware
    architecture and cfssl version you are using.
-->
<ol>
<li>
<p>下载、解压并准备如下所示的命令行工具。
注意：你可能需要根据所用的硬件体系架构和 cfssl 版本调整示例命令。</p>
<pre><code>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o cfssl
chmod +x cfssl
curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o cfssljson
chmod +x cfssljson
curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl-certinfo_1.5.0_linux_amd64 -o cfssl-certinfo
chmod +x cfssl-certinfo
</code></pre>
<!-- 
1.  Create a directory to hold the artifacts and initialize cfssl:
-->
</li>
<li>
<p>创建一个目录，用它保存所生成的构件和初始化 cfssl：</p>
<pre><code>mkdir cert
cd cert
../cfssl print-defaults config &gt; config.json
../cfssl print-defaults csr &gt; csr.json
</code></pre>
<!-- 
1.  Create a JSON config file for generating the CA file, for example, `ca-config.json`:
-->
</li>
<li>
<p>创建一个 JSON 配置文件来生成 CA 文件，例如：<code>ca-config.json</code>：</p>
<pre><code>{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
          &quot;signing&quot;,
          &quot;key encipherment&quot;,
          &quot;server auth&quot;,
          &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
</code></pre>
<!-- 
1.  Create a JSON config file for CA certificate signing request (CSR), for example,
`ca-csr.json`. Be sure to replace the values marked with angle brackets with
real values you want to use.
-->
</li>
<li>
<p>创建一个 JSON 配置文件，用于 CA 证书签名请求（CSR），例如：<code>ca-csr.json</code>。
确认用你需要的值替换掉尖括号中的值。</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;:[{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre>
<!-- 
1.  Generate CA key (`ca-key.pem`) and certificate (`ca.pem`):
-->
</li>
<li>
<p>生成 CA 秘钥文件（<code>ca-key.pem</code>）和证书文件（<code>ca.pem</code>）：</p>
<pre><code>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca
</code></pre>
<!-- 
1.  Create a JSON config file for generating keys and certificates for the API
server, for example, `server-csr.json`. Be sure to replace the values in angle brackets with
real values you want to use. The `MASTER_CLUSTER_IP` is the service cluster
IP for the API server as described in previous subsection.
The sample below also assumes that you are using `cluster.local` as the default
DNS domain name.
-->
</li>
<li>
<p>创建一个 JSON 配置文件，用来为 API 服务器生成秘钥和证书，例如：<code>server-csr.json</code>。
确认用你需要的值替换掉尖括号中的值。<code>MASTER_CLUSTER_IP</code> 是为 API 服务器 指定的服务集群 IP，就像前面小节描述的那样。
以下示例假定你的默认 DSN 域名为<code>cluster.local</code>。</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;&lt;MASTER_IP&gt;&quot;,
    &quot;&lt;MASTER_CLUSTER_IP&gt;&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre>
<!-- 
1.  Generate the key and certificate for the API server, which are by default
saved into file `server-key.pem` and `server.pem` respectively:
-->
</li>
<li>
<p>为 API 服务器生成秘钥和证书，默认会分别存储为<code>server-key.pem</code> 和 <code>server.pem</code> 两个文件。</p>
<pre><code>../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \
--config=ca-config.json -profile=kubernetes \
server-csr.json | ../cfssljson -bare server
</code></pre>
</li>
</ol>
<!-- 
## Distributing Self-Signed CA Certificate
-->
<h2 id="分发自签名的-ca-证书">分发自签名的 CA 证书</h2>
<!-- 
A client node may refuse to recognize a self-signed CA certificate as valid.
For a non-production deployment, or for a deployment that runs behind a company
firewall, you can distribute a self-signed CA certificate to all clients and
refresh the local list for valid certificates.

On each client, perform the following operations:
-->
<p>客户端节点可能不认可自签名 CA 证书的有效性。
对于非生产环境，或者运行在公司防火墙后的环境，你可以分发自签名的 CA 证书到所有客户节点，并刷新本地列表以使证书生效。</p>
<p>在每一个客户节点，执行以下操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt
sudo update-ca-certificates
</code></pre></div><pre><code>Updating certificates in /etc/ssl/certs...
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....
done.
</code></pre><!-- 
## Certificates API
-->
<h2 id="certificates-api">证书 API</h2>
<!-- 
You can use the `certificates.k8s.io` API to provision
x509 certificates to use for authentication as documented
[here](/docs/tasks/tls/managing-tls-in-a-cluster).
-->
<p>你可以通过 <code>certificates.k8s.io</code> API 提供 x509 证书，用来做身份验证，
如<a href="/zh/docs/tasks/tls/managing-tls-in-a-cluster">本</a>文档所述。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8c31aafd38fad5b0de0bd191758d6f93">5 - 安装网络策略驱动</h1>
    
	<!--
title: Install a Network Policy Provider
weight: 30
-->

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-b4418905b0c14630e4e9cb1368241534">5.1 - 使用 Antrea 提供 NetworkPolicy</h1>
    
	<!--
---
title: Use Antrea for NetworkPolicy
content_type: task
weight: 10
---
-->
<!-- overview -->
<!--
This page shows how to install and use Antrea CNI plugin on Kubernetes.
For background on Project Antrea, read the [Introduction to Antrea](https://antrea.io/docs/).
-->
<p>本页展示了如何在 kubernetes 中安装和使用 Antrea CNI 插件。
要了解 Antrea 项目的背景，请阅读 <a href="https://antrea.io/docs/">Antrea 介绍</a>。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
You need to have a Kubernetes cluster. Follow the
[kubeadm getting started guide](/docs/reference/setup-tools/kubeadm/) to bootstrap one.
-->
<p>你需要拥有一个 kuernetes 集群。
遵循 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm 入门指南</a>自行创建一个。</p>
<!-- steps -->
<!--
## Deploying Antrea with kubeadm

Follow [Getting Started](https://github.com/vmware-tanzu/antrea/blob/main/docs/getting-started.md) guide to deploy Antrea for kubeadm.
-->
<h2 id="使用-kubeadm-部署-antrea">使用 kubeadm 部署 Antrea</h2>
<p>遵循<a href="https://github.com/vmware-tanzu/antrea/blob/main/docs/getting-started.md">入门</a>指南
为 kubeadm 部署 Antrea 。</p>
<h2 id="what-s-next">What's next</h2>
<!--
Once your cluster is running, you can follow the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) to try out Kubernetes NetworkPolicy.
-->
<p>一旦你的集群已经运行，你可以遵循
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
来尝试 Kubernetes NetworkPolicy。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1239a77618c6278373832a142cd85519">5.2 - 使用 Calico 提供 NetworkPolicy</h1>
    
	<!-- overview -->
<!--
This page shows a couple of quick ways to create a Calico cluster on Kubernetes.
-->
<p>本页展示了几种在 Kubernetes 上快速创建 Calico 集群的方法。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
Decide whether you want to deploy a [cloud](#creating-a-calico-cluster-with-google-kubernetes-engine-gke) or [local](#creating-a-local-calico-cluster-with-kubeadm) cluster.
-->
<p>确定你想部署一个<a href="#gke-cluster">云版本</a>还是<a href="#local-cluster">本地版本</a>的集群。</p>
<!-- steps -->
<!--
## Creating a Calico cluster with Google Kubernetes Engine (GKE)

**Prerequisite**: [gcloud](https://cloud.google.com/sdk/docs/quickstarts).
-->
<h2 id="gke-cluster">在 Google Kubernetes Engine (GKE) 上创建一个 Calico 集群</h2>
<p><strong>先决条件</strong>: <a href="https://cloud.google.com/sdk/docs/quickstarts">gcloud</a></p>
<!--
1.  To launch a GKE cluster with Calico, include the `--enable-network-policy` flag.
-->
<ol>
<li>
<p>启动一个带有 Calico 的 GKE 集群，需要加上参数 <code>--enable-network-policy</code>。</p>
<p><strong>语法</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">gcloud container clusters create <span style="color:#666">[</span>CLUSTER_NAME<span style="color:#666">]</span> --enable-network-policy
</code></pre></div><p><strong>示例</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">gcloud container clusters create my-calico-cluster --enable-network-policy
</code></pre></div></li>
</ol>
<!--
1.  To verify the deployment, use the following command.
-->
<ol start="2">
<li>
<p>使用如下命令验证部署是否正确。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!--
The Calico pods begin with `calico`. Check to make sure each one has a status of `Running`.
-->
<p>Calico 的 pods 名以 <code>calico</code> 打头，检查确认每个 pods 状态为 <code>Running</code>。</p>
</li>
</ol>
<!-- 
## Creating a local Calico cluster with kubeadm

To get a local single-host Calico cluster in fifteen minutes using kubeadm, refer to the
[Calico Quickstart](https://docs.projectcalico.org/latest/getting-started/kubernetes/).
-->
<h2 id="local-cluster">使用 kubeadm 创建一个本地 Calico 集群  </h2>
<p>使用 kubeadm 在 15 分钟内得到一个本地单主机 Calico 集群，请参考
<a href="https://docs.projectcalico.org/latest/getting-started/kubernetes/">Calico 快速入门</a>。</p>
<h2 id="what-s-next">What's next</h2>
<!--
Once your cluster is running, you can follow the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) to try out Kubernetes NetworkPolicy.
-->
<p>集群运行后，您可以按照<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
去尝试使用 Kubernetes NetworkPolicy。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-95039241255a31df196beaa405b68eba">5.3 - 使用 Cilium 提供 NetworkPolicy</h1>
    
	<!--
reviewers:
- danwent
- aanm
title: Use Cilium for NetworkPolicy
content_type: task
weight: 20
-->
<!-- overview -->
<!--
This page shows how to use Cilium for NetworkPolicy.

For background on Cilium, read the [Introduction to Cilium](https://docs.cilium.io/en/stable/intro).
-->
<p>本页展示如何使用 Cilium 提供 NetworkPolicy。</p>
<p>关于 Cilium 的背景知识，请阅读 <a href="https://docs.cilium.io/en/stable/intro">Cilium 介绍</a>。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Deploying Cilium on Minikube for Basic Testing

To get familiar with Cilium easily you can follow the
[Cilium Kubernetes Getting Started Guide](https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/)
to perform a basic DaemonSet installation of Cilium in minikube.

To start minikube, minimal version required is >= v1.5.2, run the with the
following arguments:
-->
<h2 id="在-minikube-上部署-cilium-用于基本测试">在 Minikube 上部署 Cilium 用于基本测试</h2>
<p>为了轻松熟悉 Cilium 你可以根据
<a href="https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/s">Cilium Kubernetes 入门指南</a>
在 minikube 中执行一个 cilium 的基本 DaemonSet 安装。</p>
<p>要启动 minikube，需要的最低版本为 1.5.2，使用下面的参数运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube version
</code></pre></div><pre><code>minikube version: v1.5.2
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube start --network-plugin<span style="color:#666">=</span>cni --memory<span style="color:#666">=</span><span style="color:#666">4096</span>
</code></pre></div><!--
For minikube you can install Cilium using its CLI tool. Cilium will
automatically detect the cluster configuration and will install the appropriate
components for a successful installation:
-->
<p>对于 minikube 你可以使用 Cilium 的 CLI 工具安装它。
Cilium 将自动检测集群配置并为成功的集群部署选择合适的组件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -LO https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz
sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
rm cilium-linux-amd64.tar.gz
cilium install
</code></pre></div><pre><code>🔮 Auto-detected Kubernetes kind: minikube
✨ Running &quot;minikube&quot; validation checks
✅ Detected minikube version &quot;1.20.0&quot;
ℹ️  Cilium version not set, using default version &quot;v1.10.0&quot;
🔮 Auto-detected cluster name: minikube
🔮 Auto-detected IPAM mode: cluster-pool
🔮 Auto-detected datapath mode: tunnel
🔑 Generating CA...
2021/05/27 02:54:44 [INFO] generate received request
2021/05/27 02:54:44 [INFO] received CSR
2021/05/27 02:54:44 [INFO] generating key: ecdsa-256
2021/05/27 02:54:44 [INFO] encoded CSR
2021/05/27 02:54:44 [INFO] signed certificate with serial number 48713764918856674401136471229482703021230538642
🔑 Generating certificates for Hubble...
2021/05/27 02:54:44 [INFO] generate received request
2021/05/27 02:54:44 [INFO] received CSR
2021/05/27 02:54:44 [INFO] generating key: ecdsa-256
2021/05/27 02:54:44 [INFO] encoded CSR
2021/05/27 02:54:44 [INFO] signed certificate with serial number 3514109734025784310086389188421560613333279574
🚀 Creating Service accounts...
🚀 Creating Cluster roles...
🚀 Creating ConfigMap...
🚀 Creating Agent DaemonSet...
🚀 Creating Operator Deployment...
⌛ Waiting for Cilium to be installed...
</code></pre><!--
The remainder of the Getting Started Guide explains how to enforce both L3/L4
(i.e., IP address + port) security policies, as well as L7 (e.g., HTTP) security
policies using an example application.
-->
<p>入门指南其余的部分用一个示例应用说明了如何强制执行 L3/L4（即 IP 地址+端口）的安全策略
以及L7 （如 HTTP）的安全策略。</p>
<!--
## Deploying Cilium for Production Use

For detailed instructions around deploying Cilium for production, see:
[Cilium Kubernetes Installation Guide](https://docs.cilium.io/en/stable/concepts/kubernetes/intro/)
This documentation includes detailed requirements, instructions and example
production DaemonSet files.
 -->
<h2 id="部署-cilium-用于生产用途">部署 Cilium 用于生产用途</h2>
<p>关于部署 Cilium 用于生产的详细说明，请见
<a href="https://docs.cilium.io/en/stable/concepts/kubernetes/intro/">Cilium Kubernetes 安装指南</a>
此文档包括详细的需求、说明和生产用途 DaemonSet 文件示例。</p>
<!-- discussion -->
<!--
##  Understanding Cilium components

Deploying a cluster with Cilium adds Pods to the `kube-system` namespace. To see
this list of Pods run:
 -->
<h2 id="了解-cilium-组件">了解 Cilium 组件</h2>
<p>部署使用 Cilium 的集群会添加 Pods 到 <code>kube-system</code> 命名空间。要查看 Pod 列表，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --namespace<span style="color:#666">=</span>kube-system -l k8s-app<span style="color:#666">=</span>cilium
</code></pre></div><!-- You'll see a list of Pods similar to this: -->
<p>你将看到像这样的 Pods 列表：</p>
<pre><code class="language-console" data-lang="console">NAME           READY   STATUS    RESTARTS   AGE
cilium-kkdhz   1/1     Running   0          3m23s
...
</code></pre><!--
A `cilium` Pod runs on each node in your cluster and enforces network policy
on the traffic to/from Pods on that node using Linux BPF.
-->
<p>你的集群中的每个节点上都会运行一个 <code>cilium</code> Pod，通过使用 Linux BPF
针对该节点上的 Pod 的入站、出站流量实施网络策略控制。</p>
<h2 id="what-s-next">What's next</h2>
<!--
Once your cluster is running, you can follow the
[Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/)
to try out Kubernetes NetworkPolicy with Cilium.
Have fun, and if you have questions, contact us using the
[Cilium Slack Channel](https://cilium.herokuapp.com/).
-->
<p>集群运行后，你可以按照
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
试用基于 Cilium 的 Kubernetes NetworkPolicy。
玩得开心，如果你有任何疑问，请到 <a href="https://cilium.herokuapp.com/">Cilium Slack 频道</a>
联系我们。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-505a0a6a7e6eff361bbb3be81c84b2e0">5.4 - 使用 kube-router 提供 NetworkPolicy</h1>
    
	<!-- overview -->
<!--
This page shows how to use [Kube-router](https://github.com/cloudnativelabs/kube-router) for NetworkPolicy.
-->
<p>本页展示如何使用 <a href="https://github.com/cloudnativelabs/kube-router">Kube-router</a> 提供 NetworkPolicy。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
You need to have a Kubernetes cluster running. If you do not already have a cluster, you can create one by using any of the cluster installers like Kops, Bootkube, Kubeadm etc.
-->
<p>你需要拥有一个运行中的 Kubernetes 集群。如果你还没有集群，可以使用任意的集群
安装程序如 Kops、Bootkube、Kubeadm 等创建一个。</p>
<!-- steps -->
<!--
## Installing Kube-router addon

The Kube-router Addon comes with a Network Policy Controller that watches Kubernetes API server for any NetworkPolicy and pods updated and configures iptables rules and ipsets to allow or block traffic as directed by the policies. Please follow the [trying Kube-router with cluster installers](https://www.kube-router.io/docs/user-guide/#try-kube-router-with-cluster-installers) guide to install Kube-router addon.
-->
<h2 id="安装-kube-router-插件">安装 kube-router 插件</h2>
<p>kube-router 插件自带一个网络策略控制器，监视来自于 Kubernetes API 服务器的
NetworkPolicy 和 Pod 的变化，根据策略指示配置 iptables 规则和 ipsets 来允许或阻止流量。
请根据 <a href="https://www.kube-router.io/docs/user-guide/#try-kube-router-with-cluster-installers">通过集群安装程序尝试 kube-router</a> 指南安装 kube-router 插件。</p>
<h2 id="what-s-next">What's next</h2>
<!--
Once you have installed the Kube-router addon, you can follow the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) to try out Kubernetes NetworkPolicy.
-->
<p>在你安装了 kube-router 插件后，可以参考
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
去尝试使用 Kubernetes NetworkPolicy。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2842eac98aa0e229a5c6755c4c83d2a7">5.5 - 使用 Romana 提供 NetworkPolicy</h1>
    
	<!--
reviewers:
- chrismarino
title: Romana for NetworkPolicy
content_type: task
weight: 40
-->
<!-- overview -->
<!--
This page shows how to use Romana for NetworkPolicy.
-->
<p>本页展示如何使用 Romana 作为 NetworkPolicy。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
Complete steps 1, 2, and 3 of  the [kubeadm getting started guide](/docs/getting-started-guides/kubeadm/).
-->
<p>完成 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm 入门指南</a>中的 1、2、3 步。</p>
<!-- steps -->
<!--
## Installing Romana with kubeadm

Follow the [containerized installation guide](https://github.com/romana/romana/tree/master/containerize) for kubeadm.

## Applying network policies

To apply network policies use one of the following:

* [Romana network policies](https://github.com/romana/romana/wiki/Romana-policies).
    * [Example of Romana network policy](https://github.com/romana/core/blob/master/doc/policy.md).
* The NetworkPolicy API.
 -->
<h2 id="使用-kubeadm-安装-romana">使用 kubeadm 安装 Romana</h2>
<p>按照<a href="https://github.com/romana/romana/tree/master/containerize">容器化安装指南</a>，
使用 kubeadm 安装。</p>
<h2 id="应用网络策略">应用网络策略</h2>
<p>使用以下的一种方式应用网络策略：</p>
<ul>
<li><a href="https://github.com/romana/romana/wiki/Romana-policies">Romana 网络策略</a>
<ul>
<li><a href="https://github.com/romana/core/blob/master/doc/policy.md">Romana 网络策略例子</a></li>
</ul>
</li>
<li>NetworkPolicy API</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
Once you have installed Romana, you can follow the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) to try out Kubernetes NetworkPolicy.
 -->
<p>Romana 安装完成后，你可以按照
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
去尝试使用 Kubernetes NetworkPolicy。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ac075c3fdfd0d41aa753cc70e42be064">5.6 - 使用 Weave Net 提供 NetworkPolicy</h1>
    
	<!--
reviewers:
- bboreham
title: Weave Net for NetworkPolicy
content_type: task
weight: 50
-->
<!-- overview -->
<!--
This page shows how to use Weave Net for NetworkPolicy.
-->
<p>本页展示如何使用使用 Weave Net 提供 NetworkPolicy。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
You need to have a Kubernetes cluster. Follow the
[kubeadm getting started guide](/docs/reference/setup-tools/kubeadm/) to bootstrap one.
 -->
<p>你需要拥有一个 Kubernetes 集群。按照
<a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm 入门指南</a>
来启动一个。</p>
<!-- steps -->
<!--
## Install the Weave Net addon

Follow the [Integrating Kubernetes via the Addon](https://www.weave.works/docs/net/latest/kubernetes/kube-addon/) guide.

The Weave Net addon for Kubernetes comes with a
[Network Policy Controller](https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#npc)
that automatically monitors Kubernetes for any NetworkPolicy annotations on all
namespaces and configures `iptables` rules to allow or block traffic as directed by the policies.
-->
<h2 id="安装-weave-net-插件">安装 Weave Net 插件</h2>
<p>按照<a href="https://www.weave.works/docs/net/latest/kubernetes/kube-addon/">通过插件集成 Kubernetes</a>
指南执行安装。</p>
<p>Kubernetes 的 Weave Net 插件带有
<a href="https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#npc">网络策略控制器</a>，
可自动监控 Kubernetes 所有名字空间的 NetworkPolicy 注释，
配置 <code>iptables</code> 规则以允许或阻止策略指示的流量。</p>
<!--
## Test the installation

Verify that the weave works.

Enter the following command:
-->
<h2 id="测试安装">测试安装</h2>
<p>验证 weave 是否有效。</p>
<p>输入以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get po -n kube-system -o wide
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似这样：</p>
<pre><code>NAME                                    READY     STATUS    RESTARTS   AGE       IP              NODE
weave-net-1t1qg                         2/2       Running   0          9d        192.168.2.10    worknode3
weave-net-231d7                         2/2       Running   1          7d        10.2.0.17       worknodegpu
weave-net-7nmwt                         2/2       Running   3          9d        192.168.2.131   masternode
weave-net-pmw8w                         2/2       Running   0          9d        192.168.2.216   worknode2
</code></pre><!--
Each Node has a weave Pod, and all Pods are `Running` and `2/2 READY`. (`2/2` means that each Pod has `weave` and `weave-npc`.)
-->
<p>每个 Node 都有一个 weave Pod，所有 Pod 都是<code>Running</code> 和 <code>2/2 READY</code>。
（<code>2/2</code> 表示每个 Pod 都有 <code>weave</code> 和 <code>weave-npc</code>）</p>
<h2 id="what-s-next">What's next</h2>
<!--
Once you have installed the Weave Net addon, you can follow the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) to try out Kubernetes NetworkPolicy. If you have any question, contact us at [#weave-community on Slack or Weave User Group](https://github.com/weaveworks/weave#getting-help).
 -->
<p>安装 Weave Net 插件后，你可以参考
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
来试用 Kubernetes NetworkPolicy。
如果你有任何疑问，请通过
<a href="https://github.com/weaveworks/weave#getting-help">Slack 上的 #weave-community 频道或者 Weave 用户组</a>
联系我们。</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b45f024608e1b367cdacb1fd9d77278a">6 - IP Masquerade Agent 用户指南</h1>
    
	<!--
title: IP Masquerade Agent User Guide
content_type: task
-->
<!-- overview -->
<!--
This page shows how to configure and enable the ip-masq-agent.
-->
<p>此页面展示如何配置和启用 ip-masq-agent。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- discussion -->
<!--
## IP Masquerade Agent User Guide
-->
<h2 id="ip-masquerade-agent-用户指南">IP Masquerade Agent 用户指南</h2>
<!--
The ip-masq-agent configures iptables rules to hide a pod's IP address behind the cluster node's IP address. This is typically done when sending traffic to destinations outside the cluster's pod [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) range.
-->
<p>ip-masq-agent 配置 iptables 规则以隐藏位于集群节点 IP 地址后面的 Pod 的 IP 地址。
这通常在将流量发送到集群的 Pod
<a href="https://zh.wikipedia.org/wiki/%E6%97%A0%E7%B1%BB%E5%88%AB%E5%9F%9F%E9%97%B4%E8%B7%AF%E7%94%B1">CIDR</a>
范围之外的目的地时使用。</p>
<!--
### **Key Terms**
-->
<h3 id="关键术语"><strong>关键术语</strong></h3>
<!--
* **NAT (Network Address Translation)**
  Is a method of remapping one IP address to another by modifying either the source and/or destination address information in the IP header.  Typically performed by a device doing IP routing.
-->
<ul>
<li><strong>NAT (网络地址转译)</strong>
是一种通过修改 IP 地址头中的源和/或目标地址信息将一个 IP 地址重新映射
到另一个 IP 地址的方法。通常由执行 IP 路由的设备执行。</li>
</ul>
<!--
* **Masquerading**
  A form of NAT that is typically used to perform a many to one address translation, where multiple source IP addresses are masked behind a single address, which is typically the device doing the IP routing. In Kubernetes this is the Node's IP address.
-->    
<ul>
<li><strong>伪装</strong>
NAT 的一种形式，通常用于执行多对一地址转换，其中多个源 IP 地址被隐藏在
单个地址后面，该地址通常是执行 IP 路由的设备。在 Kubernetes 中，
这是节点的 IP 地址。</li>
</ul>
<!--
* **CIDR (Classless Inter-Domain Routing)**
  Based on the variable-length subnet masking, allows specifying arbitrary-length prefixes. CIDR introduced a new method of representation for IP addresses, now commonly known as **CIDR notation**, in which an address or routing prefix is written with a suffix indicating the number of bits of the prefix, such as 192.168.2.0/24.
-->
<ul>
<li><strong>CIDR (无类别域间路由)</strong>
基于可变长度子网掩码，允许指定任意长度的前缀。
CIDR 引入了一种新的 IP 地址表示方法，现在通常称为<strong>CIDR表示法</strong>，
其中地址或路由前缀后添加一个后缀，用来表示前缀的位数，例如 192.168.2.0/24。</li>
</ul>
<!--
* **Link Local**
  A link-local address is a network address that is valid only for communications within the network segment or the broadcast domain that the host is connected to. Link-local addresses for IPv4 are defined in the address block 169.254.0.0/16 in CIDR notation.
-->
<ul>
<li><strong>本地链路</strong>
本地链路是仅对网段或主机所连接的广播域内的通信有效的网络地址。
IPv4 的本地链路地址在 CIDR 表示法的地址块 169.254.0.0/16 中定义。</li>
</ul>
<!--
The ip-masq-agent configures iptables rules to handle masquerading node/pod IP addresses when sending traffic to destinations outside the cluster node's IP and the Cluster IP range.  This essentially hides pod IP addresses behind the cluster node's IP address.  In some environments, traffic to "external" addresses must come from a known machine address. For example, in Google Cloud, any traffic to the internet must come from a VM's IP.  When containers are used, as in Google Kubernetes Engine, the Pod IP will be rejected for egress. To avoid this, we must hide the Pod IP behind the VM's own IP address - generally known as "masquerade". By default, the agent is configured to treat the three private IP ranges specified by [RFC 1918](https://tools.ietf.org/html/rfc1918) as non-masquerade [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing).  These ranges are 10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16. The agent will also treat link-local (169.254.0.0/16) as a non-masquerade CIDR by default.  The agent is configured to reload its configuration from the location */etc/config/ip-masq-agent* every 60 seconds, which is also configurable.
-->
<p>ip-masq-agent 配置 iptables 规则，以便在将流量发送到集群节点的 IP 和集群 IP 范围之外的目标时
处理伪装节点或 Pod 的 IP 地址。这本质上隐藏了集群节点 IP 地址后面的 Pod IP 地址。
在某些环境中，去往“外部”地址的流量必须从已知的机器地址发出。
例如，在 Google Cloud 中，任何到互联网的流量都必须来自 VM 的 IP。
使用容器时，如 Google Kubernetes Engine，从 Pod IP 发出的流量将被拒绝出站。
为了避免这种情况，我们必须将 Pod IP 隐藏在 VM 自己的 IP 地址后面 - 通常称为“伪装”。
默认情况下，代理配置为将
<a href="https://tools.ietf.org/html/rfc1918">RFC 1918</a>
指定的三个私有 IP 范围视为非伪装
<a href="https://zh.wikipedia.org/wiki/%E6%97%A0%E7%B1%BB%E5%88%AB%E5%9F%9F%E9%97%B4%E8%B7%AF%E7%94%B1">CIDR</a>。
这些范围是 10.0.0.0/8,172.16.0.0/12 和 192.168.0.0/16。
默认情况下，代理还将链路本地地址（169.254.0.0/16）视为非伪装 CIDR。
代理程序配置为每隔 60 秒从 <em>/etc/config/ip-masq-agent</em> 重新加载其配置，
这也是可修改的。</p>
<p><img src="/images/docs/ip-masq.png" alt="masq/non-masq example"></p>
<!--
The agent configuration file must be written in YAML or JSON syntax, and may contain three optional keys:
-->
<p>代理配置文件必须使用 YAML 或 JSON 语法编写，并且可能包含三个可选值：</p>
<!--
*   **nonMasqueradeCIDRs:** A list of strings in [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) notation that specify the non-masquerade ranges.
-->
<ul>
<li><strong>nonMasqueradeCIDRs:</strong>
<a href="https://zh.wikipedia.org/wiki/%E6%97%A0%E7%B1%BB%E5%88%AB%E5%9F%9F%E9%97%B4%E8%B7%AF%E7%94%B1">CIDR</a>
表示法中的字符串列表，用于指定不需伪装的地址范围。</li>
</ul>
<!--
*   **masqLinkLocal:** A Boolean (true / false) which indicates whether to masquerade traffic to the link local prefix 169.254.0.0/16. False by default.
-->
<ul>
<li><strong>masqLinkLocal:</strong> 布尔值 (true / false)，表示是否将流量伪装到
本地链路前缀 169.254.0.0/16。默认为 false。</li>
</ul>
<!--
*   **resyncInterval:** An interval at which the agent attempts to reload config from disk. e.g. '30s' where 's' is seconds, 'ms' is milliseconds etc...
-->
<ul>
<li><strong>resyncInterval:</strong> 代理尝试从磁盘重新加载配置的时间间隔。
例如 '30s'，其中 's' 是秒，'ms' 是毫秒等...</li>
</ul>
<!--
Traffic to 10.0.0.0/8, 172.16.0.0/12 and 192.168.0.0/16) ranges will NOT be masqueraded. Any other traffic (assumed to be internet) will be masqueraded.  An example of a local destination from a pod could be its Node's IP address as well as another node's address or one of the IP addresses in Cluster's IP range.   Any other traffic will be masqueraded by default.  The below entries show the default set of rules that are applied by the ip-masq-agent:
-->
<p>10.0.0.0/8、172.16.0.0/12 和 192.168.0.0/16 范围内的流量不会被伪装。
任何其他流量（假设是互联网）将被伪装。
Pod 访问本地目的地的例子，可以是其节点的 IP 地址、另一节点的地址或集群的 IP 地址范围内的一个 IP 地址。
默认情况下，任何其他流量都将伪装。以下条目展示了 ip-masq-agent 的默认使用的规则：</p>
<pre><code>iptables -t nat -L IP-MASQ-AGENT
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             172.16.0.0/12        /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             192.168.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL

</code></pre><!--
By default, in GCE/Google Kubernetes Engine starting with Kubernetes version 1.7.0, if network policy is enabled or you are using a cluster CIDR not in the 10.0.0.0/8 range, the ip-masq-agent will run in your cluster.  If you are running in another environment, you can add the ip-masq-agent [DaemonSet](/docs/concepts/workloads/controllers/daemonset/) to your cluster:
-->
<p>默认情况下，从 Kubernetes 1.7.0 版本开始的 GCE/Google Kubernetes Engine 中，
如果启用了网络策略，或者你使用的集群 CIDR 不在 10.0.0.0/8 范围内，
则 ip-masq-agent 将在你的集群中运行。
如果你在其他环境中运行，则可以将 ip-masq-agent
<a href="/zh/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> 添加到你的集群：</p>
<!-- steps -->
<!--
## Create an ip-masq-agent
To create an ip-masq-agent, run the following kubectl command:
-->
<h2 id="创建-ip-masq-agent">创建 ip-masq-agent</h2>
<p>通过运行以下 kubectl 指令创建 ip-masq-agent:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml
</code></pre></div><!--
You must also apply the appropriate node label to any nodes in your cluster that you want the agent to run on.
-->
<p>你必须同时将适当的节点标签应用于集群中希望代理运行的任何节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl label nodes my-node beta.kubernetes.io/masq-agent-ds-ready<span style="color:#666">=</span><span style="color:#a2f">true</span>
</code></pre></div><!--
More information can be found in the ip-masq-agent documentation [here](https://github.com/kubernetes-sigs/ip-masq-agent)
-->
<p>更多信息可以通过 ip-masq-agent 文档 <a href="https://github.com/kubernetes-sigs/ip-masq-agent">这里</a> 找到。</p>
<!--
In most cases, the default set of rules should be sufficient; however, if this is not the case for your cluster, you can create and apply a [ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/) to customize the IP ranges that are affected.  For example, to allow only 10.0.0.0/8 to be considered by the ip-masq-agent, you can create the following [ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/) in a file called "config".
-->
<p>在大多数情况下，默认的规则集应该足够；但是，如果你的群集不是这种情况，则可以创建并应用
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>
来自定义受影响的 IP 范围。
例如，要允许 ip-masq-agent 仅作用于 10.0.0.0/8，你可以在一个名为 “config” 的文件中创建以下
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a> 。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
It is important that the file is called config since, by default, that will be used as the key for lookup by the ip-masq-agent:
-->
<p>重要的是，该文件之所以被称为 config，因为默认情况下，该文件将被用作
ip-masq-agent 查找的主键：</p>
<pre><code>nonMasqueradeCIDRs:
  - 10.0.0.0/8
resyncInterval: 60s
</code></pre>
</div>
<!--
Run the following command to add the config map to your cluster:
-->
<p>运行以下命令将配置映射添加到你的集群：</p>
<pre><code>kubectl create configmap ip-masq-agent --from-file=config --namespace=kube-system
</code></pre><!--
This will update a file located at */etc/config/ip-masq-agent* which is periodically checked every *resyncInterval* and applied to the cluster node.
After the resync interval has expired, you should see the iptables rules reflect your changes:
-->
<p>这将更新位于 <em>/etc/config/ip-masq-agent</em> 的一个文件，该文件以 <em>resyncInterval</em>
为周期定期检查并应用于集群节点。
重新同步间隔到期后，你应该看到你的更改在 iptables 规则中体现：</p>
<pre><code>iptables -t nat -L IP-MASQ-AGENT
Chain IP-MASQ-AGENT (1 references)
target     prot opt source               destination
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre><!--
By default, the link local range (169.254.0.0/16) is also handled by the ip-masq agent, which sets up the appropriate iptables rules.  To have the ip-masq-agent ignore link local, you can set *masqLinkLocal*  to true in the config map.
-->
<p>默认情况下，本地链路范围 (169.254.0.0/16) 也由 ip-masq agent 处理，
该代理设置适当的 iptables 规则。 要使 ip-masq-agent 忽略本地链路，
可以在配置映射中将 <em>masqLinkLocal</em> 设置为 true。</p>
<pre><code>nonMasqueradeCIDRs:
  - 10.0.0.0/8
resyncInterval: 60s
masqLinkLocal: true
</code></pre>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ce4cd28c8feb9faa783e79b48af37961">7 - Kubernetes 云管理控制器</h1>
    
	<!--
reviewers:
- luxas
- thockin
- wlan0
title: Kubernetes Cloud Controller Manager
content_type: concept
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code>
</div>


<!--
Since cloud providers develop and release at a different pace compared to the Kubernetes project, abstracting the provider-specific code to the `<a class='glossary-tooltip' title='将 Kubernetes 与第三方云提供商进行集成的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/cloud-controller/' target='_blank' aria-label='cloud-controller-manager'>cloud-controller-manager</a>` binary allows cloud vendors to evolve independently from the core Kubernetes code.
-->
<p>由于云驱动的开发和发布的步调与 Kubernetes 项目不同，将服务提供商专用代码抽象到
<code><a class='glossary-tooltip' title='将 Kubernetes 与第三方云提供商进行集成的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/cloud-controller/' target='_blank' aria-label='cloud-controller-manager'>cloud-controller-manager</a></code>
二进制中有助于云服务厂商在 Kubernetes 核心代码之外独立进行开发。</p>
<!--
The `cloud-controller-manager` can be linked to any cloud provider that satisfies [cloudprovider.Interface](https://github.com/kubernetes/cloud-provider/blob/master/cloud.go). For backwards compatibility, the [cloud-controller-manager](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager) provided in the core Kubernetes project uses the same cloud libraries as `kube-controller-manager`. Cloud providers already supported in Kubernetes core are expected to use the in-tree cloud-controller-manager to transition out of Kubernetes core.
-->
<p><code>cloud-controller-manager</code> 可以被链接到任何满足
<a href="https://github.com/kubernetes/cloud-provider/blob/master/cloud.go">cloudprovider.Interface</a>
约束的云服务提供商。为了兼容旧版本，Kubernetes 核心项目中提供的
<a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager">cloud-controller-manager</a>
使用和 <code>kube-controller-manager</code> 相同的云服务类库。
已经在 Kubernetes 核心项目中支持的云服务提供商预计将通过使用 in-tree 的 cloud-controller-manager
过渡为非 Kubernetes 核心代码。</p>
<!-- body -->
<!--
## Administration

### Requirements

Every cloud has their own set of requirements for running their own cloud provider integration, it should not be too different from the requirements when running `kube-controller-manager`. As a general rule of thumb you'll need:

* cloud authentication/authorization: your cloud may require a token or IAM rules to allow access to their APIs
* kubernetes authentication/authorization: cloud-controller-manager may need RBAC rules set to speak to the kubernetes apiserver
* high availability: like kube-controller-manager, you may want a high available setup for cloud controller manager using leader election (on by default).
-->
<h2 id="管理">管理</h2>
<h3 id="需求">需求</h3>
<p>每个云服务都有一套各自的需求用于系统平台的集成，这不应与运行
<code>kube-controller-manager</code> 的需求有太大差异。作为经验法则，你需要：</p>
<ul>
<li>云服务认证/授权：你的云服务可能需要使用令牌或者 IAM 规则以允许对其 API 的访问</li>
<li>kubernetes 认证/授权：cloud-controller-manager 可能需要 RBAC 规则以访问 kubernetes apiserver</li>
<li>高可用：类似于 kube-controller-manager，你可能希望通过主节点选举（默认开启）配置一个高可用的云管理控制器。</li>
</ul>
<!--
### Running cloud-controller-manager

Successfully running cloud-controller-manager requires some changes to your cluster configuration.
-->
<h3 id="运行云管理控制器">运行云管理控制器</h3>
<p>你需要对集群配置做适当的修改以成功地运行云管理控制器：</p>
<!--
* `kube-apiserver` and `kube-controller-manager` MUST NOT specify the `--cloud-provider` flag. This ensures that it does not run any cloud specific loops that would be run by cloud controller manager. In the future, this flag will be deprecated and removed.
* `kubelet` must run with `--cloud-provider=external`. This is to ensure that the kubelet is aware that it must be initialized by the cloud controller manager before it is scheduled any work.
-->
<ul>
<li>一定不要为 <code>kube-apiserver</code> 和 <code>kube-controller-manager</code> 指定 <code>--cloud-provider</code> 标志。
这将保证它们不会运行任何云服务专用循环逻辑，这将会由云管理控制器运行。未来这个标记将被废弃并去除。</li>
<li><code>kubelet</code> 必须使用 <code>--cloud-provider=external</code> 运行。
这是为了保证让 kubelet 知道在执行任何任务前，它必须被云管理控制器初始化。</li>
</ul>
<!--
Keep in mind that setting up your cluster to use cloud controller manager will change your cluster behaviour in a few ways:
-->
<p>请记住，设置群集使用云管理控制器将用多种方式更改群集行为：</p>
<!--
* kubelets specifying `--cloud-provider=external` will add a taint `node.cloudprovider.kubernetes.io/uninitialized` with an effect `NoSchedule` during initialization. This marks the node as needing a second initialization from an external controller before it can be scheduled work. Note that in the event that cloud controller manager is not available, new nodes in the cluster will be left unschedulable. The taint is important since the scheduler may require cloud specific information about nodes such as their region or type (high cpu, gpu, high memory, spot instance, etc).
-->
<ul>
<li>指定了 <code>--cloud-provider=external</code> 的 kubelet 将被添加一个 <code>node.cloudprovider.kubernetes.io/uninitialized</code>
的污点，导致其在初始化过程中不可调度（<code>NoSchedule</code>）。
这将标记该节点在能够正常调度前，需要外部的控制器进行二次初始化。
请注意，如果云管理控制器不可用，集群中的新节点会一直处于不可调度的状态。
这个污点很重要，因为调度器可能需要关于节点的云服务特定的信息，比如他们的区域或类型
（高端 CPU、GPU 支持、内存较大、临时实例等）。</li>
</ul>
<!--
* cloud information about nodes in the cluster will no longer be retrieved using local metadata, but instead all API calls to retrieve node information will go through cloud controller manager. This may mean you can restrict access to your cloud API on the kubelets for better security. For larger clusters you may want to consider if cloud controller manager will hit rate limits since it is now responsible for almost all API calls to your cloud from within the cluster.
-->
<ul>
<li>集群中节点的云服务信息将不再能够从本地元数据中获取，取而代之的是所有获取节点信息的
API 调用都将通过云管理控制器。这意味着你可以通过限制到 kubelet 云服务 API 的访问来提升安全性。
在更大的集群中你可能需要考虑云管理控制器是否会遇到速率限制，
因为它现在负责集群中几乎所有到云服务的 API 调用。</li>
</ul>
<!--
Cloud controller manager can implement:

* node controller - responsible for updating kubernetes nodes using cloud APIs and deleting kubernetes nodes that were deleted on your cloud.
* service controller - responsible for loadbalancers on your cloud against services of type LoadBalancer.
* route controller - responsible for setting up network routes on your cloud
* any other features you would like to implement if you are running an out-of-tree provider.
-->
<p>云管理控制器可以实现：</p>
<ul>
<li>节点控制器 - 负责使用云服务 API 更新 kubernetes 节点并删除在云服务上已经删除的 kubernetes 节点。</li>
<li>服务控制器 - 负责在云服务上为类型为 LoadBalancer 的 service 提供负载均衡器。</li>
<li>路由控制器 - 负责在云服务上配置网络路由。</li>
<li>如果你使用的是 out-of-tree 提供商，请按需实现其余任意特性。</li>
</ul>
<!--
## Examples

If you are using a cloud that is currently supported in Kubernetes core and would like to adopt cloud controller manager, see the [cloud controller manager in kubernetes core](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager).

For cloud controller managers not in Kubernetes core, you can find the respective projects in repos maintained by cloud vendors or sig leads.
-->
<h2 id="示例">示例</h2>
<p>如果当前 Kubernetes 内核支持你使用的云服务，并且想要采用云管理控制器，请参见
<a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager">kubernetes 内核中的云管理控制器</a>。</p>
<p>对于不在 Kubernetes 核心代码库中的云管理控制器，你可以在云服务厂商或 SIG 领导者的源中找到对应的项目。</p>
<ul>
<li><a href="https://github.com/digitalocean/digitalocean-cloud-controller-manager">DigitalOcean</a></li>
<li><a href="https://github.com/munnerz/keepalived-cloud-provider">keepalived</a></li>
<li><a href="https://github.com/oracle/oci-cloud-controller-manager">Oracle Cloud Infrastructure</a></li>
<li><a href="https://github.com/rancher/rancher-cloud-controller-manager">Rancher</a></li>
</ul>
<!--
For providers already in Kubernetes core, you can run the in-tree cloud controller manager as a Daemonset in your cluster, use the following as a guideline:
-->
<p>对于已经存在于 Kubernetes 内核中的提供商，你可以在集群中将 in-tree 云管理控制器作为守护进程运行。请使用如下指南：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/cloud/ccm-example.yaml" download="admin/cloud/ccm-example.yaml"><code>admin/cloud/ccm-example.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-cloud-ccm-example-yaml')" title="Copy admin/cloud/ccm-example.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-cloud-ccm-example-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># This is an example of how to setup cloud-controller-manager as a Daemonset in your cluster.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># It assumes that your masters can run pods and has the role node-role.kubernetes.io/master</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># Note that this Daemonset will not work straight out of the box for your cloud, this is</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># meant to be a guideline.</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRoleBinding<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">roleRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cluster-admin<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">subjects</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">serviceAccountName</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># for in-tree providers we use k8s.gcr.io/cloud-controller-manager</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># this can be replaced with any other image for out-of-tree providers</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/cloud-controller-manager:v1.8.0<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- /usr/local/bin/cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --cloud-provider=[YOUR_CLOUD_PROVIDER] <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Add your own cloud provider here!</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --leader-elect=true<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --use-service-account-credentials<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># these flags will vary for every cloud provider</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --allocate-node-cidrs=true<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --configure-cloud-routes=true<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --cluster-cidr=172.17.0.0/16<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this is required so CCM can bootstrap itself</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>node.cloudprovider.kubernetes.io/uninitialized<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this is to have the daemonset runnable on master nodes</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># the taint may vary depending on your cluster setup</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>node-role.kubernetes.io/master<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this is to restrict CCM to only run on master nodes</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># the node selector may vary depending on your cluster setup</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">node-role.kubernetes.io/master</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
## Limitations

Running cloud controller manager comes with a few possible limitations. Although these limitations are being addressed in upcoming releases, it's important that you are aware of these limitations for production workloads.
-->
<h2 id="限制">限制</h2>
<p>运行云管理控制器会有一些可能的限制。虽然以后的版本将处理这些限制，但是知道这些生产负载的限制很重要。</p>
<!--
### Support for Volumes

Cloud controller manager does not implement any of the volume controllers found in `kube-controller-manager` as the volume integrations also require coordination with kubelets. As we evolve CSI (container storage interface) and add stronger support for flex volume plugins, necessary support will be added to cloud controller manager so that clouds can fully integrate with volumes. Learn more about out-of-tree CSI volume plugins [here](https://github.com/kubernetes/features/issues/178).
-->
<h3 id="对-volume-的支持">对 Volume 的支持</h3>
<p>云管理控制器未实现 <code>kube-controller-manager</code> 中的任何 volume 控制器，因为和 volume 的集成还需要与 kubelet 协作。由于我们引入了 CSI (容器存储接口，container storage interface) 并对弹性 volume 插件添加了更强大的支持，云管理控制器将添加必要的支持，以使云服务同 volume 更好的集成。请在 <a href="https://github.com/kubernetes/features/issues/178">这里</a> 了解更多关于 out-of-tree CSI volume 插件的信息。</p>
<!--
### Scalability

In the previous architecture for cloud providers, we relied on kubelets using a local metadata service to retrieve node information about itself. With this new architecture, we now fully rely on the cloud controller managers to retrieve information for all nodes. For very larger clusters, you should consider possible bottle necks such as resource requirements and API rate limiting.
-->
<h3 id="可扩展性">可扩展性</h3>
<p>在以前为云服务提供商提供的架构中，我们依赖 kubelet 的本地元数据服务来获取关于它本身的节点信息。通过这个新的架构，现在我们完全依赖云管理控制器来获取所有节点的信息。对于非常大的集群，你需要考虑可能的瓶颈，例如资源需求和 API 速率限制。</p>
<!--
### Chicken and Egg

The goal of the cloud controller manager project is to decouple development of cloud features from the core Kubernetes project. Unfortunately, many aspects of the Kubernetes project has assumptions that cloud provider features are tightly integrated into the project. As a result, adopting this new architecture can create several situations where a request is being made for information from a cloud provider, but the cloud controller manager may not be able to return that information without the original request being complete.
-->
<h3 id="鸡和蛋的问题">鸡和蛋的问题</h3>
<p>云管理控制器的目标是将云服务特性的开发从 Kubernetes 核心项目中解耦。
不幸的是，Kubernetes 项目的许多方面都假设云服务提供商的特性同项目紧密结合。
因此，这种新架构的采用可能导致某些场景下，当一个请求需要从云服务提供商获取信息时，
在该请求没有完成的情况下云管理控制器不能返回那些信息。</p>
<!--
A good example of this is the TLS bootstrapping feature in the Kubelet. Currently, TLS bootstrapping assumes that the Kubelet has the ability to ask the cloud provider (or a local metadata service) for all its address types (private, public, etc) but cloud controller manager cannot set a node's address types without being initialized in the first place which requires that the kubelet has TLS certificates to communicate with the apiserver.

As this initiative evolves, changes will be made to address these issues in upcoming releases.
-->
<p>Kubelet 中的 TLS 引导特性是一个很好的例子。
目前，TLS 引导认为 kubelet 有能力从云提供商（或本地元数据服务）获取所有的地址类型（私有、公用等），
但在被初始化之前，云管理控制器不能设置节点地址类型，而这需要 kubelet 拥有
TLS 证书以和 API 服务器通信。</p>
<p>随着整个动议的演进，将来的发行版中将作出改变来解决这些问题。</p>
<h2 id="what-s-next">What's next</h2>
<!--
To build and develop your own cloud controller manager, read the [Developing Cloud Controller Manager](/docs/tasks/administer-cluster/developing-cloud-controller-manager.md) doc.
-->
<p>要构建和开发你自己的云管理控制器，请阅读
<a href="/zh/docs/tasks/administer-cluster/developing-cloud-controller-manager/">开发云管理控制器</a>
文档。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c4d0832845adc92b7ccd54aed63fc932">8 - 为 Kubernetes 运行 etcd 集群</h1>
    
	<!--
---
reviewers:
- mml
- wojtek-t
title: Operating etcd clusters for Kubernetes
content_type: task
---
-->
<!-- overview -->
<!--
---
title: etcd
id: etcd
date: 2018-04-12
full_link: /docs/tasks/administer-cluster/configure-upgrade-etcd/
short_description: >
  Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.

aka: 
tags:
- architecture
- storage
---
-->
<!--
 Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.
-->
<p>etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。</p>
<!--
If your Kubernetes cluster uses etcd as its backing store, make sure you have a
[back up](/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster) plan
for those data.
-->	
<p>您的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。</p>
<!--
You can find in-depth information about etcd in the official [documentation](https://etcd.io/docs/).
-->
<p>要了解 etcd 更深层次的信息，请参考 <a href="https://etcd.io/docs/">etcd 文档</a>。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Prerequisites

* Run etcd as a cluster of odd members.

* etcd is a leader-based distributed system. Ensure that the leader
  periodically send heartbeats on time to all followers to keep the cluster
  stable.

* Ensure that no resource starvation occurs.

  Performance and stability of the cluster is sensitive to network and disk
  I/O. Any resource starvation can lead to heartbeat timeout, causing instability
  of the cluster. An unstable etcd indicates that no leader is elected. Under
  such circumstances, a cluster cannot make any changes to its current state,
  which implies no new pods can be scheduled.

* Keeping etcd clusters stable is critical to the stability of Kubernetes
  clusters. Therefore, run etcd clusters on dedicated machines or isolated
  environments for [guaranteed resource requirements](https://etcd.io/docs/current/op-guide/hardware/).

* The minimum recommended version of etcd to run in production is `3.2.10+`.
-->
<h2 id="先决条件">先决条件</h2>
<ul>
<li>
<p>运行的 etcd 集群个数成员为奇数。</p>
</li>
<li>
<p>etcd 是一个 leader-based 分布式系统。确保主节点定期向所有从节点发送心跳，以保持集群稳定。</p>
</li>
<li>
<p>确保不发生资源不足。</p>
<p>集群的性能和稳定性对网络和磁盘 I/O 非常敏感。任何资源匮乏都会导致心跳超时，
从而导致集群的不稳定。不稳定的情况表明没有选出任何主节点。
在这种情况下，集群不能对其当前状态进行任何更改，这意味着不能调度新的 pod。</p>
</li>
<li>
<p>保持 etcd 集群的稳定对 Kubernetes 集群的稳定性至关重要。
因此，请在专用机器或隔离环境上运行 etcd 集群，以满足
<a href="https://etcd.io/docs/current/op-guide/hardware/">所需资源需求</a>。</p>
</li>
<li>
<p>在生产中运行的 etcd 的最低推荐版本是 <code>3.2.10+</code>。</p>
</li>
</ul>
<!--
## Resource requirements

Operating etcd with limited resources is suitable only for testing purposes.
For deploying in production, advanced hardware configuration is required.
Before deploying etcd in production, see
[resource requirement reference](https://etcd.io/docs/current/op-guide/hardware/#example-hardware-configurations).

## Starting etcd clusters

This section covers starting a single-node and multi-node etcd cluster. 
-->
<h2 id="资源要求">资源要求</h2>
<p>使用有限的资源运行 etcd 只适合测试目的。为了在生产中部署，需要先进的硬件配置。
在生产中部署 etcd 之前，请查看
<a href="https://etcd.io/docs/current/op-guide/hardware/#example-hardware-configurations">所需资源参考文档</a>。</p>
<h2 id="启动-etcd-集群">启动 etcd 集群</h2>
<p>本节介绍如何启动单节点和多节点 etcd 集群。</p>
<!--
### Single-node etcd cluster

Use a single-node etcd cluster only for testing purpose.

1. Run the following:

   ```sh
   etcd --listen-client-urls=http://$PRIVATE_IP:2379 \
      --advertise-client-urls=http://$PRIVATE_IP:2379
   ```

2. Start the Kubernetes API server with the flag
   `--etcd-servers=$PRIVATE_IP:2379`.

    Make sure `PRIVATE_IP` is set to your etcd client IP.
-->
<h3 id="单节点-etcd-集群">单节点 etcd 集群</h3>
<p>只为测试目的使用单节点 etcd 集群。</p>
<ol>
<li>
<p>运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">etcd --listen-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$PRIVATE_IP</span>:2379 <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   --advertise-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$PRIVATE_IP</span>:2379
</code></pre></div></li>
<li>
<p>使用参数 <code>--etcd-servers=$PRIVATE_IP:2379</code> 启动 Kubernetes API 服务器。</p>
<p>确保将 <code>PRIVATE_IP</code> 设置为etcd客户端 IP。</p>
</li>
</ol>
<!--
### Multi-node etcd cluster

For durability and high availability, run etcd as a multi-node cluster in
production and back it up periodically. A five-member cluster is recommended
in production. For more information, see
[FAQ documentation](https://etcd.io/docs/current/faq/#what-is-failure-tolerance).

Configure an etcd cluster either by static member information or by dynamic
discovery. For more information on clustering, see
[etcd clustering documentation](https://etcd.io/docs/current/op-guide/clustering/).

For an example, consider a five-member etcd cluster running with the following
client URLs: `http://$IP1:2379`, `http://$IP2:2379`, `http://$IP3:2379`,
`http://$IP4:2379`, and `http://$IP5:2379`. To start a Kubernetes API server:

1. Run the following:

   ```shell
   etcd --listen-client-urls=http://$IP1:2379,http://$IP2:2379,http://$IP3:2379,http://$IP4:2379,http://$IP5:2379 --advertise-client-urls=http://$IP1:2379,http://$IP2:2379,http://$IP3:2379,http://$IP4:2379,http://$IP5:2379
   ```

2. Start the Kubernetes API servers with the flag
   `--etcd-servers=$IP1:2379,$IP2:2379,$IP3:2379,$IP4:2379,$IP5:2379`.

   Make sure the `IP<n>` variables are set to your client IP addresses.
-->
<h3 id="多节点-etcd-集群">多节点 etcd 集群</h3>
<p>为了耐用性和高可用性，在生产中将以多节点集群的方式运行 etcd，并且定期备份。
建议在生产中使用五个成员的集群。
有关该内容的更多信息，请参阅
<a href="https://etcd.io/docs/current/faq/#what-is-failure-tolerance">常见问题文档</a>。</p>
<p>可以通过静态成员信息或动态发现的方式配置 etcd 集群。
有关集群的详细信息，请参阅
<a href="https://etcd.io/docs/current/op-guide/clustering/">etcd 集群文档</a>。</p>
<p>例如，考虑运行以下客户端 URL 的五个成员的 etcd 集群：<code>http://$IP1:2379</code>，
<code>http://$IP2:2379</code>，<code>http://$IP3:2379</code>，<code>http://$IP4:2379</code> 和 <code>http://$IP5:2379</code>。
要启动 Kubernetes API 服务器：</p>
<ol>
<li>
<p>运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">etcd --listen-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$IP1</span>:2379,http://<span style="color:#b8860b">$IP2</span>:2379,http://<span style="color:#b8860b">$IP3</span>:2379,http://<span style="color:#b8860b">$IP4</span>:2379,http://<span style="color:#b8860b">$IP5</span>:2379 --advertise-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$IP1</span>:2379,http://<span style="color:#b8860b">$IP2</span>:2379,http://<span style="color:#b8860b">$IP3</span>:2379,http://<span style="color:#b8860b">$IP4</span>:2379,http://<span style="color:#b8860b">$IP5</span>:2379
</code></pre></div></li>
<li>
<p>使用参数 <code>--etcd-servers=$IP1:2379,$IP2:2379,$IP3:2379,$IP4:2379,$IP5:2379</code>
启动 Kubernetes API 服务器。</p>
<p>确保将 <code>IP&lt;n&gt;</code> 变量设置为客户端 IP 地址。</p>
</li>
</ol>
<!--
### Multi-node etcd cluster with load balancer

To run a load balancing etcd cluster:

1. Set up an etcd cluster.
2. Configure a load balancer in front of the etcd cluster.
   For example, let the address of the load balancer be `$LB`.
3. Start Kubernetes API Servers with the flag `--etcd-servers=$LB:2379`.
-->
<h3 id="使用负载均衡的多节点-etcd-集群">使用负载均衡的多节点 etcd 集群</h3>
<p>要运行负载均衡的 etcd 集群：</p>
<ol>
<li>建立一个 etcd 集群。</li>
<li>在 etcd 集群前面配置负载均衡器。例如，让负载均衡器的地址为 <code>$LB</code>。</li>
<li>使用参数 <code>--etcd-servers=$LB:2379</code> 启动 Kubernetes API 服务器。</li>
</ol>
<!--
## Securing etcd clusters

Access to etcd is equivalent to root permission in the cluster so ideally only
the API server should have access to it. Considering the sensitivity of the
data, it is recommended to grant permission to only those nodes that require
access to etcd clusters.

To secure etcd, either set up firewall rules or use the security features
provided by etcd. etcd security features depend on x509 Public Key
Infrastructure (PKI). To begin, establish secure communication channels by
generating a key and certificate pair. For example, use key pairs `peer.key`
and `peer.cert` for securing communication between etcd members, and
`client.key` and `client.cert` for securing communication between etcd and its
clients. See the [example scripts](https://github.com/coreos/etcd/tree/master/hack/tls-setup)
provided by the etcd project to generate key pairs and CA files for client
authentication.
-->
<h2 id="安全的-etcd-集群">安全的 etcd 集群</h2>
<p>对 etcd 的访问相当于集群中的 root 权限，因此理想情况下只有 API 服务器才能访问它。
考虑到数据的敏感性，建议只向需要访问 etcd 集群的节点授予权限。</p>
<p>想要确保 etcd 的安全，可以设置防火墙规则或使用 etcd 提供的安全特性，这些安全特性依赖于 x509 公钥基础设施（PKI）。
首先，通过生成密钥和证书对来建立安全的通信通道。
例如，使用密钥对 <code>peer.key</code> 和 <code>peer.cert</code> 来保护 etcd 成员之间的通信，
而 <code>client.key</code> 和 <code>client.cert</code> 用于保护 etcd 与其客户端之间的通信。
请参阅 etcd 项目提供的<a href="https://github.com/coreos/etcd/tree/master/hack/tls-setup">示例脚本</a>，
以生成用于客户端身份验证的密钥对和 CA 文件。</p>
<!--
### Securing communication

To configure etcd with secure peer communication, specify flags
`--peer-key-file=peer.key` and `--peer-cert-file=peer.cert`, and use HTTPS as
the URL schema.

Similarly, to configure etcd with secure client communication, specify flags
`--key-file=k8sclient.key` and `--cert-file=k8sclient.cert`, and use HTTPS as
the URL schema. Here is an example on a client command that uses secure
communication:

```
ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
  --cert=/etc/kubernetes/pki/etcd/client.crt \
  --key=/etc/kubernetes/pki/etcd/client.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  member list
```
-->
<h3 id="安全通信">安全通信</h3>
<p>若要使用安全对等通信对 etcd 进行配置，请指定参数 <code>--peer-key-file=peer.key</code>
和 <code>--peer-cert-file=peer.cert</code>，并使用 HTTPS 作为 URL 模式。</p>
<p>类似地，要使用安全客户端通信对 etcd 进行配置，请指定参数 <code>--key-file=k8sclient.key</code>
和 <code>--cert-file=k8sclient.cert</code>，并使用 HTTPS 作为 URL 模式。
使用安全通信的客户端命令的示例：</p>
<pre><code>ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
  --cert=/etc/kubernetes/pki/etcd/client.crt \
  --key=/etc/kubernetes/pki/etcd/client.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  member list
</code></pre><!--
### Limiting access of etcd clusters

After configuring secure communication, restrict the access of etcd cluster to
only the Kubernetes API servers. Use TLS authentication to do so.

For example, consider key pairs `k8sclient.key` and `k8sclient.cert` that are
trusted by the CA `etcd.ca`. When etcd is configured with `--client-cert-auth`
along with TLS, it verifies the certificates from clients by using system CAs
or the CA passed in by `--trusted-ca-file` flag. Specifying flags
`--client-cert-auth=true` and `--trusted-ca-file=etcd.ca` will restrict the
access to clients with the certificate `k8sclient.cert`.

Once etcd is configured correctly, only clients with valid certificates can
access it. To give Kubernetes API servers the access, configure them with the
flags `--etcd-certfile=k8sclient.cert`,`--etcd-keyfile=k8sclient.key` and
`--etcd-cafile=ca.cert`.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> etcd authentication is not currently supported by Kubernetes. For more
information, see the related issue
<a href="https://github.com/kubernetes/kubernetes/issues/23398">Support Basic Auth for Etcd v2</a>.
</div>
-->
<h3 id="限制-etcd-集群的访问">限制 etcd 集群的访问</h3>
<p>配置安全通信后，将 etcd 集群的访问限制在 Kubernetes API 服务器上。使用 TLS 身份验证来完成此任务。</p>
<p>例如，考虑由 CA <code>etcd.ca</code> 信任的密钥对 <code>k8sclient.key</code> 和 <code>k8sclient.cert</code>。
当 etcd 配置为 <code>--client-cert-auth</code> 和 TLS 时，它使用系统 CA 或由 <code>--trusted-ca-file</code> 参数传入的 CA 验证来自客户端的证书。
指定参数 <code>--client-cert-auth=true</code> 和 <code>--trusted-ca-file=etcd.ca</code> 将限制对具有证书 <code>k8sclient.cert</code> 的客户端的访问。</p>
<p>一旦正确配置了 etcd，只有具有有效证书的客户端才能访问它。要让 Kubernetes API 服务器访问，
可以使用参数 <code>--etcd-certfile=k8sclient.cert</code>,<code>--etcd-keyfile=k8sclient.key</code> 和 <code>--etcd-cafile=ca.cert</code> 配置。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Kubernetes 目前不支持 etcd 身份验证。
想要了解更多信息，请参阅相关的问题
<a href="https://github.com/kubernetes/kubernetes/issues/23398">支持 etcd v2 的基本认证</a>。
</div>
<!--
## Replacing a failed etcd member

etcd cluster achieves high availability by tolerating minor member failures.
However, to improve the overall health of the cluster, replace failed members
immediately. When multiple members fail, replace them one by one. Replacing a
failed member involves two steps: removing the failed member and adding a new
member.

Though etcd keeps unique member IDs internally, it is recommended to use a
unique name for each member to avoid human errors. For example, consider a
three-member etcd cluster. Let the URLs be, `member1=http://10.0.0.1`,
`member2=http://10.0.0.2`, and `member3=http://10.0.0.3`. When `member1` fails,
replace it with `member4=http://10.0.0.4`.

1. Get the member ID of the failed `member1`:

   ```shell
   etcdctl --endpoints=http://10.0.0.2,http://10.0.0.3 member list
   ```

   The following message is displayed:

   ```console
   8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379
   91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379
   fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379
   ```

2. Remove the failed member:

   ```shell
   etcdctl member remove 8211f1d0f64f3269
   ```

   The following message is displayed:

   ```console
   Removed member 8211f1d0f64f3269 from cluster
   ```

3. Add the new member:

   ```shell
   etcdctl member add member4 --peer-urls=http://10.0.0.4:2380
   ```

   The following message is displayed:

   ```console
   Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4
   ```

4. Start the newly added member on a machine with the IP `10.0.0.4`:

   ```shell
   export ETCD_NAME="member4"
   export ETCD_INITIAL_CLUSTER="member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380"
   export ETCD_INITIAL_CLUSTER_STATE=existing
   etcd [flags]
   ```

5. Do either of the following:

   1. Update the `--etcd-servers` flag for the Kubernetes API servers to make
      Kubernetes aware of the configuration changes, then restart the
      Kubernetes API servers.
   2. Update the load balancer configuration if a load balancer is used in the
      deployment.

For more information on cluster reconfiguration, see
[etcd reconfiguration documentation](https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member).
-->
<h2 id="替换失败的-etcd-成员">替换失败的 etcd 成员</h2>
<p>etcd 集群通过容忍少数成员故障实现高可用性。
但是，要改善集群的整体健康状况，请立即替换失败的成员。当多个成员失败时，逐个替换它们。
替换失败成员需要两个步骤：删除失败成员和添加新成员。</p>
<p>虽然 etcd 在内部保留唯一的成员 ID，但建议为每个成员使用唯一的名称，以避免人为错误。
例如，考虑一个三成员的 etcd 集群。让 URL 为：<code>member1=http://10.0.0.1</code>， <code>member2=http://10.0.0.2</code>
和 <code>member3=http://10.0.0.3</code>。当 <code>member1</code> 失败时，将其替换为 <code>member4=http://10.0.0.4</code>。</p>
<ol>
<li>
<p>获取失败的 <code>member1</code> 的成员 ID：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">etcdctl --endpoints<span style="color:#666">=</span>http://10.0.0.2,http://10.0.0.3 member list
</code></pre></div><p>显示以下信息：</p>
<pre><code class="language-console" data-lang="console">8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379
91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379
fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379
</code></pre></li>
<li>
<p>移除失败的成员</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">etcdctl member remove 8211f1d0f64f3269
</code></pre></div><p>显示以下信息：</p>
<pre><code class="language-console" data-lang="console">Removed member 8211f1d0f64f3269 from cluster
</code></pre></li>
<li>
<p>增加新成员：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">etcdctl member add member4 --peer-urls<span style="color:#666">=</span>http://10.0.0.4:2380
</code></pre></div><p>显示以下信息：</p>
<pre><code class="language-console" data-lang="console">Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4
</code></pre></li>
<li>
<p>在 IP 为 <code>10.0.0.4</code> 的机器上启动新增加的成员：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_NAME</span><span style="color:#666">=</span><span style="color:#b44">&#34;member4&#34;</span>
<span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_INITIAL_CLUSTER</span><span style="color:#666">=</span><span style="color:#b44">&#34;member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380&#34;</span>
<span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_INITIAL_CLUSTER_STATE</span><span style="color:#666">=</span>existing
etcd <span style="color:#666">[</span>flags<span style="color:#666">]</span>
</code></pre></div></li>
<li>
<p>做以下事情之一：</p>
<ol>
<li>更新 Kubernetes API 服务器的 <code>--etcd-servers</code> 参数，使 Kubernetes 知道配置进行了更改，然后重新启动 Kubernetes API 服务器。</li>
<li>如果在 deployment 中使用了负载均衡，更新负载均衡配置。</li>
</ol>
</li>
</ol>
<p>有关集群重新配置的详细信息，请参阅 <a href="https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member">etcd 重构文档</a>。</p>
<!--
## Backing up an etcd cluster

All Kubernetes objects are stored on etcd. Periodically backing up the etcd
cluster data is important to recover Kubernetes clusters under disaster
scenarios, such as losing all control plane nodes. The snapshot file contains
all the Kubernetes states and critical information. In order to keep the
sensitive Kubernetes data safe, encrypt the snapshot files.

Backing up an etcd cluster can be accomplished in two ways: etcd built-in
snapshot and volume snapshot.
-->
<h2 id="备份-etcd-集群">备份 etcd 集群</h2>
<p>所有 Kubernetes 对象都存储在 etcd 上。定期备份 etcd 集群数据对于在灾难场景（例如丢失所有控制平面节点）下恢复 Kubernetes 集群非常重要。
快照文件包含所有 Kubernetes 状态和关键信息。为了保证敏感的 Kubernetes 数据的安全，可以对快照文件进行加密。</p>
<p>备份 etcd 集群可以通过两种方式完成：etcd 内置快照和卷快照。</p>
<!--
### Built-in snapshot

etcd supports built-in snapshot. A snapshot may either be taken from a live
member with the `etcdctl snapshot save` command or by copying the
`member/snap/db` file from an etcd
[data directory](https://etcd.io/docs/current/op-guide/configuration/#--data-dir)
that is not currently used by an etcd process. Taking the snapshot will
not affect the performance of the member.

Below is an example for taking a snapshot of the keyspace served by
`$ENDPOINT` to the file `snapshotdb`:

```shell
ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshotdb
```

Verify the snapshot:

```shell
ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb
```

```console
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| fe01cf57 |       10 |          7 | 2.1 MB     |
+----------+----------+------------+------------+
```
-->
<h3 id="内置快照">内置快照</h3>
<p>etcd 支持内置快照。快照可以从使用 <code>etcdctl snapshot save</code> 命令的活动成员中获取，
也可以通过从 etcd <a href="https://etcd.io/docs/current/op-guide/configuration/#--data-dir">数据目录</a>
复制 <code>member/snap/db</code> 文件，该 etcd 数据目录目前没有被 etcd 进程使用。获取快照不会影响成员的性能。</p>
<p>下面是一个示例，用于获取 <code>$ENDPOINT</code> 所提供的键空间的快照到文件 <code>snapshotdb</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --endpoints <span style="color:#b8860b">$ENDPOINT</span> snapshot save snapshotdb
</code></pre></div><p>验证快照:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --write-out<span style="color:#666">=</span>table snapshot status snapshotdb
</code></pre></div><pre><code class="language-console" data-lang="console">+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| fe01cf57 |       10 |          7 | 2.1 MB     |
+----------+----------+------------+------------+
</code></pre><!--
### Volume snapshot

If etcd is running on a storage volume that supports backup, such as Amazon
Elastic Block Store, back up etcd data by taking a snapshot of the storage
volume.

### Snapshot using etcdctl options

We can also take the snapshot using various options given by etcdctl. For example 

```shell
ETCDCTL_API=3 etcdctl -h 
``` 

will list various options available from etcdctl. For example, you can take a snapshot by specifying
the endpoint, certificates etc as shown below:

```shell
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
  snapshot save <backup-file-location>
```
where `trusted-ca-file`, `cert-file` and `key-file` can be obtained from the description of the etcd Pod.

## Scaling up etcd clusters

Scaling up etcd clusters increases availability by trading off performance.
Scaling does not increase cluster performance nor capability. A general rule
is not to scale up or down etcd clusters. Do not configure any auto scaling
groups for etcd clusters. It is highly recommended to always run a static
five-member etcd cluster for production Kubernetes clusters at any officially
supported scale.

A reasonable scaling is to upgrade a three-member cluster to a five-member
one, when more reliability is desired. See
[etcd reconfiguration documentation](https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member)
for information on how to add members into an existing cluster.
-->
<h3 id="卷快照">卷快照</h3>
<p>如果 etcd 运行在支持备份的存储卷（如 Amazon Elastic Block 存储）上，则可以通过获取存储卷的快照来备份 etcd 数据。</p>
<h3 id="使用-etcdctl-选项的快照">使用 etcdctl 选项的快照</h3>
<p>我们还可以使用 etcdctl 提供的各种选项来拍摄快照。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl -h 
</code></pre></div><p>列出 etcdctl 可用的各种选项。例如，你可以通过指定端点，证书等来拍摄快照，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --endpoints<span style="color:#666">=</span>https://127.0.0.1:2379 <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --cacert<span style="color:#666">=</span>&lt;trusted-ca-file&gt; --cert<span style="color:#666">=</span>&lt;cert-file&gt; --key<span style="color:#666">=</span>&lt;key-file&gt; <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  snapshot save &lt;backup-file-location&gt;
</code></pre></div><p>可以从 etcd Pod 的描述中获得 <code>trusted-ca-file</code>, <code>cert-file</code> 和 <code>key-file</code> 。</p>
<h2 id="扩展-etcd-集群">扩展 etcd 集群</h2>
<p>通过交换性能，扩展 etcd 集群可以提高可用性。缩放不会提高集群性能和能力。
一般情况下不要扩大或缩小 etcd 集群的集合。不要为 etcd 集群配置任何自动缩放组。
强烈建议始终在任何官方支持的规模上运行生产 Kubernetes 集群时使用静态的五成员 etcd 集群。</p>
<p>合理的扩展是在需要更高可靠性的情况下，将三成员集群升级为五成员集群。
请参阅 <a href="https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member">etcd 重新配置文档</a>
以了解如何将成员添加到现有集群中的信息。</p>
<!--
## Restoring an etcd cluster

etcd supports restoring from snapshots that are taken from an etcd process of
the [major.minor](http://semver.org/) version. Restoring a version from a
different patch version of etcd also is supported. A restore operation is
employed to recover the data of a failed cluster.

Before starting the restore operation, a snapshot file must be present. It can
either be a snapshot file from a previous backup operation, or from a remaining
[data directory]( https://etcd.io/docs/current/op-guide/configuration/#--data-dir).
Here is an example:

```shell
ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 snapshot restore snapshotdb
```

For more information and examples on restoring a cluster from a snapshot file, see
[etcd disaster recovery documentation](https://etcd.io/docs/current/op-guide/recovery/#restoring-a-cluster).

If the access URLs of the restored cluster is changed from the previous
cluster, the Kubernetes API server must be reconfigured accordingly. In this
case, restart Kubernetes API servers with the flag
`--etcd-servers=$NEW_ETCD_CLUSTER` instead of the flag
`--etcd-servers=$OLD_ETCD_CLUSTER`. Replace `$NEW_ETCD_CLUSTER` and
`$OLD_ETCD_CLUSTER` with the respective IP addresses. If a load balancer is
used in front of an etcd cluster, you might need to update the load balancer
instead.

If the majority of etcd members have permanently failed, the etcd cluster is
considered failed. In this scenario, Kubernetes cannot make any changes to its
current state. Although the scheduled pods might continue to run, no new pods
can be scheduled. In such cases, recover the etcd cluster and potentially
reconfigure Kubernetes API servers to fix the issue.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>If any API servers are running in your cluster, you should not attempt to
restore instances of etcd. Instead, follow these steps to restore etcd:</p>
<ul>
<li>stop <em>all</em> API server instances</li>
<li>restore state in all etcd instances</li>
<li>restart all API server instances</li>
</ul>
<p>We also recommend restarting any components (e.g. <code>kube-scheduler</code>,
<code>kube-controller-manager</code>, <code>kubelet</code>) to ensure that they don't rely on some
stale data. Note that in practice, the restore takes a bit of time.  During the
restoration, critical components will lose leader lock and restart themselves.</p>

</div>
-->
<h2 id="恢复-etcd-集群">恢复 etcd 集群</h2>
<p>etcd 支持从 <a href="http://semver.org/">major.minor</a> 或其他不同 patch 版本的 etcd 进程中获取的快照进行恢复。
还原操作用于恢复失败的集群的数据。</p>
<p>在启动还原操作之前，必须有一个快照文件。它可以是来自以前备份操作的快照文件，
也可以是来自剩余<a href="https://etcd.io/docs/current/op-guide/configuration/#--data-dir">数据目录</a>的快照文件。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --endpoints 10.2.0.9:2379 snapshot restore snapshotdb
</code></pre></div><p>恢复时也可以指定操作选项，例如：</p>
<pre><code>ETCDCTL_API=3 etcdctl --data-dir &lt;data-dir-location&gt; snapshot restore snapshotdb
</code></pre><p>有关从快照文件还原集群的详细信息和示例，请参阅
<a href="https://etcd.io/docs/current/op-guide/recovery/#restoring-a-cluster">etcd 灾难恢复文档</a>。</p>
<p>如果还原的集群的访问 URL 与前一个集群不同，则必须相应地重新配置 Kubernetes API 服务器。
在本例中，使用参数 <code>--etcd-servers=$NEW_ETCD_CLUSTER</code> 而不是参数 <code>--etcd-servers=$OLD_ETCD_CLUSTER</code> 重新启动 Kubernetes API 服务器。
用相应的 IP 地址替换 <code>$NEW_ETCD_CLUSTER</code> 和 <code>$OLD_ETCD_CLUSTER</code>。如果在 etcd 集群前面使用负载平衡，则可能需要更新负载均衡器。</p>
<p>如果大多数 etcd 成员永久失败，则认为 etcd 集群失败。在这种情况下，Kubernetes 不能对其当前状态进行任何更改。
虽然已调度的 pod 可能继续运行，但新的 pod 无法调度。在这种情况下，恢复 etcd 集群并可能需要重新配置 Kubernetes API 服务器以修复问题。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>如果集群中正在运行任何 API 服务器，则不应尝试还原 etcd 的实例。相反，请按照以下步骤还原 etcd：</p>
<ul>
<li>停止 <em>所有</em> API 服务实例</li>
<li>在所有 etcd 实例中恢复状态</li>
<li>重启所有 API 服务实例</li>
</ul>
<p>我们还建议重启所有组件（例如 <code>kube-scheduler</code>、<code>kube-controller-manager</code>、<code>kubelet</code>），以确保它们不会
依赖一些过时的数据。请注意，实际中还原会花费一些时间。
在还原过程中，关键组件将丢失领导锁并自行重启。</p>

</div>
<!--

## Upgrading etcd clusters

For more details on etcd upgrade, please refer to the [etcd upgrades](https://etcd.io/docs/latest/upgrades/) documentation.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Before you start an upgrade, please back up your etcd cluster first.
</div>
-->
<h2 id="升级-etcd-集群">升级 etcd 集群</h2>
<p>有关 etcd 升级的更多详细信息，请参阅 <a href="https://etcd.io/docs/latest/upgrades/">etcd 升级</a>文档。
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在开始升级之前，请先备份你的 etcd 集群。
</div></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b64a1d2bb3f4ed9f7021134e09a75c36">9 - 为系统守护进程预留计算资源</h1>
    
	<!--
reviewers:
- vishh
- derekwaynecarr
- dashpole
title: Reserve Compute Resources for System Daemons
content_type: task
min-kubernetes-server-version: 1.8
-->
<!-- overview -->
<!--
Kubernetes nodes can be scheduled to `Capacity`. Pods can consume all the
available capacity on a node by default. This is an issue because nodes
typically run quite a few system daemons that power the OS and Kubernetes
itself. Unless resources are set aside for these system daemons, pods and system
daemons compete for resources and lead to resource starvation issues on the
node.

The `kubelet` exposes a feature named 'Node Allocatable' that helps to reserve
compute resources for system daemons. Kubernetes recommends cluster
administrators to configure `Node Allocatable` based on their workload density
on each node.
-->
<p>Kubernetes 的节点可以按照 <code>Capacity</code> 调度。默认情况下 pod 能够使用节点全部可用容量。
这是个问题，因为节点自己通常运行了不少驱动 OS 和 Kubernetes 的系统守护进程。
除非为这些系统守护进程留出资源，否则它们将与 pod 争夺资源并导致节点资源短缺问题。</p>
<p><code>kubelet</code> 公开了一个名为 'Node Allocatable' 的特性，有助于为系统守护进程预留计算资源。
Kubernetes 推荐集群管理员按照每个节点上的工作负载密度配置 <code>Node Allocatable</code>。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

Your Kubernetes server must be at or later than version 1.8.
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- 
Your Kubernetes server must be at or later than version 1.17 to use
the kubelet command line option `--reserved-cpus` to set an
[explicitly reserved CPU list](#explicitly-reserved-cpu-list).
-->
<p>您的 kubernetes 服务器版本必须至少是 1.17 版本，才能使用 kubelet
命令行选项 <code>--reserved-cpus</code> 设置
<a href="#explicitly-reserved-cpu-list">显式预留 CPU 列表</a>。</p>
<!-- steps -->
<!--
## Node Allocatable

![node capacity](/images/docs/node-capacity.svg)

'Allocatable' on a Kubernetes node is defined as the amount of compute resources
that are available for pods. The scheduler does not over-subscribe
'Allocatable'. 'CPU', 'memory' and 'ephemeral-storage' are supported as of now.

Node Allocatable is exposed as part of `v1.Node` object in the API and as part
of `kubectl describe node` in the CLI.

Resources can be reserved for two categories of system daemons in the `kubelet`.
-->
<h2 id="node-allocatable">节点可分配  </h2>
<p><img src="/images/docs/node-capacity.svg" alt="节点容量"></p>
<p>Kubernetes 节点上的 'Allocatable' 被定义为 pod 可用计算资源量。
调度器不会超额申请 'Allocatable'。
目前支持 'CPU', 'memory' 和 'ephemeral-storage' 这几个参数。</p>
<p>可分配的节点暴露为 API 中 <code>v1.Node</code> 对象的一部分，也是 CLI 中
<code>kubectl describe node</code> 的一部分。</p>
<p>在 <code>kubelet</code> 中，可以为两类系统守护进程预留资源。</p>
<!--
### Enabling QoS and Pod level cgroups

To properly enforce node allocatable constraints on the node, you must
enable the new cgroup hierarchy via the `--cgroups-per-qos` flag. This flag is
enabled by default. When enabled, the `kubelet` will parent all end-user pods
under a cgroup hierarchy managed by the `kubelet`.
-->
<h3 id="启用-qos-和-pod-级别的-cgroups">启用 QoS 和 Pod 级别的 cgroups</h3>
<p>为了恰当的在节点范围实施节点可分配约束，你必须通过 <code>--cgroups-per-qos</code>
标志启用新的 cgroup 层次结构。这个标志是默认启用的。
启用后，<code>kubelet</code> 将在其管理的 cgroup 层次结构中创建所有终端用户的 Pod。</p>
<!--
### Configuring a cgroup driver

The `kubelet` supports manipulation of the cgroup hierarchy on
the host using a cgroup driver. The driver is configured via the
`--cgroup-driver` flag.

The supported values are the following:

* `cgroupfs` is the default driver that performs direct manipulation of the
cgroup filesystem on the host in order to manage cgroup sandboxes.
* `systemd` is an alternative driver that manages cgroup sandboxes using
transient slices for resources that are supported by that init system.

Depending on the configuration of the associated container runtime,
operators may have to choose a particular cgroup driver to ensure
proper system behavior. For example, if operators use the `systemd`
cgroup driver provided by the `docker` runtime, the `kubelet` must
be configured to use the `systemd` cgroup driver.
-->
<h3 id="配置-cgroup-驱动">配置 cgroup 驱动</h3>
<p><code>kubelet</code> 支持在主机上使用 cgroup 驱动操作 cgroup 层次结构。
驱动通过 <code>--cgroup-driver</code> 标志配置。</p>
<p>支持的参数值如下：</p>
<ul>
<li><code>cgroupfs</code> 是默认的驱动，在主机上直接操作 cgroup 文件系统以对 cgroup
沙箱进行管理。</li>
<li><code>systemd</code> 是可选的驱动，使用 init 系统支持的资源的瞬时切片管理
cgroup 沙箱。</li>
</ul>
<p>取决于相关容器运行时的配置，操作员可能需要选择一个特定的 cgroup 驱动
来保证系统正常运行。
例如，如果操作员使用 <code>docker</code> 运行时提供的 <code>systemd</code> cgroup 驱动时，
必须配置 <code>kubelet</code> 使用 <code>systemd</code> cgroup 驱动。</p>
<!--
### Kube Reserved

- **Kubelet Flag**: `--kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]`
- **Kubelet Flag**: `--kube-reserved-cgroup=`

`kube-reserved` is meant to capture resource reservation for kubernetes system
daemons like the `kubelet`, `container runtime`, `node problem detector`, etc.
It is not meant to reserve resources for system daemons that are run as pods.
`kube-reserved` is typically a function of `pod density` on the nodes.

In addition to `cpu`, `memory`, and `ephemeral-storage`, `pid` may be
specified to reserve the specified number of process IDs for
kubernetes system daemons.
-->
<h3 id="kube-reserved">Kube 预留值 </h3>
<ul>
<li><strong>Kubelet 标志</strong>: <code>--kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]</code></li>
<li><strong>Kubelet 标志</strong>: <code>--kube-reserved-cgroup=</code></li>
</ul>
<p><code>kube-reserved</code> 用来给诸如 <code>kubelet</code>、容器运行时、节点问题监测器等
kubernetes 系统守护进程记述其资源预留值。
该配置并非用来给以 Pod 形式运行的系统守护进程保留资源。<code>kube-reserved</code>
通常是节点上 <code>pod 密度</code> 的函数。</p>
<p>除了 <code>cpu</code>，<code>内存</code> 和 <code>ephemeral-storage</code> 之外，<code>pid</code> 可用来指定为
kubernetes 系统守护进程预留指定数量的进程 ID。</p>
<!--
To optionally enforce `kube-reserved` on kubernetes system daemons, specify the parent
control group for kube daemons as the value for `--kube-reserved-cgroup` kubelet
flag.

It is recommended that the kubernetes system daemons are placed under a top
level control group (`runtime.slice` on systemd machines for example). Each
system daemon should ideally run within its own child control group. Refer to
[the design proposal](https://git.k8s.io/design-proposals-archive/node/node-allocatable.md#recommended-cgroups-setup)
for more details on recommended control group hierarchy.

Note that Kubelet **does not** create `--kube-reserved-cgroup` if it doesn't
exist. Kubelet will fail if an invalid cgroup is specified.
-->
<p>要选择性地对 kubernetes 系统守护进程上执行 <code>kube-reserved</code> 保护，需要把 kubelet 的
<code>--kube-reserved-cgroup</code> 标志的值设置为 kube 守护进程的父控制组。</p>
<p>推荐将 kubernetes 系统守护进程放置于顶级控制组之下（例如 systemd 机器上的
<code>runtime.slice</code>）。
理想情况下每个系统守护进程都应该在其自己的子控制组中运行。
请参考
<a href="https://git.k8s.io/design-proposals-archive/node/node-allocatable.md#recommended-cgroups-setup">这个设计方案</a>，
进一步了解关于推荐控制组层次结构的细节。</p>
<p>请注意，如果 <code>--kube-reserved-cgroup</code> 不存在，Kubelet 将 <strong>不会</strong> 创建它。
如果指定了一个无效的 cgroup，Kubelet 将会失败。</p>
<!--
### System Reserved

- **Kubelet Flag**: `--system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]`
- **Kubelet Flag**: `--system-reserved-cgroup=`


`system-reserved` is meant to capture resource reservation for OS system daemons
like `sshd`, `udev`, etc. `system-reserved` should reserve `memory` for the
`kernel` too since `kernel` memory is not accounted to pods in Kubernetes at this time.
Reserving resources for user login sessions is also recommended (`user.slice` in
systemd world).

In addition to `cpu`, `memory`, and `ephemeral-storage`, `pid` may be
specified to reserve the specified number of process IDs for OS system
daemons.
-->
<h3 id="system-reserved">系统预留值 </h3>
<ul>
<li><strong>Kubelet 标志</strong>: <code>--system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]</code></li>
<li><strong>Kubelet 标志</strong>: <code>--system-reserved-cgroup=</code></li>
</ul>
<p><code>system-reserved</code> 用于为诸如 <code>sshd</code>、<code>udev</code> 等系统守护进程记述其资源预留值。
<code>system-reserved</code> 也应该为 <code>kernel</code> 预留 <code>内存</code>，因为目前 <code>kernel</code>
使用的内存并不记在 Kubernetes 的 Pod 上。
同时还推荐为用户登录会话预留资源（systemd 体系中的 <code>user.slice</code>）。</p>
<p>除了 <code>cpu</code>，<code>内存</code> 和 <code>ephemeral-storage</code> 之外，<code>pid</code> 可用来指定为
kubernetes 系统守护进程预留指定数量的进程 ID。</p>
<!--
To optionally enforce `system-reserved` on system daemons, specify the parent
control group for OS system daemons as the value for `--system-reserved-cgroup`
kubelet flag.

It is recommended that the OS system daemons are placed under a top level
control group (`system.slice` on systemd machines for example).

Note that `kubelet` **does not** create `--system-reserved-cgroup` if it doesn't
exist. `kubelet` will fail if an invalid cgroup is specified.
-->
<p>要想为系统守护进程上可选地实施 <code>system-reserved</code> 约束，请指定 kubelet 的
<code>--system-reserved-cgroup</code> 标志值为 OS 系统守护进程的父级控制组。</p>
<p>推荐将 OS 系统守护进程放在一个顶级控制组之下（例如 systemd 机器上的
<code>system.slice</code>）。</p>
<p>请注意，如果 <code>--system-reserved-cgroup</code> 不存在，<code>kubelet</code> <strong>不会</strong> 创建它。
如果指定了无效的 cgroup，<code>kubelet</code> 将会失败。</p>
<!--
### Explicitly Reserved CPU List

-**Kubelet Flag**: `--reserved-cpus=0-3`
-->
<h3 id="explicitly-reserved-cpu-list">显式保留的 CPU 列表</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code>
</div>


<p>-<strong>Kubelet 标志</strong>: <code>--reserved-cpus=0-3</code></p>
<!--
`reserved-cpus` is meant to define an explicit CPU set for OS system daemons and
kubernetes system daemons. `reserved-cpus` is for systems that do not intend to
define separate top level cgroups for OS system daemons and kubernetes system daemons
with regard to cpuset resource.
If the Kubelet **does not** have `--system-reserved-cgroup` and `--kube-reserved-cgroup`,
the explicit cpuset provided by `reserved-cpus` will take precedence over the CPUs
defined by `--kube-reserved` and `--system-reserved` options.
-->
<p><code>reserved-cpus</code> 旨在为操作系统守护程序和 kubernetes 系统守护程序保留一组明确指定编号的
CPU。<code>reserved-cpus</code> 适用于不打算针对 cpuset 资源为操作系统守护程序和 kubernetes
系统守护程序定义独立的顶级 cgroups 的系统。
如果 Kubelet <strong>没有</strong> 指定参数 <code>--system-reserved-cgroup</code> 和 <code>--kube-reserved-cgroup</code>，
则 <code>reserved-cpus</code> 的设置将优先于 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 选项。</p>
<!--
This option is specifically designed for Telco/NFV use cases where uncontrolled
interrupts/timers may impact the workload performance. you can use this option
to define the explicit cpuset for the system/kubernetes daemons as well as the
interrupts/timers, so the rest CPUs on the system can be used exclusively for
workloads, with less impact from uncontrolled interrupts/timers. To move the
system daemon, kubernetes daemons and interrupts/timers to the explicit cpuset
defined by this option, other mechanism outside Kubernetes should be used.
For example: in Centos, you can do this using the tuned toolset.
-->
<p>此选项是专门为电信/NFV 用例设计的，在这些用例中不受控制的中断或计时器可能会
影响其工作负载性能。
你可以使用此选项为系统或 kubernetes 守护程序以及中断或计时器显式定义 cpuset，
这样系统上的其余 CPU 可以专门用于工作负载，因不受控制的中断或计时器的影响得以
降低。
要将系统守护程序、kubernetes 守护程序和中断或计时器移动到此选项定义的显式
cpuset 上，应使用 Kubernetes 之外的其他机制。
例如：在 Centos 系统中，可以使用 tuned 工具集来执行此操作。</p>
<!--
### Eviction Thresholds

- **Kubelet Flag**: `--eviction-hard=[memory.available<500Mi]`

Memory pressure at the node level leads to System OOMs which affects the entire
node and all pods running on it. Nodes can go offline temporarily until memory
has been reclaimed. To avoid (or reduce the probability of) system OOMs kubelet
provides [out of resource](/docs/concepts/scheduling-eviction/node-pressure-eviction/)
management. Evictions are
supported for `memory` and `ephemeral-storage` only. By reserving some memory via
`--eviction-hard` flag, the `kubelet` attempts to evict pods whenever memory
availability on the node drops below the reserved value. Hypothetically, if
system daemons did not exist on a node, pods cannot use more than `capacity -
eviction-hard`. For this reason, resources reserved for evictions are not
available for pods.
-->
<h3 id="eviction-Thresholds">驱逐阈值  </h3>
<ul>
<li><strong>Kubelet 标志</strong>: <code>--eviction-hard=[memory.available&lt;500Mi]</code></li>
</ul>
<p>节点级别的内存压力将导致系统内存不足，这将影响到整个节点及其上运行的所有 Pod。
节点可以暂时离线直到内存已经回收为止。
为了防止（或减少可能性）系统内存不足，kubelet 提供了
<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/">资源不足</a>管理。
驱逐操作只支持 <code>memory</code> 和 <code>ephemeral-storage</code>。
通过 <code>--eviction-hard</code> 标志预留一些内存后，当节点上的可用内存降至保留值以下时，
<code>kubelet</code> 将尝试驱逐 Pod。
如果节点上不存在系统守护进程，Pod 将不能使用超过 <code>capacity-eviction-hard</code> 所
指定的资源量。因此，为驱逐而预留的资源对 Pod 是不可用的。</p>
<!--
### Enforcing Node Allocatable

-**Kubelet Flag**: `--enforce-node-allocatable=pods[,][system-reserved][,][kube-reserved]`

The scheduler treats 'Allocatable' as the available `capacity` for pods.

`kubelet` enforce 'Allocatable' across pods by default. Enforcement is performed
by evicting pods whenever the overall usage across all pods exceeds
'Allocatable'. More details on eviction policy can be found
on the [node pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/)
page. This enforcement is controlled by
specifying `pods` value to the kubelet flag `--enforce-node-allocatable`.

Optionally, `kubelet` can be made to enforce `kube-reserved` and
`system-reserved` by specifying `kube-reserved` & `system-reserved` values in
the same flag. Note that to enforce `kube-reserved` or `system-reserved`,
`--kube-reserved-cgroup` or `--system-reserved-cgroup` needs to be specified
respectively.
-->
<h3 id="enforcing-node-allocatable">实施节点可分配约束  </h3>
<p>-<strong>Kubelet 标志</strong>: <code>--enforce-node-allocatable=pods[,][system-reserved][,][kube-reserved]</code></p>
<p>调度器将 'Allocatable' 视为 Pod 可用的 <code>capacity</code>（资源容量）。</p>
<p><code>kubelet</code> 默认对 Pod 执行 'Allocatable' 约束。
无论何时，如果所有 Pod 的总用量超过了 'Allocatable'，驱逐 Pod 的措施将被执行。
有关驱逐策略的更多细节可以在
<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">节点压力驱逐</a>页找到。
可通过设置 kubelet <code>--enforce-node-allocatable</code> 标志值为 <code>pods</code> 控制这个措施。</p>
<p>可选地，通过在同一标志中同时指定 <code>kube-reserved</code> 和 <code>system-reserved</code> 值，
可以使 <code>kubelet</code> 强制实施 <code>kube-reserved</code> 和 <code>system-reserved</code>约束。
请注意，要想执行 <code>kube-reserved</code> 或者 <code>system-reserved</code> 约束，
需要对应设置 <code>--kube-reserved-cgroup</code> 或者 <code>--system-reserved-cgroup</code>。</p>
<!--
## General Guidelines

System daemons are expected to be treated similar to
[Guaranteed pods](/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed). 
System daemons can burst within their bounding control groups and this behavior needs
to be managed as part of kubernetes deployments. For example, `kubelet` should
have its own control group and share `Kube-reserved` resources with the
container runtime. However, Kubelet cannot burst and use up all available Node
resources if `kube-reserved` is enforced.
-->
<h2 id="general-guidelines">一般原则  </h2>
<p>系统守护进程一般会被按照类似
<a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed">Guaranteed pods</a>
一样对待。
系统守护进程可以在与其对应的控制组中出现突发资源用量，这一行为要作为
kubernetes 部署的一部分进行管理。
例如，<code>kubelet</code> 应该有它自己的控制组并和容器运行时共享 <code>Kube-reserved</code> 资源。
不过，如果执行了 <code>kube-reserved</code> 约束，则 kubelet 不可出现突发负载并用光
节点的所有可用资源。</p>
<!--
Be extra careful while enforcing `system-reserved` reservation since it can lead
to critical system services being CPU starved, OOM killed, or unable
to fork on the node. The
recommendation is to enforce `system-reserved` only if a user has profiled their
nodes exhaustively to come up with precise estimates and is confident in their
ability to recover if any process in that group is oom-killed.

* To begin with enforce 'Allocatable' on `pods`.
* Once adequate monitoring and alerting is in place to track kube system
  daemons, attempt to enforce `kube-reserved` based on usage heuristics.
* If absolutely necessary, enforce `system-reserved` over time.
-->
<p>在执行 <code>system-reserved</code> 预留策略时请加倍小心，因为它可能导致节点上的
关键系统服务出现 CPU 资源短缺、因为内存不足而被终止或者无法在节点上创建进程。
建议只有当用户详尽地描述了他们的节点以得出精确的估计值，
并且对该组中进程因内存不足而被杀死时，有足够的信心将其恢复时，
才可以强制执行 <code>system-reserved</code> 策略。</p>
<ul>
<li>作为起步，可以先针对 <code>pods</code> 上执行 'Allocatable' 约束。</li>
<li>一旦用于追踪系统守护进程的监控和告警的机制到位，可尝试基于用量估计的
方式执行 <code>kube-reserved</code>策略。</li>
<li>随着时间推进，在绝对必要的时候可以执行 <code>system-reserved</code> 策略。</li>
</ul>
<!--
The resource requirements of kube system daemons may grow over time as more and
more features are added. Over time, kubernetes project will attempt to bring
down utilization of node system daemons, but that is not a priority as of now.
So expect a drop in `Allocatable` capacity in future releases.
-->
<p>随着时间推进和越来越多特性被加入，kube 系统守护进程对资源的需求可能也会增加。
以后 kubernetes 项目将尝试减少对节点系统守护进程的利用，但目前这件事的优先级
并不是最高。
所以，将来的发布版本中 <code>Allocatable</code> 容量是有可能降低的。</p>
<!-- discussion -->
<!--
## Example Scenario

Here is an example to illustrate Node Allocatable computation:

* Node has `32Gi` of `memory`, `16 CPUs` and `100Gi` of `Storage`
* `--kube-reserved` is set to `cpu=1,memory=2Gi,ephemeral-storage=1Gi`
* `--system-reserved` is set to `cpu=500m,memory=1Gi,ephemeral-storage=1Gi`
* `--eviction-hard` is set to `memory.available<500Mi,nodefs.available<10%`
-->
<h2 id="example-scenario">示例场景  </h2>
<p>这是一个用于说明节点可分配（Node Allocatable）计算方式的示例：</p>
<ul>
<li>节点拥有 <code>32Gi</code> <code>memeory</code>，<code>16 CPU</code> 和 <code>100Gi</code> <code>Storage</code> 资源</li>
<li><code>--kube-reserved</code> 被设置为 <code>cpu=1,memory=2Gi,ephemeral-storage=1Gi</code></li>
<li><code>--system-reserved</code> 被设置为 <code>cpu=500m,memory=1Gi,ephemeral-storage=1Gi</code></li>
<li><code>--eviction-hard</code> 被设置为 <code>memory.available&lt;500Mi,nodefs.available&lt;10%</code></li>
</ul>
<!--
Under this scenario, 'Allocatable' will be 14.5 CPUs, 28.5Gi of memory and
`88Gi` of local storage.
Scheduler ensures that the total memory `requests` across all pods on this node does
not exceed 28.5Gi and storage doesn't exceed 88Gi.
Kubelet evicts pods whenever the overall memory usage across pods exceeds 28.5Gi,
or if overall disk usage exceeds 88Gi If all processes on the node consume as
much CPU as they can, pods together cannot consume more than 14.5 CPUs.

If `kube-reserved` and/or `system-reserved` is not enforced and system daemons
exceed their reservation, `kubelet` evicts pods whenever the overall node memory
usage is higher than 31.5Gi or `storage` is greater than 90Gi
-->
<p>在这个场景下，'Allocatable' 将会是 14.5 CPUs、28.5Gi 内存以及 <code>88Gi</code> 本地存储。
调度器保证这个节点上的所有 Pod 的内存 <code>requests</code> 总量不超过 28.5Gi，
存储不超过 '88Gi'。
当 Pod 的内存使用总量超过 28.5Gi 或者磁盘使用总量超过 88Gi 时，
kubelet 将会驱逐它们。
如果节点上的所有进程都尽可能多地使用 CPU，则 Pod 加起来不能使用超过
14.5 CPUs 的资源。</p>
<p>当没有执行 <code>kube-reserved</code> 和/或 <code>system-reserved</code> 策略且系统守护进程
使用量超过其预留时，如果节点内存用量高于 31.5Gi 或<code>存储</code>大于 90Gi，
kubelet 将会驱逐 Pod。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a8f6511197efcd7d0db80ade49620f9d">10 - 为节点发布扩展资源</h1>
    
	<!--
title: Advertise Extended Resources for a Node
content_type: task
-->
<!-- overview -->
<!--
This page shows how to specify extended resources for a Node.
Extended resources allow cluster administrators to advertise node-level
resources that would otherwise be unknown to Kubernetes.
-->
<p>本文展示了如何为节点指定扩展资源（Extended Resource）。
扩展资源允许集群管理员发布节点级别的资源，这些资源在不进行发布的情况下无法被 Kubernetes 感知。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Get the names of your Nodes

Choose one of your Nodes to use for this exercise.
-->
<h2 id="获取你的节点名称">获取你的节点名称</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><p>选择一个节点用于此练习。</p>
<!--
## Advertise a new extended resource on one of your Nodes

To advertise a new extended resource on a Node, send an HTTP PATCH request to
the Kubernetes API server. For example, suppose one of your Nodes has four dongles
attached. Here's an example of a PATCH request that advertises four dongle resources
for your Node.
-->
<h2 id="在你的一个节点上发布一种新的扩展资源">在你的一个节点上发布一种新的扩展资源</h2>
<p>为在一个节点上发布一种新的扩展资源，需要发送一个 HTTP PATCH 请求到 Kubernetes API server。
例如：假设你的一个节点上带有四个 dongle 资源。
下面是一个 PATCH 请求的示例，该请求为你的节点发布四个 dongle 资源。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

<span style="color:#666">[</span>
  <span style="color:#666">{</span>
    <span style="color:#b44">&#34;op&#34;</span>: <span style="color:#b44">&#34;add&#34;</span>,
    <span style="color:#b44">&#34;path&#34;</span>: <span style="color:#b44">&#34;/status/capacity/example.com~1dongle&#34;</span>,
    <span style="color:#b44">&#34;value&#34;</span>: <span style="color:#b44">&#34;4&#34;</span>
  <span style="color:#666">}</span>
<span style="color:#666">]</span>
</code></pre></div><!--
Note that Kubernetes does not need to know what a dongle is or what a dongle is for.
The preceding PATCH request tells Kubernetes that your Node has four things that
you call dongles.

Start a proxy, so that you can easily send requests to the Kubernetes API server:
-->
<p>注意：Kubernetes 不需要了解 dongle 资源的含义和用途。
前面的 PATCH 请求告诉 Kubernetes 你的节点拥有四个你称之为 dongle 的东西。</p>
<p>启动一个代理（proxy），以便你可以很容易地向 Kubernetes API server 发送请求：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy
</code></pre></div><!--
In another command window, send the HTTP PATCH request.
Replace `<your-node-name>` with the name of your Node:
-->
<p>在另一个命令窗口中，发送 HTTP PATCH 请求。 用你的节点名称替换 <code>&lt;your-node-name&gt;</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl --header <span style="color:#b44">&#34;Content-Type: application/json-patch+json&#34;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --request PATCH <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --data <span style="color:#b44">&#39;[{&#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1dongle&#34;, &#34;value&#34;: &#34;4&#34;}]&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status
</code></pre></div><!--
In the preceding request, `~1` is the encoding for the character / in
the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
[IETF RFC 6901](https://tools.ietf.org/html/rfc6901), section 3.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在前面的请求中，<code>~1</code> 为 patch 路径中 “/” 符号的编码。
JSON-Patch 中的操作路径值被解析为 JSON 指针。
更多细节，请查看 <a href="https://tools.ietf.org/html/rfc6901">IETF RFC 6901</a> 的第 3 节。
</div>
<!--
The output shows that the Node has a capacity of 4 dongles:
-->
<p>输出显示该节点的 dongle 资源容量（capacity）为 4：</p>
<pre><code>&quot;capacity&quot;: {
  &quot;cpu&quot;: &quot;2&quot;,
  &quot;memory&quot;: &quot;2049008Ki&quot;,
  &quot;example.com/dongle&quot;: &quot;4&quot;,
</code></pre><!-- Describe your Node: -->
<p>描述你的节点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe node &lt;your-node-name&gt;
</code></pre></div><!-- Once again, the output shows the dongle resource: -->
<p>输出再次展示了 dongle 资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">Capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb">  </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb">  </span>2049008Ki<span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">example.com/dongle</span>:<span style="color:#bbb">  </span><span style="color:#666">4</span><span style="color:#bbb">
</span></code></pre></div><!--
Now, application developers can create Pods that request a certain
number of dongles. See
[Assign Extended Resources to a Container](/docs/tasks/configure-pod-container/extended-resource/).
-->
<p>现在，应用开发者可以创建请求一定数量 dongle 资源的 Pod 了。
参见<a href="/zh/docs/tasks/configure-pod-container/extended-resource/">将扩展资源分配给容器</a>。</p>
<!--
## Discussion

Extended resources are similar to memory and CPU resources. For example,
just as a Node has a certain amount of memory and CPU to be shared by all components
running on the Node, it can have a certain number of dongles to be shared
by all components running on the Node. And just as application developers
can create Pods that request a certain amount of memory and CPU, they can
create Pods that request a certain number of dongles.
-->
<h2 id="讨论">讨论</h2>
<p>扩展资源类似于内存和 CPU 资源。例如，正如一个节点拥有一定数量的内存和 CPU 资源，
它们被节点上运行的所有组件共享，该节点也可以拥有一定数量的 dongle 资源，
这些资源同样被节点上运行的所有组件共享。
此外，正如应用开发者可以创建请求一定数量的内存和 CPU 资源的 Pod，
他们也可以创建请求一定数量 dongle 资源的 Pod。</p>
<!--
Extended resources are opaque to Kubernetes; Kubernetes does not
know anything about what they are. Kubernetes knows only that a Node
has a certain number of them. Extended resources must be advertised in integer
amounts. For example, a Node can advertise four dongles, but not 4.5 dongles.
-->
<p>扩展资源对 Kubernetes 是不透明的。Kubernetes 不知道扩展资源含义相关的任何信息。
Kubernetes 只了解一个节点拥有一定数量的扩展资源。
扩展资源必须以整形数量进行发布。
例如，一个节点可以发布 4 个 dongle 资源，但是不能发布 4.5 个。</p>
<!--
### Storage example

Suppose a Node has 800 GiB of a special kind of disk storage. You could
create a name for the special storage, say example.com/special-storage.
Then you could advertise it in chunks of a certain size, say 100 GiB. In that case,
your Node would advertise that it has eight resources of type
example.com/special-storage.
-->
<h3 id="存储示例">存储示例</h3>
<p>假设一个节点拥有一种特殊类型的磁盘存储，其容量为 800 GiB。
你可以为该特殊存储创建一个名称，如 <code>example.com/special-storage</code>。
然后你就可以按照一定规格的块（如 100 GiB）对其进行发布。
在这种情况下，你的节点将会通知它拥有八个 <code>example.com/special-storage</code> 类型的资源。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">Capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">example.com/special-storage</span>:<span style="color:#bbb"> </span><span style="color:#666">8</span><span style="color:#bbb">
</span></code></pre></div><!--
If you want to allow arbitrary requests for special storage, you
could advertise special storage in chunks of size 1 byte. In that case, you would advertise
800Gi resources of type example.com/special-storage.
-->
<p>如果你想要允许针对特殊存储任意（数量）的请求，你可以按照 1 字节大小的块来发布特殊存储。
在这种情况下，你将会发布 800Gi 数量的 example.com/special-storage 类型的资源。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">Capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">example.com/special-storage</span>:<span style="color:#bbb">  </span>800Gi<span style="color:#bbb">
</span></code></pre></div><!--
Then a Container could request any number of bytes of special storage, up to 800Gi.
-->
<p>然后，容器就能够请求任意数量（多达 800Gi）字节的特殊存储。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">Capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">example.com/special-storage</span>:<span style="color:#bbb">  </span>800Gi<span style="color:#bbb">
</span></code></pre></div><!--
## Clean up

Here is a PATCH request that removes the dongle advertisement from a Node.
-->
<h2 id="清理">清理</h2>
<p>这里是一个从节点移除 dongle 资源发布的 PATCH 请求。</p>
<pre><code>PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    &quot;op&quot;: &quot;remove&quot;,
    &quot;path&quot;: &quot;/status/capacity/example.com~1dongle&quot;,
  }
]
</code></pre><!--
Start a proxy, so that you can easily send requests to the Kubernetes API server:
-->
<p>启动一个代理，以便你可以很容易地向 Kubernetes API 服务器发送请求：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy
</code></pre></div><!--
In another command window, send the HTTP PATCH request.
Replace `<your-node-name>` with the name of your Node:
-->
<p>在另一个命令窗口中，发送 HTTP PATCH 请求。用你的节点名称替换 <code>&lt;your-node-name&gt;</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl --header <span style="color:#b44">&#34;Content-Type: application/json-patch+json&#34;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--request PATCH <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--data <span style="color:#b44">&#39;[{&#34;op&#34;: &#34;remove&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1dongle&#34;}]&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status
</code></pre></div><!--
Verify that the dongle advertisement has been removed:
-->
<p>验证 dongle 资源的发布已经被移除：</p>
<pre><code>kubectl describe node &lt;your-node-name&gt; | grep dongle
</code></pre><!--
(you should not see any output)  
-->
<p>(你应该看不到任何输出)</p>
<h2 id="what-s-next">What's next</h2>
<!--
### For application developers

* [Assign Extended Resources to a Container](/docs/tasks/configure-pod-container/extended-resource/)

### For cluster administrators

* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)
-->
<h3 id="针对应用开发人员">针对应用开发人员</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/extended-resource/">将扩展资源分配给容器</a></li>
</ul>
<h3 id="针对集群管理员">针对集群管理员</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为名字空间配置最小和最大内存约束</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为名字空间配置最小和最大 CPU 约束</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f6f3b8f9789fda4286bf410b8e108f69">11 - 以非root用户身份运行 Kubernetes 节点组件</h1>
    
	<!--
---
title: Running Kubernetes Node Components as a Non-root User
content_type: task
min-kubernetes-server-version: 1.22
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>


<!--
This document describes how to run Kubernetes Node components such as kubelet, CRI, OCI, and CNI
without root privileges, by using a <a class='glossary-tooltip' title='一种为非特权用户模拟超级用户特权的 Linux 内核功能特性。' data-toggle='tooltip' data-placement='top' href='https://man7.org/linux/man-pages/man7/user_namespaces.7.html' target='_blank' aria-label='user namespace'>user namespace</a>.

This technique is also known as _rootless mode_.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>This document describes how to run Kubernetes Node components (and hence pods) a non-root user.</p>
<p>If you are just looking for how to run a pod as a non-root user, see <a href="/docs/tasks/configure-pod-container/security-context/">SecurityContext</a>.</p>

</div>
-->
<p>这个文档描述了怎样不使用 root 特权，而是通过使用 <a class='glossary-tooltip' title='一种为非特权用户模拟超级用户特权的 Linux 内核功能特性。' data-toggle='tooltip' data-placement='top' href='https://man7.org/linux/man-pages/man7/user_namespaces.7.html' target='_blank' aria-label='用户命名空间'>用户命名空间</a>
去运行 Kubernetes 节点组件（例如 kubelet、CRI、OCI、CNI）。</p>
<p>这种技术也叫做 <em>rootless 模式（Rootless mode）</em>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 这个文档描述了怎么以非 root 用户身份运行 Kubernetes 节点组件以及 Pod。
如果你只是想了解如何以非 root 身份运行 Pod，请参阅 <a href="/zh/docs/tasks/configure-pod-container/security-context/">SecurityContext</a>。
</div>
<!--
## Before you begin



Your Kubernetes server must be at or later than version 1.22.
 To check the version, enter <code>kubectl version</code>.


* [Enable Cgroup v2](https://rootlesscontaine.rs/getting-started/common/cgroup2/)
* [Enable systemd with user session](https://rootlesscontaine.rs/getting-started/common/login/)
* [Configure several sysctl values, depending on host Linux distribution](https://rootlesscontaine.rs/getting-started/common/sysctl/)
* [Ensure that your unprivileged user is listed in `/etc/subuid` and `/etc/subgid`](https://rootlesscontaine.rs/getting-started/common/subuid/)
* Enable the `KubeletInUserNamespace` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
-->
<h2 id="before-you-begin">Before you begin</h2>
<p>Your Kubernetes server must be at or later than version 1.22.
To check the version, enter <code>kubectl version</code>.</p>
<ul>
<li><a href="https://rootlesscontaine.rs/getting-started/common/cgroup2/">启用 Cgroup v2</a></li>
<li><a href="https://rootlesscontaine.rs/getting-started/common/login/">在 systemd 中启用 user session</a></li>
<li><a href="https://rootlesscontaine.rs/getting-started/common/sysctl/">根据不同的 Linux 发行版，配置 sysctl 的值</a></li>
<li><a href="https://rootlesscontaine.rs/getting-started/common/subuid/">确保你的非特权用户被列在 <code>/etc/subuid</code> 和 <code>/etc/subgid</code> 文件中</a></li>
<li>启用 <code>KubeletInUserNamespace</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a></li>
</ul>
<!-- steps -->
<!--
## Running Kubernetes inside Rootless Docker/Podman

### kind

[kind](https://kind.sigs.k8s.io/) supports running Kubernetes inside Rootless Docker or Rootless Podman.

See [Running kind with Rootless Docker](https://kind.sigs.k8s.io/docs/user/rootless/).

### minikube

[minikube](https://minikube.sigs.k8s.io/) also supports running Kubernetes inside Rootless Docker.

See the page about the [docker](https://minikube.sigs.k8s.io/docs/drivers/docker/) driver in the Minikube documentation.

Rootless Podman is not supported.
-->
<h2 id="使用-rootless-模式的-docker-podman-运行-kubernetes">使用 Rootless 模式的 Docker/Podman 运行 Kubernetes</h2>
<h3 id="kind">kind</h3>
<p><a href="https://kind.sigs.k8s.io/">kind</a> 支持使用 Rootless 模式的 Docker 或者 Podman 运行 Kubernetes。</p>
<p>请参阅<a href="https://kind.sigs.k8s.io/docs/user/rootless/">使用 Rootless 模式的 Docker 运行 kind</a>。</p>
<h3 id="minikube">minikube</h3>
<p><a href="https://minikube.sigs.k8s.io/">minikube</a> 也支持使用 Rootless 模式的 Docker 运行 Kubernetes。</p>
<p>请参阅 Minikube 文档中的 <a href="https://minikube.sigs.k8s.io/docs/drivers/docker/">docker</a> 驱动页面。</p>
<p>它不支持 Rootless 模式的 Podman。</p>
<!-- Supporting rootless podman is discussed in https://github.com/kubernetes/minikube/issues/8719 -->
<!--
## Running Kubernetes inside Unprivileged Containers

<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>


### sysbox

-->
<h2 id="在非特权容器内运行-kubernetes">在非特权容器内运行 Kubernetes</h2>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<h3 id="sysbox">sysbox</h3>
<!--
[Sysbox](https://github.com/nestybox/sysbox) is an open-source container runtime
(similar to "runc") that supports running system-level workloads such as Docker
and Kubernetes inside unprivileged containers isolated with the Linux user
namespace.
-->
<p><a href="https://github.com/nestybox/sysbox">Sysbox</a> 是一个开源容器运行时
(类似于 “runc”），支持在 Linux 用户命名空间隔离的非特权容器内运行系统级工作负载，
比如 Docker 和 Kubernetes。</p>
<!--
See [Sysbox Quick Start Guide: Kubernetes-in-Docker](https://github.com/nestybox/sysbox/blob/master/docs/quickstart/kind.md) for more info.
-->
<p>查看 <a href="https://github.com/nestybox/sysbox/blob/master/docs/quickstart/kind.md">Sysbox 快速入门指南: Kubernetes-in-Docker</a>
了解更多细节。</p>
<!--
Sysbox supports running Kubernetes inside unprivileged containers without
requiring Cgroup v2 and without the `KubeletInUserNamespace` feature gate. It
does this by exposing specially crafted `/proc` and `/sys` filesystems inside
the container plus several other advanced OS virtualization techniques.
-->
<p>Sysbox 支持在非特权容器内运行 Kubernetes，
而不需要 Cgroup v2 和 “KubeletInUserNamespace” 特性门控。
Sysbox 通过在容器内暴露特定的 <code>/proc</code> 和 <code>/sys</code> 文件系统，
以及其它一些先进的操作系统虚拟化技术来实现。</p>
<!--
## Running Rootless Kubernetes directly on a host

<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>


### K3s

[K3s](https://k3s.io/) experimentally supports rootless mode.

See [Running K3s with Rootless mode](https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental) for the usage.

### Usernetes
[Usernetes](https://github.com/rootless-containers/usernetes) is a reference distribution of Kubernetes that can be installed under `$HOME` directory without the root privilege.

Usernetes supports both containerd and CRI-O as CRI runtimes.
Usernetes supports multi-node clusters using Flannel (VXLAN).

See [the Usernetes repo](https://github.com/rootless-containers/usernetes) for the usage.
-->
<h2 id="直接在主机上运行-rootless-模式的-kubernetes">直接在主机上运行 Rootless 模式的 Kubernetes</h2>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<h3 id="k3s">K3s</h3>
<p><a href="https://k3s.io/">K3s</a> 实验性支持了 Rootless 模式。</p>
<p>请参阅<a href="https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental">使用 Rootless 模式运行 K3s</a>
页面中的用法.</p>
<h3 id="usernetes">Usernetes</h3>
<p><a href="https://github.com/rootless-containers/usernetes">Usernetes</a> 是 Kubernetes 的一个参考发行版，
它可以在不使用 root 特权的情况下安装在 <code>$HOME</code> 目录下。</p>
<p>Usernetes 支持使用 containerd 和 CRI-O 作为 CRI 运行时。
Usernetes 支持配置了 Flannel (VXLAN)的多节点集群。</p>
<p>关于用法，请参阅 <a href="https://github.com/rootless-containers/usernetes">Usernetes 仓库</a>。</p>
<!--
## Manually deploy a node that runs the kubelet in a user namespace {#userns-the-hard-way}

This section provides hints for running Kubernetes in a user namespace manually.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> This section is intended to be read by developers of Kubernetes distributions, not by end users.
</div>
-->
<h2 id="userns-the-hard-way">手动部署一个在用户命名空间运行 kubelet 的节点</h2>
<p>本节提供在用户命名空间手动运行 Kubernetes 的注意事项。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 本节是面向 Kubernetes 发行版的开发者，而不是最终用户。
</div>
<!--
### Creating a user namespace

The first step is to create a <a class='glossary-tooltip' title='一种为非特权用户模拟超级用户特权的 Linux 内核功能特性。' data-toggle='tooltip' data-placement='top' href='https://man7.org/linux/man-pages/man7/user_namespaces.7.html' target='_blank' aria-label='user namespace'>user namespace</a>.

If you are trying to run Kubernetes in a user-namespaced container such as
Rootless Docker/Podman or LXC/LXD, you are all set, and you can go to the next subsection.

Otherwise you have to create a user namespace by yourself, by calling `unshare(2)` with `CLONE_NEWUSER`.

A user namespace can be also unshared by using command line tools such as:

- [`unshare(1)`](https://man7.org/linux/man-pages/man1/unshare.1.html)
- [RootlessKit](https://github.com/rootless-containers/rootlesskit)
- [become-root](https://github.com/giuseppe/become-root)

After unsharing the user namespace, you will also have to unshare other namespaces such as mount namespace.

You do *not* need to call `chroot()` nor `pivot_root()` after unsharing the mount namespace,
however, you have to mount writable filesystems on several directories *in* the namespace.

At least, the following directories need to be writable *in* the namespace (not *outside* the namespace):

- `/etc`
- `/run`
- `/var/logs`
- `/var/lib/kubelet`
- `/var/lib/cni`
- `/var/lib/containerd` (for containerd)
- `/var/lib/containers` (for CRI-O)
-->
<h3 id="创建用户命名空间">创建用户命名空间</h3>
<p>第一步是创建一个 <a class='glossary-tooltip' title='一种为非特权用户模拟超级用户特权的 Linux 内核功能特性。' data-toggle='tooltip' data-placement='top' href='https://man7.org/linux/man-pages/man7/user_namespaces.7.html' target='_blank' aria-label='用户命名空间'>用户命名空间</a>。</p>
<p>如果你正在尝试使用用户命名空间的容器（例如 Rootless 模式的 Docker/Podman 或 LXC/LXD）
运行 Kubernetes，那么你已经准备就绪，可以直接跳到下一小节。</p>
<p>否则你需要通过传递参数 <code>CLONE_NEWUSER</code> 调用 <code>unshare(2)</code>，自己创建一个命名空间。</p>
<p>用户命名空间也可以通过如下所示的命令行工具取消共享：</p>
<ul>
<li><a href="https://man7.org/linux/man-pages/man1/unshare.1.html"><code>unshare(1)</code></a></li>
<li><a href="https://github.com/rootless-containers/rootlesskit">RootlessKit</a></li>
<li><a href="https://github.com/giuseppe/become-root">become-root</a></li>
</ul>
<p>在取消命名空间的共享之后，你也必须对其它的命名空间例如 mount 命名空间取消共享。</p>
<p>在取消 mount 命名空间的共享之后，你<em>不</em>需要调用 <code>chroot()</code> 或者 <code>pivot_root()</code>，
但是你必须<em>在这个命名空间内</em>挂载可写的文件系统到几个目录上。</p>
<p>请确保<em>这个命名空间内</em>(不是这个命名空间外部)至少以下几个目录是可写的：</p>
<ul>
<li><code>/etc</code></li>
<li><code>/run</code></li>
<li><code>/var/logs</code></li>
<li><code>/var/lib/kubelet</code></li>
<li><code>/var/lib/cni</code></li>
<li><code>/var/lib/containerd</code> (参照 containerd )</li>
<li><code>/var/lib/containers</code> (参照 CRI-O )</li>
</ul>
<!--
### Creating a delegated cgroup tree

In addition to the user namespace, you also need to have a writable cgroup tree with cgroup v2.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Kubernetes support for running Node components in user namespaces requires cgroup v2.
Cgroup v1 is not supported.
</div>

If you are trying to run Kubernetes in Rootless Docker/Podman or LXC/LXD on a systemd-based host, you are all set.

Otherwise you have to create a systemd unit with `Delegate=yes` property to delegate a cgroup tree with writable permission.

On your node, systemd must already be configured to allow delegation; for more details, see
[cgroup v2](https://rootlesscontaine.rs/getting-started/common/cgroup2/) in the Rootless
Containers documentation.
-->
<h3 id="创建委派-cgroup-树">创建委派 cgroup 树</h3>
<p>除了用户命名空间，你也需要有一个版本为 cgroup v2 的可写 cgroup 树。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Kubernetes 需要 cgroup v2 才支持在用户命名空间运行节点组件。
cgroup v1 是不支持的。
</div>
<p>如果你在一个采用 systemd 机制的主机上使用用户命名空间的容器（例如 Rootless 模式的 Docker/Podman
或 LXC/LXD）来运行 Kubernetes，那么你已经准备就绪。</p>
<p>否则你必须创建一个具有 <code>Delegate=yes</code> 属性的 systemd 单元，来委派一个具有可写权限的 cgroup 树。</p>
<p>在你的节点上，systemd 必须已经配置为允许委派。更多细节请参阅 Rootless 容器文档的
<a href="https://rootlesscontaine.rs/getting-started/common/cgroup2/">cgroup v2</a> 部分。</p>
<!--
### Configuring network

<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>


The network namespace of the Node components has to have a non-loopback interface, which can be for example configured with
[slirp4netns](https://github.com/rootless-containers/slirp4netns),
[VPNKit](https://github.com/moby/vpnkit), or
[lxc-user-nic(1)](https://www.man7.org/linux/man-pages/man1/lxc-user-nic.1.html).

The network namespaces of the Pods can be configured with regular CNI plugins.
For multi-node networking, Flannel (VXLAN, 8472/UDP) is known to work.

Ports such as the kubelet port (10250/TCP) and `NodePort` service ports have to be exposed from the Node network namespace to
the host with an external port forwarder, such as RootlessKit, slirp4netns, or
[socat(1)](https://linux.die.net/man/1/socat).

You can use the port forwarder from K3s.
See [Running K3s in Rootless Mode](https://rancher.com/docs/k3s/latest/en/advanced/#known-issues-with-rootless-mode)
for more details.

### Configuring CRI

The kubelet relies on a container runtime. You should deploy a container runtime such as
containerd or CRI-O and ensure that it is running within the user namespace before the kubelet starts.
-->
<h3 id="配置网络">配置网络</h3>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<p>节点组件的网络命名空间必须有一个非本地回路的网卡。它可以使用
<a href="https://github.com/rootless-containers/slirp4netns">slirp4netns</a>、
<a href="https://github.com/moby/vpnkit">VPNKit</a>、
<a href="https://www.man7.org/linux/man-pages/man1/lxc-user-nic.1.html">lxc-user-nic(1)</a>
等工具进行配置。</p>
<p>Pod 的网络命名空间可以使用常规的 CNI 插件配置。对于多节点的网络，已知 Flannel (VXLAN、8472/UDP) 可以正常工作。</p>
<p>诸如 kubelet 端口（10250/TCP）和 <code>NodePort</code> 服务端口之类的端口必须通过外部端口转发器
（例如 RootlessKit、 slirp4netns 或
<a href="https://linux.die.net/man/1/socat">socat(1)</a>) 从节点网络命名空间暴露给主机。</p>
<p>你可以使用 K3s 的端口转发器。更多细节请参阅
<a href="https://rancher.com/docs/k3s/latest/en/advanced/#known-issues-with-rootless-mode">在 Rootless 模式下运行 K3s</a>。</p>
<h3 id="配置-cri">配置 CRI</h3>
<p>kubelet 依赖于容器运行时。你需要部署一个容器运行时（例如 containerd 或 CRI-O），
并确保它在 kubelet 启动之前已经在用户命名空间内运行。</p>
<!--
<ul class="nav nav-tabs" id="cri" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#cri-0" role="tab" aria-controls="cri-0" aria-selected="true">containerd</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#cri-1" role="tab" aria-controls="cri-1">CRI-O</a></li></ul>
<div class="tab-content" id="cri"><div id="cri-0" class="tab-pane show active" role="tabpanel" aria-labelledby="cri-0">

<p><p>Running CRI plugin of containerd in a user namespace is supported since containerd 1.4.</p>
<p>Running containerd within a user namespace requires the following configurations
in <code>/etc/containerd/containerd-config.toml</code>.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">version = <span style="color:#666">2</span>

[plugins.<span style="color:#b44">&#34;io.containerd.grpc.v1.cri&#34;</span>]
<span style="color:#080;font-style:italic"># Disable AppArmor</span>
  disable_apparmor = <span style="color:#a2f;font-weight:bold">true</span>
<span style="color:#080;font-style:italic"># Ignore an error during setting oom_score_adj</span>
  restrict_oom_score_adj = <span style="color:#a2f;font-weight:bold">true</span>
<span style="color:#080;font-style:italic"># Disable hugetlb cgroup v2 controller (because systemd does not support delegating hugetlb controller)</span>
  disable_hugetlb_controller = <span style="color:#a2f;font-weight:bold">true</span>

[plugins.<span style="color:#b44">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd]
<span style="color:#080;font-style:italic"># Using non-fuse overlayfs is also possible for kernel &gt;= 5.11, but requires SELinux to be disabled</span>
  snapshotter = <span style="color:#b44">&#34;fuse-overlayfs&#34;</span>

[plugins.<span style="color:#b44">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.runc.options]
<span style="color:#080;font-style:italic"># We use cgroupfs that is delegated by systemd, so we do not use SystemdCgroup driver</span>
<span style="color:#080;font-style:italic"># (unless you run another systemd in the namespace)</span>
  SystemdCgroup = <span style="color:#a2f;font-weight:bold">false</span>
</code></pre></div></div>
  <div id="cri-1" class="tab-pane" role="tabpanel" aria-labelledby="cri-1">

<p><p>Running CRI-O in a user namespace is supported since CRI-O 1.22.</p>
<p>CRI-O requires an environment variable <code>_CRIO_ROOTLESS=1</code> to be set.</p>
<p>The following configurations (in <code>/etc/crio/crio.conf</code>) are also recommended:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">[crio]
  storage_driver = <span style="color:#b44">&#34;overlay&#34;</span>
<span style="color:#080;font-style:italic"># Using non-fuse overlayfs is also possible for kernel &gt;= 5.11, but requires SELinux to be disabled</span>
  storage_option = [<span style="color:#b44">&#34;overlay.mount_program=/usr/local/bin/fuse-overlayfs&#34;</span>]

[crio.runtime]
<span style="color:#080;font-style:italic"># We use cgroupfs that is delegated by systemd, so we do not use &#34;systemd&#34; driver</span>
<span style="color:#080;font-style:italic"># (unless you run another systemd in the namespace)</span>
  cgroup_manager = <span style="color:#b44">&#34;cgroupfs&#34;</span>
</code></pre></div></div></div>

-->
<ul class="nav nav-tabs" id="cri" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#cri-0" role="tab" aria-controls="cri-0" aria-selected="true">containerd</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#cri-1" role="tab" aria-controls="cri-1">CRI-O</a></li></ul>
<div class="tab-content" id="cri"><div id="cri-0" class="tab-pane show active" role="tabpanel" aria-labelledby="cri-0">

<p><p>containerd 1.4 开始支持在用户命名空间运行 containerd 的 CRI 插件。</p>
<p>在用户命名空间运行 containerd 需要在 <code>/etc/containerd/containerd-config.toml</code> 文件包含以下配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">version = <span style="color:#666">2</span>

[plugins.<span style="color:#b44">&#34;io.containerd.grpc.v1.cri&#34;</span>]
<span style="color:#080;font-style:italic"># 禁用 AppArmor</span>
  disable_apparmor = <span style="color:#a2f;font-weight:bold">true</span>
<span style="color:#080;font-style:italic"># 忽略配置 oom_score_adj 时的错误</span>
  restrict_oom_score_adj = <span style="color:#a2f;font-weight:bold">true</span>
<span style="color:#080;font-style:italic"># 禁用 hugetlb cgroup v2 控制器（因为 systemd 不支持委派 hugetlb controller）</span>
  disable_hugetlb_controller = <span style="color:#a2f;font-weight:bold">true</span>

[plugins.<span style="color:#b44">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd]
<span style="color:#080;font-style:italic"># 如果内核 &gt;= 5.11 , 也可以使用 non-fuse overlayfs， 但需要禁用 SELinux</span>
  snapshotter = <span style="color:#b44">&#34;fuse-overlayfs&#34;</span>

[plugins.<span style="color:#b44">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.runc.options]
<span style="color:#080;font-style:italic"># 我们使用的 cgroupfs 已经被 systemd 委派，所以我们不使用 SystemdCgroup 驱动</span>
<span style="color:#080;font-style:italic"># (除非你在命名空间内运行了另一个 systemd)</span>
  SystemdCgroup = <span style="color:#a2f;font-weight:bold">false</span>
</code></pre></div></div>
  <div id="cri-1" class="tab-pane" role="tabpanel" aria-labelledby="cri-1">

<p><p>CRI-O 1.22 开始支持在用户命名空间运行 CRI-O。</p>
<p>CRI-O 必须配置一个环境变量 <code>_CRIO_ROOTLESS=1</code>。</p>
<p>也推荐使用 <code>/etc/crio/crio.conf</code> 文件内的以下配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">[crio]
  storage_driver = <span style="color:#b44">&#34;overlay&#34;</span>
<span style="color:#080;font-style:italic"># 如果内核 &gt;= 5.11 , 也可以使用 non-fuse overlayfs， 但需要禁用 SELinux</span>
  storage_option = [<span style="color:#b44">&#34;overlay.mount_program=/usr/local/bin/fuse-overlayfs&#34;</span>]

[crio.runtime]
<span style="color:#080;font-style:italic"># 我们使用的 cgroupfs 已经被 systemd 委派，所以我们不使用 &#34;systemd&#34; 驱动</span>
<span style="color:#080;font-style:italic"># (除非你在命名空间内运行了另一个 systemd)</span>
  cgroup_manager = <span style="color:#b44">&#34;cgroupfs&#34;</span>
</code></pre></div></div></div>

<!--
### Configuring kubelet

Running kubelet in a user namespace requires the following configuration:

```yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
featureGates:
  KubeletInUserNamespace: true
# We use cgroupfs that is delegated by systemd, so we do not use "systemd" driver
# (unless you run another systemd in the namespace)
cgroupDriver: "cgroupfs"
```

When the `KubeletInUserNamespace` feature gate is enabled, the kubelet ignores errors
that may happen during setting the following sysctl values on the node.

- `vm.overcommit_memory`
- `vm.panic_on_oom`
- `kernel.panic`
- `kernel.panic_on_oops`
- `kernel.keys.root_maxkeys`
- `kernel.keys.root_maxbytes`.

Within a user namespace, the kubelet also ignores any error raised from trying to open `/dev/kmsg`.
This feature gate also allows kube-proxy to ignore an error during setting `RLIMIT_NOFILE`.

The `KubeletInUserNamespace` feature gate was introduced in Kubernetes v1.22 with "alpha" status.

Running kubelet in a user namespace without using this feature gate is also possible
by mounting a specially crafted proc filesystem (as done by [Sysbox](https://github.com/nestybox/sysbox)), but not officially supported.
-->
<h3 id="配置-kubelet">配置 kubelet</h3>
<p>在用户命名空间运行 kubelet 必须进行如下配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">featureGates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">KubeletInUserNamespace</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 我们使用的 cgroupfs 已经被 systemd 委派，所以我们不使用 &#34;systemd&#34; 驱动</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># (除非你在命名空间内运行了另一个 systemd)</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">cgroupDriver</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;cgroupfs&#34;</span><span style="color:#bbb">
</span></code></pre></div><p>当 <code>KubeletInUserNamespace</code> 特性门控被启用时， kubelet 会忽略节点内由于配置如下几个 sysctl
参数值而可能产生的错误。</p>
<ul>
<li><code>vm.overcommit_memory</code></li>
<li><code>vm.panic_on_oom</code></li>
<li><code>kernel.panic</code></li>
<li><code>kernel.panic_on_oops</code></li>
<li><code>kernel.keys.root_maxkeys</code></li>
<li><code>kernel.keys.root_maxbytes</code>.</li>
</ul>
<p>在用户命名空间内， kubelet 也会忽略任何由于打开 <code>/dev/kmsg</code> 而产生的错误。
这个特性门控也允许 kube-proxy 忽略由于配置 <code>RLIMIT_NOFILE</code> 而产生的一个错误。</p>
<p><code>KubeletInUserNamespace</code> 特性门控从 Kubernetes v1.22 被引入， 标记为 &quot;alpha&quot; 状态。</p>
<p>通过挂载特制的 proc 文件系统 （比如 <a href="https://github.com/nestybox/sysbox">Sysbox</a>），
也可以在不使用这个特性门控的情况下在用户命名空间运行 kubelet，但这不受官方支持。</p>
<!--
### Configuring kube-proxy

Running kube-proxy in a user namespace requires the following configuration:

```yaml
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "iptables" # or "userspace"
conntrack:
# Skip setting sysctl value "net.netfilter.nf_conntrack_max"
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s
```
-->
<h3 id="配置-kube-proxy">配置 kube-proxy</h3>
<p>在用户命名空间运行 kube-proxy 需要进行以下配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeproxy.config.k8s.io/v1alpha1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeProxyConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">mode</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;iptables&#34;</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># or &#34;userspace&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">conntrack</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 跳过配置 sysctl 的值 &#34;net.netfilter.nf_conntrack_max&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">maxPerCore</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 跳过配置 &#34;net.netfilter.nf_conntrack_tcp_timeout_established&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tcpEstablishedTimeout</span>:<span style="color:#bbb"> </span>0s<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 跳过配置 &#34;net.netfilter.nf_conntrack_tcp_timeout_close&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tcpCloseWaitTimeout</span>:<span style="color:#bbb"> </span>0s<span style="color:#bbb">
</span></code></pre></div><!--
## Caveats

- Most of "non-local" volume drivers such as `nfs` and `iscsi` do not work.
  Local volumes like `local`, `hostPath`, `emptyDir`, `configMap`, `secret`, and `downwardAPI` are known to work.

- Some CNI plugins may not work. Flannel (VXLAN) is known to work.

For more on this, see the [Caveats and Future work](https://rootlesscontaine.rs/caveats/) page
on the rootlesscontaine.rs website.
-->
<h2 id="caveats">注意事项  </h2>
<ul>
<li>
<p>大部分“非本地”的卷驱动（例如 <code>nfs</code> 和 <code>iscsi</code>）不能正常工作。
已知诸如 <code>local</code>、<code>hostPath</code>、<code>emptyDir</code>、<code>configMap</code>、<code>secret</code> 和 <code>downwardAPI</code>
这些本地卷是能正常工作的。</p>
</li>
<li>
<p>一些 CNI 插件可能不正常工作。已知 Flannel (VXLAN) 是能正常工作的。</p>
</li>
</ul>
<p>更多细节请参阅 rootlesscontaine.rs 站点的 <a href="https://rootlesscontaine.rs/caveats/">Caveats and Future work</a> 页面。</p>
<!--
## See Also

- [rootlesscontaine.rs](https://rootlesscontaine.rs/)
- [Rootless Containers 2020 (KubeCon NA 2020)](https://www.slideshare.net/AkihiroSuda/kubecon-na-2020-containerd-rootless-containers-2020)
- [Running kind with Rootless Docker](https://kind.sigs.k8s.io/docs/user/rootless/)
- [Usernetes](https://github.com/rootless-containers/usernetes)
- [Running K3s with rootless mode](https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental)
- [KEP-2033: Kubelet-in-UserNS (aka Rootless mode)](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2033-kubelet-in-userns-aka-rootless)
-->
<h2 id="see-also">See Also</h2>
<ul>
<li><a href="https://rootlesscontaine.rs/">rootlesscontaine.rs</a></li>
<li><a href="https://www.slideshare.net/AkihiroSuda/kubecon-na-2020-containerd-rootless-containers-2020">Rootless Containers 2020 (KubeCon NA 2020)</a></li>
<li><a href="https://kind.sigs.k8s.io/docs/user/rootless/">使用 Rootless 模式的 Docker 运行 kind</a></li>
<li><a href="https://github.com/rootless-containers/usernetes">Usernetes</a></li>
<li><a href="https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental">使用 Rootless 模式运行 K3s</a></li>
<li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2033-kubelet-in-userns-aka-rootless">KEP-2033: Kubelet-in-UserNS (aka Rootless mode)</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e1afcdac8d5e8458274b3c481c5ebcda">12 - 使用 CoreDNS 进行服务发现</h1>
    
	<!--
reviewers:
- johnbelamaric
title: Using CoreDNS for Service Discovery
min-kubernetes-server-version: v1.9
content_type: task
-->
<!-- overview -->
<!--
This page describes the CoreDNS upgrade process and how to install CoreDNS instead of kube-dns.
-->
<p>此页面介绍了 CoreDNS 升级过程以及如何安装 CoreDNS 而不是 kube-dns。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

Your Kubernetes server must be at or later than version v1.9.
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## About CoreDNS

[CoreDNS](https://coredns.io) is a flexible, extensible DNS server
that can serve as the Kubernetes cluster DNS.
Like Kubernetes, the CoreDNS project is hosted by the
<a class='glossary-tooltip' title='云原生计算基金会' data-toggle='tooltip' data-placement='top' href='https://cncf.io/' target='_blank' aria-label='CNCF'>CNCF</a>.
-->
<h2 id="关于-coredns">关于 CoreDNS</h2>
<p><a href="https://coredns.io">CoreDNS</a> 是一个灵活可扩展的 DNS 服务器，可以作为 Kubernetes 集群 DNS。
与 Kubernetes 一样，CoreDNS 项目由 <a class='glossary-tooltip' title='云原生计算基金会' data-toggle='tooltip' data-placement='top' href='https://cncf.io/' target='_blank' aria-label='CNCF'>CNCF</a> 托管。</p>
<!--
You can use CoreDNS instead of kube-dns in your cluster by replacing
kube-dns in an existing deployment, or by using tools like kubeadm
that will deploy and upgrade the cluster for you.
-->
<p>通过替换现有集群部署中的 kube-dns，或者使用 kubeadm 等工具来为你部署和升级集群，
可以在你的集群中使用 CoreDNS 而非 kube-dns，</p>
<!--
## Installing CoreDNS

For manual deployment or replacement of kube-dns, see the documentation at the
[CoreDNS GitHub project.](https://github.com/coredns/deployment/tree/master/kubernetes)
-->
<h2 id="安装-coredns">安装 CoreDNS</h2>
<p>有关手动部署或替换 kube-dns，请参阅
<a href="https://github.com/coredns/deployment/tree/master/kubernetes">CoreDNS GitHub 项目</a>。</p>
<!--
## Migrating to CoreDNS

### Upgrading an existing cluster with kubeadm
-->
<h2 id="迁移到-coredns">迁移到 CoreDNS</h2>
<h3 id="使用-kubeadm-升级现有集群">使用 kubeadm 升级现有集群</h3>
<!--
In Kubernetes version 1.21, kubeadm removed its support for `kube-dns` as a DNS application.
For `kubeadm` v1.23, the only supported cluster DNS application
is CoreDNS.
-->
<p>在 Kubernetes 1.21 版本中，kubeadm 移除了对将 <code>kube-dns</code> 作为 DNS 应用的支持。
对于 <code>kubeadm</code> v1.23，所支持的唯一的集群 DNS 应用是 CoreDNS。</p>
<!--
You can move to CoreDNS when you use `kubeadm` to upgrade a cluster that is
using `kube-dns`. In this case, `kubeadm` generates the CoreDNS configuration
("Corefile") based upon the `kube-dns` ConfigMap, preserving configurations for
stub domains, and upstream name server.
-->
<p>当你使用 <code>kubeadm</code> 升级使用 <code>kube-dns</code> 的集群时，你还可以执行到 CoreDNS 的迁移。
在这种场景中，<code>kubeadm</code> 将基于 <code>kube-dns</code> ConfigMap 生成 CoreDNS 配置（&quot;Corefile&quot;），
保存存根域和上游名称服务器的配置。</p>
<!--
## Upgrading CoreDNS 

You can check the version of CoreDNS that kubeadm installs for each version of
Kubernetes in the page
[CoreDNS version in Kubernetes](https://github.com/coredns/deployment/blob/master/kubernetes/CoreDNS-k8s_version.md).
-->
<h2 id="升级-coredns">升级 CoreDNS</h2>
<p>你可以在 <a href="https://github.com/coredns/deployment/blob/master/kubernetes/CoreDNS-k8s_version.md">CoreDNS version in Kubernetes</a>
页面查看 kubeadm 为不同版本 Kubernetes 所安装的 CoreDNS 版本。</p>
<!--
CoreDNS can be upgraded manually in case you want to only upgrade CoreDNS
or use your own custom image.
There is a helpful [guideline and walkthrough](https://github.com/coredns/deployment/blob/master/kubernetes/Upgrading_CoreDNS.md)
available to ensure a smooth upgrade.
Make sure the existing CoreDNS configuration ("Corefile") is retained when
upgrading your cluster.
-->
<p>如果你只想升级 CoreDNS 或使用自己的定制镜像，也可以手动升级 CoreDNS。
参看<a href="https://github.com/coredns/deployment/blob/master/kubernetes/Upgrading_CoreDNS.md">指南和演练</a>
文档了解如何平滑升级。
在升级你的集群过程中，请确保现有 CoreDNS 的配置（&quot;Corefile&quot;）被保留下来。</p>
<!--
If you are upgrading your cluster using the `kubeadm` tool, `kubeadm`
can take care of retaining the existing CoreDNS configuration automatically.
-->
<p>如果使用 <code>kubeadm</code> 工具来升级集群，则 <code>kubeadm</code> 可以自动处理保留现有 CoreDNS
配置这一事项。</p>
<!--
## Tuning CoreDNS

When resource utilisation is a concern, it may be useful to tune
the configuration of CoreDNS. For more details, check out the
[documentation on scaling CoreDNS]((https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md)).
-->
<h2 id="coredns-调优">CoreDNS 调优</h2>
<p>当资源利用方面有问题时，优化 CoreDNS 的配置可能是有用的。
有关详细信息，请参阅有关<a href="https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md">扩缩 CoreDNS 的文档</a>。</p>
<h2 id="what-s-next">What's next</h2>
<!--
You can configure [CoreDNS](https://coredns.io) to support many more use cases than
kube-dns does by modifying the CoreDNS configuration ("Corefile").
For more information, see the [documentation](https://coredns.io/plugins/kubernetes/)
for the `kubernetes` CoreDNS plugin, or read the 
[Custom DNS Entries for Kubernetes](https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/)
in the CoreDNS blog.
-->
<p>你可以通过修改 CoreDNS 的配置（&quot;Corefile&quot;）来配置 <a href="https://coredns.io">CoreDNS</a>，
以支持比 kube-dns 更多的用例。
请参考 <code>kubernetes</code> CoreDNS 插件的<a href="https://coredns.io/plugins/kubernetes/">文档</a>
或者 CoreDNS 博客上的博文
<a href="https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/">Custom DNS Entries for Kubernetes</a>，
以了解更多信息。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-669c88964b4a9eb2b040057266e4b60d">13 - 使用 KMS 驱动进行数据加密</h1>
    
	<!--
reviewers:
- smarterclayton
title: Using a KMS provider for data encryption
content_type: task
-->
<!-- overview -->
<!-- This page shows how to configure a Key Management Service (KMS) provider and plugin to enable secret data encryption. -->
<p>本页展示了如何配置秘钥管理服务—— Key Management Service (KMS) 驱动和插件以启用
Secret 数据加密。</p>
<h2 id="before-you-begin">Before you begin</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</li>
</ul>
<!-- * Kubernetes version 1.10.0 or later is required -->
<!-- * etcd v3 or later is required -->
<ul>
<li>需要 Kubernetes 1.10.0 或更新版本</li>
<li>需要 etcd v3 或更新版本</li>
</ul>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.12 [beta]</code>
</div>


<!-- steps -->
<!--
The KMS encryption provider uses an envelope encryption scheme to encrypt data in etcd. The data is encrypted using a data encryption key (DEK); a new DEK is generated for each encryption. The DEKs are encrypted with a key encryption key (KEK) that is stored and managed in a remote KMS. The KMS provider uses gRPC to communicate with a specific KMS 
plugin. The KMS plugin, which is implemented as a gRPC server and deployed on the same host(s) as the Kubernetes master(s), is responsible for all communication with the remote KMS.
-->
<p>KMS 加密驱动使用封套加密模型来加密 etcd 中的数据。
数据使用数据加密秘钥（DEK）加密；每次加密都生成一个新的 DEK。
这些 DEK 经一个秘钥加密秘钥（KEK）加密后在一个远端的 KMS 中存储和管理。
KMS 驱动使用 gRPC 与一个特定的 KMS 插件通信。这个 KMS 插件作为一个 gRPC
服务器被部署在 Kubernetes 主服务器的同一个主机上，负责与远端 KMS 的通信。</p>
<!--
## Configuring the KMS provider

To configure a KMS provider on the API server, include a provider of type ```kms``` in the providers array in the encryption configuration file and set the following properties:
-->
<h2 id="配置-kms-驱动">配置 KMS 驱动</h2>
<p>为了在 API 服务器上配置 KMS 驱动，在加密配置文件中的驱动数组中加入一个类型为 <code>kms</code>
的驱动，并设置下列属性：</p>
<!--
* `name`: Display name of the KMS plugin.
* `endpoint`: Listen address of the gRPC server (KMS plugin). The endpoint is a UNIX domain socket.
* `cachesize`: Number of data encryption keys (DEKs) to be cached in the clear. When cached, DEKs can be used without another call to the KMS; whereas DEKs that are not cached require a call to the KMS to unwrap.
* `timeout`: How long should kube-apiserver wait for kms-plugin to respond before returning an error (default is 3 seconds).
-->
<ul>
<li><code>name</code>: KMS 插件的显示名称。</li>
<li><code>endpoint</code>: gRPC 服务器（KMS 插件）的监听地址。该端点是一个 UNIX 域套接字。</li>
<li><code>cachesize</code>: 以明文缓存的数据加密秘钥（DEKs）的数量。一旦被缓存，
就可以直接使用 DEKs 而无需另外调用 KMS；而未被缓存的 DEKs 需要调用一次 KMS 才能解包。</li>
<li><code>timeout</code>: 在返回一个错误之前，kube-apiserver 等待 kms-plugin 响应的时间（默认是 3 秒）。</li>
</ul>
<!-- See [Understanding the encryption at rest configuration.](/docs/tasks/administer-cluster/encrypt-data) -->
<p>参见<a href="/zh/docs/tasks/administer-cluster/encrypt-data">理解静态数据加密配置</a></p>
<!--
## Implementing a KMS plugin

To implement a KMS plugin, you can develop a new plugin gRPC server or enable a KMS plugin already provided by your cloud provider. You then integrate the plugin with the remote KMS and deploy it on the Kubernetes master.
-->
<h2 id="实现-kms-插件">实现 KMS 插件</h2>
<p>为实现一个 KMS 插件，你可以开发一个新的插件 gRPC 服务器或启用一个由你的云服务驱动提供的 KMS 插件。
你可以将这个插件与远程 KMS 集成，并把它部署到 Kubernetes 的主服务器上。</p>
<!--
### Enabling the KMS supported by your cloud provider 
Refer to your cloud provider for instructions on enabling the cloud provider-specific KMS plugin.
-->
<h3 id="启用由云服务驱动支持的-kms">启用由云服务驱动支持的 KMS</h3>
<p>有关启用云服务驱动特定的 KMS 插件的说明，请咨询你的云服务驱动商。</p>
<!--
### Developing a KMS plugin gRPC server

You can develop a KMS plugin gRPC server using a stub file available for Go. For other languages, you use a proto file to create a stub file that you can use to develop the gRPC server code.
-->
<h3 id="开发-kms-插件-grpc-服务器">开发 KMS 插件 gRPC 服务器</h3>
<p>你可以使用 Go 语言的存根文件开发 KMS 插件 gRPC 服务器。
对于其他语言，你可以用 proto 文件创建可以用于开发 gRPC 服务器代码的存根文件。</p>
<!--
* Using Go: Use the functions and data structures in the stub file:
[service.pb.go](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.pb.go) to develop the gRPC server code
-->
<ul>
<li>使用 Go：使用存根文件 <a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.pb.go">service.pb.go</a>
中的函数和数据结构开发 gRPC 服务器代码。</li>
</ul>
<!--
* Using languages other than Go: Use the protoc compiler with the proto file: [service.proto](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.proto) to generate a stub file for the specific language
-->
<ul>
<li>使用 Go 以外的其他语言：用 protoc 编译器编译 proto 文件：
<a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.proto">service.proto</a>
为指定语言生成存根文件。</li>
</ul>
<!--
Then use the functions and data structures in the stub file to develop the server code.
-->
<p>然后使用存根文件中的函数和数据结构开发服务器代码。</p>
<!-- **Notes:** -->
<p><strong>注意：</strong></p>
<!--
* kms plugin version: `v1beta1`

  In response to procedure call Version, a compatible KMS plugin should return v1beta1 as VersionResponse.version.

* message version: `v1beta1`

  All messages from KMS provider have the version field set to current version v1beta1.

* protocol: UNIX domain socket (`unix`)

  The gRPC server should listen at UNIX domain socket.
-->
<ul>
<li>
<p>kms 插件版本：<code>v1beta1</code></p>
<p>作为对过程调用 Version 的响应，兼容的 KMS 插件应把 v1beta1 作为 VersionResponse.version 返回</p>
</li>
<li>
<p>消息版本：<code>v1beta1</code></p>
<p>所有来自 KMS 驱动的消息都把 version 字段设置为当前版本 v1beta1</p>
</li>
<li>
<p>协议：UNIX 域套接字 (<code>unix</code>)</p>
<p>gRPC 服务器应监听 UNIX 域套接字</p>
</li>
</ul>
<!--
### Integrating a KMS plugin with the remote KMS

The KMS plugin can communicate with the remote KMS using any protocol supported by the KMS.
All configuration data, including authentication credentials the KMS plugin uses to communicate with the remote KMS, 
are stored and managed by the KMS plugin independently. The KMS plugin can encode the ciphertext with additional metadata that may be required before sending it to the KMS for decryption.
-->
<h3 id="将-kms-插件与远程-kms-整合">将 KMS 插件与远程 KMS 整合</h3>
<p>KMS 插件可以用任何受 KMS 支持的协议与远程 KMS 通信。
所有的配置数据，包括 KMS 插件用于与远程 KMS 通信的认证凭据，都由 KMS 插件独立地存储和管理。
KMS 插件可以用额外的元数据对密文进行编码，这些元数据是在把它发往 KMS 进行解密之前可能要用到的。</p>
<!--
### Deploying the KMS plugin 

Ensure that the KMS plugin runs on the same host(s) as the Kubernetes master(s).
-->
<h3 id="部署-kms-插件">部署 KMS 插件</h3>
<p>确保 KMS 插件与 Kubernetes 主服务器运行在同一主机上。</p>
<!--
## Encrypting your data with the KMS provider

To encrypt the data:
-->
<h2 id="使用-kms-驱动加密数据">使用 KMS 驱动加密数据</h2>
<p>为了加密数据：</p>
<!--
1. Create a new encryption configuration file using the appropriate properties for the `kms` provider:
-->
<ol>
<li>
<p>使用 <code>kms</code> 驱动的相应的属性创建一个新的加密配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myKmsPlugin<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile.sock<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cachesize</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">timeout</span>:<span style="color:#bbb"> </span>3s<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
2. Set the `--encryption-provider-config` flag on the kube-apiserver to point to the location of the configuration file.
3. Restart your API server.
-->
<ol start="2">
<li>设置 kube-apiserver 的 <code>--encryption-provider-config</code> 参数指向配置文件的位置。</li>
<li>重启 API 服务器。</li>
</ol>
<!--
## Verifying that the data is encrypted

Data is encrypted when written to etcd. After restarting your kube-apiserver, any newly created or updated secret should be encrypted when stored. To verify, you can use the etcdctl command line program to retrieve the contents of your secret.
-->
<h2 id="验证数据已经加密">验证数据已经加密</h2>
<p>写入 etcd 时数据被加密。重启 kube-apiserver 后，任何新建或更新的 Secret 在存储时应该已被加密。
要验证这点，你可以用 etcdctl 命令行程序获取 Secret 内容。</p>
<!--
1. Create a new secret called secret1 in the default namespace:
-->
<ol>
<li>
<p>在默认的命名空间里创建一个名为 secret1 的 Secret：</p>
<pre><code>kubectl create secret generic secret1 -n default --from-literal=mykey=mydata
</code></pre></li>
</ol>
<!--
2. Using the etcdctl command line, read that secret out of etcd:
-->
<ol start="2">
<li>
<p>用 etcdctl 命令行，从 etcd 读取出 Secret：</p>
<pre><code>ETCDCTL_API=3 etcdctl get /kubernetes.io/secrets/default/secret1 [...] | hexdump -C
</code></pre><!--
where `[...]` must be the additional arguments for connecting to the etcd server.
-->
<p>其中 <code>[...]</code> 是用于连接 etcd 服务器的额外参数。</p>
</li>
</ol>
<!--
3. Verify the stored secret is prefixed with `k8s:enc:kms:v1:`, which indicates that the `kms` provider has encrypted the resulting data.
-->
<ol start="3">
<li>验证保存的 Secret 是否是以 <code>k8s:enc:kms:v1:</code> 开头的，这表明 <code>kms</code> 驱动已经对结果数据加密。</li>
</ol>
<!--
4. Verify that the secret is correctly decrypted when retrieved via the API:
-->
<ol start="4">
<li>
<p>验证 Secret 在被 API 获取时已被正确解密：</p>
<pre><code>kubectl describe secret secret1 -n default
</code></pre><p>结果应该是 <code>mykey: mydata</code>。</p>
</li>
</ol>
<!--
## Ensuring all secrets are encrypted

Because secrets are encrypted on write, performing an update on a secret encrypts that content.
-->
<h2 id="确保所有-secret-都已被加密">确保所有 Secret 都已被加密</h2>
<p>因为 Secret 是在写入时被加密的，所以在更新 Secret 时也会加密该内容。</p>
<!--
The following command reads all secrets and then updates them to apply server side encryption. If an error occurs due to a conflicting write, retry the command.
For larger clusters, you may wish to subdivide the secrets by namespace or script an update.
-->
<p>下列命令读取所有 Secret 并更新它们以便应用服务器端加密。如果因为写入冲突导致错误发生，
请重试此命令。对较大的集群，你可能希望根据命名空间或脚本更新去细分 Secret 内容。</p>
<pre><code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</code></pre><!--
## Switching from a local encryption provider to the KMS provider

To switch from a local encryption provider to the `kms` provider and re-encrypt all of the secrets:
-->
<h2 id="从本地加密驱动切换到-kms-驱动">从本地加密驱动切换到 KMS 驱动</h2>
<p>为了从本地加密驱动切换到 <code>kms</code> 驱动并重新加密所有 Secret 内容：</p>
<!--
1. Add the `kms` provider as the first entry in the configuration file as shown in the following example.
-->
<ol>
<li>
<p>在配置文件中加入 <code>kms</code> 驱动作为第一个条目，如下列样例所示</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name </span>:<span style="color:#bbb"> </span>myKmsPlugin<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile.sock<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cachesize</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">aescbc</span>:<span style="color:#bbb">
</span><span style="color:#bbb">         </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">         </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">           </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
2. Restart all kube-apiserver processes.
3. Run the following command to force all secrets to be re-encrypted using the `kms` provider.
-->
<ol start="2">
<li>
<p>重启所有 kube-apiserver 进程。</p>
</li>
<li>
<p>运行下列命令使用 <code>kms</code> 驱动强制重新加密所有 Secret。</p>
<pre><code>kubectl get secrets --all-namespaces -o json| kubectl replace -f -
</code></pre></li>
</ol>
<!--
## Disabling encryption at rest
To disable encryption at rest:
-->
<h2 id="禁用静态数据加密">禁用静态数据加密</h2>
<p>要禁用静态数据加密：</p>
<!--
1. Place the `identity` provider as the first entry in the configuration file:
-->
<ol>
<li>
<p>将 <code>identity</code> 驱动作为配置文件中的第一个条目：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name </span>:<span style="color:#bbb"> </span>myKmsPlugin<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile.sock<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cachesize</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
2.  Restart all kube-apiserver processes.
3. Run the following command to force all secrets to be decrypted.
-->
<ol start="2">
<li>
<p>重启所有 kube-apiserver 进程。</p>
</li>
<li>
<p>运行下列命令强制重新加密所有 Secret。</p>
<pre><code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</code></pre></li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e77685d5b88d2db5c7631a27b9472eea">14 - 使用 Kubernetes API 访问集群</h1>
    
	<!--
title: Access Clusters Using the Kubernetes API
content_type: task
-->
<!-- overview -->
<!--
This page shows how to access clusters using the Kubernetes API.
-->
<p>本页展示了如何使用 Kubernetes API 访问集群</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Accessing the cluster API

### Accessing for the first time with kubectl
-->
<h2 id="访问集群-api">访问集群 API</h2>
<h3 id="使用-kubectl-进行首次访问">使用 kubectl 进行首次访问</h3>
<!--
When accessing the Kubernetes API for the first time, use the
Kubernetes command-line tool, `kubectl`.
-->
<p>首次访问 Kubernetes API 时，请使用 Kubernetes 命令行工具 <code>kubectl</code> 。</p>
<!--
To access a cluster, you need to know the location of the cluster and have credentials
to access it. Typically, this is automatically set-up when you work through
a [Getting started guide](/docs/setup/),
or someone else setup the cluster and provided you with credentials and a location.
-->
<p>要访问集群，你需要知道集群位置并拥有访问它的凭证。
通常，当你完成<a href="/zh/docs/setup/">入门指南</a>时，这会自动设置完成，或者由其他人设置好集群并将凭证和位置提供给你。</p>
<!--
Check the location and credentials that kubectl knows about with this command:
-->
<p>使用此命令检查 kubectl 已知的位置和凭证：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div><!--
Many of the [examples](https://github.com/kubernetes/examples/tree/master/) provide an introduction to using
kubectl. Complete documentation is found in the [kubectl manual](/docs/reference/kubectl/).
-->
<p>许多<a href="https://github.com/kubernetes/examples/tree/master/">样例</a>
提供了使用 kubectl 的介绍。完整文档请见 <a href="/zh/docs/reference/kubectl/">kubectl 手册</a>。</p>
<!--
### Directly accessing the REST API

kubectl handles locating and authenticating to the API server. If you want to directly access the REST API with an http client like
`curl` or `wget`, or a browser, there are multiple ways you can locate and authenticate against the API server:
-->
<h3 id="直接访问-rest-api">直接访问 REST API</h3>
<p>kubectl 处理对 API 服务器的定位和身份验证。如果你想通过 http 客户端（如 <code>curl</code> 或 <code>wget</code>，或浏览器）直接访问 REST API，你可以通过多种方式对 API 服务器进行定位和身份验证：</p>
 <!--
1. Run kubectl in proxy mode (recommended). This method is recommended, since it uses the stored apiserver location and verifies the identity of the API server using a self-signed cert. No man-in-the-middle (MITM) attack is possible using this method.
 1. Alternatively, you can provide the location and credentials directly to the http client. This works with client code that is confused by proxies. To protect against man in the middle attacks, you'll need to import a root cert into your browser.
-->
<ol>
<li>以代理模式运行 kubectl（推荐）。
推荐使用此方法，因为它用存储的 apiserver 位置并使用自签名证书验证 API 服务器的标识。
使用这种方法无法进行中间人（MITM）攻击。</li>
<li>另外，你可以直接为 HTTP 客户端提供位置和身份认证。
这适用于被代理混淆的客户端代码。
为防止中间人攻击，你需要将根证书导入浏览器。</li>
</ol>
<!--
Using the Go or Python client libraries provides accessing kubectl in proxy mode.
-->
<p>使用 Go 或 Python 客户端库可以在代理模式下访问 kubectl。</p>
<!--
#### Using kubectl proxy

The following command runs kubectl in a mode where it acts as a reverse proxy. It handles
locating the API server and authenticating.
-->
<h4 id="使用-kubectl-代理">使用 kubectl 代理</h4>
<p>下列命令使 kubectl 运行在反向代理模式下。它处理 API 服务器的定位和身份认证。</p>
<!-- Run it like this: -->
<p>像这样运行它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span> &amp;
</code></pre></div><!--
See [kubectl proxy](/docs/reference/generated/kubectl/kubectl-commands/#proxy) for more details.
-->
<p>参见 <a href="/docs/reference/generated/kubectl/kubectl-commands/#proxy">kubectl 代理</a> 获取更多细节。</p>
<!--
Then you can explore the API with curl, wget, or a browser, like so:
-->
<p>然后你可以通过 curl，wget，或浏览器浏览 API，像这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl http://localhost:8080/api/
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;versions&#34;</span>: [
    <span style="color:#b44">&#34;v1&#34;</span>
  ],
  <span style="color:#008000;font-weight:bold">&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;clientCIDR&#34;</span>: <span style="color:#b44">&#34;0.0.0.0/0&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;serverAddress&#34;</span>: <span style="color:#b44">&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div><!--
#### Without kubectl proxy

It is possible to avoid using kubectl proxy by passing an authentication token
directly to the API server, like this:

Using `grep/cut` approach:
-->
<h4 id="不使用-kubectl-代理">不使用 kubectl 代理</h4>
<p>通过将身份认证令牌直接传给 API 服务器，可以避免使用 kubectl 代理，像这样：</p>
<p>使用 <code>grep/cut</code> 方式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 查看所有的集群，因为你的 .kubeconfig 文件中可能包含多个上下文</span>
kubectl config view -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{&#34;Cluster name\tServer\n&#34;}{range .clusters[*]}{.name}{&#34;\t&#34;}{.cluster.server}{&#34;\n&#34;}{end}&#39;</span>

<span style="color:#080;font-style:italic"># 从上述命令输出中选择你要与之交互的集群的名称</span>
<span style="color:#a2f">export</span> <span style="color:#b8860b">CLUSTER_NAME</span><span style="color:#666">=</span><span style="color:#b44">&#34;some_server_name&#34;</span>

<span style="color:#080;font-style:italic"># 指向引用该集群名称的 API 服务器</span>
<span style="color:#b8860b">APISERVER</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl config view -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#34;{.clusters[?(@.name==\&#34;</span><span style="color:#b8860b">$CLUSTER_NAME</span><span style="color:#b44">\&#34;)].cluster.server}&#34;</span><span style="color:#a2f;font-weight:bold">)</span>

<span style="color:#080;font-style:italic"># 创建一个 secret 来保存默认服务账户的令牌</span>
kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: Secret
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: default-token
</span><span style="color:#b44">  annotations:
</span><span style="color:#b44">    kubernetes.io/service-account.name: default
</span><span style="color:#b44">type: kubernetes.io/service-account-token
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 等待令牌控制器使用令牌填充 secret:</span>
<span style="color:#a2f;font-weight:bold">while</span> ! kubectl describe secret default-token | grep -E <span style="color:#b44">&#39;^token&#39;</span> &gt;/dev/null; <span style="color:#a2f;font-weight:bold">do</span>
  <span style="color:#a2f">echo</span> <span style="color:#b44">&#34;waiting for token...&#34;</span> &gt;&amp;<span style="color:#666">2</span>
  sleep <span style="color:#666">1</span>
<span style="color:#a2f;font-weight:bold">done</span>

<span style="color:#080;font-style:italic"># 获取令牌</span>
<span style="color:#b8860b">TOKEN</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get secret default-token -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.data.token}&#39;</span> | base64 --decode<span style="color:#a2f;font-weight:bold">)</span>

<span style="color:#080;font-style:italic"># 使用令牌玩转 API</span>
curl -X GET <span style="color:#b8860b">$APISERVER</span>/api --header <span style="color:#b44">&#34;Authorization: Bearer </span><span style="color:#b8860b">$TOKEN</span><span style="color:#b44">&#34;</span> --insecure
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;APIVersions&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;versions&#34;</span>: [
    <span style="color:#b44">&#34;v1&#34;</span>
  ],
  <span style="color:#008000;font-weight:bold">&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;clientCIDR&#34;</span>: <span style="color:#b44">&#34;0.0.0.0/0&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;serverAddress&#34;</span>: <span style="color:#b44">&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div><!--
The above example uses the `--insecure` flag. This leaves it subject to MITM
attacks. When kubectl accesses the cluster it uses a stored root certificate
and client certificates to access the server. (These are installed in the
`~/.kube` directory). Since cluster certificates are typically self-signed, it
may take special configuration to get your http client to use root
certificate.
-->
<p>上面例子使用了 <code>--insecure</code> 标志位。这使它易受到 MITM 攻击。
当 kubectl 访问集群时，它使用存储的根证书和客户端证书访问服务器。
（已安装在 <code>~/.kube</code> 目录下）。
由于集群认证通常是自签名的，因此可能需要特殊设置才能让你的 http 客户端使用根证书。</p>
<!--
On some clusters, the API server does not require authentication; it may serve
on localhost, or be protected by a firewall. There is not a standard
for this. [Controlling Access to the Kubernetes API](/docs/concepts/security/controlling-access)
describes how you can configure this as a cluster administrator.
-->
<p>在一些集群中，API 服务器不需要身份认证；它运行在本地，或由防火墙保护着。
对此并没有一个标准。
<a href="/zh/docs/concepts/security/controlling-access/">配置对 API 的访问</a>
讲解了作为集群管理员可如何对此进行配置。</p>
<!--
### Programmatic access to the API

Kubernetes officially supports client libraries for [Go](#go-client), [Python](#python-client), [Java](#java-client), [dotnet](#dotnet-client), [JavaScript](#javascript-client), and [Haskell](#haskell-client). There are other client libraries that are provided and maintained by their authors, not the Kubernetes team. See [client libraries](/docs/reference/using-api/client-libraries/) for accessing the API from other languages and how they authenticate.
-->
<h3 id="编程方式访问-api">编程方式访问 API</h3>
<p>Kubernetes 官方支持 <a href="#go-client">Go</a>、<a href="#python-client">Python</a>、<a href="#java-client">Java</a>、
<a href="#dotnet-client">dotnet</a>、<a href="#javascript-client">JavaScript</a> 和 <a href="#haskell-client">Haskell</a>
语言的客户端库。还有一些其他客户端库由对应作者而非 Kubernetes 团队提供并维护。
参考<a href="/zh/docs/reference/using-api/client-libraries/">客户端库</a>了解如何使用其他语言
来访问 API 以及如何执行身份认证。</p>
<!-- #### Go client -->
<h4 id="go-client">Go 客户端 </h4>
<!--
* To get the library, run the following command: `go get k8s.io/client-go@kubernetes-<kubernetes-version-number>` See [https://github.com/kubernetes/client-go/releases](https://github.com/kubernetes/client-go/releases) to see which versions are supported.
* Write an application atop of the client-go clients.
-->
<ul>
<li>要获取库，运行下列命令：<code>go get k8s.io/client-go/kubernetes-&lt;kubernetes 版本号&gt;</code>，
参见 <a href="https://github.com/kubernetes/client-go/releases">https://github.com/kubernetes/client-go/releases</a> 查看受支持的版本。</li>
<li>基于 client-go 客户端编写应用程序。</li>
</ul>
<!--
Note that client-go defines its own API objects, so if needed, please import API definitions from client-go rather than from the main repository, e.g., `import "k8s.io/client-go/kubernetes"` is correct.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 注意 client-go 定义了自己的 API 对象，因此如果需要，请从 client-go 而不是主仓库导入
API 定义，例如 <code>import &quot;k8s.io/client-go/kubernetes&quot;</code> 是正确做法。
</div>
<!--
The Go client can use the same [kubeconfig file](/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go):
-->
<p>Go 客户端可以使用与 kubectl 命令行工具相同的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
定位和验证 API 服务器。参见这个
<a href="https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go">例子</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-golang" data-lang="golang"><span style="color:#a2f;font-weight:bold">package</span> main

<span style="color:#a2f;font-weight:bold">import</span> (
   <span style="color:#b44">&#34;context&#34;</span>
   <span style="color:#b44">&#34;fmt&#34;</span>
   <span style="color:#b44">&#34;k8s.io/apimachinery/pkg/apis/meta/v1&#34;</span>
   <span style="color:#b44">&#34;k8s.io/client-go/kubernetes&#34;</span>
   <span style="color:#b44">&#34;k8s.io/client-go/tools/clientcmd&#34;</span>
)

<span style="color:#a2f;font-weight:bold">func</span> <span style="color:#00a000">main</span>() {
  <span style="color:#080;font-style:italic">// uses the current context in kubeconfig
</span><span style="color:#080;font-style:italic"></span>  <span style="color:#080;font-style:italic">// path-to-kubeconfig -- for example, /root/.kube/config
</span><span style="color:#080;font-style:italic"></span>  config, _ <span style="color:#666">:=</span> clientcmd.<span style="color:#00a000">BuildConfigFromFlags</span>(<span style="color:#b44">&#34;&#34;</span>, <span style="color:#b44">&#34;&lt;path-to-kubeconfig&gt;&#34;</span>)
  <span style="color:#080;font-style:italic">// creates the clientset
</span><span style="color:#080;font-style:italic"></span>  clientset, _ <span style="color:#666">:=</span> kubernetes.<span style="color:#00a000">NewForConfig</span>(config)
  <span style="color:#080;font-style:italic">// access the API to list pods
</span><span style="color:#080;font-style:italic"></span>  pods, _ <span style="color:#666">:=</span> clientset.<span style="color:#00a000">CoreV1</span>().<span style="color:#00a000">Pods</span>(<span style="color:#b44">&#34;&#34;</span>).<span style="color:#00a000">List</span>(context.<span style="color:#00a000">TODO</span>(), v1.ListOptions{})
  fmt.<span style="color:#00a000">Printf</span>(<span style="color:#b44">&#34;There are %d pods in the cluster\n&#34;</span>, <span style="color:#a2f">len</span>(pods.Items))
}
</code></pre></div><!--
If the application is deployed as a Pod in the cluster, see [Accessing the API from within a Pod](/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod).
-->
<p>如果该应用程序部署为集群中的一个
Pod，请参阅<a href="/zh/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod">从 Pod 内访问 API</a>。</p>
<!-- #### Python client -->
<h4 id="python-client">Python 客户端</h4>
<!--
To use [Python client](https://github.com/kubernetes-client/python), run the following command: `pip install kubernetes` See [Python Client Library page](https://github.com/kubernetes-client/python) for more installation options.
-->
<p>要使用 <a href="https://github.com/kubernetes-client/python">Python 客户端</a>，运行下列命令：
<code>pip install kubernetes</code>。
参见 <a href="https://github.com/kubernetes-client/python">Python 客户端库主页</a> 了解更多安装选项。</p>
<!--
The Python client can use the same [kubeconfig file](/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://github.com/kubernetes-client/python/blob/master/examples/out_of_cluster_config.py):
-->
<p>Python 客户端可以使用与 kubectl 命令行工具相同的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
定位和验证 API 服务器。参见这个
<a href="https://github.com/kubernetes-client/python/blob/master/examples/out_of_cluster_config.py">例子</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">kubernetes</span> <span style="color:#a2f;font-weight:bold">import</span> client, config

config<span style="color:#666">.</span>load_kube_config()

v1<span style="color:#666">=</span>client<span style="color:#666">.</span>CoreV1Api()
<span style="color:#a2f">print</span>(<span style="color:#b44">&#34;Listing pods with their IPs:&#34;</span>)
ret <span style="color:#666">=</span> v1<span style="color:#666">.</span>list_pod_for_all_namespaces(watch<span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">False</span>)
<span style="color:#a2f;font-weight:bold">for</span> i <span style="color:#a2f;font-weight:bold">in</span> ret<span style="color:#666">.</span>items:
    <span style="color:#a2f">print</span>(<span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">%s</span><span style="color:#b62;font-weight:bold">\t</span><span style="color:#b68;font-weight:bold">%s</span><span style="color:#b62;font-weight:bold">\t</span><span style="color:#b68;font-weight:bold">%s</span><span style="color:#b44">&#34;</span> <span style="color:#666">%</span> (i<span style="color:#666">.</span>status<span style="color:#666">.</span>pod_ip, i<span style="color:#666">.</span>metadata<span style="color:#666">.</span>namespace, i<span style="color:#666">.</span>metadata<span style="color:#666">.</span>name))
</code></pre></div><!-- #### Java client -->
<h4 id="java-client">Java 客户端   </h4>
<!--
To install the [Java Client](https://github.com/kubernetes-client/java), run:
-->
<p>要安装 <a href="https://github.com/kubernetes-client/java">Java 客户端</a>，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 克隆 Java 库</span>
git clone --recursive https://github.com/kubernetes-client/java

<span style="color:#080;font-style:italic"># 安装项目文件、POM 等</span>
<span style="color:#a2f">cd</span> java
mvn install
</code></pre></div><!--
See [https://github.com/kubernetes-client/java/releases](https://github.com/kubernetes-client/java/releases) to see which versions are supported.

The Java client can use the same [kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/KubeConfigFileClientExample.java):
-->
<p>参阅<a href="https://github.com/kubernetes-client/java/releases">https://github.com/kubernetes-client/java/releases</a>
了解当前支持的版本。</p>
<p>Java 客户端可以使用 kubectl 命令行所使用的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
以定位 API 服务器并向其认证身份。
参看此<a href="https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/KubeConfigFileClientExample.java">示例</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-java" data-lang="java"><span style="color:#a2f;font-weight:bold">package</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.examples</span><span style="color:#666">;</span>

<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.ApiClient</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.ApiException</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.Configuration</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.apis.CoreV1Api</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.models.V1Pod</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.models.V1PodList</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.util.ClientBuilder</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.util.KubeConfig</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">java.io.FileReader</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">java.io.IOException</span><span style="color:#666">;</span>

<span style="color:#080;font-style:italic">/**
</span><span style="color:#080;font-style:italic"> * A simple example of how to use the Java API from an application outside a kubernetes cluster
</span><span style="color:#080;font-style:italic"> *
</span><span style="color:#080;font-style:italic"> * &lt;p&gt;Easiest way to run this: mvn exec:java
</span><span style="color:#080;font-style:italic"> * -Dexec.mainClass=&#34;io.kubernetes.client.examples.KubeConfigFileClientExample&#34;
</span><span style="color:#080;font-style:italic"> *
</span><span style="color:#080;font-style:italic"> */</span>
<span style="color:#a2f;font-weight:bold">public</span> <span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">KubeConfigFileClientExample</span> <span style="color:#666">{</span>
  <span style="color:#a2f;font-weight:bold">public</span> <span style="color:#a2f;font-weight:bold">static</span> <span style="color:#0b0;font-weight:bold">void</span> <span style="color:#00a000">main</span><span style="color:#666">(</span>String<span style="color:#666">[]</span> args<span style="color:#666">)</span> <span style="color:#a2f;font-weight:bold">throws</span> IOException<span style="color:#666">,</span> ApiException <span style="color:#666">{</span>

    <span style="color:#080;font-style:italic">// file path to your KubeConfig
</span><span style="color:#080;font-style:italic"></span>    String kubeConfigPath <span style="color:#666">=</span> <span style="color:#b44">&#34;~/.kube/config&#34;</span><span style="color:#666">;</span>

    <span style="color:#080;font-style:italic">// loading the out-of-cluster config, a kubeconfig from file-system
</span><span style="color:#080;font-style:italic"></span>    ApiClient client <span style="color:#666">=</span>
        ClientBuilder<span style="color:#666">.</span><span style="color:#b44">kubeconfig</span><span style="color:#666">(</span>KubeConfig<span style="color:#666">.</span><span style="color:#b44">loadKubeConfig</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:bold">new</span> FileReader<span style="color:#666">(</span>kubeConfigPath<span style="color:#666">))).</span><span style="color:#b44">build</span><span style="color:#666">();</span>

    <span style="color:#080;font-style:italic">// set the global default api-client to the in-cluster one from above
</span><span style="color:#080;font-style:italic"></span>    Configuration<span style="color:#666">.</span><span style="color:#b44">setDefaultApiClient</span><span style="color:#666">(</span>client<span style="color:#666">);</span>

    <span style="color:#080;font-style:italic">// the CoreV1Api loads default api-client from global configuration.
</span><span style="color:#080;font-style:italic"></span>    CoreV1Api api <span style="color:#666">=</span> <span style="color:#a2f;font-weight:bold">new</span> CoreV1Api<span style="color:#666">();</span>

    <span style="color:#080;font-style:italic">// invokes the CoreV1Api client
</span><span style="color:#080;font-style:italic"></span>    V1PodList list <span style="color:#666">=</span> api<span style="color:#666">.</span><span style="color:#b44">listPodForAllNamespaces</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">);</span>
    System<span style="color:#666">.</span><span style="color:#b44">out</span><span style="color:#666">.</span><span style="color:#b44">println</span><span style="color:#666">(</span><span style="color:#b44">&#34;Listing all pods: &#34;</span><span style="color:#666">);</span>
    <span style="color:#a2f;font-weight:bold">for</span> <span style="color:#666">(</span>V1Pod item <span style="color:#666">:</span> list<span style="color:#666">.</span><span style="color:#b44">getItems</span><span style="color:#666">())</span> <span style="color:#666">{</span>
      System<span style="color:#666">.</span><span style="color:#b44">out</span><span style="color:#666">.</span><span style="color:#b44">println</span><span style="color:#666">(</span>item<span style="color:#666">.</span><span style="color:#b44">getMetadata</span><span style="color:#666">().</span><span style="color:#b44">getName</span><span style="color:#666">());</span>
    <span style="color:#666">}</span>
  <span style="color:#666">}</span>
<span style="color:#666">}</span>
</code></pre></div><!--
#### dotnet client

To use [dotnet client](https://github.com/kubernetes-client/csharp), run the following command: `dotnet add package KubernetesClient --version 1.6.1` See [dotnet Client Library page](https://github.com/kubernetes-client/csharp) for more installation options. See [https://github.com/kubernetes-client/csharp/releases](https://github.com/kubernetes-client/csharp/releases) to see which versions are supported.

The dotnet client can use the same [kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://github.com/kubernetes-client/csharp/blob/master/examples/simple/PodList.cs):
-->
<h4 id="dotnet-client">.Net 客户端   </h4>
<p>要使用<a href="https://github.com/kubernetes-client/csharp">.Net 客户端</a>，运行下面的命令：
<code>dotnet add package KubernetesClient --version 1.6.1</code>。
参见<a href="https://github.com/kubernetes-client/csharp">.Net 客户端库页面</a>了解更多安装选项。
关于可支持的版本，参见<a href="https://github.com/kubernetes-client/csharp/releases">https://github.com/kubernetes-client/csharp/releases</a>。</p>
<p>.Net 客户端可以使用与 kubectl CLI 相同的 <a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
来定位并验证 API 服务器。
参见<a href="https://github.com/kubernetes-client/csharp/blob/master/examples/simple/PodList.cs">样例</a>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#a2f;font-weight:bold">using</span> <span style="color:#00f;font-weight:bold">System</span>;
<span style="color:#a2f;font-weight:bold">using</span> <span style="color:#00f;font-weight:bold">k8s</span>;

<span style="color:#a2f;font-weight:bold">namespace</span> <span style="color:#00f;font-weight:bold">simple</span>
{
    <span style="color:#a2f;font-weight:bold">internal</span> <span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">PodList</span>
    {
        <span style="color:#a2f;font-weight:bold">private</span> <span style="color:#a2f;font-weight:bold">static</span> <span style="color:#a2f;font-weight:bold">void</span> Main(<span style="color:#0b0;font-weight:bold">string</span>[] args)
        {
            <span style="color:#0b0;font-weight:bold">var</span> config = KubernetesClientConfiguration.BuildDefaultConfig();
            IKubernetes client = <span style="color:#a2f;font-weight:bold">new</span> Kubernetes(config);
            Console.WriteLine(<span style="color:#b44">&#34;Starting Request!&#34;</span>);

            <span style="color:#0b0;font-weight:bold">var</span> list = client.ListNamespacedPod(<span style="color:#b44">&#34;default&#34;</span>);
            <span style="color:#a2f;font-weight:bold">foreach</span> (<span style="color:#0b0;font-weight:bold">var</span> item <span style="color:#a2f;font-weight:bold">in</span> list.Items)
            {
                Console.WriteLine(item.Metadata.Name);
            }
            <span style="color:#a2f;font-weight:bold">if</span> (list.Items.Count == <span style="color:#666">0</span>)
            {
                Console.WriteLine(<span style="color:#b44">&#34;Empty!&#34;</span>);
            }
        }
    }
}
</code></pre></div><!--
#### JavaScript client

To install [JavaScript client](https://github.com/kubernetes-client/javascript), run the following command: `npm install @kubernetes/client-node`. See [https://github.com/kubernetes-client/javascript/releases](https://github.com/kubernetes-client/javascript/releases) to see which versions are supported.

The JavaScript client can use the same [kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://github.com/kubernetes-client/javascript/blob/master/examples/example.js):
-->
<h4 id="javascript-client">JavaScript 客户端   </h4>
<p>要安装 <a href="https://github.com/kubernetes-client/javascript">JavaScript 客户端</a>，运行下面的命令：
<code>npm install @kubernetes/client-node</code>。
参考<a href="https://github.com/kubernetes-client/javascript/releases">https://github.com/kubernetes-client/javascript/releases</a>了解可支持的版本。</p>
<p>JavaScript 客户端可以使用 kubectl 命令行所使用的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
以定位 API 服务器并向其认证身份。
参见<a href="https://github.com/kubernetes-client/javascript/blob/master/examples/example.js">此例</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#a2f;font-weight:bold">const</span> k8s <span style="color:#666">=</span> require(<span style="color:#b44">&#39;@kubernetes/client-node&#39;</span>);

<span style="color:#a2f;font-weight:bold">const</span> kc <span style="color:#666">=</span> <span style="color:#a2f;font-weight:bold">new</span> k8s.KubeConfig();
kc.loadFromDefault();

<span style="color:#a2f;font-weight:bold">const</span> k8sApi <span style="color:#666">=</span> kc.makeApiClient(k8s.CoreV1Api);

k8sApi.listNamespacedPod(<span style="color:#b44">&#39;default&#39;</span>).then((res) =&gt; {
    console.log(res.body);
});
</code></pre></div><!--
#### Haskell client

See [https://github.com/kubernetes-client/haskell/releases](https://github.com/kubernetes-client/haskell/releases) to see which versions are supported.

The [Haskell client](https://github.com/kubernetes-client/haskell) can use the same [kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://github.com/kubernetes-client/haskell/blob/master/kubernetes-client/example/App.hs):
-->
<h4 id="haskell-client">Haskell 客户端   </h4>
<p>参考 <a href="https://github.com/kubernetes-client/haskell/releases">https://github.com/kubernetes-client/haskell/releases</a> 了解支持的版本。</p>
<p><a href="https://github.com/kubernetes-client/haskell">Haskell 客户端</a>
可以使用 kubectl 命令行所使用的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
以定位 API 服务器并向其认证身份。
参见<a href="https://github.com/kubernetes-client/haskell/blob/master/kubernetes-client/example/App.hs">此例</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#00a000">exampleWithKubeConfig</span> <span style="color:#a2f;font-weight:bold">::</span> <span style="color:#0b0;font-weight:bold">IO</span> <span style="color:#a2f">()</span>
<span style="color:#00a000">exampleWithKubeConfig</span> <span style="color:#a2f;font-weight:bold">=</span> <span style="color:#a2f;font-weight:bold">do</span>
    oidcCache <span style="color:#a2f;font-weight:bold">&lt;-</span> atomically <span style="color:#666">$</span> newTVar <span style="color:#666">$</span> <span style="color:#0b0;font-weight:bold">Map</span><span style="color:#666">.</span>fromList <span style="color:#0b0;font-weight:bold">[]</span>
    (mgr, kcfg) <span style="color:#a2f;font-weight:bold">&lt;-</span> mkKubeClientConfig oidcCache <span style="color:#666">$</span> <span style="color:#0b0;font-weight:bold">KubeConfigFile</span> <span style="color:#b44">&#34;/path/to/kubeconfig&#34;</span>
    dispatchMime
            mgr
            kcfg
            (<span style="color:#0b0;font-weight:bold">CoreV1</span><span style="color:#666">.</span>listPodForAllNamespaces (<span style="color:#0b0;font-weight:bold">Accept</span> <span style="color:#0b0;font-weight:bold">MimeJSON</span>))
        <span style="color:#666">&gt;&gt;=</span> print
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
* [Accessing the Kubernetes API from a Pod](/docs/tasks/run-application/access-api-from-pod/)
-->
<ul>
<li><a href="/zh/docs/tasks/run-application/access-api-from-pod/">从 Pod 中访问 API</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-778055e4a4415ca195169b42cd42ddf9">15 - 使用 NUMA 感知的内存管理器</h1>
    
	<!--
title: Utilizing the NUMA-aware Memory Manager

reviewers:
- klueska
- derekwaynecarr

content_type: task
min-kubernetes-server-version: v1.21
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
The Kubernetes *Memory Manager* enables the feature of guaranteed memory (and hugepages)
allocation for pods in the `Guaranteed` <a class='glossary-tooltip' title='QoS 类（Quality of Service Class）为 Kubernetes 提供了一种将集群中的 Pod 分为几个类并做出有关调度和驱逐决策的方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-qos-class' target='_blank' aria-label='QoS class'>QoS class</a>.

The Memory Manager employs hint generation protocol to yield the most suitable NUMA affinity for a pod.
The Memory Manager feeds the central manager (*Topology Manager*) with these affinity hints.
Based on both the hints and Topology Manager policy, the pod is rejected or admitted to the node.
-->
<p>Kubernetes 内存管理器（Memory Manager）为 <code>Guaranteed</code>
<a class='glossary-tooltip' title='QoS 类（Quality of Service Class）为 Kubernetes 提供了一种将集群中的 Pod 分为几个类并做出有关调度和驱逐决策的方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-qos-class' target='_blank' aria-label='QoS 类'>QoS 类</a>
的 Pods 提供可保证的内存（及大页面）分配能力。</p>
<p>内存管理器使用提示生成协议来为 Pod 生成最合适的 NUMA 亲和性配置。
内存管理器将这类亲和性提示输入给中央管理器（即 Topology Manager）。
基于所给的提示和 Topology Manager（拓扑管理器）的策略设置，Pod
或者会被某节点接受，或者被该节点拒绝。</p>
<!--
Moreover, the Memory Manager ensures that the memory which a pod
requests is allocated from
a minimum number of NUMA nodes.

The Memory Manager is only pertinent to Linux based hosts.
-->
<p>此外，内存管理器还确保 Pod 所请求的内存是从尽量少的 NUMA 节点分配而来。</p>
<p>内存管理器仅能用于 Linux 主机。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

Your Kubernetes server must be at or later than version v1.21.
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
To align memory resources with other requested resources in a Pod Spec:

- the CPU Manager should be enabled and proper CPU Manager policy should be configured on a Node.
  See [control CPU Management Policies](/docs/tasks/administer-cluster/cpu-management-policies/);
- the Topology Manager should be enabled and proper Topology Manager policy should be configured on a Node.
  See [control Topology Management Policies](/docs/tasks/administer-cluster/topology-manager/).
-->
<p>为了使得内存资源与 Pod 规约中所请求的其他资源对齐：</p>
<ul>
<li>CPU 管理器应该被启用，并且在节点（Node）上要配置合适的 CPU 管理器策略，
参见<a href="/zh/docs/tasks/administer-cluster/cpu-management-policies/">控制 CPU 管理策略</a>；</li>
<li>拓扑管理器要被启用，并且要在节点上配置合适的拓扑管理器策略，参见
<a href="/zh/docs/tasks/administer-cluster/topology-manager/">控制拓扑管理器策略</a>。</li>
</ul>
<!--
Starting from v1.22, the Memory Manager is enabled by default through `MemoryManager`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/).

Preceding v1.22, the `kubelet` must be started with the following flag:

`--feature-gates=MemoryManager=true`

in order to enable the Memory Manager feature.
-->
<p>从 v1.22 开始，内存管理器通过
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>MemoryManager</code> 默认启用。</p>
<p>在 v1.22 之前，<code>kubelet</code> 必须在启动时设置如下标志：</p>
<p><code>--feature-gates=MemoryManager=true</code></p>
<p>这样内存管理器特性才会被启用。</p>
<!--
## How Memory Manager Operates?
-->
<h2 id="内存管理器如何运作">内存管理器如何运作？</h2>
<!--
The Memory Manager currently offers the guaranteed memory (and hugepages) allocation
for Pods in Guaranteed QoS class.
To immediately put the Memory Manager into operation follow the guidelines in the section
[Memory Manager configuration](#memory-manager-configuration), and subsequently,
prepare and deploy a `Guaranteed` pod as illustrated in the section
[Placing a Pod in the Guaranteed QoS class](#placing-a-pod-in-the-guaranteed-qos-class).
-->
<p>内存管理器目前为 Guaranteed QoS 类中的 Pod 提供可保证的内存（和大页面）分配能力。
若要立即将内存管理器启用，可参照<a href="#memory-manager-configuration">内存管理器配置</a>节中的指南，
之后按<a href="#placing-a-pod-in-the-guaranteed-qos-class">将 Pod 放入 Guaranteed QoS 类</a>
节中所展示的，准备并部署一个 <code>Guaranteed</code> Pod。</p>
<!--
The Memory Manager is a Hint Provider, and it provides topology hints for
the Topology Manager which then aligns the requested resources according to these topology hints.
It also enforces `cgroups` (i.e. `cpuset.mems`) for pods.
The complete flow diagram concerning pod admission and deployment process is illustrated in
[Memory Manager KEP: Design Overview][4] and below:
-->
<p>内存管理器是一个提示驱动组件（Hint Provider），负责为拓扑管理器提供拓扑提示，
后者根据这些拓扑提示对所请求的资源执行对齐操作。
内存管理器也会为 Pods 应用 <code>cgroups</code> 设置（即 <code>cpuset.mems</code>）。
与 Pod 准入和部署流程相关的完整流程图在<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#design-overview">Memory Manager KEP: Design Overview</a>
和下面。</p>
<!--
![Memory Manager in the pod admission and deployment process](/images/docs/memory-manager-diagram.svg)
-->
<p><img src="/images/docs/memory-manager-diagram.svg" alt="Pod 准入与部署流程中的内存管理器"></p>
<!--
During this process, the Memory Manager updates its internal counters stored in
[Node Map and Memory Maps][2] to manage guaranteed memory allocation. 

The Memory Manager updates the Node Map during the startup and runtime as follows.
-->
<p>在这个过程中，内存管理器会更新其内部存储于<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">节点映射和内存映射</a>中的计数器，
从而管理有保障的内存分配。</p>
<p>内存管理器在启动和运行期间按下述逻辑更新节点映射（Node Map）。</p>
<!--
### Startup
-->
<h3 id="startup">启动 </h3>
<!--
This occurs once a node administrator employs `--reserved-memory` (section
[Reserved memory flag](#reserved-memory-flag)).
In this case, the Node Map becomes updated to reflect this reservation as illustrated in
[Memory Manager KEP: Memory Maps at start-up (with examples)][5].

The administrator must provide `--reserved-memory` flag when `Static` policy is configured.
-->
<p>当节点管理员应用 <code>--reserved-memory</code> <a href="#reserved-memory-flag">预留内存标志</a>时执行此逻辑。
这时，节点映射会被更新以反映内存的预留，如
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-start-up-with-examples">Memory Manager KEP: Memory Maps at start-up (with examples)</a>
所说明。</p>
<p>当配置了 <code>Static</code> 策略时，管理员必须提供 <code>--reserved-memory</code> 标志设置。</p>
<!--
### Runtime
-->
<h3 id="runtime">运行时 </h3>
<!--
Reference [Memory Manager KEP: Memory Maps at runtime (with examples)][6]
illustrates how a successful pod deployment affects the Node Map, and it also relates to
how potential Out-of-Memory (OOM) situations are handled further by Kubernetes or operating system.
-->
<p>参考文献 <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-runtime-with-examples">Memory Manager KEP: Memory Maps at runtime (with examples)</a>
中说明了成功的 Pod 部署是如何影响节点映射的，该文档也解释了可能发生的内存不足
（Out-of-memory，OOM）情况是如何进一步被 Kubernetes 或操作系统处理的。</p>
<!--
Important topic in the context of Memory Manager operation is the management of NUMA groups.
Each time pod's memory request is in excess of single NUMA node capacity, the Memory Manager
attempts to create a group that comprises several NUMA nodes and features extend memory capacity.
The problem has been solved as elaborated in
[Memory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?][3].
Also, reference [Memory Manager KEP: Simulation - how the Memory Manager works? (by examples)][1]
illustrates how the management of groups occurs. 
-->
<p>在内存管理器运作的语境中，一个重要的话题是对 NUMA 分组的管理。
每当 Pod 的内存请求超出单个 NUMA 节点容量时，内存管理器会尝试创建一个包含多个
NUMA 节点的分组，从而扩展内存容量。解决这个问题的详细描述在文档
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes">Memory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?</a>
中。同时，关于 NUMA 分组是如何管理的，你还可以参考文档
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples">Memory Manager KEP: Simulation - how the Memory Manager works? (by examples)</a>。</p>
<!--
## Memory Manager configuration
-->
<h2 id="memory-manager-configuration">内存管理器配置  </h2>
<!--
Other Managers should be first pre-configured. Next, the Memory Manger feature should be enabled
and be run with `Static` policy (section [Static policy](#static-policy)).
Optionally, some amount of memory can be reserved for system or kubelet processes to increase
node stability (section [Reserved memory flag](#reserved-memory-flag)).
-->
<p>其他管理器也要预先配置。接下来，内存管理器特性需要被启用，
并且采用 <code>Static</code> 策略（<a href="#policy-static">静态策略</a>）运行。
作为可选操作，可以预留一定数量的内存给系统或者 kubelet 进程以增强节点的
稳定性（<a href="#reserved-memory-flag">预留内存标志</a>）。</p>
<!--
### Policies 
-->
<h3 id="policies">策略   </h3>
<!--
Memory Manager supports two policies. You can select a policy via a `kubelet` flag `--memory-manager-policy`:

* `None` (default)
* `Static`
-->
<p>内存管理器支持两种策略。你可以通过 <code>kubelet</code> 标志 <code>--memory-manager-policy</code> 来
选择一种策略：</p>
<ul>
<li><code>None</code> （默认）</li>
<li><code>Static</code></li>
</ul>
<!--
#### None policy {#policy-none}

This is the default policy and does not affect the memory allocation in any way.
It acts the same as if the Memory Manager is not present at all.

The `None` policy returns default topology hint. This special hint denotes that Hint Provider
(Memory Manger in this case) has no preference for NUMA affinity with any resource.
-->
<h4 id="policy-none">None 策略   </h4>
<p>这是默认的策略，并且不会以任何方式影响内存分配。该策略的行为好像内存管理器不存在一样。</p>
<p><code>None</code> 策略返回默认的拓扑提示信息。这种特殊的提示会表明拓扑驱动组件（Hint Provider）
（在这里是内存管理器）对任何资源都没有与 NUMA 亲和性关联的偏好。</p>
<!--
#### Static policy {#policy-static}

In the case of the `Guaranteed` pod, the `Static` Memory Manger policy returns topology hints
relating to the set of NUMA nodes where the memory can be guaranteed,
and reserves the memory through updating the internal [NodeMap][2] object.

In the case of the `BestEffort` or `Burstable` pod, the `Static` Memory Manager policy sends back
the default topology hint as there is no request for the guaranteed memory,
and does not reserve the memory in the internal [NodeMap][2] object.
-->
<h4 id="policy-static">Static 策略   </h4>
<p>对 <code>Guaranteed</code> Pod 而言，<code>Static</code> 内存管理器策略会返回拓扑提示信息，该信息
与内存分配有保障的 NUMA 节点集合有关，并且内存管理器还通过更新内部的
<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">节点映射</a> 对象来完成内存预留。</p>
<p>对 <code>BestEffort</code> 或 <code>Burstable</code> Pod 而言，因为不存在对有保障的内存资源的请求，
<code>Static</code> 内存管理器策略会返回默认的拓扑提示，并且不会通过内部的<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">节点映射</a>对象
来预留内存。</p>
<!--
### Reserved memory flag
-->
<h3 id="reserved-memory-flag">预留内存标志   </h3>
<!--
The [Node Allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/) mechanism
is commonly used by node administrators to reserve K8S node system resources for the kubelet
or operating system processes in order to enhance the node stability.
A dedicated set of flags can be used for this purpose to set the total amount of reserved memory
for a node. This pre-configured value is subsequently utilized to calculate
the real amount of node's "allocatable" memory available to pods.
-->
<p><a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/">节点可分配</a>机制通常
被节点管理员用来为 kubelet 或操作系统进程预留 K8S 节点上的系统资源，目的是提高节点稳定性。
有一组专用的标志可用于这个目的，为节点设置总的预留内存量。
此预配置的值接下来会被用来计算节点上对 Pods “可分配的”内存。</p>
<!--
The Kubernetes scheduler incorporates "allocatable" to optimise pod scheduling process.
The foregoing flags include `--kube-reserved`, `--system-reserved` and `--eviction-threshold`.
The sum of their values will account for the total amount of reserved memory.

A new `--reserved-memory` flag was added to Memory Manager to allow for this total reserved memory
to be split (by a node administrator) and accordingly reserved across many NUMA nodes. 
-->
<p>Kubernetes 调度器在优化 Pod 调度过程时，会考虑“可分配的”内存。
前面提到的标志包括 <code>--kube-reserved</code>、<code>--system-reserved</code> 和 <code>--eviction-threshold</code>。
这些标志值的综合计作预留内存的总量。</p>
<p>为内存管理器而新增加的 <code>--reserved-memory</code> 标志可以（让节点管理员）将总的预留内存进行划分，
并完成跨 NUMA 节点的预留操作。</p>
<!--
The flag specifies a comma-separated list of memory reservations of different memory types per NUMA node.
Memory reservations across multiple NUMA nodes can be specified using semicolon as separator.
This parameter is only useful in the context of the Memory Manager feature. 
The Memory Manager will not use this reserved memory for the allocation of container workloads.

For example, if you have a NUMA node "NUMA0" with `10Gi` of memory available, and
the `--reserved-memory` was specified to reserve `1Gi` of memory at "NUMA0",
the Memory Manager assumes that only `9Gi` is available for containers.
-->
<p>标志设置的值是一个按 NUMA 节点的不同内存类型所给的内存预留的值的列表，用逗号分开。
可以使用分号作为分隔符来指定跨多个 NUMA 节点的内存预留。
只有在内存管理器特性被启用的语境下，这个参数才有意义。
内存管理器不会使用这些预留的内存来为容器负载分配内存。</p>
<p>例如，如果你有一个可用内存为 <code>10Gi</code> 的 NUMA 节点 &quot;NUMA0&quot;，而参数 <code>--reserved-memory</code>
被设置成要在 &quot;NUMA0&quot; 上预留 <code>1Gi</code> 的内存，那么内存管理器会假定节点上只有 <code>9Gi</code>
内存可用于容器负载。</p>
<!--
You can omit this parameter, however, you should be aware that the quantity of reserved memory
from all NUMA nodes should be equal to the quantity of memory specified by the
[Node Allocatable feature](/docs/tasks/administer-cluster/reserve-compute-resources/).
If at least one node allocatable parameter is non-zero, you will need to specify
`--reserved-memory` for at least one NUMA node.
In fact, `eviction-hard` threshold value is equal to `100Mi` by default, so
if `Static` policy is used, `--reserved-memory` is obligatory.
-->
<p>你也可以忽略此参数，不过这样做时，你要清楚，所有 NUMA 节点上预留内存的数量要等于
<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/">节点可分配特性</a>
所设定的内存量。如果至少有一个节点可分配参数值为非零，你就需要至少为一个 NUMA
节点设置 <code>--reserved-memory</code>。实际上，<code>eviction-hard</code> 阈值默认为 <code>100Mi</code>，
所以当使用 <code>Static</code> 策略时，<code>--reserved-memory</code> 是必须设置的。</p>
<!--
Also, avoid the following configurations:

1. duplicates, i.e. the same NUMA node or memory type, but with a different value;
1. setting zero limit for any of memory types;
1. NUMA node IDs that do not exist in the machine hardware;
1. memory type names different than `memory` or `hugepages-<size>`
   (hugepages of particular `<size>` should also exist).
-->
<p>此外，应尽量避免如下配置：</p>
<ol>
<li>重复的配置，即同一 NUMA 节点或内存类型被设置不同的取值；</li>
<li>为某种内存类型设置约束值为零；</li>
<li>使用物理硬件上不存在的 NUMA 节点 ID；</li>
<li>使用名字不是 <code>memory</code> 或 <code>hugepages-&lt;size&gt;</code> 的内存类型名称
（特定的 <code>&lt;size&gt;</code> 的大页面也必须存在）。</li>
</ol>
<!--
Syntax: 
-->
<p>语法：</p>
<p><code>--reserved-memory N:memory-type1=value1,memory-type2=value2,...</code></p>
<!--
* `N` (integer) - NUMA node index, e.g. `0`
* `memory-type` (string) - represents memory type:
  * `memory` - conventional memory
  * `hugepages-2Mi` or `hugepages-1Gi` - hugepages 
* `value` (string) - the quantity of reserved memory, e.g. `1Gi`
-->
<ul>
<li><code>N</code>（整数）- NUMA 节点索引，例如，<code>0</code></li>
<li><code>memory-type</code>（字符串）- 代表内存类型：
<ul>
<li><code>memory</code> - 常规内存；</li>
<li><code>hugepages-2Mi</code> 或 <code>hugepages-1Gi</code> - 大页面</li>
</ul>
</li>
<li><code>value</code>（字符串） - 预留内存的量，例如 <code>1Gi</code></li>
</ul>
<!--
Example usage:
-->
<p>用法示例：</p>
<p><code>--reserved-memory 0:memory=1Gi,hugepages-1Gi=2Gi</code></p>
<!-- or -->
<p>或者</p>
<p><code>--reserved-memory 0:memory=1Gi --reserved-memory 1:memory=2Gi</code></p>
<!--
When you specify values for `--reserved-memory` flag, you must comply with the setting that
you prior provided via Node Allocatable Feature flags.
That is, the following rule must be obeyed for each memory type: 

`sum(reserved-memory(i)) = kube-reserved + system-reserved + eviction-threshold`, 

where `i` is an index of a NUMA node. 
-->
<p>当你为 <code>--reserved-memory</code> 标志指定取值时，必须要遵从之前通过节点可分配特性标志所设置的值。
换言之，对每种内存类型而言都要遵从下面的规则：</p>
<p><code>sum(reserved-memory(i)) = kube-reserved + system-reserved + eviction-threshold</code></p>
<p>其中，<code>i</code> 是 NUMA 节点的索引。</p>
<!--
If you do not follow the formula above, the Memory Manager will show an error on startup.

In other words, the example above illustrates that for the conventional memory (`type=memory`),
we reserve `3Gi` in total, i.e.: 
-->
<p>如果你不遵守上面的公示，内存管理器会在启动时输出错误信息。</p>
<p>换言之，上面的例子我们一共要预留 <code>3Gi</code> 的常规内存（<code>type=memory</code>），即：</p>
<p><code>sum(reserved-memory(i)) = reserved-memory(0) + reserved-memory(1) = 1Gi + 2Gi = 3Gi</code></p>
<!--
An example of kubelet command-line arguments relevant to the node Allocatable configuration:
-->
<p>下面的例子中给出与节点可分配配置相关的 kubelet 命令行参数：</p>
<ul>
<li><code>--kube-reserved=cpu=500m,memory=50Mi</code></li>
<li><code>--system-reserved=cpu=123m,memory=333Mi</code></li>
<li><code>--eviction-hard=memory.available&lt;500Mi</code></li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The default hard eviction threshold is 100MiB, and **not** zero.
Remember to increase the quantity of memory that you reserve by setting `--reserved-memory`
by that hard eviction threshold. Otherwise, the kubelet will not start Memory Manager and
display an error. 
-->
<p>默认的硬性驱逐阈值是 100MiB，<strong>不是</strong>零。
请记得在使用 <code>--reserved-memory</code> 设置要预留的内存量时，加上这个硬性驱逐阈值。
否则 kubelet 不会启动内存管理器，而会输出一个错误信息。
</div>
<!--
Here is an example of a correct configuration:
-->
<p>下面是一个正确配置的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">--feature-gates<span style="color:#666">=</span><span style="color:#b8860b">MemoryManager</span><span style="color:#666">=</span><span style="color:#a2f">true</span> 
--kube-reserved<span style="color:#666">=</span><span style="color:#b8860b">cpu</span><span style="color:#666">=</span>4,memory<span style="color:#666">=</span>4Gi 
--system-reserved<span style="color:#666">=</span><span style="color:#b8860b">cpu</span><span style="color:#666">=</span>1,memory<span style="color:#666">=</span>1Gi 
--memory-manager-policy<span style="color:#666">=</span>Static 
--reserved-memory <span style="color:#b44">&#39;0:memory=3Gi;1:memory=2148Mi&#39;</span>
</code></pre></div><!--
Let us validate the configuration above:
-->
<p>我们对上面的配置做一个检查：</p>
<ol>
<li><code>kube-reserved + system-reserved + eviction-hard(default) = reserved-memory(0) + reserved-memory(1)</code></li>
<li><code>4GiB + 1GiB + 100MiB = 3GiB + 2148MiB</code></li>
<li><code>5120MiB + 100MiB = 3072MiB + 2148MiB</code></li>
<li><code>5220MiB = 5220MiB</code> （这是对的）</li>
</ol>
<!--
## Placing a Pod in the Guaranteed QoS class

If the selected policy is anything other than `None`, the Memory Manager identifies pods
that are in the `Guaranteed` QoS class.
The Memory Manager provides specific topology hints to the Topology Manager for each `Guaranteed` pod.
For pods in a QoS class other than `Guaranteed`, the Memory Manager provides default topology hints
to the Topology Manager.
-->
<h2 id="placing-a-pod-in-the-guaranteed-qos-class">将 Pod 放入 Guaranteed QoS 类 </h2>
<p>若所选择的策略不是 <code>None</code>，则内存管理器会辨识处于 <code>Guaranteed</code> QoS 类中的 Pod。
内存管理器为每个 <code>Guaranteed</code> Pod 向拓扑管理器提供拓扑提示信息。
对于不在 <code>Guaranteed</code> QoS 类中的其他 Pod，内存管理器向拓扑管理器提供默认的 
拓扑提示信息。</p>
<!--
The following excerpts from pod manifests assign a pod to the `Guaranteed` QoS class.

Pod with integer CPU(s) runs in the `Guaranteed` QoS class, when `requests` are equal to `limits`:
-->
<p>下面的来自 Pod 清单的片段将 Pod 加入到 <code>Guaranteed</code> QoS 类中。</p>
<p>当 Pod 的 CPU <code>requests</code> 等于 <code>limits</code> 且为整数值时，Pod 将运行在 <code>Guaranteed</code>
QoS 类中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
Also, a pod sharing CPU(s) runs in the `Guaranteed` QoS class, when `requests` are equal to `limits`.
-->
<p>此外，共享 CPU 的 Pods 在 <code>requests</code> 等于 <code>limits</code> 值时也运行在 <code>Guaranteed</code>
QoS 类中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;300m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;300m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
Notice that both CPU and memory requests must be specified for a Pod to lend it to Guaranteed QoS class.
-->
<p>要注意的是，只有 CPU 和内存请求都被设置时，Pod 才会进入 Guaranteed QoS 类。</p>
<!--
## Troubleshooting

The following means can be used to troubleshoot the reason why a pod could not be deployed or
became rejected at a node:
-->
<h2 id="troubleshooting">故障排查  </h2>
<p>下面的方法可用来排查为什么 Pod 无法被调度或者被节点拒绝：</p>
<!--
- pod status - indicates topology affinity errors
- system logs - include valuable information for debugging, e.g., about generated hints
- state file - the dump of internal state of the Memory Manager
  (includes [Node Map and Memory Maps][2]) 
- starting from v1.22, the [device plugin resource API](#device-plugin-resource-api) can be used
  to retrieve information about the memory reserved for containers
-->
<ul>
<li>Pod 状态 - 可表明拓扑亲和性错误</li>
<li>系统日志 - 包含用来调试的有价值的信息，例如，关于所生成的提示信息</li>
<li>状态文件 - 其中包含内存管理器内部状态的转储（包含<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">节点映射和内存映射</a>）</li>
<li>从 v1.22 开始，<a href="#device-plugin-resource-api">设备插件资源 API</a> 可以用来
检索关于为容器预留的内存的信息</li>
</ul>
<!--
### Pod status (TopologyAffinityError) {#TopologyAffinityError}

This error typically occurs in the following situations:

* a node has not enough resources available to satisfy the pod's request
* the pod's request is rejected due to particular Topology Manager policy constraints 

The error appears in the status of a pod:
-->
<h3 id="TopologyAffinityError">Pod 状态 （TopologyAffinityError）  </h3>
<p>这类错误通常在以下情形出现：</p>
<ul>
<li>节点缺少足够的资源来满足 Pod 请求</li>
<li>Pod 的请求因为特定的拓扑管理器策略限制而被拒绝</li>
</ul>
<p>错误信息会出现在 Pod 的状态中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><pre><code class="language-none" data-lang="none">NAME         READY   STATUS                  RESTARTS   AGE
guaranteed   0/1     TopologyAffinityError   0          113s
</code></pre><!--
Use `kubectl describe pod <id>` or `kubectl get events` to obtain detailed error message:
-->
<p>使用 <code>kubectl describe pod &lt;id&gt;</code> 或 <code>kubectl get events</code> 可以获得详细的错误信息。</p>
<pre><code class="language-none" data-lang="none">Warning  TopologyAffinityError  10m   kubelet, dell8  Resources cannot be allocated with Topology locality
</code></pre><!--
### System logs

Search system logs with respect to a particular pod.

The set of hints that Memory Manager generated for the pod can be found in the logs. 
Also, the set of hints generated by CPU Manager should be present in the logs.
-->
<h3 id="system-logs">系统日志    </h3>
<p>针对特定的 Pod 搜索系统日志。</p>
<p>内存管理器为 Pod 所生成的提示信息可以在日志中找到。
此外，日志中应该也存在 CPU 管理器所生成的提示信息。</p>
<!--
Topology Manager merges these hints to calculate a single best hint.
The best hint should be also present in the logs.

The best hint indicates where to allocate all the resources.
Topology Manager tests this hint against its current policy, and based on the verdict,
it either admits the pod to the node or rejects it.  

Also, search the logs for occurrences associated with the Memory Manager,
e.g. to find out information about `cgroups` and `cpuset.mems` updates.
-->
<p>拓扑管理器将这些提示信息进行合并，计算得到唯一的最合适的提示数据。
此最佳提示数据也应该出现在日志中。</p>
<p>最佳提示表明要在哪里分配所有的资源。拓扑管理器会用当前的策略来测试此数据，
并基于得出的结论或者接纳 Pod 到节点，或者将其拒绝。</p>
<p>此外，你可以搜索日志查找与内存管理器相关的其他条目，例如 <code>cgroups</code> 和
<code>cpuset.mems</code> 的更新信息等。</p>
<!--
### Examine the memory manager state on a node

Let us first deploy a sample `Guaranteed` pod whose specification is as follows:
-->
<h3 id="检查节点上内存管理器状态">检查节点上内存管理器状态</h3>
<p>我们首先部署一个 <code>Guaranteed</code> Pod 示例，其规约如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>guaranteed<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>guaranteed<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>consumer<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>150Gi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>150Gi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;sleep&#34;</span>,<span style="color:#b44">&#34;infinity&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
Next, let us log into the node where it was deployed and examine the state file in
`/var/lib/kubelet/memory_manager_state`:
-->
<p>接下来，我们登录到 Pod 运行所在的节点，检查位于
<code>/var/lib/kubelet/memory_manager_state</code> 的状态文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#008000;font-weight:bold">&#34;policyName&#34;</span>:<span style="color:#b44">&#34;Static&#34;</span>,
   <span style="color:#008000;font-weight:bold">&#34;machineState&#34;</span>:{
      <span style="color:#008000;font-weight:bold">&#34;0&#34;</span>:{
         <span style="color:#008000;font-weight:bold">&#34;numberOfAssignments&#34;</span>:<span style="color:#666">1</span>,
         <span style="color:#008000;font-weight:bold">&#34;memoryMap&#34;</span>:{
            <span style="color:#008000;font-weight:bold">&#34;hugepages-1Gi&#34;</span>:{
               <span style="color:#008000;font-weight:bold">&#34;total&#34;</span>:<span style="color:#666">0</span>,
               <span style="color:#008000;font-weight:bold">&#34;systemReserved&#34;</span>:<span style="color:#666">0</span>,
               <span style="color:#008000;font-weight:bold">&#34;allocatable&#34;</span>:<span style="color:#666">0</span>,
               <span style="color:#008000;font-weight:bold">&#34;reserved&#34;</span>:<span style="color:#666">0</span>,
               <span style="color:#008000;font-weight:bold">&#34;free&#34;</span>:<span style="color:#666">0</span>
            },
            <span style="color:#008000;font-weight:bold">&#34;memory&#34;</span>:{
               <span style="color:#008000;font-weight:bold">&#34;total&#34;</span>:<span style="color:#666">134987354112</span>,
               <span style="color:#008000;font-weight:bold">&#34;systemReserved&#34;</span>:<span style="color:#666">3221225472</span>,
               <span style="color:#008000;font-weight:bold">&#34;allocatable&#34;</span>:<span style="color:#666">131766128640</span>,
               <span style="color:#008000;font-weight:bold">&#34;reserved&#34;</span>:<span style="color:#666">131766128640</span>,
               <span style="color:#008000;font-weight:bold">&#34;free&#34;</span>:<span style="color:#666">0</span>
            }
         },
         <span style="color:#008000;font-weight:bold">&#34;nodes&#34;</span>:[
            <span style="color:#666">0</span>,
            <span style="color:#666">1</span>
         ]
      },
      <span style="color:#008000;font-weight:bold">&#34;1&#34;</span>:{
         <span style="color:#008000;font-weight:bold">&#34;numberOfAssignments&#34;</span>:<span style="color:#666">1</span>,
         <span style="color:#008000;font-weight:bold">&#34;memoryMap&#34;</span>:{
            <span style="color:#008000;font-weight:bold">&#34;hugepages-1Gi&#34;</span>:{
               <span style="color:#008000;font-weight:bold">&#34;total&#34;</span>:<span style="color:#666">0</span>,
               <span style="color:#008000;font-weight:bold">&#34;systemReserved&#34;</span>:<span style="color:#666">0</span>,
               <span style="color:#008000;font-weight:bold">&#34;allocatable&#34;</span>:<span style="color:#666">0</span>,
               <span style="color:#008000;font-weight:bold">&#34;reserved&#34;</span>:<span style="color:#666">0</span>,
               <span style="color:#008000;font-weight:bold">&#34;free&#34;</span>:<span style="color:#666">0</span>
            },
            <span style="color:#008000;font-weight:bold">&#34;memory&#34;</span>:{
               <span style="color:#008000;font-weight:bold">&#34;total&#34;</span>:<span style="color:#666">135286722560</span>,
               <span style="color:#008000;font-weight:bold">&#34;systemReserved&#34;</span>:<span style="color:#666">2252341248</span>,
               <span style="color:#008000;font-weight:bold">&#34;allocatable&#34;</span>:<span style="color:#666">133034381312</span>,
               <span style="color:#008000;font-weight:bold">&#34;reserved&#34;</span>:<span style="color:#666">29295144960</span>,
               <span style="color:#008000;font-weight:bold">&#34;free&#34;</span>:<span style="color:#666">103739236352</span>
            }
         },
         <span style="color:#008000;font-weight:bold">&#34;nodes&#34;</span>:[
            <span style="color:#666">0</span>,
            <span style="color:#666">1</span>
         ]
      }
   },
   <span style="color:#008000;font-weight:bold">&#34;entries&#34;</span>:{
      <span style="color:#008000;font-weight:bold">&#34;fa9bdd38-6df9-4cf9-aa67-8c4814da37a8&#34;</span>:{
         <span style="color:#008000;font-weight:bold">&#34;guaranteed&#34;</span>:[
            {
               <span style="color:#008000;font-weight:bold">&#34;numaAffinity&#34;</span>:[
                  <span style="color:#666">0</span>,
                  <span style="color:#666">1</span>
               ],
               <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>:<span style="color:#b44">&#34;memory&#34;</span>,
               <span style="color:#008000;font-weight:bold">&#34;size&#34;</span>:<span style="color:#666">161061273600</span>
            }
         ]
      }
   },
   <span style="color:#008000;font-weight:bold">&#34;checksum&#34;</span>:<span style="color:#666">4142013182</span>
}
</code></pre></div><!--
It can be deduced from the state file that the pod was pinned to both NUMA nodes, i.e.:
-->
<p>从这个状态文件，可以推断 Pod 被同时绑定到两个 NUMA 节点，即：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#b44">&#34;numaAffinity&#34;</span><span style="">:</span>[
   <span style="color:#666">0</span>,
   <span style="color:#666">1</span>
]<span style="">,</span>
</code></pre></div><!--
Pinned term means that pod's memory consumption is constrained (through `cgroups` configuration)
to these NUMA nodes.

This automatically implies that Memory Manager instantiated a new group that
comprises these two NUMA nodes, i.e. `0` and `1` indexed NUMA nodes. 
-->
<p>术语绑定（pinned）意味着 Pod 的内存使用被（通过 <code>cgroups</code> 配置）限制到
这些 NUMA 节点。</p>
<p>这也直接意味着内存管理器已经创建了一个 NUMA 分组，由这两个 NUMA 节点组成，
即索引值分别为 <code>0</code> 和 <code>1</code> 的 NUMA 节点。</p>
<!--
Notice that the management of groups is handled in a relatively complex manner, and
further elaboration is provided in Memory Manager KEP in [this][1] and [this][3] sections.

In order to analyse memory resources available in a group,the corresponding entries from
NUMA nodes belonging to the group must be added up.  
-->
<p>注意 NUMA 分组的管理是有一个相对复杂的管理器处理的，相关逻辑的进一步细节可在内存管理器的
KEP 中<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples">示例1</a>和<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes">跨 NUMA 节点</a>节找到。</p>
<p>为了分析 NUMA 组中可用的内存资源，必须对分组内 NUMA 节点对应的条目进行汇总。</p>
<!--
For example, the total amount of free "conventional" memory in the group can be computed
by adding up the free memory available at every NUMA node in the group,
i.e., in the `"memory"` section of NUMA node `0` (`"free":0`) and NUMA node `1` (`"free":103739236352`).
So, the total amount of free "conventional" memory in this group is equal to `0 + 103739236352` bytes.
-->
<p>例如，NUMA 分组中空闲的“常规”内存的总量可以通过将分组内所有 NUMA
节点上空闲内存加和来计算，即将 NUMA 节点 <code>0</code> 和 NUMA 节点 <code>1</code>  的 <code>&quot;memory&quot;</code> 节
（分别是 <code>&quot;free&quot;:0</code> 和 <code>&quot;free&quot;: 103739236352</code>）相加，得到此分组中空闲的“常规”
内存总量为 <code>0 + 103739236352</code> 字节。</p>
<!--
The line `"systemReserved":3221225472` indicates that the administrator of this node reserved
`3221225472` bytes (i.e. `3Gi`) to serve kubelet and system processes at NUMA node `0`,
by using `--reserved-memory` flag.
-->
<p><code>&quot;systemReserved&quot;: 3221225472</code> 这一行表明节点的管理员使用 <code>--reserved-memory</code> 为 NUMA
节点 <code>0</code> 上运行的 kubelet 和系统进程预留了 <code>3221225472</code> 字节 （即 <code>3Gi</code>）。</p>
<!--
### Device plugin resource API

By employing the [API](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/),
the information about reserved memory for each container can be retrieved, which is contained
in protobuf `ContainerMemory` message.
This information can be retrieved solely for pods in Guaranteed QoS class.   
-->
<h3 id="device-plugin-resource-api">设备插件资源 API    </h3>
<p>通过使用此 <a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">API</a>，
可以获得每个容器的预留内存信息，该信息位于 protobuf 协议的 <code>ContainerMemory</code> 消息中。
只能针对 Guaranteed QoS 类中的 Pod 来检索此信息。</p>
<h2 id="what-s-next">What's next</h2>
<!--
以下均为英文设计文档，因此其标题不翻译。
-->
<ul>
<li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#design-overview">Memory Manager KEP: Design Overview</a></li>
<li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-start-up-with-examples">Memory Manager KEP: Memory Maps at start-up (with examples)</a></li>
<li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#memory-maps-at-runtime-with-examples">Memory Manager KEP: Memory Maps at runtime (with examples)</a></li>
<li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#simulation---how-the-memory-manager-works-by-examples">Memory Manager KEP: Simulation - how the Memory Manager works? (by examples)</a></li>
<li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#the-concept-of-node-map-and-memory-maps">Memory Manager KEP: The Concept of Node Map and Memory Maps</a></li>
<li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1769-memory-manager#how-to-enable-the-guaranteed-memory-allocation-over-many-numa-nodes">Memory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-12001be83d15fcd7f3242313a55777df">16 - 保护集群</h1>
    
	<!--
reviewers:
- smarterclayton
- liggitt
- ericchiang
- destijl
title: Securing a Cluster
content_type: task
-->
<!-- overview -->
<!--
This document covers topics related to protecting a cluster from accidental or malicious access
and provides recommendations on overall security.
-->
<p>本文档涉及与保护集群免受意外或恶意访问有关的主题，并对总体安全性提出建议。</p>
<h2 id="before-you-begin">Before you begin</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</li>
</ul>
<!-- steps -->
<!--
## Controlling access to the Kubernetes API

As Kubernetes is entirely API-driven, controlling and limiting who can access the cluster and what actions
they are allowed to perform is the first line of defense.
-->
<h2 id="控制对-kubernetes-api-的访问">控制对 Kubernetes API 的访问</h2>
<p>因为 Kubernetes 是完全通过 API 驱动的，所以，控制和限制谁可以通过 API 访问集群，
以及允许这些访问者执行什么样的 API 动作，就成为了安全控制的第一道防线。</p>
<!--
### Use Transport Layer Security (TLS) for all API traffic

Kubernetes expects that all API communication in the cluster is encrypted by default with TLS, and the
majority of installation methods will allow the necessary certificates to be created and distributed to
the cluster components. Note that some components and installation methods may enable local ports over
HTTP and administrators should familiarize themselves with the settings of each component to identify
potentially unsecured traffic.
-->
<h3 id="为所有-api-交互使用传输层安全-tls">为所有 API 交互使用传输层安全 （TLS）</h3>
<p>Kubernetes 期望集群中所有的 API 通信在默认情况下都使用 TLS 加密，
大多数安装方法也允许创建所需的证书并且分发到集群组件中。
请注意，某些组件和安装方法可能使用 HTTP 来访问本地端口，
管理员应该熟悉每个组件的设置，以识别可能不安全的流量。</p>
<!--
### API Authentication

Choose an authentication mechanism for the API servers to use that matches the common access patterns
when you install a cluster. For instance, small single-user clusters may wish to use a simple certificate
or static Bearer token approach. Larger clusters may wish to integrate an existing OIDC or LDAP server that
allow users to be subdivided into groups.

All API clients must be authenticated, even those that are part of the infrastructure like nodes,
proxies, the scheduler, and volume plugins. These clients are typically [service accounts](/docs/reference/access-authn-authz/service-accounts-admin/) or use x509 client certificates, and they are created automatically at cluster startup or are setup as part of the cluster installation.

Consult the [authentication reference document](/docs/reference/access-authn-authz/authentication/) for more information.
-->
<h3 id="api-认证">API 认证</h3>
<p>安装集群时，选择一个 API 服务器的身份验证机制，去使用与之匹配的公共访问模式。
例如，小型的单用户集群可能希望使用简单的证书或静态承载令牌方法。
更大的集群则可能希望整合现有的、OIDC、LDAP 等允许用户分组的服务器。</p>
<p>所有 API 客户端都必须经过身份验证，即使它是基础设施的一部分，比如节点、代理、调度程序和卷插件。
这些客户端通常使用 <a href="/zh/docs/reference/access-authn-authz/service-accounts-admin/">服务帐户</a>
或 X509 客户端证书，并在集群启动时自动创建或是作为集群安装的一部分进行设置。</p>
<p>如果你希望获取更多信息，请参考<a href="/zh/docs/reference/access-authn-authz/authentication/">认证参考文档</a>。</p>
<!--
### API Authorization

Once authenticated, every API call is also expected to pass an authorization check. Kubernetes ships
an integrated [Role-Based Access Control (RBAC)](/docs/reference/access-authn-authz/rbac/) component that matches an incoming user or group to a
set of permissions bundled into roles. These permissions combine verbs (get, create, delete) with
resources (pods, services, nodes) and can be namespace-scoped or cluster-scoped. A set of out-of-the-box
roles are provided that offer reasonable default separation of responsibility depending on what
actions a client might want to perform. It is recommended that you use the [Node](/docs/reference/access-authn-authz/node/) and [RBAC](/docs/reference/access-authn-authz/rbac/) authorizers together, in combination with the
[NodeRestriction](/docs/reference/access-authn-authz/admission-controllers/#noderestriction) admission plugin.
-->
<h3 id="api-授权">API 授权</h3>
<p>一旦通过身份认证，每个 API 的调用都将通过鉴权检查。
Kubernetes 集成<a href="/zh/docs/reference/access-authn-authz/rbac/">基于角色的访问控制（RBAC）</a>组件，
将传入的用户或组与一组绑定到角色的权限匹配。
这些权限将动作（get、create、delete）和资源（Pod、Service、Node）进行组合，并可在名字空间或者集群范围生效。
Kubernetes 提供了一组可直接使用的角色，这些角色根据客户可能希望执行的操作提供合理的责任划分。
建议你同时使用 <a href="/zh/docs/reference/access-authn-authz/node/">Node</a> 和
<a href="/zh/docs/reference/access-authn-authz/rbac/">RBAC</a> 两个鉴权组件，再与
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction</a>
准入插件结合使用。</p>
<!--
As with authentication, simple and broad roles may be appropriate for smaller clusters, but as
more users interact with the cluster, it may become necessary to separate teams into separate
<a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespaces'>namespaces</a> with more limited roles.
-->
<p>与身份验证一样，简单而广泛的角色可能适合于较小的集群，但是随着更多的用户与集群交互，
可能需要将团队划分到有更多角色限制的、
单独的<a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>中去。</p>
<!--
With authorization, it is important to understand how updates on one object may cause actions in
other places. For instance, a user may not be able to create pods directly, but allowing them to
create a deployment, which creates pods on their behalf, will let them create those pods
indirectly. Likewise, deleting a node from the API will result in the pods scheduled to that node
being terminated and recreated on other nodes. The out-of-the-box roles represent a balance
between flexibility and common use cases, but more limited roles should be carefully reviewed
to prevent accidental escalation. You can make roles specific to your use case if the out-of-box ones don't meet your needs.

Consult the [authorization reference section](/docs/reference/access-authn-authz/authorization/) for more information.
-->
<p>就鉴权而言，很重要的一点是理解对象上的更新操作如何导致在其它地方发生对应行为。
例如，用户可能不能直接创建 Pod，但允许他们通过创建 Deployment 来创建这些 Pod，
这将让他们间接创建这些 Pod。
同样地，从 API 删除一个节点将导致调度到这些节点上的 Pod 被中止，并在其他节点上重新创建。
原生的角色设计代表了灵活性和常见用例之间的平衡，但须限制的角色应该被仔细审查，
以防止意外的权限升级。如果内置的角色无法满足你的需求，则可以根据使用场景需要创建特定的角色。</p>
<p>如果你希望获取更多信息，请参阅<a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权参考</a>。</p>
<!--
## Controlling access to the Kubelet

Kubelets expose HTTPS endpoints which grant powerful control over the node and containers. By default Kubelets allow unauthenticated access to this API.

Production clusters should enable Kubelet authentication and authorization.

Consult the [Kubelet authentication/authorization reference](/docs/admin/kubelet-authentication-authorization) for more information.
-->
<h2 id="控制对-kubelet-的访问">控制对 Kubelet 的访问</h2>
<p>Kubelet 公开 HTTPS 端点，这些端点提供了对节点和容器的强大的控制能力。
默认情况下，Kubelet 允许对此 API 进行未经身份验证的访问。</p>
<p>生产级别的集群应启用 Kubelet 身份认证和授权。</p>
<p>进一步的信息，请参考
<a href="/zh/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/">Kubelet 身份验证/授权参考</a>。</p>
<!--
## Controlling the capabilities of a workload or user at runtime

Authorization in Kubernetes is intentionally high level, focused on coarse actions on resources.
More powerful controls exist as **policies** to limit by use case how those objects act on the
cluster, themselves, and other resources.
-->
<h2 id="控制运行时负载或用户的能力">控制运行时负载或用户的能力</h2>
<p>Kubernetes 中的授权故意设计成较高抽象级别，侧重于对资源的粗粒度行为。
更强大的控制是 <strong>策略</strong> 的形式呈现的，根据使用场景限制这些对象如何作用于集群、自身和其他资源。</p>
<!--
### Limiting resource usage on a cluster

[Resource quota](/docs/concepts/policy/resource-quotas/) limits the number or capacity of
resources granted to a namespace. This is most often used to limit the amount of CPU, memory,
or persistent disk a namespace can allocate, but can also control how many pods, services, or
volumes exist in each namespace.

[Limit ranges](/docs/tasks/administer-cluster/memory-default-namespace/) restrict the maximum or minimum size of some of the
resources above, to prevent users from requesting unreasonably high or low values for commonly
reserved resources like memory, or to provide default limits when none are specified.
-->
<h3 id="限制集群上的资源使用">限制集群上的资源使用</h3>
<p><a href="/zh/docs/concepts/policy/resource-quotas/">资源配额（Resource Quota）</a>限制了赋予命名空间的资源的数量或容量。
资源配额通常用于限制名字空间可以分配的 CPU、内存或持久磁盘的数量，
但也可以控制每个名字空间中存在多少个 Pod、Service 或 Volume。</p>
<p><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">限制范围（Limit Range）</a>
限制上述某些资源的最大值或者最小值，以防止用户使用类似内存这样的通用保留资源时请求不合理的过高或过低的值，
或者在没有指定的情况下提供默认限制。</p>
<!--
### Controlling what privileges containers run with

A pod definition contains a [security context](/docs/tasks/configure-pod-container/security-context/)
that allows it to request access to run as a specific Linux user on a node (like root),
access to run privileged or access the host network, and other controls that would otherwise
allow it to run unfettered on a hosting node.

You can configure [Pod security admission](/docs/concepts/security/pod-security-admission/)
to enforce use of a particular [Pod Security Standard](/docs/concepts/security/pod-security-standards/)
in a <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespace'>namespace</a>, or to detect breaches.
-->
<h3 id="控制容器运行的特权">控制容器运行的特权</h3>
<p>Pod 定义包含了一个<a href="/zh/docs/tasks/configure-pod-container/security-context/">安全上下文</a>，
用于描述一些访问请求，如以某个节点上的特定 Linux 用户（如 root）身份运行，
以特权形式运行，访问主机网络，以及一些在宿主节点上不受约束地运行的其它控制权限等等。</p>
<p>你可以配置 <a href="/zh/docs/concepts/security/pod-security-admission/">Pod 安全准入</a>来在某个
<a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>中
强制实施特定的
<a href="/zh/docs/concepts/security/pod-security-standards/">Pod 安全标准（Pod Security Standard）</a>，
或者检查安全上的缺陷。</p>
<!--
Generally, most application workloads need limited access to host resources so they can
successfully run as a root process (uid 0) without access to host information. However,
considering the privileges associated with the root user, you should write application
containers to run as a non-root user. Similarly, administrators who wish to prevent
client applications from escaping their containers should apply the **Baseline**
or **Restricted** Pod Security Standard.
-->
<p>一般来说，大多数应用程序需要对主机资源的有限制的访问，
这样它们可以在不访问主机信息的情况下，成功地以 root 账号（UID 0）运行。
但是，考虑到与 root 用户相关的特权，在编写应用程序容器时，你应该使用非 root 用户运行。
类似地，希望阻止客户端应用程序从其容器中逃逸的管理员，应该应用 <strong>Baseline</strong>
或 <strong>Restricted</strong> Pod 安全标准。</p>
<!--
### Restricting network access

The [network policies](/docs/tasks/administer-cluster/declare-network-policy/) for a namespace
allows application authors to restrict which pods in other namespaces may access pods and ports
within their namespaces. Many of the supported [Kubernetes networking providers](/docs/concepts/cluster-administration/networking/)
now respect network policy.
-->
<h3 id="限制网络访问">限制网络访问</h3>
<p>基于名字空间的<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">网络策略</a>
允许应用程序作者限制其它名字空间中的哪些 Pod 可以访问自身名字空间内的 Pod 和端口。
现在已经有许多支持网络策略的
<a href="/zh/docs/concepts/cluster-administration/networking/">Kubernetes 网络驱动</a>。</p>
<!--
Quota and limit ranges can also be used to control whether users may request node ports or
load-balanced services, which on many clusters can control whether those users applications
are visible outside of the cluster.

Additional protections may be available that control network rules on a per-plugin or
per-environment basis, such as per-node firewalls, physically separating cluster nodes to
prevent cross talk, or advanced networking policy.
-->
<p>配额（Quota）和限制范围（Limit Range）也可用于控制用户是否可以请求节点端口或负载均衡服务。
在很多集群上，节点端口和负载均衡服务也可控制用户的应用程序是否在集群之外可见。</p>
<p>此外也可能存在一些基于插件或基于环境的网络规则，能够提供额外的保护能力。
例如各节点上的防火墙、物理隔离群集节点以防止串扰或者高级的网络策略等。</p>
<!--
### Restricting cloud metadata API access

Cloud platforms (AWS, Azure, GCE, etc.) often expose metadata services locally to instances.
By default these APIs are accessible by pods running on an instance and can contain cloud
credentials for that node, or provisioning data such as kubelet credentials. These credentials
can be used to escalate within the cluster or to other cloud services under the same account.

When running Kubernetes on a cloud platform, limit permissions given to instance credentials, use
[network policies](/docs/tasks/administer-cluster/declare-network-policy/) to restrict pod access
to the metadata API, and avoid using provisioning data to deliver secrets.
-->
<h3 id="限制云元数据-api-访问">限制云元数据 API 访问</h3>
<p>云平台（AWS,  Azure, GCE 等）经常将 metadata 本地服务暴露给实例。
默认情况下，这些 API 可由运行在实例上的 Pod 访问，并且可以包含
该云节点的凭据或配置数据（如 kubelet 凭据）。
这些凭据可以用于在集群内升级或在同一账户下升级到其他云服务。</p>
<p>在云平台上运行 Kubernetes 时，需要限制对实例凭据的权限，使用
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">网络策略</a>
限制 Pod 对元数据 API 的访问，并避免使用配置数据来传递机密信息。</p>
<!--
### Controlling which nodes pods may access

By default, there are no restrictions on which nodes may run a pod.  Kubernetes offers a
[rich set of policies for controlling placement of pods onto nodes](/docs/concepts/configuration/assign-pod-node/)
and the [taint-based pod placement and eviction](/docs/concepts/configuration/taint-and-toleration/)
that are available to end users. For many clusters use of these policies to separate workloads
can be a convention that authors adopt or enforce via tooling.

As an administrator, a beta admission plugin `PodNodeSelector` can be used to force pods
within a namespace to default or require a specific node selector, and if end users cannot
alter namespaces, this can strongly limit the placement of all of the pods in a specific workload.
-->
<h3 id="控制-pod-可以访问的节点">控制 Pod 可以访问的节点</h3>
<p>默认情况下，对 Pod 可以运行在哪些节点上是没有任何限制的。
Kubernetes 给最终用户提供了
一组丰富的策略用于<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">控制 Pod 所放置的节点位置</a>，
以及<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">基于污点的 Pod 放置和驱逐</a>。
对于许多集群，使用这些策略来分离工作负载可以作为一种约定，要求作者遵守或者通过工具强制。</p>
<p>对于管理员，Beta 阶段的准入插件 <code>PodNodeSelector</code> 可用于强制某名字空间中的 Pod
使用默认的或特定的节点选择算符。
如果最终用户无法改变名字空间，这一机制可以有效地限制特定工作负载中所有 Pod 的放置位置。</p>
<!--
## Protecting cluster components from compromise

This section describes some common patterns for protecting clusters from compromise.
-->
<h2 id="保护集群组件免受破坏">保护集群组件免受破坏</h2>
<p>本节描述保护集群免受破坏的一些常用模式。</p>
<!--
### Restrict access to etcd

Write access to the etcd backend for the API is equivalent to gaining root on the entire cluster,
and read access can be used to escalate fairly quickly. Administrators should always use strong
credentials from the API servers to their etcd server, such as mutual auth via TLS client certificates,
and it is often recommended to isolate the etcd servers behind a firewall that only the API servers
may access.

<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> Allowing other components within the cluster to access the master etcd instance with
read or write access to the full keyspace is equivalent to granting cluster-admin access. Using
separate etcd instances for non-master components or using etcd ACLs to restrict read and write
access to a subset of the keyspace is strongly recommended.
</div>

-->
<h3 id="限制访问-etcd">限制访问 etcd</h3>
<p>拥有对 API 的 etcd 后端的写访问权限相当于获得了整个集群的 root 权限，
读访问权限也可能被利用，实现相当快速的权限提升。
对于从 API 服务器访问其 etcd 服务器，管理员应该总是使用比较强的凭证，如通过 TLS
客户端证书来实现双向认证。
通常，我们建议将 etcd 服务器隔离到只有 API 服务器可以访问的防火墙后面。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 允许集群中其它组件对整个主键空间（keyspace）拥有读或写权限去访问 etcd 实例，
相当于授予这些组件群集管理员的访问权限。
对于非主控组件，强烈推荐使用不同的 etcd 实例，或者使用 etcd 的访问控制列表
来限制这些组件只能读或写主键空间的一个子集。
</div>

<!--
### Enable audit logging

The [audit logger](/docs/tasks/debug-application-cluster/audit/) is a beta feature that records actions taken by the
API for later analysis in the event of a compromise. It is recommended to enable audit logging
and archive the audit file on a secure server.
-->
<h3 id="启用审计日志">启用审计日志</h3>
<p><a href="/zh/docs/tasks/debug-application-cluster/audit/">审计日志</a>是 Beta 特性，
负责记录 API 操作以便在发生破坏时进行事后分析。
建议启用审计日志，并将审计文件归档到安全服务器上。</p>
<!--
### Restrict access to alpha or beta features

Alpha and beta Kubernetes features are in active development and may have limitations or bugs
that result in security vulnerabilities. Always assess the value an alpha or beta feature may
provide against the possible risk to your security posture. When in doubt, disable features you
do not use.
-->
<h3 id="限制使用-alpha-和-beta-特性">限制使用 alpha 和 beta 特性</h3>
<p>Kubernetes 的 alpha 和 beta 特性还在努力开发中，可能存在导致安全漏洞的缺陷或错误。
要始终评估 alpha 和 beta 特性可能给你的安全态势带来的风险。
当你怀疑存在风险时，可以禁用那些不需要使用的特性。</p>
<!--
### Rotate infrastructure credentials frequently

The shorter the lifetime of a secret or credential the harder it is for an attacker to make
use of that credential. Set short lifetimes on certificates and automate their rotation. Use
an authentication provider that can control how long issued tokens are available and use short
lifetimes where possible. If you use service-account tokens in external integrations, plan to
rotate those tokens frequently. For example, once the bootstrap phase is complete, a bootstrap token used for setting up nodes should be revoked or its authorization removed.
-->
<h3 id="经常轮换基础设施证书">经常轮换基础设施证书</h3>
<p>一项机密信息或凭据的生命期越短，攻击者就越难使用该凭据。
在证书上设置较短的生命期并实现自动轮换是控制安全的一个好方法。
使用身份验证提供程序时，应该使用那些可以控制所发布令牌的合法时长的提供程序，
并尽可能设置较短的生命期。
如果在外部集成场景中使用服务帐户令牌，则应该经常性地轮换这些令牌。
例如，一旦引导阶段完成，就应该撤销用于配置节点的引导令牌，或者取消它的授权。</p>
<!--
### Review third party integrations before enabling them

Many third party integrations to Kubernetes may alter the security profile of your cluster. When
enabling an integration, always review the permissions that an extension requests before granting
it access. For example, many security integrations may request access to view all secrets on
your cluster which is effectively making that component a cluster admin. When in doubt,
restrict the integration to functioning in a single namespace if possible.

Components that create pods may also be unexpectedly powerful if they can do so inside namespaces
like the `kube-system` namespace, because those pods can gain access to service account secrets
or run with elevated permissions if those service accounts are granted access to permissive
[PodSecurityPolicies](/docs/concepts/security/pod-security-policy/).
-->
<h3 id="在启用第三方集成之前-请先审查它们">在启用第三方集成之前，请先审查它们</h3>
<p>许多集成到 Kubernetes 的第三方软件或服务都可能改变你的集群的安全配置。
启用集成时，在授予访问权限之前，你应该始终检查扩展所请求的权限。
例如，许多安全性集成中可能要求查看集群上的所有 Secret 的访问权限，
本质上该组件便成为了集群的管理员。
当有疑问时，如果可能的话，将要集成的组件限制在某指定名字空间中运行。</p>
<p>如果执行 Pod 创建操作的组件能够在 <code>kube-system</code> 这类名字空间中创建 Pod，
则这类组件也可能获得意外的权限，因为这些 Pod 可以访问服务账户的 Secret，
或者，如果对应服务帐户被授权访问宽松的
<a href="/zh/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy</a>，
它们就能以较高的权限运行。</p>
<!--
If you use [Pod Security admission](/docs/concepts/security/pod-security-admission/) and allow
any component to create Pods within a namespace that permits privileged Pods, those Pods may
be able to escape their containers and use this widened access to elevate their privileges.
-->
<p>如果你使用 <a href="/zh/docs/concepts/security/pod-security-admission/">Pod 安全准入</a>，
并且允许任何组件在一个允许执行特权 Pod 的名字空间中创建 Pod，这些 Pod
就可能从所在的容器中逃逸，利用被拓宽的访问权限来实现特权提升。</p>
<!--
You should not allow untrusted components to create Pods in any system namespace (those with
names that start with `kube-`) nor in any namespace where that access grant allows the possibility
of privilege escalation.
-->
<p>你不应该允许不可信的组件在任何系统名字空间（名字以 <code>kube-</code> 开头）中创建 Pod，
也不允许它们在访问权限授权可被利用来提升特权的名字空间中创建 Pod。</p>
<!--
### Encrypt secrets at rest

In general, the etcd database will contain any information accessible via the Kubernetes API
and may grant an attacker significant visibility into the state of your cluster. Always encrypt
your backups using a well reviewed backup and encryption solution, and consider using full disk
encryption where possible.

Kubernetes supports [encryption at rest](/docs/tasks/administer-cluster/encrypt-data/), a feature 
introduced in 1.7, and beta since 1.13. This will encrypt `Secret` resources in etcd, preventing
parties that gain access to your etcd backups from viewing the content of those secrets. While
this feature is currently beta, it offers an additional level of defense when backups
are not encrypted or an attacker gains read access to etcd.
-->
<h3 id="对-secret-进行静态加密">对 Secret 进行静态加密</h3>
<p>一般情况下，etcd 数据库包含了通过 Kubernetes API 可以访问到的所有信息，
并且可能为攻击者提供对你的集群的状态的较多的可见性。
你要始终使用经过充分审查的备份和加密方案来加密备份数据，
并考虑在可能的情况下使用全盘加密。</p>
<p>Kubernetes 支持<a href="/zh/docs/tasks/administer-cluster/encrypt-data/">静态数据加密</a>。
该功能在 1.7 版本引入，并在 1.13 版本成为 Beta。
它会加密 etcd 里面的 <code>Secret</code> 资源，以防止某一方通过查看 etcd 的备份文件查看到这些
Secret 的内容。虽然目前该功能还只是 Beta 阶段，
在备份未被加密或者攻击者获取到 etcd 的读访问权限时，它仍能提供额外的防御层级。</p>
<!--
### Receiving alerts for security updates and reporting vulnerabilities

Join the [kubernetes-announce](https://groups.google.com/forum/#!forum/kubernetes-announce)
group for emails about security announcements. See the [security reporting](/security/)
page for more on how to report vulnerabilities.
-->
<h3 id="接收安全更新和报告漏洞的警报">接收安全更新和报告漏洞的警报</h3>
<p>请加入 <a href="https://groups.google.com/forum/#!forum/kubernetes-announce">kubernetes-announce</a>
组，这样你就能够收到有关安全公告的邮件。有关如何报告漏洞的更多信息，
请参见<a href="/zh/docs/reference/issues-security/security/">安全报告</a>页面。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4a02bcca41439e16655f43fa37c81da4">17 - 关键插件 Pod 的调度保证</h1>
    
	<!-- overview -->
<!-- 
Kubernetes core components such as the API server, scheduler, and controller-manager run on a control plane node. 
However, add-ons must run on a regular cluster node.
Some of these add-ons are critical to a fully functional cluster, such as metrics-server, DNS, and UI.
A cluster may stop working properly if a critical add-on is evicted (either manually or as a side effect of another operation like upgrade)
and becomes pending (for example when the cluster is highly utilized and either there are other pending pods that schedule into the space
vacated by the evicted critical add-on pod or the amount of resources available on the node changed for some other reason).
-->
<p>Kubernetes 核心组件（如 API 服务器、调度器、控制器管理器）在控制平面节点上运行。
但是插件必须在常规集群节点上运行。
其中一些插件对于功能完备的群集至关重要，例如 Heapster、DNS 和 UI。
如果关键插件被逐出（手动或作为升级等其他操作的副作用）或者变成挂起状态，群集可能会停止正常工作。
关键插件进入挂起状态的例子有：集群利用率过高；被逐出的关键插件 Pod 释放了空间，但该空间被之前悬决的 Pod 占用；由于其它原因导致节点上可用资源的总量发生变化。</p>
<!--
Note that marking a pod as critical is not meant to prevent evictions entirely; it only prevents the pod from becoming permanently unavailable.
A static pod marked as critical, can't be evicted. However, a non-static pods marked as critical are always rescheduled.
-->
<p>注意，把某个 Pod 标记为关键 Pod 并不意味着完全避免该 Pod 被逐出；它只能防止该 Pod 变成永久不可用。
被标记为关键性的静态 Pod 不会被逐出。但是，被标记为关键性的非静态 Pod 总是会被重新调度。</p>
<!-- body -->
<!--
### Marking pod as critical
-->
<h3 id="标记关键-pod">标记关键 Pod</h3>
<!--
To mark a Pod as critical, set priorityClassName for that Pod to `system-cluster-critical` or `system-node-critical`. `system-node-critical` is the highest available priority, even higher than `system-cluster-critical`
-->
<p>要将 Pod 标记为关键性（critical），设置 Pod 的 priorityClassName 为 <code>system-cluster-critical</code> 或者 <code>system-node-critical</code>。
<code>system-node-critical</code> 是最高级别的可用性优先级，甚至比 <code>system-cluster-critical</code> 更高。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fe6b50655c29ab0b7c1ee549ff64c138">18 - 升级集群</h1>
    
	<!-- 
---
title: Upgrade A Cluster
content_type: task
---
-->
<!-- overview -->
<!-- 
This page provides an overview of the steps you should follow to upgrade a
Kubernetes cluster.

The way that you upgrade a cluster depends on how you initially deployed it
and on any subsequent changes.

At a high level, the steps you perform are:
-->
<p>本页概述升级 Kubernetes 集群的步骤。</p>
<p>升级集群的方式取决于你最初部署它的方式、以及后续更改它的方式。</p>
<p>从高层规划的角度看，要执行的步骤是：</p>
<!-- 
- Upgrade the <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>
- Upgrade the nodes in your cluster
- Upgrade clients such as <a class='glossary-tooltip' title='kubectl 是用来和 Kubernetes 集群进行通信的命令行工具。' data-toggle='tooltip' data-placement='top' href='/docs/user-guide/kubectl-overview/' target='_blank' aria-label='kubectl'>kubectl</a>
- Adjust manifests and other resources based on the API changes that accompany the
  new Kubernetes version
-->
<ul>
<li>升级<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a></li>
<li>升级集群中的节点</li>
<li>升级 <a class='glossary-tooltip' title='kubectl 是用来和 Kubernetes 集群进行通信的命令行工具。' data-toggle='tooltip' data-placement='top' href='/docs/user-guide/kubectl-overview/' target='_blank' aria-label='kubectl'>kubectl</a> 之类的客户端</li>
<li>根据新 Kubernetes 版本带来的 API 变化，调整清单文件和其他资源</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<!-- 
You must have an existing cluster. This page is about upgrading from Kubernetes
1.22 to Kubernetes 1.23. If your cluster
is not currently running Kubernetes 1.22 then please check
the documentation for the version of Kubernetes that you plan to upgrade to.
-->
<p>你必须有一个集群。
本页内容涉及从 Kubernetes 1.22
升级到 Kubernetes 1.23。
如果你的集群未运行 Kubernetes 1.22，
那请参考目标 Kubernetes 版本的文档。</p>
<!-- ## Upgrade approaches -->
<h2 id="upgrade-approaches">升级方法</h2>
<h3 id="upgrade-kubeadm">kubeadm</h3>
<!-- 
If your cluster was deployed using the `kubeadm` tool, refer to 
[Upgrading kubeadm clusters](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
for detailed information on how to upgrade the cluster.

Once you have upgraded the cluster, remember to
[install the latest version of `kubectl`](/docs/tasks/tools/).
-->
<p>如果你的集群是使用 <code>kubeadm</code> 安装工具部署而来，
那么升级群集的详细信息，请参阅
<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">升级 kubeadm 集群</a>。</p>
<p>升级集群之后，要记得
<a href="/zh/docs/tasks/tools/">安装最新版本的 <code>kubectl</code></a>.</p>
<!-- ### Manual deployments -->
<h3 id="manual-deployments">手动部署</h3>
<!-- 
These steps do not account for third-party extensions such as network and storage
plugins.

You should manually update the control plane following this sequence:
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 这些步骤不考虑第三方扩展，例如网络和存储插件。
</div>

<p>你应该跟随下面操作顺序，手动更新控制平面：</p>
<!-- 
- etcd (all instances)
- kube-apiserver (all control plane hosts)
- kube-controller-manager
- kube-scheduler
- cloud controller manager, if you use one
-->
<ul>
<li>etcd (所有实例)</li>
<li>kube-apiserver (所有控制平面的宿主机)</li>
<li>kube-controller-manager</li>
<li>kube-scheduler</li>
<li>cloud controller manager, 在你用到时</li>
</ul>
<!-- 
At this point you should
[install the latest version of `kubectl`](/docs/tasks/tools/).

For each node in your cluster, [drain](/docs/tasks/administer-cluster/safely-drain-node/)
that node and then either replace it with a new node that uses the 1.23
kubelet, or upgrade the 1.23
kubelet on that node and bring the node back into service.
-->
<p>现在，你应该
<a href="/zh/docs/tasks/tools/">安装最新版本的 <code>kubectl</code></a>.</p>
<p>对于群集中的每个节点，
<a href="/zh/docs/tasks/administer-cluster/safely-drain-node/">排空</a>
节点，然后，或者用一个运行了 1.23 kubelet 的新节点替换它；
或者升级此节点的 kubelet，并使节点恢复服务。</p>
<!-- 
### Other deployments {#upgrade-other}

Refer to the documentation for your cluster deployment tool to learn the recommended set
up steps for maintenance.

## Post-upgrade tasks

### Switch your cluster's storage API version
-->
<h3 id="upgrade-other">其他部署方式</h3>
<p>参阅你的集群部署工具对应的文档，了解用于维护的推荐设置步骤。</p>
<h2 id="post-upgrade-tasks">升级后的任务</h2>
<h3 id="switch-your-clusters-storage-api-version">切换群集的存储 API 版本</h3>
<!-- 
The objects that are serialized into etcd for a cluster's internal
representation of the Kubernetes resources active in the cluster are
written using a particular version of the API.

When the supported API changes, these objects may need to be rewritten
in the newer API. Failure to do this will eventually result in resources
that are no longer decodable or usable by the Kubernetes API server.

For each affected object, fetch it using the latest supported API and then
write it back also using the latest supported API.
-->
<p>对象序列化到 etcd，是为了提供集群中活动 Kubernetes 资源的内部表示法，
这些对象都使用特定版本的 API 编写。</p>
<p>当底层的 API 更改时，这些对象可能需要用新 API 重写。
如果不能做到这一点，会导致再也不能用 Kubernetes API 服务器解码、使用该对象。</p>
<p>对于每个受影响的对象，用最新支持的 API 获取它，然后再用最新支持的 API 写回来。</p>
<!-- 
### Update manifests

Upgrading to a new Kubernetes version can provide new APIs.

You can use `kubectl convert` command to convert manifests between different API versions.
For example:
-->
<h3 id="update-manifests">更新清单</h3>
<p>升级到新版本 Kubernetes 就可以提供新的 API。</p>
<p>你可以使用 <code>kubectl convert</code> 命令在不同 API 版本之间转换清单。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl convert -f pod.yaml --output-version v1
</code></pre></div><!-- 
The `kubectl` tool replaces the contents of `pod.yaml` with a manifest that sets `kind` to
Pod (unchanged), but with a revised `apiVersion`.
-->
<p><code>kubectl</code> 替换了 <code>pod.yaml</code> 的内容，
在新的清单文件中，<code>kind</code> 被设置为 Pod（未变），
但 <code>apiVersion</code> 则被修订了。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-56de8c25b1486599777034111645b803">19 - 名字空间演练</h1>
    
	<!--
reviewers:
- derekwaynecarr
- janetkuo
title: Namespaces Walkthrough
content_type: task
-->
<!-- overview -->
<!--
Kubernetes <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespaces'>namespaces</a>
help different projects, teams, or customers to share a Kubernetes cluster.
-->
<p>Kubernetes <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>
有助于不同的项目、团队或客户去共享 Kubernetes 集群。</p>
<!--
It does this by providing the following:

1. A scope for [Names](/docs/concepts/overview/working-with-objects/names/).
2. A mechanism to attach authorization and policy to a subsection of the cluster.
-->
<p>名字空间通过以下方式实现这点：</p>
<ol>
<li>为<a href="/zh/docs/concepts/overview/working-with-objects/names/">名字</a>设置作用域.</li>
<li>为集群中的部分资源关联鉴权和策略的机制。</li>
</ol>
<!--
Use of multiple namespaces is optional.

This example demonstrates how to use Kubernetes namespaces to subdivide your cluster.
-->
<p>使用多个名字空间是可选的。</p>
<p>此示例演示了如何使用 Kubernetes 名字空间细分群集。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Prerequisites

This example assumes the following:

1. You have an [existing Kubernetes cluster](/docs/setup/).
2. You have a basic understanding of Kubernetes _[Pods](/docs/concepts/workloads/pods/pod/)_, _[Services](/docs/concepts/services-networking/service/)_, and _[Deployments](/docs/concepts/workloads/controllers/deployment/)_.
-->
<h2 id="环境准备">环境准备</h2>
<p>此示例作如下假设：</p>
<ol>
<li>你已拥有一个<a href="/zh/docs/setup/">配置好的 Kubernetes 集群</a>。</li>
<li>你已对 Kubernetes 的 <em><a href="/zh/docs/concepts/workloads/pods/">Pods</a></em>、
<em><a href="/zh/docs/concepts/services-networking/service/">Services</a></em> 和
<em><a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployments</a></em>
有基本理解。</li>
</ol>
<!--
## Understand the default namespace

By default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods,
Services, and Deployments used by the cluster.
-->
<h2 id="理解默认名字空间">理解默认名字空间</h2>
<p>默认情况下，Kubernetes 集群会在配置集群时实例化一个默认名字空间，用以存放集群所使用的默认
Pod、Service 和 Deployment 集合。</p>
<!--
Assuming you have a fresh cluster, you can inspect the available namespaces by doing the following:
-->
<p>假设你有一个新的集群，你可以通过执行以下操作来检查可用的名字空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces
</code></pre></div><pre><code>NAME      STATUS    AGE
default   Active    13m
</code></pre><!--
## Create new namespaces

For this exercise, we will create two additional Kubernetes namespaces to hold our content.
-->
<h2 id="创建新的名字空间">创建新的名字空间</h2>
<p>在本练习中，我们将创建两个额外的 Kubernetes 名字空间来保存我们的内容。</p>
<!--
Let's imagine a scenario where an organization is using a shared Kubernetes cluster for development and production use cases.
-->
<p>我们假设一个场景，某组织正在使用共享的 Kubernetes 集群来支持开发和生产：</p>
<!--
The development team would like to maintain a space in the cluster where they can get a view on the list of Pods, Services, and Deployments
they use to build and run their application.  In this space, Kubernetes resources come and go, and the restrictions on who can or cannot modify resources
are relaxed to enable agile development.
-->
<p>开发团队希望在集群中维护一个空间，以便他们可以查看用于构建和运行其应用程序的 Pod、Service
和 Deployment 列表。在这个空间里，Kubernetes 资源被自由地加入或移除，
对谁能够或不能修改资源的限制被放宽，以实现敏捷开发。</p>
<!--
The operations team would like to maintain a space in the cluster where they can enforce strict procedures on who can or cannot manipulate the set of
Pods, Services, and Deployments that run the production site.
-->
<p>运维团队希望在集群中维护一个空间，以便他们可以强制实施一些严格的规程，
对谁可以或谁不可以操作运行生产站点的 Pod、Service 和 Deployment 集合进行控制。</p>
<!--
One pattern this organization could follow is to partition the Kubernetes cluster into two namespaces: `development` and `production`.
-->
<p>该组织可以遵循的一种模式是将 Kubernetes 集群划分为两个名字空间：<code>development</code> 和 <code>production</code>。</p>
<!--
Let's create two new namespaces to hold our work.
-->
<p>让我们创建两个新的名字空间来保存我们的工作。</p>
<!--
Use the file [`namespace-dev.json`](/examples/admin/namespace-dev.json) which describes a `development` namespace:
-->
<p>文件 <a href="/examples/admin/namespace-dev.json"><code>namespace-dev.json</code></a> 描述了 <code>development</code> 名字空间:</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/namespace-dev.json" download="admin/namespace-dev.json"><code>admin/namespace-dev.json</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-namespace-dev-json')" title="Copy admin/namespace-dev.json to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-namespace-dev-json">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;v1&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Namespace&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;metadata&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;development&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;labels&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;development&#34;</span>
    }
  }
}
</code></pre></div>
    </div>
</div>


<!--
Create the `development` namespace using kubectl.
-->
<p>使用 kubectl 创建 <code>development</code> 名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
</code></pre></div><!--
Save the following contents into file [`namespace-prod.json`](/examples/admin/namespace-prod.json) which describes a `production` namespace:
-->
<p>将下列的内容保存到文件 <a href="/examples/admin/namespace-prod.json"><code>namespace-prod.json</code></a> 中，
这些内容是对 <code>production</code> 名字空间的描述：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/namespace-prod.json" download="admin/namespace-prod.json"><code>admin/namespace-prod.json</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-namespace-prod-json')" title="Copy admin/namespace-prod.json to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-namespace-prod-json">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;v1&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Namespace&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;metadata&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;production&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;labels&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;production&#34;</span>
    }
  }
}
</code></pre></div>
    </div>
</div>


<!--
And then let's create the `production` namespace using kubectl.
-->
<p>让我们使用 kubectl 创建 <code>production</code> 名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
</code></pre></div><!--
To be sure things are right, let's list all of the namespaces in our cluster.
-->
<p>为了确保一切正常，我们列出集群中的所有名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces --show-labels
</code></pre></div><pre><code>NAME          STATUS    AGE       LABELS
default       Active    32m       &lt;none&gt;
development   Active    29s       name=development
production    Active    23s       name=production
</code></pre><!--
## Create pods in each namespace

A Kubernetes namespace provides the scope for Pods, Services, and Deployments in the cluster.

Users interacting with one namespace do not see the content in another namespace.

To demonstrate this, let's spin up a simple Deployment and Pods in the `development` namespace.
-->
<h2 id="在每个名字空间中创建-pod">在每个名字空间中创建 pod</h2>
<p>Kubernetes 名字空间为集群中的 Pod、Service 和 Deployment 提供了作用域。</p>
<p>与一个名字空间交互的用户不会看到另一个名字空间中的内容。</p>
<p>为了演示这一点，让我们在 development 名字空间中启动一个简单的 Deployment 和 Pod。</p>
<!--
We first check what is the current context:
-->
<p>我们首先检查一下当前的上下文：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">clusters</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">certificate-authority-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">server</span>:<span style="color:#bbb"> </span>https://130.211.122.180<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">contexts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">current-context</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">users</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-certificate-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-key-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">token</span>:<span style="color:#bbb"> </span>65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes-basic-auth<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>h5M0FtUUIflBSdI7<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>admin<span style="color:#bbb">
</span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config current-context
</code></pre></div><pre><code>lithe-cocoa-92103_kubernetes
</code></pre><!--
The next step is to define a context for the kubectl client to work in each namespace. The value of "cluster" and "user" fields are copied from the current context.
-->
<p>下一步是为 kubectl 客户端定义一个上下文，以便在每个名字空间中工作。
&quot;cluster&quot; 和 &quot;user&quot; 字段的值将从当前上下文中复制。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config set-context dev --namespace<span style="color:#666">=</span>development <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --cluster<span style="color:#666">=</span>lithe-cocoa-92103_kubernetes <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --user<span style="color:#666">=</span>lithe-cocoa-92103_kubernetes

kubectl config set-context prod --namespace<span style="color:#666">=</span>production <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --cluster<span style="color:#666">=</span>lithe-cocoa-92103_kubernetes <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --user<span style="color:#666">=</span>lithe-cocoa-92103_kubernetes
</code></pre></div><!--
By default, the above commands adds two contexts that are saved into file
`.kube/config`. You can now view the contexts and alternate against the two
new request contexts depending on which namespace you wish to work against.
-->
<p>默认情况下，上述命令会添加两个上下文到 <code>.kube/config</code> 文件中。
你现在可以查看上下文并根据你希望使用的名字空间并在这两个新的请求上下文之间切换。</p>
<!--
To view the new contexts:
-->
<p>查看新的上下文：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">clusters</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">certificate-authority-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">server</span>:<span style="color:#bbb"> </span>https://130.211.122.180<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">contexts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>production<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>prod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">current-context</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">users</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-certificate-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-key-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">token</span>:<span style="color:#bbb"> </span>65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes-basic-auth<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>h5M0FtUUIflBSdI7<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>admin<span style="color:#bbb">
</span></code></pre></div><!--
Let's switch to operate in the `development` namespace.
-->
<p>让我们切换到 <code>development</code> 名字空间进行操作。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config use-context dev
</code></pre></div><!--
You can verify your current context by doing the following:
-->
<p>你可以使用下列命令验证当前上下文：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config current-context
</code></pre></div><pre><code>dev
</code></pre><!--
At this point, all requests we make to the Kubernetes cluster from the command line are scoped to the `development` namespace.
-->
<p>此时，我们从命令行向 Kubernetes 集群发出的所有请求都限定在 <code>development</code> 名字空间中。</p>
<!--
Let's create some contents.
-->
<p>让我们创建一些内容。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/snowflake-deployment.yaml" download="admin/snowflake-deployment.yaml"><code>admin/snowflake-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-snowflake-deployment-yaml')" title="Copy admin/snowflake-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-snowflake-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>snowflake<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>snowflake<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>snowflake<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>snowflake<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/serve_hostname<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>snowflake<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Apply the manifest to create a Deployment 
-->
<p>应用清单文件来创建 Deployment。</p>
<!--
We have created a deployment whose replica size is 2 that is running the pod called `snowflake` with a basic container that serves the hostname.
-->
<p>我们创建了一个副本大小为 2 的 Deployment，该 Deployment 运行名为 <code>snowflake</code> 的 Pod，
其中包含一个仅提供主机名服务的基本容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment
</code></pre></div><pre><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
snowflake    2/2     2            2           2m
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>snowflake
</code></pre></div><pre><code>NAME                         READY     STATUS    RESTARTS   AGE
snowflake-3968820950-9dgr8   1/1       Running   0          2m
snowflake-3968820950-vgc4n   1/1       Running   0          2m
</code></pre><!--
And this is great, developers are able to do what they want, and they do not have to worry about affecting content in the `production` namespace.
-->
<p>这很棒，开发人员可以做他们想要的事情，而不必担心影响 <code>production</code> 名字空间中的内容。</p>
<!--
Let's switch to the `production` namespace and show how resources in one namespace are hidden from the other.
-->
<p>让我们切换到 <code>production</code> 名字空间，展示一个名字空间中的资源如何对另一个名字空间不可见。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config use-context prod
</code></pre></div><!--
The `production` namespace should be empty, and the following commands should return nothing.
-->
<p><code>production</code> 名字空间应该是空的，下列命令应该返回的内容为空。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment
kubectl get pods
</code></pre></div><!--
Production likes to run cattle, so let's create some cattle pods.
-->
<p>生产环境需要以放牛的方式运维，让我们创建一些名为 <code>cattle</code> 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment cattle --image<span style="color:#666">=</span>k8s.gcr.io/serve_hostname --replicas<span style="color:#666">=</span><span style="color:#666">5</span>
kubectl get deployment
</code></pre></div><pre><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
cattle       5/5     5            5           10s
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">run</span><span style="color:#666">=</span>cattle
</code></pre></div><pre><code>NAME                      READY     STATUS    RESTARTS   AGE
cattle-2263376956-41xy6   1/1       Running   0          34s
cattle-2263376956-kw466   1/1       Running   0          34s
cattle-2263376956-n4v97   1/1       Running   0          34s
cattle-2263376956-p5p3i   1/1       Running   0          34s
cattle-2263376956-sxpth   1/1       Running   0          34s
</code></pre><!--
At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.
-->
<p>此时，应该很清楚的展示了用户在一个名字空间中创建的资源对另一个名字空间是不可见的。</p>
<!--
As the policy support in Kubernetes evolves, we will extend this scenario to show how you can provide different
authorization rules for each namespace.
-->
<p>随着 Kubernetes 中的策略支持的发展，我们将扩展此场景，以展示如何为每个名字空间提供不同的授权规则。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-09cc2cf3e0f23a3996e6cb31dc4d867c">20 - 启用/禁用 Kubernetes API</h1>
    
	<!-- 
---
title: Enable Or Disable A Kubernetes API
content_type: task
---
-->
<!-- overview -->
<!-- 
This page shows how to enable or disable an API version from your cluster's
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>.
-->
<p>本页展示怎么用集群的
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>.
启用/禁用 API 版本。</p>
<!-- steps -->
<!-- 
Specific API versions can be turned on or off by passing `--runtime-config=api/<version>` as a
command line argument to the API server. The values for this argument are a comma-separated
list of API versions. Later values override earlier values.

The `runtime-config` command line argument also supports 2 special keys:
-->
<p>通过 API 服务器的命令行参数 <code>--runtime-config=api/&lt;version&gt;</code> ，
可以开启/关闭某个指定的 API 版本。
此参数的值是一个逗号分隔的 API 版本列表。
此列表中，后面的值可以覆盖前面的值。</p>
<p>命令行参数 <code>runtime-config</code> 支持两个特殊的值（keys）：</p>
<!-- 
- `api/all`, representing all known APIs
- `api/legacy`, representing only legacy APIs. Legacy APIs are any APIs that have been
   explicitly [deprecated](/zh/docs/reference/using-api/deprecation-policy/).

For example, to turning off all API versions except v1, pass `--runtime-config=api/all=false,api/v1=true`
to the `kube-apiserver`.
-->
<ul>
<li><code>api/all</code>：指所有已知的 API</li>
<li><code>api/legacy</code>：指过时的 API。过时的 API 就是明确地
<a href="/zh/docs/reference/using-api/deprecation-policy/">弃用</a>
的 API。</li>
</ul>
<p>例如：为了停用除去 v1 版本之外的全部其他 API 版本，
就用参数 <code>--runtime-config=api/all=false,api/v1=true</code> 启动 <code>kube-apiserver</code>。</p>
<h2 id="what-s-next">What's next</h2>
<!-- 
Read the [full documentation](/docs/reference/command-line-tools-reference/kube-apiserver/)
for the `kube-apiserver` component.
-->
<p>阅读<a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">完整的文档</a>,
以了解 <code>kube-apiserver</code> 组件。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9ceed97f912df7289ed8872e290cfbad">21 - 在 Kubernetes 集群中使用 NodeLocal DNSCache</h1>
    
	<!--
reviewers:
- bowei
- zihongz
title: Using NodeLocal DNSCache in Kubernetes clusters
content_type: task
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>


<!--
This page provides an overview of NodeLocal DNSCache feature in Kubernetes.
-->
<p>本页概述了 Kubernetes 中的 NodeLocal DNSCache 功能。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
 <!-- steps -->
<!--
## Introduction
-->
<h2 id="引言">引言</h2>
<!--
NodeLocal DNSCache improves Cluster DNS performance by running a dns caching agent on cluster nodes as a DaemonSet. In today's architecture, Pods in ClusterFirst DNS mode reach out to a kube-dns serviceIP for DNS queries. This is translated to a kube-dns/CoreDNS endpoint via iptables rules added by kube-proxy. With this new architecture, Pods will reach out to the dns caching agent running on the same node, thereby avoiding iptables DNAT rules and connection tracking. The local caching agent will query kube-dns service for cache misses of cluster hostnames(cluster.local suffix by default).
-->
<p>NodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 DNS 缓存代理来提高集群 DNS 性能。
在当今的体系结构中，运行在 ClusterFirst DNS 模式下的 Pod 可以连接到 kube-dns <code>serviceIP</code> 进行 DNS 查询。
通过 kube-proxy 添加的 iptables 规则将其转换为 kube-dns/CoreDNS 端点。
借助这种新架构，Pods 将可以访问在同一节点上运行的 DNS 缓存代理，从而避免 iptables DNAT 规则和连接跟踪。
本地缓存代理将查询 kube-dns 服务以获取集群主机名的缓存缺失（默认为 &quot;<code>cluster.local</code>&quot; 后缀）。</p>
<!--
## Motivation
-->
<h2 id="动机">动机</h2>
<!--
* With the current DNS architecture, it is possible that Pods with the highest DNS QPS have to reach out to a different node, if there is no local kube-dns/CoreDNS instance.
Having a local cache will help improve the latency in such scenarios.
-->
<ul>
<li>使用当前的 DNS 体系结构，如果没有本地 kube-dns/CoreDNS 实例，则具有最高 DNS QPS
的 Pod 可能必须延伸到另一个节点。
在这种场景下，拥有本地缓存将有助于改善延迟。</li>
</ul>
<!--
* Skipping iptables DNAT and connection tracking will help reduce [conntrack races](https://github.com/kubernetes/kubernetes/issues/56903) and avoid UDP DNS entries filling up conntrack table.
-->
<ul>
<li>跳过 iptables DNAT 和连接跟踪将有助于减少
<a href="https://github.com/kubernetes/kubernetes/issues/56903">conntrack 竞争</a>
并避免 UDP DNS 条目填满 conntrack 表。</li>
</ul>
<!--
* Connections from local caching agent to kube-dns servie can be upgraded to TCP. TCP conntrack entries will be removed on connection close in contrast with UDP entries that have to timeout ([default](https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt) `nf_conntrack_udp_timeout` is 30 seconds)
-->
<ul>
<li>从本地缓存代理到 kube-dns 服务的连接可以升级为 TCP 。
TCP conntrack 条目将在连接关闭时被删除，相反 UDP 条目必须超时
（<a href="https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt">默认</a>
<code>nf_conntrack_udp_timeout</code> 是 30 秒）。</li>
</ul>
<!--
* Upgrading DNS queries from UDP to TCP would reduce tail latency attributed to dropped UDP packets and DNS timeouts usually up to 30s (3 retries + 10s timeout). Since the nodelocal cache listens for UDP DNS queries, applications don't need to be changed.
-->
<ul>
<li>将 DNS 查询从 UDP 升级到 TCP 将减少由于被丢弃的 UDP 包和 DNS 超时而带来的尾部等待时间；
这类延时通常长达 30 秒（3 次重试 + 10 秒超时）。
由于 nodelocal 缓存监听 UDP DNS 查询，应用不需要变更。</li>
</ul>
<!--
* Metrics & visibility into dns requests at a node level.
-->
<ul>
<li>在节点级别对 DNS 请求的度量和可见性。</li>
</ul>
<!--
* Negative caching can be re-enabled, thereby reducing number of queries to kube-dns service.
-->
<ul>
<li>可以重新启用负缓存，从而减少对 kube-dns 服务的查询数量。</li>
</ul>
<!--
## Architecture Diagram
-->
<h2 id="架构图">架构图</h2>
<!--
This is the path followed by DNS Queries after NodeLocal DNSCache is enabled:
-->
<p>启用 NodeLocal DNSCache 之后，DNS 查询所遵循的路径如下：</p>
<!--

<figure class="diagram-medium">
    <img src="/images/docs/nodelocaldns.svg"
         alt="NodeLocal DNSCache flow"/> <figcaption>
            <h4>Nodelocal DNSCache flow</h4><p>This image shows how NodeLocal DNSCache handles DNS queries.</p>
        </figcaption>
</figure>

-->

<figure class="diagram-medium">
    <img src="/images/docs/nodelocaldns.svg"
         alt="NodeLocal DNSCache 流"/> <figcaption>
            <h4>Nodelocal DNSCache 流</h4><p>此图显示了 NodeLocal DNSCache 如何处理 DNS 查询。</p>
        </figcaption>
</figure>

<!--
## Configuration
-->
<h2 id="配置">配置</h2>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> The local listen IP address for NodeLocal DNSCache can be any address that can be guaranteed to not collide with any existing IP in your cluster. It's recommended to use an address with a local scope, per example, from the link-local range 169.254.0.0/16 for IPv4 or from the Unique Local Address range in IPv6 fd00::/8.
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> NodeLocal DNSCache 的本地侦听 IP 地址可以是任何地址，只要该地址不和你的集群里现有的 IP 地址发生冲突。
推荐使用本地范围内的地址，例如，IPv4 链路本地区段 '169.254.0.0/16' 内的地址，
或者 IPv6 唯一本地地址区段 'fd00::/8' 内的地址。
</div>
<!--
This feature can be enabled using the following steps:
-->
<p>可以使用以下步骤启动此功能：</p>
<!--
* Prepare a manifest similar to the sample [`nodelocaldns.yaml`](https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml) and save it as `nodelocaldns.yaml.`
-->
<ul>
<li>根据示例 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml"><code>nodelocaldns.yaml</code></a>
准备一个清单，把它保存为 <code>nodelocaldns.yaml</code>。</li>
</ul>
<!--
* If using IPv6, the CoreDNS configuration file need to enclose all the IPv6 addresses into square brackets if used in IP:Port format. 
If you are using the sample manifest from the previous point, this will require to modify [the configuration line L70](https://github.com/kubernetes/kubernetes/blob/b2ecd1b3a3192fbbe2b9e348e095326f51dc43dd/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml#L70) like this `health [__PILLAR__LOCAL__DNS__]:8080`
-->
<ul>
<li>如果使用 IPv6，在使用 IP:Port 格式的时候需要把 CoreDNS 配置文件里的所有 IPv6 地址用方括号包起来。
如果你使用上述的示例清单，需要把
<a href="https://github.com/kubernetes/kubernetes/blob/b2ecd1b3a3192fbbe2b9e348e095326f51dc43dd/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml#L70">配置行 L70</a>
修改为 <code>health [__PILLAR__LOCAL__DNS__]:8080</code>。</li>
</ul>
<!--
* Substitute the variables in the manifest with the right values:

     * kubedns=`kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}`

     * domain=`<cluster-domain>`

     * localdns=`<node-local-address>`

     `<cluster-domain>` is "cluster.local" by default. `<node-local-address>` is the local listen IP address chosen for NodeLocal DNSCache.
-->
<ul>
<li>
<p>把清单里的变量更改为正确的值：</p>
<pre><code>kubedns=`kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}`
domain=&lt;cluster-domain&gt;
localdns=&lt;node-local-address&gt;
</code></pre><p><code>&lt;cluster-domain&gt;</code> 的默认值是 &quot;<code>cluster.local</code>&quot;。<code>&lt;node-local-address&gt;</code> 是
NodeLocal DNSCache 选择的本地侦听 IP 地址。</p>
</li>
</ul>
<!--
   * If kube-proxy is running in IPTABLES mode:

     ``` bash
     sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/__PILLAR__DNS__SERVER__/$kubedns/g" nodelocaldns.yaml
     ```

     `__PILLAR__CLUSTER__DNS__` and `__PILLAR__UPSTREAM__SERVERS__` will be populated by the node-local-dns pods.
     In this mode, node-local-dns pods listen on both the kube-dns service IP as well as `<node-local-address>`, so pods can lookup DNS records using either IP address.
-->   
<ul>
<li>
<p>如果 kube-proxy 运行在 IPTABLES 模式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sed -i <span style="color:#b44">&#34;s/__PILLAR__LOCAL__DNS__/</span><span style="color:#b8860b">$localdns</span><span style="color:#b44">/g; s/__PILLAR__DNS__DOMAIN__/</span><span style="color:#b8860b">$domain</span><span style="color:#b44">/g; s/__PILLAR__DNS__SERVER__/</span><span style="color:#b8860b">$kubedns</span><span style="color:#b44">/g&#34;</span> nodelocaldns.yaml
</code></pre></div><p>node-local-dns Pods 会设置 <code>__PILLAR__CLUSTER__DNS__</code> 和 <code>__PILLAR__UPSTREAM__SERVERS__</code>。
在此模式下, node-local-dns Pods 会同时侦听 kube-dns 服务的 IP 地址和
<code>&lt;node-local-address&gt;</code> 的地址，以便 Pods 可以使用其中任何一个 IP 地址来查询 DNS 记录。</p>
</li>
</ul>
  <!--
  * If kube-proxy is running in IPVS mode:

    ``` bash
     sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/,__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/$kubedns/g" nodelocaldns.yaml
    ```
     In this mode, node-local-dns pods listen only on `<node-local-address>`. The node-local-dns interface cannot bind the kube-dns cluster IP since the interface used for IPVS loadbalancing already uses this address.
     `__PILLAR__UPSTREAM__SERVERS__` will be populated by the node-local-dns pods.
-->
<ul>
<li>
<p>如果 kube-proxy 运行在 IPVS 模式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sed -i <span style="color:#b44">&#34;s/__PILLAR__LOCAL__DNS__/</span><span style="color:#b8860b">$localdns</span><span style="color:#b44">/g; s/__PILLAR__DNS__DOMAIN__/</span><span style="color:#b8860b">$domain</span><span style="color:#b44">/g; s/__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/</span><span style="color:#b8860b">$kubedns</span><span style="color:#b44">/g&#34;</span> nodelocaldns.yaml
</code></pre></div><p>在此模式下，node-local-dns Pods 只会侦听 <code>&lt;node-local-address&gt;</code> 的地址。
node-local-dns 接口不能绑定 kube-dns 的集群 IP 地址，因为 IPVS 负载均衡
使用的接口已经占用了该地址。
node-local-dns Pods 会设置 <code>__PILLAR__UPSTREAM__SERVERS__</code>。</p>
</li>
</ul>
<!--
* Run `kubectl create -f nodelocaldns.yaml`
* If using kube-proxy in IPVS mode, `--cluster-dns` flag to kubelet needs to be modified to use `<node-local-address>` that NodeLocal DNSCache is listening on.
  Otherwise, there is no need to modify the value of the `--cluster-dns` flag, since NodeLocal DNSCache listens on both the kube-dns service IP as well as `<node-local-address>`.
-->
<ul>
<li>运行 <code>kubectl create -f nodelocaldns.yaml</code></li>
<li>如果 kube-proxy 运行在 IPVS 模式，需要修改 kubelet 的 <code>--cluster-dns</code> 参数
NodeLocal DNSCache 正在侦听的 <code>&lt;node-local-address&gt;</code> 地址。
否则，不需要修改 <code>--cluster-dns</code> 参数，因为 NodeLocal DNSCache 会同时侦听
kube-dns 服务的 IP 地址和 <code>&lt;node-local-address&gt;</code> 的地址。</li>
</ul>
<!--
Once enabled, node-local-dns Pods will run in the kube-system namespace on each of the cluster nodes. This Pod runs [CoreDNS](https://github.com/coredns/coredns) in cache mode, so all CoreDNS metrics exposed by the different plugins will be available on a per-node basis.

You can disable this feature by removing the DaemonSet, using `kubectl delete -f <manifest>` . You should also revert any changes you made to the kubelet configuration.
-->
<p>启用后，node-local-dns Pods 将在每个集群节点上的 kube-system 名字空间中运行。
此 Pod 在缓存模式下运行 <a href="https://github.com/coredns/coredns">CoreDNS</a> ，
因此每个节点都可以使用不同插件公开的所有 CoreDNS 指标。</p>
<p>如果要禁用该功能，你可以使用 <code>kubectl delete -f &lt;manifest&gt;</code> 来删除 DaemonSet。
你还应该回滚你对 kubelet 配置所做的所有改动。</p>
<!--
## StubDomains and Upstream server Configuration
-->
<h2 id="stubdomains-和上游服务器配置">StubDomains 和上游服务器配置</h2>
<!--
StubDomains and upstream servers specified in the `kube-dns` ConfigMap in the `kube-system` namespace
are automatically picked up by `node-local-dns` pods. The ConfigMap contents need to follow the format
shown in [the example](/docs/tasks/administer-cluster/dns-custom-nameservers/#example-1).
The `node-local-dns` ConfigMap can also be modified directly with the stubDomain configuration
in the Corefile format. Some cloud providers might not allow modifying `node-local-dns` ConfigMap directly.
In those cases, the `kube-dns` ConfigMap can be updated.
-->
<p><code>node-local-dns</code> Pod 能够自动读取 <code>kube-system</code> 名字空间中 <code>kube-dns</code> ConfigMap
中保存的 StubDomains 和上游服务器信息。ConfigMap 中的内容需要遵从
<a href="/zh/docs/tasks/administer-cluster/dns-custom-nameservers/#example-1">此示例</a>
中所给的格式。
<code>node-local-dns</code> ConfigMap 也可被直接修改，使用 Corefile 格式设置 stubDomain 配置。
某些云厂商可能不允许直接修改 <code>node-local-dns</code> ConfigMap 的内容。
在这种情况下，可以更新 <code>kube-dns</code> ConfigMap。</p>
<!--
## Setting memory limits
-->
<h2 id="设置内存限制">设置内存限制</h2>
<!--
node-local-dns pods use memory for storing cache entries and processing queries. Since they do not watch Kubernetes objects, the cluster size or the number of Services/Endpoints do not directly affect memory usage. Memory usage is influenced by the DNS query pattern.
From [CoreDNS docs](https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md),
> The default cache size is 10000 entries, which uses about 30 MB when completely filled.
-->
<p><code>node-local-dns</code> Pod 使用内存来保存缓存项并处理查询。
由于它们并不监视 Kubernetes 对象变化，集群规模或者 Service/Endpoints
的数量都不会直接影响内存用量。内存用量会受到 DNS 查询模式的影响。
根据 <a href="https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md">CoreDNS 文档</a>,</p>
<blockquote>
<p>The default cache size is 10000 entries, which uses about 30 MB when completely filled.
（默认的缓存大小是 10000 个表项，当完全填充时会使用约 30 MB 内存）</p>
</blockquote>
<!--
This would be the memory usage for each server block (if the cache gets completely filled).
Memory usage can be reduced by specifying smaller cache sizes.

The number of concurrent queries is linked to the memory demand, because each extra
goroutine used for handling a query requires an amount of memory. You can set an upper limit
using the `max_concurrent` option in the forward plugin.
-->
<p>这一数值是（缓存完全被填充时）每个服务器块的内存用量。
通过设置小一点的缓存大小可以降低内存用量。</p>
<p>并发查询的数量会影响内存需求，因为用来处理查询请求而创建的 Go 协程都需要一定量的内存。
你可以在 forward 插件中使用 <code>max_concurrent</code> 选项设置并发查询数量上限。</p>
<!--
If a node-local-dns pod attempts to use more memory than is available (because of total system
resources, or because of a configured
[resource limit](/docs/concepts/configuration/manage-resources-containers/)), the operating system
may shut down that pod's container.
If this happens, the container that is terminated (“OOMKilled”) does not clean up the custom
packet filtering rules that it previously added during startup.
The node-local-dns container should get restarted (since managed as part of a DaemonSet), but this
will lead to a brief DNS downtime each time that the container fails: the packet filtering rules direct
DNS queries to a local Pod that is unhealthy.
-->
<p>如果一个 <code>node-local-dns</code> Pod 尝试使用的内存超出可提供的内存量
（因为系统资源总量的，或者所配置的<a href="/zh/docs/concepts/configuration/manage-resources-containers/">资源约束</a>）的原因，
操作系统可能会关闭这一 Pod 的容器。
发生这种情况时，被终止的（&quot;OOMKilled&quot;）容器不会清理其启动期间所添加的定制包过滤规则。
该 <code>node-local-dns</code> 容器应该会被重启（因其作为 DaemonSet 的一部分被管理），
但因上述原因可能每次容器失败时都会导致 DNS 有一小段时间不可用：
the packet filtering rules direct DNS queries to a local Pod that is unhealthy
（包过滤器规则将 DNS 查询转发到本地某个不健康的 Pod）。</p>
<!--
You can determine a suitable memory limit by running node-local-dns pods without a limit and
measuring the peak usage. You can also set up and use a
[VerticalPodAutoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)
in _recommender mode_, and then check its recommendations.
-->
<p>通过不带限制地运行 <code>node-local-dns</code> Pod 并度量其内存用量峰值，你可以为其确定一个合适的内存限制值。
你也可以安装并使用一个运行在 “Recommender Mode（建议者模式）” 的
<a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">VerticalPodAutoscaler</a>，
并查看该组件输出的建议信息。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fe5ad73163d38596340536ec03a205f0">22 - 在 Kubernetes 集群中使用 sysctl</h1>
    
	<!--
title: Using sysctls in a Kubernetes Cluster
reviewers:
- sttts
content_type: task
--->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>


<!--
This document describes how to configure and use kernel parameters within a
Kubernetes cluster using the <a class='glossary-tooltip' title='用于获取和设置 Unix 内核参数的接口' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/sysctl-cluster/' target='_blank' aria-label='sysctl'>sysctl</a>
interface.
-->
<p>本文档介绍如何通过 <a class='glossary-tooltip' title='用于获取和设置 Unix 内核参数的接口' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/sysctl-cluster/' target='_blank' aria-label='sysctl'>sysctl</a>
接口在 Kubernetes 集群中配置和使用内核参数。</p>
<!--
Starting from Kubernetes version 1.23, the kubelet supports the use of either `/` or `.`
as separators for sysctl names.
For example, you can represent the same sysctl name as `kernel.shm_rmid_forced` using a
period as the separator, or as `kernel/shm_rmid_forced` using a slash as a separator.
For more sysctl parameter conversion method details, please refer to
the page [sysctl.d(5)](https://man7.org/linux/man-pages/man5/sysctl.d.5.html) from
the Linux man-pages project.
Setting Sysctls for a Pod and PodSecurityPolicy features do not yet support
setting sysctls with slashes.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 从 Kubernetes 1.23 版本开始，kubelet 支持使用 <code>/</code> 或 <code>.</code> 作为 sysctl 参数的分隔符。
例如，你可以使用点或者斜线作为分隔符表示相同的 sysctl 参数，以点作为分隔符表示为： <code>kernel.shm_rmid_forced</code>，
或者以斜线作为分隔符表示为：<code>kernel/shm_rmid_forced</code>。
更多 sysctl 参数转换方法详情请参考 Linux man-pages
<a href="https://man7.org/linux/man-pages/man5/sysctl.d.5.html">sysctl.d(5)</a> 。
设置 Pod 的 Sysctl 参数 和 PodSecurityPolicy 功能尚不支持设置包含斜线的 Sysctl 参数。
</div>
<h2 id="before-you-begin">Before you begin</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!--
For some steps, you also need to be able to reconfigure the command line
options for the kubelets running on your cluster.
-->
<p>对一些步骤，你需要能够重新配置在你的集群里运行的 kubelet 命令行的选项。</p>
<!-- steps -->
<!--
## Listing all Sysctl Parameters
-->
<h2 id="获取-sysctl-的参数列表">获取 Sysctl 的参数列表</h2>
<!--
In Linux, the sysctl interface allows an administrator to modify kernel
parameters at runtime. Parameters are available via the `/proc/sys/` virtual
process file system. The parameters cover various subsystems such as:
-->
<p>在 Linux 中，管理员可以通过 sysctl 接口修改内核运行时的参数。在 <code>/proc/sys/</code>
虚拟文件系统下存放许多内核参数。这些参数涉及了多个内核子系统，如：</p>
<!--
- kernel (common prefix: `kernel.`)
- networking (common prefix: `net.`)
- virtual memory (common prefix: `vm.`)
- MDADM (common prefix: `dev.`)
- More subsystems are described in [Kernel docs](https://www.kernel.org/doc/Documentation/sysctl/README).
-->
<ul>
<li>内核子系统（通常前缀为: <code>kernel.</code>）</li>
<li>网络子系统（通常前缀为: <code>net.</code>）</li>
<li>虚拟内存子系统（通常前缀为: <code>vm.</code>）</li>
<li>MDADM 子系统（通常前缀为: <code>dev.</code>）</li>
<li>更多子系统请参见<a href="https://www.kernel.org/doc/Documentation/sysctl/README">内核文档</a>。</li>
</ul>
<!--
To get a list of all parameters, you can run
--->
<p>若要获取完整的参数列表，请执行以下命令</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo sysctl -a
</code></pre></div><!--
## Enabling Unsafe Sysctls

Sysctls are grouped into _safe_  and _unsafe_ sysctls. In addition to proper
namespacing a _safe_ sysctl must be properly _isolated_ between pods on the same
node. This means that setting a _safe_ sysctl for one pod
-->
<h2 id="启用非安全的-sysctl-参数">启用非安全的 Sysctl 参数</h2>
<p>sysctl 参数分为 <em>安全</em> 和 <em>非安全的</em>。
<em>安全</em> sysctl 参数除了需要设置恰当的命名空间外，在同一 node 上的不同 Pod
之间也必须是 <em>相互隔离的</em>。这意味着在 Pod 上设置 <em>安全</em> sysctl 参数</p>
<!--
- must not have any influence on any other pod on the node
- must not allow to harm the node's health
- must not allow to gain CPU or memory resources outside of the resource limits
  of a pod.
-->
<ul>
<li>必须不能影响到节点上的其他 Pod</li>
<li>必须不能损害节点的健康</li>
<li>必须不允许使用超出 Pod 的资源限制的 CPU 或内存资源。</li>
</ul>
<!--
By far, most of the _namespaced_ sysctls are not necessarily considered _safe_.
The following sysctls are supported in the _safe_ set:
-->
<p>至今为止，大多数 <em>有命名空间的</em> sysctl 参数不一定被认为是 <em>安全</em> 的。
以下几种 sysctl 参数是 <em>安全的</em>：</p>
<ul>
<li><code>kernel.shm_rmid_forced</code></li>
<li><code>net.ipv4.ip_local_port_range</code></li>
<li><code>net.ipv4.tcp_syncookies</code></li>
<li><code>net.ipv4.ping_group_range</code> （从 Kubernetes 1.18 开始）</li>
<li><code>net.ipv4.ip_unprivileged_port_start</code> （从 Kubernetes 1.22 开始）。</li>
</ul>
<!--
The example `net.ipv4.tcp_syncookies` is not namespaced on Linux kernel version 4.4 or lower.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 示例中的 <code>net.ipv4.tcp_syncookies</code> 在Linux 内核 4.4 或更低的版本中是无命名空间的。
</div>
<!--
This list will be extended in future Kubernetes versions when the kubelet
supports better isolation mechanisms.
-->
<p>在未来的 Kubernetes 版本中，若 kubelet 支持更好的隔离机制，则上述列表中将会
列出更多 <em>安全的</em> sysctl 参数。</p>
<!--
All _safe_ sysctls are enabled by default.
-->
<p>所有 <em>安全的</em> sysctl 参数都默认启用。</p>
<!--
All _unsafe_ sysctls are disabled by default and must be allowed manually by the
cluster admin on a per-node basis. Pods with disabled unsafe sysctls will be
scheduled, but will fail to launch.
-->
<p>所有 <em>非安全的</em> sysctl 参数都默认禁用，且必须由集群管理员在每个节点上手动开启。
那些设置了不安全 sysctl 参数的 Pod 仍会被调度，但无法正常启动。</p>
<!--
With the warning above in mind, the cluster admin can allow certain _unsafe_
sysctls for very special situations like e.g. high-performance or real-time
application tuning. _Unsafe_ sysctls are enabled on a node-by-node basis with a
flag of the kubelet, e.g.:
-->
<p>参考上述警告，集群管理员只有在一些非常特殊的情况下（如：高可用或实时应用调整），
才可以启用特定的 <em>非安全的</em> sysctl 参数。
如需启用 <em>非安全的</em> sysctl 参数，请你在每个节点上分别设置 kubelet 命令行参数，例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubelet --allowed-unsafe-sysctls <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#b44">&#39;kernel.msg*,net.core.somaxconn&#39;</span> ...
</code></pre></div><!--
For <a class='glossary-tooltip' title='Minikube 是用来在本地运行 Kubernetes 的一种工具。' data-toggle='tooltip' data-placement='top' href='/docs/getting-started-guides/minikube/' target='_blank' aria-label='Minikube'>Minikube</a>, this can be done via the `extra-config` flag:
-->
<p>如果你使用 <a class='glossary-tooltip' title='Minikube 是用来在本地运行 Kubernetes 的一种工具。' data-toggle='tooltip' data-placement='top' href='/docs/getting-started-guides/minikube/' target='_blank' aria-label='Minikube'>Minikube</a>，可以通过 <code>extra-config</code> 参数来配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube start --extra-config<span style="color:#666">=</span><span style="color:#b44">&#34;kubelet.allowed-unsafe-sysctls=kernel.msg*,net.core.somaxconn&#34;</span>...
</code></pre></div><!--
Only _namespaced_ sysctls can be enabled this way.
-->
<p>只有 <em>有命名空间的</em> sysctl 参数可以通过该方式启用。</p>
<!--
## Setting Sysctls for a Pod

A number of sysctls are _namespaced_ in today's Linux kernels. This means that
they can be set independently for each pod on a node. Only namespaced sysctls
are configurable via the pod securityContext within Kubernetes.
-->
<h2 id="设置-pod-的-sysctl-参数">设置 Pod 的 Sysctl 参数</h2>
<p>目前，在 Linux 内核中，有许多的 sysctl 参数都是 <em>有命名空间的</em> 。
这就意味着可以为节点上的每个 Pod 分别去设置它们的 sysctl 参数。
在 Kubernetes 中，只有那些有命名空间的 sysctl 参数可以通过 Pod 的 securityContext 对其进行配置。</p>
<!--
The following sysctls are known to be namespaced. This list could change
in future versions of the Linux kernel.
-->
<p>以下列出有命名空间的 sysctl 参数，在未来的 Linux 内核版本中，此列表可能会发生变化。</p>
<ul>
<li><code>kernel.shm*</code>,</li>
<li><code>kernel.msg*</code>,</li>
<li><code>kernel.sem</code>,</li>
<li><code>fs.mqueue.*</code>,</li>
<li><code>net.*</code>（内核中可以在容器命名空间里被更改的网络配置项相关参数）。然而也有一些特例
（例如，<code>net.netfilter.nf_conntrack_max</code> 和 <code>net.netfilter.nf_conntrack_expect_max</code>
可以在容器命名空间里被更改，但它们是非命名空间的）。</li>
</ul>
<!--
Sysctls with no namespace are called _node-level_ sysctls. If you need to set
them, you must manually configure them on each node's operating system, or by
using a DaemonSet with privileged containers.
-->
<p>没有命名空间的 sysctl 参数称为 <em>节点级别的</em> sysctl 参数。
如果需要对其进行设置，则必须在每个节点的操作系统上手动地去配置它们，
或者通过在 DaemonSet 中运行特权模式容器来配置。</p>
<!--
Use the pod securityContext to configure namespaced sysctls. The securityContext
applies to all containers in the same pod.
-->
<p>可使用 Pod 的 securityContext 来配置有命名空间的 sysctl 参数，
securityContext 应用于同一个 Pod 中的所有容器。</p>
<!--
This example uses the pod securityContext to set a safe sysctl
`kernel.shm_rmid_forced` and two unsafe sysctls `net.core.somaxconn` and
`kernel.msgmax` There is no distinction between _safe_ and _unsafe_ sysctls in
the specification.
-->
<p>此示例中，使用 Pod SecurityContext 来对一个安全的 sysctl 参数
<code>kernel.shm_rmid_forced</code> 以及两个非安全的 sysctl 参数
<code>net.core.somaxconn</code> 和 <code>kernel.msgmax</code> 进行设置。
在 Pod 规约中对 <em>安全的</em> 和 <em>非安全的</em> sysctl 参数不做区分。</p>
<!--
Only modify sysctl parameters after you understand their effects, to avoid
destabilizing your operating system.
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 为了避免破坏操作系统的稳定性，请你在了解变更后果之后再修改 sysctl 参数。
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sysctl-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">sysctls</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kernel.shm_rmid_forced<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>net.core.somaxconn<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1024&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kernel.msgmax<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;65536&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!-- discussion -->
<!--
Due to their nature of being _unsafe_, the use of _unsafe_ sysctls
is at-your-own-risk and can lead to severe problems like wrong behavior of
containers, resource shortage or complete breakage of a node.
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 由于 <em>非安全的</em> sysctl 参数其本身具有不稳定性，在使用 <em>非安全的</em> sysctl 参数
时可能会导致一些严重问题，如容器的错误行为、机器资源不足或节点被完全破坏，
用户需自行承担风险。
</div>


<!--
It is good practice to consider nodes with special sysctl settings as
_tainted_ within a cluster, and only schedule pods onto them which need those
sysctl settings. It is suggested to use the Kubernetes [_taints and toleration_
feature](/docs/reference/generated/kubectl/kubectl-commands/#taint) to implement this.
-->
<p>最佳实践方案是将集群中具有特殊 sysctl 设置的节点视为 <em>有污点的</em>，并且只调度
需要使用到特殊 sysctl 设置的 Pod 到这些节点上。
建议使用 Kubernetes 的
<a href="/docs/reference/generated/kubectl/kubectl-commands/#taint">污点和容忍度特性</a> 来实现它。</p>
<!--
A pod with the _unsafe_ sysctls will fail to launch on any node which has not
enabled those two _unsafe_ sysctls explicitly. As with _node-level_ sysctls it
is recommended to use
[_taints and toleration_ feature](/docs/reference/generated/kubectl/kubectl-commands/#taint) or
[taints on nodes](/docs/concepts/configuration/taint-and-toleration/)
to schedule those pods onto the right nodes.
-->
<p>设置了 <em>非安全的</em> sysctl 参数的 Pod 在禁用了这两种 <em>非安全的</em> sysctl 参数配置
的节点上启动都会失败。与 <em>节点级别的</em> sysctl 一样，建议开启
<a href="/docs/reference/generated/kubectl/kubectl-commands/#taint">污点和容忍度特性</a> 或
<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">为节点配置污点</a>
以便将 Pod 调度到正确的节点之上。</p>
<h2 id="podsecuritypolicy">PodSecurityPolicy</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code>
</div>


<!--
You can further control which sysctls can be set in pods by specifying lists of
sysctls or sysctl patterns in the `forbiddenSysctls` and/or
`allowedUnsafeSysctls` fields of the PodSecurityPolicy. A sysctl pattern ends
with a `*` character, such as `kernel.*`. A `*` character on its own matches
all sysctls.
-->
<p>你可以通过在 PodSecurityPolicy 的 <code>forbiddenSysctls</code> 和/或 <code>allowedUnsafeSysctls</code>
字段中，指定 sysctl 或填写 sysctl 匹配模式来进一步为 Pod 设置 sysctl 参数。
sysctl 参数匹配模式以 <code>*</code> 字符结尾，如 <code>kernel.*</code>。
单独的 <code>*</code>  字符匹配所有 sysctl 参数。</p>
<!--
By default, all safe sysctls are allowed.
-->
<p>所有 <em>安全的</em> sysctl 参数都默认启用。</p>
<!--
Both `forbiddenSysctls` and `allowedUnsafeSysctls` are lists of plain sysctl names
or sysctl patterns (which end with `*`). The string `*` matches all sysctls.
-->
<p><code>forbiddenSysctls</code> 和 <code>allowedUnsafeSysctls</code> 的值都是字符串列表类型，
可以添加 sysctl 参数名称，也可以添加 sysctl 参数匹配模式（以<code>*</code>结尾）。
只填写 <code>*</code> 则匹配所有的 sysctl 参数。</p>
<!--
The `forbiddenSysctls` field excludes specific sysctls. You can forbid a
combination of safe and unsafe sysctls in the list. To forbid setting any
sysctls, use `*` on its own.
-->
<p><code>forbiddenSysctls</code> 字段用于禁用特定的 sysctl 参数。
你可以在列表中禁用安全和非安全的 sysctl 参数的组合。
要禁用所有的 sysctl 参数，请设置为 <code>*</code>。</p>
<!--
If you specify any unsafe sysctl in the `allowedUnsafeSysctls` field and it is
not present in the `forbiddenSysctls` field, that sysctl can be used in Pods
using this PodSecurityPolicy. To allow all unsafe sysctls in the
PodSecurityPolicy to be set, use `*` on its own.
-->
<p>如果要在 <code>allowedUnsafeSysctls</code> 字段中指定一个非安全的 sysctl 参数，
并且它在 <code>forbiddenSysctls</code> 字段中未被禁用，则可以在 Pod 中通过
PodSecurityPolicy 启用该 sysctl 参数。
若要在 PodSecurityPolicy 中开启所有非安全的 sysctl 参数，
请设 <code>allowedUnsafeSysctls</code> 字段值为 <code>*</code>。</p>
<!--
Do not configure these two fields such that there is overlap, meaning that a
given sysctl is both allowed and forbidden.
-->
<p><code>allowedUnsafeSysctls</code> 与 <code>forbiddenSysctls</code> 两字段的配置不能重叠，
否则这就意味着存在某个 sysctl 参数既被启用又被禁用。</p>
<!--
If you whitelist unsafe sysctls via the `allowedUnsafeSysctls` field
in a PodSecurityPolicy, any pod using such a sysctl will fail to start
if the sysctl is not whitelisted via the `--allowed-unsafe-sysctls` kubelet
flag as well on that node.
--->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 如果你通过 PodSecurityPolicy 中的 <code>allowedUnsafeSysctls</code> 字段将非安全的 sysctl
参数列入白名单，但该 sysctl 参数未通过 kubelet 命令行参数
<code>--allowed-unsafe-sysctls</code> 在节点上将其列入白名单，则设置了这个 sysctl
参数的 Pod 将会启动失败。
</div>


<!--
This example allows unsafe sysctls prefixed with `kernel.msg` to be set and
disallows setting of the `kernel.shm_rmid_forced` sysctl.
-->
<p>以下示例设置启用了以 <code>kernel.msg</code> 为前缀的非安全的 sysctl 参数，同时禁用了
sysctl 参数 <code>kernel.shm_rmid_forced</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>policy/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PodSecurityPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sysctl-psp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">allowedUnsafeSysctls</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- kernel.msg*<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">forbiddenSysctls</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- kernel.shm_rmid_forced<span style="color:#bbb">
</span><span style="color:#bbb"> </span>...<span style="color:#bbb">
</span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-eec61e72c300dbfbf7302400ca966432">23 - 在运行中的集群上重新配置节点的 kubelet</h1>
    
	<!--
reviewers:
- mtaufen
- dawnchen
title: Reconfigure a Node's Kubelet in a Live Cluster
content_type: task
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [deprecated]</code>
</div>


<!--
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> The <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/281-dynamic-kubelet-configuration">Dynamic Kubelet Configuration</a>
feature is deprecated and should not be used.
Please switch to alternative means distributing configuration to the Nodes of your cluster.
</div>

-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/281-dynamic-kubelet-configuration">动态 kubelet 配置</a>
已经废弃不建议使用。请选择其他方法将配置分发到集群中的节点。
</div>

<!--
[Dynamic Kubelet Configuration](https://github.com/kubernetes/enhancements/issues/281)
allows you to change the configuration of each Kubelet in a live Kubernetes
cluster by deploying a ConfigMap and configuring each Node to use it.
-->
<p><a href="https://github.com/kubernetes/enhancements/issues/281">动态 kubelet 配置</a>
允许你通过部署一个所有节点都会使用的 ConfigMap
达到在运行中的 Kubernetes 集群中更改 kubelet 配置的目的。</p>
<!--
All kubelet configuration parameters can be changed dynamically,
but this is unsafe for some parameters. Before deciding to change a parameter
dynamically, you need a strong understanding of how that change will affect your
cluster's behavior. Always carefully test configuration changes on a small set
of nodes before rolling them out cluster-wide. Advice on configuring specific
fields is available in the inline
[`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/).
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 所有 kubelet 配置参数都可以被动态更改，但对某些参数来说这类更改是不安全的。
在决定动态更改参数之前，你需要深刻理解这个改动将会如何影响集群的行为。
在将变更扩散到整个集群之前，你需要先在小规模的节点集合上仔细地测试这些配置变动。
特定字段相关的配置建议可以在文档
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>中找到。
</div>


<h2 id="before-you-begin">Before you begin</h2>
<!--
You need to have a Kubernetes cluster.
You also need `kubectl`, [installed](/docs/tasks/tools/#kubectl) and configured to communicate with your cluster.
Make sure that you are using a version of `kubectl` that is
[compatible](/releases/version-skew-policy/) with your cluster.

 To check the version, enter <code>kubectl version</code>.

-->
<p>你需要一个 Kubernetes 集群。
你还需要 <code>kubectl</code>，<a href="/zh/docs/tasks/tools/#kubectl">安装</a>并配置好与集群的通信。

 To check the version, enter <code>kubectl version</code>.

确保你使用的 <code>kubectl</code> 版本与集群 <a href="/releases/version-skew-policy/">兼容</a>。</p>
<!--
Some of the examples use the command line tool
[jq](https://stedolan.github.io/jq/). You do not need `jq` to complete the task,
because there are manual alternatives.

For each node that you're reconfiguring, you must set the kubelet
`-dynamic-config-dir` flag to a writable directory.
-->
<p>在某些例子中使用了命令行工具 <a href="https://stedolan.github.io/jq/">jq</a>。
你并不一定需要 <code>jq</code> 才能完成这些任务，因为总是有一些手工替代的方式。</p>
<p>针对你重新配置的每个节点，你必须设置 kubelet 的标志
<code>-dynamic-config-dir</code>，使之指向一个可写的目录。</p>
<!-- steps -->
<!--
## Reconfiguring the kubelet on a running node in your cluster

### Basic Workflow Overview
-->
<h2 id="重配置-集群中运行节点上的-kubelet">重配置 集群中运行节点上的 kubelet</h2>
<h3 id="基本工作流程概览">基本工作流程概览</h3>
<!--
The basic workflow for configuring a Kubelet in a live cluster is as follows:

1. Write a YAML or JSON configuration file containing the
   kubelet's configuration.
2. Wrap this file in a ConfigMap and save it to the Kubernetes control plane.
3. Update the Kubelet's corresponding Node object to use this ConfigMap.
-->
<p>在运行中的集群中配置 kubelet 的基本工作流程如下：</p>
<ol>
<li>编写一个包含 kubelet 配置的 YAML 或 JSON 文件。</li>
<li>将此文件包装在 ConfigMap 中并将其保存到 Kubernetes 控制平面。</li>
<li>更新 kubelet 所在节点对象以使用此 ConfigMap。</li>
</ol>
<!--
Each kubelet watches a configuration reference on its respective Node object.
When this reference changes, the Kubelet downloads the new configuration,
updates a local reference to refer to the file, and exits.
For the feature to work correctly, you must be running an OS-level service
manager (such as systemd), which will restart the Kubelet if it exits. When the
Kubelet is restarted, it will begin using the new configuration.
-->
<p>每个 kubelet 都会在其各自的节点对象上监测（Watch）配置引用。当引用更改时，kubelet 将下载新的配置文件，
更新本地引用指向该文件，然后退出。
为了使该功能正常地工作，你必须运行操作系统级别的服务管理器（如 systemd），
它将会在 kubelet 退出后将其重启。
kubelet 重新启动时，将开始使用新配置。</p>
<!--
The new configuration completely overrides configuration provided by `--config`,
and is overridden by command-line flags. Unspecified values in the new configuration
will receive default values appropriate to the configuration version
(e.g. `kubelet.config.k8s.io/v1beta1`), unless overridden by flags.
-->
<p>新配置将会完全地覆盖 <code>--config</code> 所提供的配置，并被命令行标志覆盖。
新配置中未指定的值将收到适合配置版本的默认值
(e.g. <code>kubelet.config.k8s.io/v1beta1</code>)，除非被命令行标志覆盖。</p>
<!--
The status of the Node's Kubelet configuration is reported via
`Node.Spec.Status.Config`. Once you have updated a Node to use the new
ConfigMap, you can observe this status to confirm that the Node is using the
intended configuration.
-->
<p>节点 kubelet 配置状态可通过 <code>node.spec.status.config</code> 获取。
一旦你更新了一个节点去使用新的 ConfigMap，
就可以通过观察此状态来确认该节点是否正在使用预期配置。</p>
<!--
This document describes editing Nodes using `kubectl edit`.
There are other ways to modify a Node's spec, including `kubectl patch`, for
example, which facilitate scripted workflows.
-->
<p>本文中使用命令 <code>kubectl edit</code> 来编辑节点，还有其他的方式可以修改节点的规约，
比如更利于脚本化工作流程的 <code>kubectl patch</code>。</p>
<!--
This document only describes a single Node consuming each ConfigMap. Keep in
mind that it is also valid for multiple Nodes to consume the same ConfigMap.
-->
<p>本文仅仅讲述在单节点上使用每个 ConfigMap。请注意对于多个节点使用相同的 ConfigMap
也是合法的。</p>
<!--
While it is *possible* to change the configuration by
updating the ConfigMap in-place, this causes all Kubelets configured with
that ConfigMap to update simultaneously. It is much safer to treat ConfigMaps
as immutable by convention, aided by `kubectl`'s `-append-hash` option,
and incrementally roll out updates to `Node.Spec.ConfigSource`.
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 尽管通过就地更新 ConfigMap 来更改配置是 <em>可能的</em>。
但是这样做会导致所有使用该 ConfigMap 配置的 kubelet 同时更新。
更安全的做法是按惯例将 ConfigMap 视为不可变更的，借助于
<code>kubectl</code> 的 <code>--append-hash</code> 选项逐步把更新推广到 <code>node.spec.configSource</code>。
</div>


<!--
### Automatic RBAC rules for Node Authorizer

Previously, you were required to manually create RBAC rules
to allow Nodes to access their assigned ConfigMaps. The Node Authorizer now
automatically configures these rules.
-->
<h3 id="节点鉴权器的自动-rbac-规则">节点鉴权器的自动 RBAC 规则</h3>
<p>以前，你需要手动创建 RBAC 规则以允许节点访问其分配的 ConfigMap。节点鉴权器现在
能够自动配置这些规则。</p>
<!--
### Generating a file that contains the current configuration

The Dynamic Kubelet Configuration feature allows you to provide an override for
the entire configuration object, rather than a per-field overlay. This is a
simpler model that makes it easier to trace the source of configuration values
and debug issues. The compromise, however, is that you must start with knowledge
of the existing configuration to ensure that you only change the fields you
intend to change.
-->
<h3 id="生成包含当前配置的文件">生成包含当前配置的文件</h3>
<p>动态 kubelet 配置特性允许你为整个配置对象提供一个重载配置，而不是靠单个字段的叠加。
这是一个更简单的模型，可以更轻松地跟踪配置值的来源，更便于调试问题。
然而，相应的代价是你必须首先了解现有配置，以确保你只更改你打算修改的字段。</p>
<!--
The kubelet loads settings from its configuration file, but you can set command
line flags to override the configuration in the file. This means that if you
only know the contents of the configuration file, and you don't know the
command line overrides, then you do not know the running configuration either.
-->
<p>组件 kubelet 从其配置文件中加载配置数据，不过你可以通过设置命令行标志
来重载文件中的一些配置。这意味着，如果你仅知道配置文件的内容，而你不知道
命令行重载了哪些配置，你就无法知道 kubelet 的运行时配置是什么。</p>
<!--
Because you need to know the running configuration in order to override it,
you can fetch the running configuration from the kubelet. You can generate a
config file containing a Node's current configuration by accessing the kubelet's
`configz` endpoint, through `kubectl proxy`. The next section explains how to
do this.
-->
<p>因为你需要知道运行时所使用的配置才能重载之，你可以从 kubelet 取回其运行时配置。
你可以通过访问 kubelet 的 <code>configz</code> 末端来生成包含节点当前配置的配置文件；
这一操作可以通过 <code>kubectl proxy</code> 来完成。
下一节解释如何完成这一操作。</p>
<!--
The kubelet's `configz` endpoint is there to help with debugging, and is not
a stable part of kubelet behavior.
Do not rely on the behavior of this endpoint for production scenarios or for
use with automated tools.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 组件 <code>kubelet</code> 上的 <code>configz</code> 末端是用来协助调试的，并非 kubelet 稳定行为的一部分。
请不要在产品环境下依赖此末端的行为，也不要在自动化工具中使用此末端。
</div>

<!--
For more information on configuring the kubelet via a configuration file, see
[Set kubelet parameters via a config file](/docs/tasks/administer-cluster/kubelet-config-file)).
-->
<p>关于如何使用配置文件来配置 kubelet 行为的更多信息可参见
<a href="/zh/docs/tasks/administer-cluster/kubelet-config-file">通过配置文件设置 kubelet 参数</a>
文档。</p>
<!-- #### Generate the configuration file -->
<h4 id="生成配置文件">生成配置文件</h4>
<!--
The steps below use the `jq` command to streamline working with JSON.
To follow the tasks as written, you need to have `jq` installed. You can
adapt the steps if you prefer to extract the `kubeletconfig` subobject manually.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 下面的任务步骤中使用了 <code>jq</code> 命令以方便处理 JSON 数据。为了完成这里讲述的任务，
你需要安装 <code>jq</code>。如果你更希望手动提取 <code>kubeletconfig</code> 子对象，也可以对这里
的对应步骤做一些调整。
</div>
<!--
1. Choose a Node to reconfigure. In this example, the name of this Node is
   referred to as `NODE_NAME`.
2. Start the kubectl proxy in the background using the following command:
-->
<ol>
<li>
<p>选择要重新配置的节点。在本例中，此节点的名称为 <code>NODE_NAME</code>。</p>
</li>
<li>
<p>使用以下命令在后台启动 kubectl 代理：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8001</span> &amp;
</code></pre></div></li>
</ol>
<!--
3. Run the following command to download and unpack the configuration from the
   `configz` endpoint. The command is long, so be careful when copying and
   pasting. **If you use zsh**, note that common zsh configurations add backslashes
   to escape the opening and closing curly braces around the variable name in the URL.
   For example: `${NODE_NAME}` will be rewritten as `$\{NODE_NAME\}` during the paste.
   You must remove the backslashes before running the command, or the command will fail.
-->
<ol start="3">
<li>
<p>运行以下命令从 <code>configz</code> 端点中下载并解压配置。这个命令很长，因此在复制粘贴时要小心。
<strong>如果你使用 zsh</strong>，请注意常见的 zsh 配置要添加反斜杠转义 URL 中变量名称周围的大括号。
例如：在粘贴时，<code>${NODE_NAME}</code> 将被重写为 <code>$\{NODE_NAME\}</code>。
你必须在运行命令之前删除反斜杠，否则命令将失败。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#b8860b">NODE_NAME</span><span style="color:#666">=</span><span style="color:#b44">&#34;the-name-of-the-node-you-are-reconfiguring&#34;</span>; curl -sSL <span style="color:#b44">&#34;http://localhost:8001/api/v1/nodes/</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">/proxy/configz&#34;</span> | jq <span style="color:#b44">&#39;.kubeletconfig|.kind=&#34;KubeletConfiguration&#34;|.apiVersion=&#34;kubelet.config.k8s.io/v1beta1&#34;&#39;</span> &gt; kubelet_configz_<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span>
</code></pre></div></li>
</ol>
<!--
You need to manually add the `kind` and `apiVersion` to the downloaded
object，because they are not reported by the `configz` endpoint。
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你需要手动将 <code>kind</code> 和 <code>apiVersion</code> 添加到下载对象中，因为它们不是由 <code>configz</code> 末端
返回的。
</div>
<!--
#### Edit the configuration file

Using a text editor, change one of the parameters in the
file generated by the previous procedure. For example, you
might edit the QPS parameter `eventRecordQPS`.
-->
<h4 id="修改配置文件">修改配置文件</h4>
<p>使用文本编辑器，改变上述操作生成的文件中一个参数。
例如，你或许会修改 QPS 参数 <code>eventRecordQPS</code>。</p>
<!--
#### Push the configuration file to the control plane

Push the edited configuration file to the control plane with the
following command:
-->
<h4 id="把配置文件推送到控制平面">把配置文件推送到控制平面</h4>
<p>用以下命令把编辑后的配置文件推送到控制平面：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl -n kube-system create configmap my-node-config <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --from-file<span style="color:#666">=</span><span style="color:#b8860b">kubelet</span><span style="color:#666">=</span>kubelet_configz_<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --append-hash -o yaml
</code></pre></div><!--
This is an example of a valid response:
-->
<p>下面是合法响应的一个例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2017-09-14T20:23:33Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-node-config-gkt4c2m4b2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;119980&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selfLink</span>:<span style="color:#bbb"> </span>/api/v1/namespaces/kube-system/configmaps/my-node-config-gkt4c2m4b2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>946d785e-998a-11e7-a8dd-42010a800006<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kubelet</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>{...}<span style="color:#bbb">
</span></code></pre></div><!--
You created that ConfigMap inside the `kube-system` namespace because the kubelet
is a Kubernetes system component.
-->
<p>你会在 <code>kube-system</code> 命名空间中创建 ConfigMap，因为 kubelet 是 Kubernetes 的系统组件。</p>
<!--
The `-append-hash` option appends a short checksum of the ConfigMap contents
to the name. This is convenient for an edit-then-push workflow, because it
automatically, yet deterministically, generates new names for new ConfigMaps.
The name that includes this generated hash is referred to as `CONFIG_MAP_NAME`
in the following examples.
-->
<p><code>--append-hash</code> 选项给 ConfigMap 内容附加了一个简短校验和。
这对于先编辑后推送的工作流程很方便，
因为它自动并确定地为新 ConfigMap 生成新的名称。
在以下示例中，包含生成的哈希字符串的对象名被称为 <code>CONFIG_MAP_NAME</code>。</p>
<!--
#### Set the Node to use the new configuration


Edit the Node's reference to point to the new ConfigMap with the
following command:
-->
<h4 id="配置节点使用新的配置">配置节点使用新的配置</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl edit node <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span>
</code></pre></div><!--
In your text editor, add the following YAML under `spec`:
-->
<p>在你的文本编辑器中，在 <code>spec</code> 下增添以下 YAML：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">configSource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>CONFIG_MAP_NAME<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kubeletConfigKey</span>:<span style="color:#bbb"> </span>kubelet<span style="color:#bbb">
</span></code></pre></div><!--
You must specify all three of `name`, `namespace`, and `kubeletConfigKey`.
The `kubeletConfigKey` parameter shows the Kubelet which key of the ConfigMap
contains its config.
-->
<p>你必须同时指定 <code>name</code>、<code>namespace</code> 和 <code>kubeletConfigKey</code> 这三个属性。
<code>kubeletConfigKey</code> 这个参数通知 kubelet ConfigMap 中的哪个键下面包含所要的配置。</p>
<!--
#### Observe that the Node begins using the new configuration

Retrieve the Node using the `kubectl get node ${NODE_NAME} -o yaml` command and inspect
`Node.Status.Config`. The config sources corresponding to the `active`,
`assigned`, and `lastKnownGood` configurations are reported in the status.

- The `active` configuration is the version the Kubelet is currently running with.
- The `assigned` configuration is the latest version the Kubelet has resolved based on
  `Node.Spec.ConfigSource`.
- The `lastKnownGood` configuration is the version the
  Kubelet will fall back to if an invalid config is assigned in `Node.Spec.ConfigSource`.
-->
<h4 id="观察节点开始使用新配置">观察节点开始使用新配置</h4>
<p>用 <code>kubectl get node ${NODE_NAME} -o yaml</code> 命令读取节点并检查 <code>node.status.config</code> 内容。
状态部分报告了对应 <code>active</code>（使用中的）配置、<code>assigned</code>（被赋予的）配置和
<code>lastKnownGood</code>（最近已知可用的）配置的配置源。</p>
<ul>
<li><code>active</code> 是 kubelet 当前运行时所使用的版本。</li>
<li><code>assigned</code> 参数是 kubelet 基于 <code>node.spec.configSource</code> 所解析出来的最新版本。</li>
<li><code>lastKnownGood</code> 参数是 kubelet 的回退版本；如果在 <code>node.spec.configSource</code> 中
包含了无效的配置值，kubelet 可以回退到这个版本。</li>
</ul>
<!--
The`lastKnownGood` configuration might not be present if it is set to its default value,
the local config deployed with the node. The status will update `lastKnownGood` to
match a valid `assigned` config after the Kubelet becomes comfortable with the config.
The details of how the Kubelet determines a config should become the `lastKnownGood` are
not guaranteed by the API, but is currently implemented as a 10-minute grace period.
-->
<p>如果用本地配置部署节点，使其设置成默认值，这个 <code>lastKnownGood</code> 配置可能不存在。
在 kubelet 配置好后，将更新 <code>lastKnownGood</code> 为一个有效的 <code>assigned</code> 配置。
决定如何确定某配置成为 <code>lastKnownGood</code> 配置的细节并不在 API 保障范畴，
不过目前实现中采用了 10 分钟的宽限期。</p>
<!--
You can use the following command (using `jq`) to filter down
to the config status:
-->
<p>你可以使用以下命令（使用 <code>jq</code>）过滤出配置状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get no <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span> -o json | jq <span style="color:#b44">&#39;.status.config&#39;</span>
</code></pre></div><!--
The following is an example response:
-->
<p>以下是一个响应示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;active&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;configMap&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;kubeletConfigKey&#34;</span>: <span style="color:#b44">&#34;kubelet&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;my-node-config-9mbkccg2cc&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;kube-system&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;resourceVersion&#34;</span>: <span style="color:#b44">&#34;1326&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;uid&#34;</span>: <span style="color:#b44">&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span>
    }
  },
  <span style="color:#008000;font-weight:bold">&#34;assigned&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;configMap&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;kubeletConfigKey&#34;</span>: <span style="color:#b44">&#34;kubelet&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;my-node-config-9mbkccg2cc&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;kube-system&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;resourceVersion&#34;</span>: <span style="color:#b44">&#34;1326&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;uid&#34;</span>: <span style="color:#b44">&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span>
    }
  },
  <span style="color:#008000;font-weight:bold">&#34;lastKnownGood&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;configMap&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;kubeletConfigKey&#34;</span>: <span style="color:#b44">&#34;kubelet&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;my-node-config-9mbkccg2cc&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;kube-system&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;resourceVersion&#34;</span>: <span style="color:#b44">&#34;1326&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;uid&#34;</span>: <span style="color:#b44">&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span>
    }
  }
}
</code></pre></div><!--
If you do not have `jq`, you can look at the whole response and find `Node.Status.Config`
by eye.
-->
<p>如果你没有安装 <code>jq</code>，你可以查看整个响应对象，查找其中的 <code>node.status.config</code>
部分。</p>
<!--
If an error occurs, the Kubelet reports it in the `Node.Status.Config.Error`
structure. Possible errors are listed in
[Understanding Node.Status.Config.Error messages](#understanding-node-config-status-errors).
You can search for the identical text in the Kubelet log for additional details
and context about the error.
-->
<p>如果发生错误，kubelet 会在 <code>Node.Status.Config.Error</code> 中显示出错误信息的结构体。
错误可能出现在列表<a href="#understanding-node-config-status-errors">理解节点状态配置错误信息</a>中。
你可以在 kubelet 日志中搜索相同的文本以获取更多详细信息和有关错误的上下文。</p>
<!--
#### Make more changes

Follow the workflow above to make more changes and push them again. Each time
you push a ConfigMap with new contents, the -append-hash kubectl option creates
the ConfigMap with a new name. The safest rollout strategy is to first create a
new ConfigMap, and then update the Node to use the new ConfigMap.
-->
<h4 id="make-more-changes">做出更多的改变  </h4>
<p>按照下面的工作流程做出更多的改变并再次推送它们。
你每次推送一个 ConfigMap 的新内容时，kubectl 的 <code>--append-hash</code> 选项都会给
ConfigMap 创建一个新的名称。
最安全的上线策略是首先创建一个新的 ConfigMap，然后更新节点以使用新的 ConfigMap。</p>
<!--
#### Reset the Node to use its local default configuration

To reset the Node to use the configuration it was provisioned with, edit the
Node using `kubectl edit node ${NODE_NAME}` and remove the
`Node.Spec.ConfigSource` field.
-->
<h4 id="重置节点以使用其本地默认配置">重置节点以使用其本地默认配置</h4>
<p>要重置节点，使其使用节点创建时使用的配置，可以用
<code>kubectl edit node $ {NODE_NAME}</code> 命令编辑节点，并删除 <code>node.spec.configSource</code>
字段。</p>
<!-- 
#### Observe that the Node is using its local default configuration

After removing this subfield, `Node.Status.Config` eventually becomes
empty, since all config sources have been reset to `nil`, which indicates that
the local default config is `assigned`, `active`, and `lastKnownGood`, and no
error is reported.
-->
<h4 id="观察节点正在使用本地默认配置">观察节点正在使用本地默认配置</h4>
<p>在删除此字段后，<code>node.status.config</code> 最终变成空，所有配置源都已重置为 <code>nil</code>。
这表示本地默认配置成为了 <code>assigned</code>、<code>active</code> 和 <code>lastKnownGood</code> 配置，
并且没有报告错误。</p>
<!-- discussion -->
<!--
## `kubectl patch` example

You can change a Node's configSource using several different mechanisms.
This example uses `kubectl patch`:
-->
<h2 id="kubectl-patch-示例"><code>kubectl patch</code> 示例</h2>
<p>你可以使用几种不同的机制来更改节点的 configSource。</p>
<p>本例使用<code>kubectl patch</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl patch node <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span> -p <span style="color:#b44">&#34;{\&#34;spec\&#34;:{\&#34;configSource\&#34;:{\&#34;configMap\&#34;:{\&#34;name\&#34;:\&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CONFIG_MAP_NAME</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">\&#34;,\&#34;namespace\&#34;:\&#34;kube-system\&#34;,\&#34;kubeletConfigKey\&#34;:\&#34;kubelet\&#34;}}}}&#34;</span>
</code></pre></div><!--
## Understanding how the Kubelet checkpoints config

When a new config is assigned to the Node, the Kubelet downloads and unpacks the
config payload as a set of files on the local disk. The Kubelet also records metadata
that locally tracks the assigned and last-known-good config sources, so that the
Kubelet knows which config to use across restarts, even if the API server becomes
unavailable. After checkpointing a config and the relevant metadata, the Kubelet
exits if it detects that the assigned config has changed. When the Kubelet is
restarted by the OS-level service manager (such as `systemd`), it reads the new
metadata and uses the new config.
-->
<h2 id="了解-kubelet-如何为配置生成检查点">了解 Kubelet 如何为配置生成检查点</h2>
<p>当为节点赋予新配置时，kubelet 会下载并解压配置负载为本地磁盘上的一组文件。
kubelet 还记录一些元数据，用以在本地跟踪已赋予的和最近已知良好的配置源，以便
kubelet 在重新启动时知道使用哪个配置，即使 API 服务器变为不可用。
在为配置信息和相关元数据生成检查点之后，如果检测到已赋予的配置发生改变，则 kubelet 退出。
当 kubelet 被 OS 级服务管理器（例如 <code>systemd</code>）重新启动时，它会读取新的元数据并使用新配置。</p>
<!--
The recorded metadata is fully resolved, meaning that it contains all necessary
information to choose a specific config version - typically a `UID` and `ResourceVersion`.
This is in contrast to `Node.Spec.ConfigSource`, where the intended config is declared
via the idempotent `namespace/name` that identifies the target ConfigMap; the Kubelet
tries to use the latest version of this ConfigMap.
-->
<p>当记录的元数据已被完全解析时，意味着它包含选择一个指定的配置版本所需的所有信息
-- 通常是 <code>UID</code> 和 <code>ResourceVersion</code>。
这与 <code>node.spec.configSource</code> 形成对比，后者通过幂等的 <code>namespace/name</code> 声明来标识
目标 ConfigMap；kubelet 尝试使用此 ConfigMap 的最新版本。</p>
<!--
When you are debugging problems on a node, you can inspect the Kubelet's config
metadata and checkpoints. The structure of the Kubelet's checkpointing directory is:
-->
<p>当你在调试节点上问题时，可以检查 kubelet 的配置元数据和检查点。kubelet 的检查点目录结构是：</p>
<!--
```none
- -dynamic-config-dir (root for managing dynamic config)
| - meta
  | - assigned (encoded kubeletconfig/v1beta1.SerializedNodeConfigSource object, indicating the assigned config)
  | - last-known-good (encoded kubeletconfig/v1beta1.SerializedNodeConfigSource object, indicating the last-known-good config)
| - checkpoints
  | - uid1 (dir for versions of object identified by uid1)
    | - resourceVersion1 (dir for unpacked files from resourceVersion1 of object with uid1)
    | - ...
  | - ...
```
-->
<pre><code class="language-none" data-lang="none">- --dynamic-config-dir （用于管理动态配置的根目录）
|-- meta
  | - assigned （编码后的 kubeletconfig/v1beta1.SerializedNodeConfigSource 对象，对应赋予的配置）
  | - last-known-good （编码后的 kubeletconfig/v1beta1.SerializedNodeConfigSource 对象，对应最近已知可用配置）
| - checkpoints
  | - uid1 （用 uid1 来标识的对象版本目录)
    | - resourceVersion1 （uid1 对象 resourceVersion1 版本下所有解压文件的目录）
    | - ...
  | - ...
</code></pre><!-- 
## Understanding `Node.Status.Config.Error` messages {#understanding-node-config-status-errors}

The following table describes error messages that can occur
when using Dynamic Kubelet Config. You can search for the identical text
in the Kubelet log for additional details and context about the error.
-->
<h2 id="understanding-node-config-status-errors">理解 <code>Node.Status.Config.Error</code> 消息</h2>
<p>下表描述了使用动态 kubelet 配置时可能发生的错误消息。
你可以在 kubelet 日志中搜索相同的文本来获取有关错误的其他详细信息和上下文。</p>
<!--
Error Message    | Possible Causes
:----------------| :----------------
failed to load config, see Kubelet log for details | The kubelet likely could not parse the downloaded config payload, or encountered a filesystem error attempting to load the payload from disk.
failed to validate config, see Kubelet log for details | The configuration in the payload, combined with any command-line flag overrides, and the sum of feature gates from flags, the config file, and the remote payload, was determined to be invalid by the kubelet.
invalid NodeConfigSource, exactly one subfield must be non-nil, but all were nil | Since Node.Spec.ConfigSource is validated by the API server to contain at least one non-nil subfield, this likely means that the kubelet is older than the API server and does not recognize a newer source type.
failed to sync: failed to download config, see Kubelet log for details | The kubelet could not download the config. It is possible that Node.Spec.ConfigSource could not be resolved to a concrete API object, or that network errors disrupted the download attempt. The kubelet will retry the download when in this error state.
failed to sync: internal failure, see Kubelet log for details | The kubelet encountered some internal problem and failed to update its config as a result. Examples include filesystem errors and reading objects from the internal informer cache.
internal failure, see Kubelet log for details | The kubelet encountered some internal problem while manipulating config, outside of the configuration sync loop.
-->





<table><caption style="display: none;">理解 node.status.config.error 消息</caption>
<thead>
<tr>
<th style="text-align:left">错误信息</th>
<th style="text-align:left">可能的原因</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">failed to load config, see Kubelet log for details</td>
<td style="text-align:left">kubelet 可能无法解析下载配置的有效负载，或者当尝试从磁盘中加载有效负载时，遇到文件系统错误。</td>
</tr>
<tr>
<td style="text-align:left">failed to validate config, see Kubelet log for details</td>
<td style="text-align:left">有效负载中的配置，与命令行标志所产生的覆盖配置以及特行门控的组合、配置文件本身、远程负载被 kubelet 判定为无效。</td>
</tr>
<tr>
<td style="text-align:left">invalid NodeConfigSource, exactly one subfield must be non-nil, but all were nil</td>
<td style="text-align:left">由于 API 服务器负责对 node.spec.configSource 执行验证，检查其中是否包含至少一个非空子字段，这个消息可能意味着 kubelet 比 API 服务器版本低，因而无法识别更新的源类型。</td>
</tr>
<tr>
<td style="text-align:left">failed to sync: failed to download config, see Kubelet log for details</td>
<td style="text-align:left">kubelet 无法下载配置数据。可能是 node.spec.configSource 无法解析为具体的 API 对象，或者网络错误破坏了下载。处于此错误状态时，kubelet 将重新尝试下载。</td>
</tr>
<tr>
<td style="text-align:left">failed to sync: internal failure, see Kubelet log for details</td>
<td style="text-align:left">kubelet 遇到了一些内部问题，因此无法更新其配置。 例如：发生文件系统错误或无法从内部缓存中读取对象。</td>
</tr>
<tr>
<td style="text-align:left">internal failure, see Kubelet log for details</td>
<td style="text-align:left">在对配置进行同步的循环之外操作配置时，kubelet 遇到了一些内部问题。</td>
</tr>
</tbody>
</table>

<h2 id="what-s-next">What's next</h2>
<!--
- [Set kubelet parameters via a config file](/docs/tasks/administer-cluster/kubelet-config-file)
  explains the supported way to configure a kubelet.
- See the reference documentation for Node, including the `configSource` field within
  the Node's [.spec](/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeSpec)
- Learn more about kubelet configuration by checking the
  [`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/)
  reference.
-->
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/kubelet-config-file">使用配置文件设置 kubelet 参数</a>说明了配置 kubelet 的方法。</li>
<li>阅读 Node 的参考文档，包括 <a href="/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeSpec">.spec</a> 里的 <code>configSource</code> 字段</li>
<li>查阅<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>文献进一步了解 kubelet
配置信息。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4e9de5bc3973e5d2bb8f09ff940c3319">24 - 在集群中使用级联删除</h1>
    
	<!--
title: Use Cascading Deletion in a Cluster
content_type: task
-->
<!--overview-->
<!--
This page shows you how to specify the type of
[cascading deletion](/docs/concepts/architecture/garbage-collection/#cascading-deletion)
to use in your cluster during <a class='glossary-tooltip' title='Kubernetes 用于清理集群资源的各种机制的统称。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/garbage-collection/' target='_blank' aria-label='garbage collection'>garbage collection</a>.
-->
<p>本页面向你展示如何设置在你的集群执行<a class='glossary-tooltip' title='Kubernetes 用于清理集群资源的各种机制的统称。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/garbage-collection/' target='_blank' aria-label='垃圾收集'>垃圾收集</a>
时要使用的<a href="/zh/docs/concepts/architecture/garbage-collection/#cascading-deletion">级联删除</a>
类型。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!--
You also need to [create a sample Deployment](/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment) 
to experiment with the different types of cascading deletion. You will need to
recreate the Deployment for each type.
-->
<p>你还需要<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment">创建一个 Deployment 示例</a>
以试验不同类型的级联删除。你需要为每种级联删除类型来重建 Deployment。</p>
<!--
## Check owner references on your pods

Check that the `ownerReferences` field is present on your pods:
-->
<h2 id="check-owner-references-on-your-pods">检查 Pod 上的属主引用   </h2>
<p>检查确认你的 Pods 上存在 <code>ownerReferences</code> 字段：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output has an `ownerReferences` field similar to this:
-->
<p>输出中包含 <code>ownerReferences</code> 字段，类似这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ownerReferences</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">blockOwnerDeletion</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">controller</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment-6b474476c4<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>4fdcd81c-bd5d-41f7-97af-3a3b759af9a7<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span></code></pre></div><!--
## Use foreground cascading deletion {#use-foreground-cascading-deletion}

By default, Kubernetes uses [background cascading deletion](/docs/concepts/architecture/garbage-collection/#background-deletion)
to delete dependents of an object. You can switch to foreground cascading deletion
using either `kubectl` or the Kubernetes API, depending on the Kubernetes
version your cluster runs. 
 To check the version, enter <code>kubectl version</code>.

-->
<h2 id="use-foreground-cascading-deletion">使用前台级联删除   </h2>
<p>默认情况下，Kubernetes 使用<a href="/zh/docs/concepts/architecture/garbage-collection/#background-deletion">后台级联删除</a>
以删除依赖某对象的其他对象。取决于你的集群所运行的 Kubernetes 版本，
你可以使用 <code>kubectl</code> 或者 Kubernetes API 来切换到前台级联删除。

 To check the version, enter <code>kubectl version</code>.
</p>
<ul class="nav nav-tabs" id="foreground-deletion" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#foreground-deletion-0" role="tab" aria-controls="foreground-deletion-0" aria-selected="true">Kubernetes 1.20.x 及更新版本</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#foreground-deletion-1" role="tab" aria-controls="foreground-deletion-1">Kubernetes 1.20.x 之前的版本</a></li></ul>
<div class="tab-content" id="foreground-deletion"><div id="foreground-deletion-0" class="tab-pane show active" role="tabpanel" aria-labelledby="foreground-deletion-0">

<p><!--
You can delete objects using foreground cascading deletion using `kubectl` or the
Kubernetes API.
-->
<p>你可以使用 <code>kubectl</code> 或者 Kubernetes API 来基于前台级联删除来删除对象。</p>
<!--
**Using kubectl**

Run the following command:
-->
<p><strong>使用 kubectl</strong></p>
<p>运行下面的命令：</p>
<!--TODO: verify release after which the --cascade flag is switched to a string in https://github.com/kubernetes/kubectl/commit/fd930e3995957b0093ecc4b9fd8b0525d94d3b4e-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment nginx-deployment --cascade<span style="color:#666">=</span>foreground
</code></pre></div><!--
**Using the Kubernetes API**
-->
<p><strong>使用 Kubernetes API</strong></p>
<!--
1. Start a local proxy session:
-->
<ol>
<li>
<p>启动一个本地代理会话：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</code></pre></div></li>
</ol>
<!--
1. Use `curl` to trigger deletion:
-->
<ol start="2">
<li>
<p>使用 <code>curl</code> 来触发删除操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -d <span style="color:#b44">&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -H <span style="color:#b44">&#34;Content-Type: application/json&#34;</span>
</code></pre></div><!--
The output contains a `foregroundDeletion` <a class='glossary-tooltip' title='一个带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后， 再完全删除被标记为删除的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/finalizers/' target='_blank' aria-label='finalizer'>finalizer</a>
like this:
-->
<p>输出中包含 <code>foregroundDeletion</code> <a class='glossary-tooltip' title='一个带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后， 再完全删除被标记为删除的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/finalizers/' target='_blank' aria-label='finalizer'>finalizer</a>，
类似这样：</p>
<pre><code>&quot;kind&quot;: &quot;Deployment&quot;,
&quot;apiVersion&quot;: &quot;apps/v1&quot;,
&quot;metadata&quot;: {
    &quot;name&quot;: &quot;nginx-deployment&quot;,
    &quot;namespace&quot;: &quot;default&quot;,
    &quot;uid&quot;: &quot;d1ce1b02-cae8-4288-8a53-30e84d8fa505&quot;,
    &quot;resourceVersion&quot;: &quot;1363097&quot;,
    &quot;creationTimestamp&quot;: &quot;2021-07-08T20:24:37Z&quot;,
    &quot;deletionTimestamp&quot;: &quot;2021-07-08T20:27:39Z&quot;,
    &quot;finalizers&quot;: [
      &quot;foregroundDeletion&quot;
    ]
    ...
</code></pre></li>
</ol>
</div>
  <div id="foreground-deletion-1" class="tab-pane" role="tabpanel" aria-labelledby="foreground-deletion-1">

<p><!--
You can delete objects using foreground cascading deletion by calling the
Kubernetes API.

For details, read the [documentation for your Kubernetes version](/docs/home/supported-doc-versions/).
-->
<p>你可以通过调用 Kubernetes API 来基于前台级联删除模式删除对象。</p>
<p>进一步的细节，可阅读<a href="/zh/docs/home/supported-doc-versions">特定于你的 Kubernetes 版本的文档</a>。</p>
<!--
1. Start a local proxy session:
-->
<ol>
<li>
<p>启动一个本地代理会话：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</code></pre></div></li>
</ol>
<!--
1. Use `curl` to trigger deletion:
-->
<ol start="2">
<li>
<p>使用 <code>curl</code> 来触发删除操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -d <span style="color:#b44">&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -H <span style="color:#b44">&#34;Content-Type: application/json&#34;</span>
</code></pre></div><!--
The output contains a `foregroundDeletion` <a class='glossary-tooltip' title='一个带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后， 再完全删除被标记为删除的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/finalizers/' target='_blank' aria-label='finalizer'>finalizer</a>
like this:
-->
<p>输出中包含 <code>foregroundDeletion</code> <a class='glossary-tooltip' title='一个带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后， 再完全删除被标记为删除的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/finalizers/' target='_blank' aria-label='finalizer'>finalizer</a>，
类似这样：</p>
<pre><code>&quot;kind&quot;: &quot;Deployment&quot;,
&quot;apiVersion&quot;: &quot;apps/v1&quot;,
&quot;metadata&quot;: {
    &quot;name&quot;: &quot;nginx-deployment&quot;,
    &quot;namespace&quot;: &quot;default&quot;,
    &quot;uid&quot;: &quot;d1ce1b02-cae8-4288-8a53-30e84d8fa505&quot;,
    &quot;resourceVersion&quot;: &quot;1363097&quot;,
    &quot;creationTimestamp&quot;: &quot;2021-07-08T20:24:37Z&quot;,
    &quot;deletionTimestamp&quot;: &quot;2021-07-08T20:27:39Z&quot;,
    &quot;finalizers&quot;: [
      &quot;foregroundDeletion&quot;
    ]
    ...
</code></pre></li>
</ol>
</div></div>

<!--
## Use background cascading deletion {#use-background-cascading-deletion}
-->
<h2 id="use-background-cascading-deletion">使用后台级联删除</h2>
<!--
1. [Create a sample Deployment](/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment).
1. Use either `kubectl` or the Kubernetes API to delete the Deployment,
   depending on the Kubernetes version your cluster runs. 
 To check the version, enter <code>kubectl version</code>.

-->
<ol>
<li><a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/#creating-and-exploring-an-nginx-deployment">创建一个 Deployment 示例</a>。</li>
<li>基于你的集群所运行的 Kubernetes 版本，使用 <code>kubectl</code> 或者 Kubernetes API 来删除 Deployment。

 To check the version, enter <code>kubectl version</code>.
</li>
</ol>
<ul class="nav nav-tabs" id="background-deletion" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#background-deletion-0" role="tab" aria-controls="background-deletion-0" aria-selected="true">Kubernetes 1.20.x 及更新版本</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#background-deletion-1" role="tab" aria-controls="background-deletion-1">Kubernetes 1.20.x 之前的版本</a></li></ul>
<div class="tab-content" id="background-deletion"><div id="background-deletion-0" class="tab-pane show active" role="tabpanel" aria-labelledby="background-deletion-0">

<p><!--
You can delete objects using background cascading deletion using `kubectl`
or the Kubernetes API.

Kubernetes uses background cascading deletion by default, and does so
even if you run the following commands without the `--cascade` flag or the
`propagationPolicy` argument.
-->
<p>你可以使用 <code>kubectl</code> 或者 Kubernetes API 来执行后台级联删除方式的对象删除操作。</p>
<p>Kubernetes 默认采用后台级联删除方式，如果你在运行下面的命令时不指定
<code>--cascade</code> 标志或者 <code>propagationPolicy</code> 参数时，用这种方式来删除对象。</p>
<!--
**Using kubectl**

Run the following command:
-->
<p><strong>使用 kubectl</strong></p>
<p>运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment nginx-deployment --cascade<span style="color:#666">=</span>background
</code></pre></div><!--
**Using the Kubernetes API**
-->
<p><strong>使用 Kubernetes API</strong></p>
<!--
1. Start a local proxy session:
-->
<ol>
<li>
<p>启动一个本地代理会话：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</code></pre></div></li>
</ol>
<!--
1. Use `curl` to trigger deletion:
-->
<ol start="2">
<li>
<p>使用 <code>curl</code> 来触发删除操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -d <span style="color:#b44">&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Background&#34;}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -H <span style="color:#b44">&#34;Content-Type: application/json&#34;</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>&quot;kind&quot;: &quot;Status&quot;,
&quot;apiVersion&quot;: &quot;v1&quot;,
...
&quot;status&quot;: &quot;Success&quot;,
&quot;details&quot;: {
    &quot;name&quot;: &quot;nginx-deployment&quot;,
    &quot;group&quot;: &quot;apps&quot;,
    &quot;kind&quot;: &quot;deployments&quot;,
    &quot;uid&quot;: &quot;cc9eefb9-2d49-4445-b1c1-d261c9396456&quot;
}
</code></pre></li>
</ol>
</div>
  <div id="background-deletion-1" class="tab-pane" role="tabpanel" aria-labelledby="background-deletion-1">

<p><!--
Kubernetes uses background cascading deletion by default, and does so
even if you run the following commands without the `--cascade` flag or the
`propagationPolicy: Background` argument.
-->
<p>Kubernetes 默认采用后台级联删除方式，如果你在运行下面的命令时不指定
<code>--cascade</code> 标志或者 <code>propagationPolicy</code> 参数时，用这种方式来删除对象。</p>
<!--
For details, read the [documentation for your Kubernetes version](/docs/home/supported-doc-versions/).
-->
<p>进一步的细节，可阅读<a href="/zh/docs/home/supported-doc-versions">特定于你的 Kubernetes 版本的文档</a>。</p>
<!--
**Using kubectl**

Run the following command:
-->
<p><strong>使用 kubectl</strong></p>
<p>运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment nginx-deployment --cascade<span style="color:#666">=</span><span style="color:#a2f">true</span>
</code></pre></div><!--
**Using the Kubernetes API**
-->
<p><strong>使用 Kubernetes API</strong></p>
<!--
1. Start a local proxy session:
-->
<ol>
<li>
<p>启动一个本地代理会话：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</code></pre></div></li>
</ol>
<!--
1. Use `curl` to trigger deletion:
-->
<ol start="2">
<li>
<p>使用 <code>curl</code> 来触发删除操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -d <span style="color:#b44">&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Background&#34;}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -H <span style="color:#b44">&#34;Content-Type: application/json&#34;</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>&quot;kind&quot;: &quot;Status&quot;,
&quot;apiVersion&quot;: &quot;v1&quot;,
...
&quot;status&quot;: &quot;Success&quot;,
&quot;details&quot;: {
    &quot;name&quot;: &quot;nginx-deployment&quot;,
    &quot;group&quot;: &quot;apps&quot;,
    &quot;kind&quot;: &quot;deployments&quot;,
    &quot;uid&quot;: &quot;cc9eefb9-2d49-4445-b1c1-d261c9396456&quot;
}
</code></pre></li>
</ol>
</div></div>

<!--
## Delete owner objects and orphan dependents {#set-orphan-deletion-policy}

By default, when you tell Kubernetes to delete an object, the
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> also deletes
dependent objects. You can make Kubernetes *orphan* these dependents using
`kubectl` or the Kubernetes API, depending on the Kubernetes version your
cluster runs. 
 To check the version, enter <code>kubectl version</code>.

-->
<h2 id="set-orphan-deletion-policy">删除属主对象和孤立的依赖对象  </h2>
<p>默认情况下，当你告诉 Kubernetes 删除某个对象时，
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a> 也会删除依赖该对象
的其他对象。
取决于你的集群所运行的 Kubernetes 版本，你也可以使用 <code>kubectl</code> 或者 Kubernetes
API 来让 Kubernetes <em>孤立</em> 这些依赖对象。
 To check the version, enter <code>kubectl version</code>.
</p>
<ul class="nav nav-tabs" id="orphan-objects" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#orphan-objects-0" role="tab" aria-controls="orphan-objects-0" aria-selected="true">Kubernetes 1.20.x 及更新版本</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#orphan-objects-1" role="tab" aria-controls="orphan-objects-1">Kubernetes 1.20.x 之前的版本</a></li></ul>
<div class="tab-content" id="orphan-objects"><div id="orphan-objects-0" class="tab-pane show active" role="tabpanel" aria-labelledby="orphan-objects-0">

<p><!--
**Using kubectl**

Run the following command:
-->
<p><strong>使用 kubectl</strong></p>
<p>运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment nginx-deployment --cascade<span style="color:#666">=</span>orphan
</code></pre></div><!--
**Using the Kubernetes API**
-->
<p><strong>使用 Kubernetes API</strong></p>
<!--
1. Start a local proxy session:
-->
<ol>
<li>
<p>启动一个本地代理会话：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</code></pre></div></li>
</ol>
<!--
1. Use `curl` to trigger deletion:
-->
<ol start="2">
<li>
<p>使用 <code>curl</code> 来触发删除操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -d <span style="color:#b44">&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -H <span style="color:#b44">&#34;Content-Type: application/json&#34;</span>
</code></pre></div><!--
The output contains `orphan` in the `finalizers` field, similar to this:
-->
<p>输出中在 <code>finalizers</code> 字段中包含 <code>orphan</code>，如下所示：</p>
<pre><code>&quot;kind&quot;: &quot;Deployment&quot;,
&quot;apiVersion&quot;: &quot;apps/v1&quot;,
&quot;namespace&quot;: &quot;default&quot;,
&quot;uid&quot;: &quot;6f577034-42a0-479d-be21-78018c466f1f&quot;,
&quot;creationTimestamp&quot;: &quot;2021-07-09T16:46:37Z&quot;,
&quot;deletionTimestamp&quot;: &quot;2021-07-09T16:47:08Z&quot;,
&quot;deletionGracePeriodSeconds&quot;: 0,
&quot;finalizers&quot;: [
  &quot;orphan&quot;
],
...
</code></pre></li>
</ol>
</div>
  <div id="orphan-objects-1" class="tab-pane" role="tabpanel" aria-labelledby="orphan-objects-1">

<p><!--
For details, read the [documentation for your Kubernetes version](/docs/home/supported-doc-versions/).
-->
<p>进一步的细节，可阅读<a href="/zh/docs/home/supported-doc-versions">特定于你的 Kubernetes 版本的文档</a>。</p>
<!--
**Using kubectl**

Run the following command:
-->
<p><strong>使用 kubectl</strong></p>
<p>运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment nginx-deployment --cascade<span style="color:#666">=</span>orphan
</code></pre></div><!--
**Using the Kubernetes API**
-->
<p><strong>使用 Kubernetes API</strong></p>
<!--
1. Start a local proxy session:
-->
<ol>
<li>
<p>启动一个本地代理会话：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</code></pre></div></li>
</ol>
<!--
1. Use `curl` to trigger deletion:
-->
<ol start="2">
<li>
<p>使用 <code>curl</code> 来触发删除操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -d <span style="color:#b44">&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -H <span style="color:#b44">&#34;Content-Type: application/json&#34;</span>
</code></pre></div><!--
The output contains `orphan` in the `finalizers` field, similar to this:
-->
<p>输出中在 <code>finalizers</code> 字段中包含 <code>orphan</code>，如下所示：</p>
<pre><code>&quot;kind&quot;: &quot;Deployment&quot;,
&quot;apiVersion&quot;: &quot;apps/v1&quot;,
&quot;namespace&quot;: &quot;default&quot;,
&quot;uid&quot;: &quot;6f577034-42a0-479d-be21-78018c466f1f&quot;,
&quot;creationTimestamp&quot;: &quot;2021-07-09T16:46:37Z&quot;,
&quot;deletionTimestamp&quot;: &quot;2021-07-09T16:47:08Z&quot;,
&quot;deletionGracePeriodSeconds&quot;: 0,
&quot;finalizers&quot;: [
  &quot;orphan&quot;
],
...
</code></pre></li>
</ol>
</div></div>

<!--
You can check that the Pods managed by the Deployment are still running:
-->
<p>你可以检查 Deployment 所管理的 Pods 仍然处于运行状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
* Learn about [owners and dependents](/docs/concepts/overview/working-with-objects/owners-dependents/) in Kubernetes.
* Learn about Kubernetes [finalizers](/docs/concepts/overview/working-with-objects/finalizers/).
* Learn about [garbage collection](/docs/concepts/workloads/controllers/garbage-collection/).
-->
<ul>
<li>了解 Kubernetes 中的<a href="/zh/docs/concepts/overview/working-with-objects/owners-dependents/">属主与依赖</a></li>
<li>了解 Kubernetes <a href="/zh/docs/concepts/overview/working-with-objects/finalizers/">finalizers</a></li>
<li>了解<a href="/zh/docs/concepts/architecture/garbage-collection/">垃圾收集</a>.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a3790dfb57271d13517e549dffa805b9">25 - 声明网络策略</h1>
    
	<!--
reviewers:
- caseydavenport
- danwinship
title: Declare Network Policy
min-kubernetes-server-version: v1.8
content_type: task
-->
<!-- overview -->
<!--
This document helps you get started using the Kubernetes [NetworkPolicy API](/docs/concepts/services-networking/network-policies/) to declare network policies that govern how pods communicate with each other.
-->
<p>本文可以帮助你开始使用 Kubernetes 的
<a href="/zh/docs/concepts/services-networking/network-policies/">NetworkPolicy API</a>
声明网络策略去管理 Pod 之间的通信</p>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

Your Kubernetes server must be at or later than version v1.8.
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
Make sure you've configured a network provider with network policy support. There are a number of network providers that support NetworkPolicy, including:

* [Antrea](/docs/tasks/administer-cluster/network-policy-provider/antrea-network-policy/)
* [Calico](/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/)
* [Cilium](/docs/tasks/administer-cluster/network-policy-provider/cilium-network-policy/)
* [Kube-router](/docs/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/)
* [Romana](/docs/tasks/administer-cluster/network-policy-provider/romana-network-policy/)
* [Weave Net](/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/)
-->
<p>你首先需要有一个支持网络策略的 Kubernetes 集群。已经有许多支持 NetworkPolicy 的网络提供商，包括：</p>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/antrea-network-policy/">Antrea</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/">Calico</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/cilium-network-policy/">Cilium</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/">Kube-router</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/romana-network-policy/">Romana</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/">Weave 网络</a></li>
</ul>
<!-- steps -->
<!--
## Create an `nginx` deployment and expose it via a service

To see how Kubernetes network policy works, start off by creating an `nginx` Deployment.
-->
<h2 id="创建一个-nginx-deployment-并且通过服务将其暴露">创建一个<code>nginx</code> Deployment 并且通过服务将其暴露</h2>
<p>为了查看 Kubernetes 网络策略是怎样工作的，可以从创建一个<code>nginx</code> Deployment 并且通过服务将其暴露开始</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment nginx --image<span style="color:#666">=</span>nginx
</code></pre></div><pre><code class="language-none" data-lang="none">deployment.apps/nginx created
</code></pre><!--
Expose the Deployment through a Service called `nginx`.
-->
<p>将此 Deployment 以名为 <code>nginx</code> 的 Service 暴露出来：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose deployment nginx --port<span style="color:#666">=</span><span style="color:#666">80</span>
</code></pre></div><pre><code class="language-none" data-lang="none">service/nginx exposed
</code></pre><!--
The above commands create a Deployment with an nginx Pod and expose the Deployment through a Service named `nginx`. The `nginx` Pod and Deployment are found in the `default` namespace.
-->
<p>上述命令创建了一个带有一个 nginx 的 Deployment，并将之通过名为 <code>nginx</code> 的
Service 暴露出来。名为 <code>nginx</code> 的 Pod 和 Deployment 都位于 <code>default</code>
名字空间内。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc,pod
</code></pre></div><pre><code class="language-none" data-lang="none">NAME                        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
svc/kubernetes              10.100.0.1    &lt;none&gt;        443/TCP    46m
svc/nginx                   10.100.0.16   &lt;none&gt;        80/TCP     33s

NAME                        READY         STATUS        RESTARTS   AGE
po/nginx-701339712-e0qfq    1/1           Running       0          35s
</code></pre><!--
## Test the service by accessing it from another Pod

You should be able to access the new `nginx` service from other Pods. To access the `nginx` Service from another Pod in the `default` namespace, start a busybox container:
-->
<h2 id="通过从-pod-访问服务对其进行测试">通过从 Pod 访问服务对其进行测试</h2>
<p>你应该可以从其它的 Pod 访问这个新的 <code>nginx</code> 服务。
要从 default 命名空间中的其它s Pod 来访问该服务。可以启动一个 busybox 容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run busybox --rm -ti --image<span style="color:#666">=</span>busybox:1.28 /bin/sh
</code></pre></div><!--
In your shell, run the following command:
-->
<p>在你的 Shell 中，运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget --spider --timeout<span style="color:#666">=</span><span style="color:#666">1</span> nginx
</code></pre></div><pre><code class="language-none" data-lang="none">Connecting to nginx (10.100.0.16:80)
remote file exists
</code></pre><!--
## Limit access to the `nginx` service

To limit the access to the `nginx` service so that only Pods with the label `access: true` can query it, create a NetworkPolicy object as follows:
-->
<h2 id="限制-nginx-服务的访问">限制 <code>nginx</code> 服务的访问</h2>
<p>如果想限制对 <code>nginx</code> 服务的访问，只让那些拥有标签 <code>access: true</code> 的 Pod 访问它，
那么可以创建一个如下所示的 NetworkPolicy 对象：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/nginx-policy.yaml" download="service/networking/nginx-policy.yaml"><code>service/networking/nginx-policy.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-nginx-policy-yaml')" title="Copy service/networking/nginx-policy.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-nginx-policy-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>access-nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ingress</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">from</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">access</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
The name of a NetworkPolicy object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>NetworkPolicy 对象的名称必须是一个合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>.</p>
<!--
NetworkPolicy includes a `podSelector` which selects the grouping of Pods to which the policy applies. You can see this policy selects Pods with the label `app=nginx`. The label was automatically added to the Pod in the `nginx` Deployment. An empty `podSelector` selects all pods in the namespace.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> NetworkPolicy 中包含选择策略所适用的 Pods 集合的 <code>podSelector</code>。
你可以看到上面的策略选择的是带有标签 <code>app=nginx</code> 的 Pods。
此标签是被自动添加到 <code>nginx</code> Deployment 中的 Pod 上的。
如果 <code>podSelector</code> 为空，则意味着选择的是名字空间中的所有 Pods。
</div>
<!--
## Assign the policy to the service

Use kubectl to create a NetworkPolicy from the above `nginx-policy.yaml` file:
-->
<h2 id="为服务指定策略">为服务指定策略</h2>
<p>使用 kubectl 根据上面的 <code>nginx-policy.yaml</code> 文件创建一个 NetworkPolicy：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/service/networking/nginx-policy.yaml
</code></pre></div><pre><code class="language-none" data-lang="none">networkpolicy.networking.k8s.io/access-nginx created
</code></pre><!--
## Test access to the service when access label is not defined

When you attempt to access the `nginx` Service from a Pod without the correct labels, the request times out:
-->
<h2 id="测试没有定义访问标签时访问服务">测试没有定义访问标签时访问服务</h2>
<p>如果你尝试从没有设定正确标签的 Pod 中去访问 <code>nginx</code> 服务，请求将会超时：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run busybox --rm -ti --image<span style="color:#666">=</span>busybox:1.28 -- /bin/sh
</code></pre></div><!--
In your shell, run the command:
-->
<p>在 Shell 中运行命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget --spider --timeout<span style="color:#666">=</span><span style="color:#666">1</span> nginx
</code></pre></div><pre><code class="language-none" data-lang="none">Connecting to nginx (10.100.0.16:80)
wget: download timed out
</code></pre><!--
## Define access label and test again

You can create a Pod with the correct labels to see that the request is allowed:
-->
<h2 id="定义访问标签后再次测试">定义访问标签后再次测试</h2>
<p>创建一个拥有正确标签的 Pod，你将看到请求是被允许的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run busybox --rm -ti --labels<span style="color:#666">=</span><span style="color:#b44">&#34;access=true&#34;</span> --image<span style="color:#666">=</span>busybox:1.28 -- /bin/sh
</code></pre></div><!--
In your shell, run the command:
-->
<p>在 Shell 中运行命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget --spider --timeout<span style="color:#666">=</span><span style="color:#666">1</span> nginx
</code></pre></div><pre><code class="language-none" data-lang="none">Connecting to nginx (10.100.0.16:80)
remote file exists
</code></pre>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b35b8ddb9bbc15620ce9636f4346c05c">26 - 安全地清空一个节点</h1>
    
	<!--
reviewers:
- davidopp
- mml
- foxish
- kow3ns
title: Safely Drain a Node
content_type: task
min-kubernetes-server-version: 1.5
-->
<!-- overview -->
<!-- 
This page shows how to safely drain a node, respecting the PodDisruptionBudget you have defined.
 -->
<p>本页展示了如何在确保 PodDisruptionBudget 的前提下，安全地清空一个<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p>Your Kubernetes server must be at or later than version 1.5.
To check the version, enter <code>kubectl version</code>.</p>
<!-- 
This task assumes that you have met the following prerequisites:

* You are using Kubernetes release >= 1.5.
* Either:
  1. You do not require your applications to be highly available during the
     node drain, or
  2. You have read about the [PodDisruptionBudget concept](/docs/concepts/workloads/pods/disruptions/)
     and [Configured PodDisruptionBudgets](/docs/tasks/run-application/configure-pdb/) for
     applications that need them.
-->
<p>此任务假定你已经满足了以下先决条件：</p>
<ul>
<li>使用的 Kubernetes 版本 &gt;= 1.5。</li>
<li>以下两项，具备其一：
<ol>
<li>在节点清空期间，不要求应用程序具有高可用性</li>
<li>你已经了解了 <a href="/zh/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget 的概念</a>，
并为需要它的应用程序<a href="/zh/docs/tasks/run-application/configure-pdb/">配置了 PodDisruptionBudget</a>。</li>
</ol>
</li>
</ul>
<!-- steps -->
<!--
## (Optional) Configure a disruption budget {#configure-poddisruptionbudget}

To endure that your workloads remain available during maintenance, you can
configure a [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/).

If availability is important for any applications that run or could run on the node(s)
that you are draining, [configure a PodDisruptionBudgets](/docs/tasks/run-application/configure-pdb/)
first and then continue following this guide.
-->
<h2 id="configure-poddisruptionbudget">（可选） 配置干扰预算</h2>
<p>为了确保你的负载在维护期间仍然可用，你可以配置一个 <a href="/zh/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>。
如果可用性对于正在清空的该节点上运行或可能在该节点上运行的任何应用程序很重要，
首先 <a href="/zh/docs/tasks/run-application/configure-pdb/">配置一个 PodDisruptionBudgets</a> 并继续遵循本指南。</p>
<!-- 
## Use `kubectl drain` to remove a node from service

You can use `kubectl drain` to safely evict all of your pods from a
node before you perform maintenance on the node (e.g. kernel upgrade,
hardware maintenance, etc.). Safe evictions allow the pod's containers
to [gracefully terminate](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)
and will respect the `PodDisruptionBudgets` you have specified.
-->
<h2 id="use-kubectl-drain-to-remove-a-node-from-service">使用 <code>kubectl drain</code> 从服务中删除一个节点</h2>
<p>在对节点执行维护（例如内核升级、硬件维护等）之前，
可以使用 <code>kubectl drain</code> 从节点安全地逐出所有 Pods。
安全的驱逐过程允许 Pod 的容器
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">体面地终止</a>，
并确保满足指定的 PodDisruptionBudgets。</p>
<!-- 
By default `kubectl drain` will ignore certain system pods on the node
that cannot be killed; see
the [kubectl drain](/docs/reference/generated/kubectl/kubectl-commands/#drain)
documentation for more details.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 默认情况下， <code>kubectl drain</code> 将忽略节点上不能杀死的特定系统 Pod；
有关更多细节，请参阅
<a href="/docs/reference/generated/kubectl/kubectl-commands/#drain">kubectl drain</a> 文档。
</div>
<!-- 
When `kubectl drain` returns successfully, that indicates that all of
the pods (except the ones excluded as described in the previous paragraph)
have been safely evicted (respecting the desired graceful termination period,
and respecting the PodDisruptionBudget you have defined). It is then safe to
bring down the node by powering down its physical machine or, if running on a
cloud platform, deleting its virtual machine.

First, identify the name of the node you wish to drain. You can list all of the nodes in your cluster with
-->
<p><code>kubectl drain</code> 的成功返回，表明所有的 Pods（除了上一段中描述的被排除的那些），
已经被安全地逐出（考虑到期望的终止宽限期和你定义的 PodDisruptionBudget）。
然后就可以安全地关闭节点，
比如关闭物理机器的电源，如果它运行在云平台上，则删除它的虚拟机。</p>
<p>首先，确定想要清空的节点的名称。可以用以下命令列出集群中的所有节点:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><!-- 
Next, tell Kubernetes to drain the node:
-->
<p>接下来，告诉 Kubernetes 清空节点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain &lt;node name&gt;
</code></pre></div><!-- 
Once it returns (without giving an error), you can power down the node
(or equivalently, if on a cloud platform, delete the virtual machine backing the node).
If you leave the node in the cluster during the maintenance operation, you need to run
-->
<p>一旦它返回（没有报错），
你就可以下线此节点（或者等价地，如果在云平台上，删除支持该节点的虚拟机）。
如果要在维护操作期间将节点留在集群中，则需要运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl uncordon &lt;node name&gt;
</code></pre></div><!-- 
afterwards to tell Kubernetes that it can resume scheduling new pods onto the node.
-->
<p>然后告诉 Kubernetes，它可以继续在此节点上调度新的 Pods。</p>
<!-- 
## Draining multiple nodes in parallel

The `kubectl drain` command should only be issued to a single node at a
time. However, you can run multiple `kubectl drain` commands for
different nodes in parallel, in different terminals or in the
background. Multiple drain commands running concurrently will still
respect the `PodDisruptionBudget` you specify.
-->
<h2 id="draining-multiple-nodes-in-parallel">并行清空多个节点 </h2>
<p><code>kubectl drain</code> 命令一次只能发送给一个节点。
但是，你可以在不同的终端或后台为不同的节点并行地运行多个 <code>kubectl drain</code> 命令。
同时运行的多个 drain 命令仍然遵循你指定的 PodDisruptionBudget 。</p>
<!-- 
For example, if you have a StatefulSet with three replicas and have
set a PodDisruptionBudget for that set specifying `minAvailable: 2`,
`kubectl drain` only evicts a pod from the StatefulSet if all three
replicas pods are ready; if then you issue multiple drain commands in
parallel, Kubernetes respects the PodDisruptionBudget and ensure
that only 1 (calculated as `replicas - minAvailable`) Pod is unavailable
at any given time. Any drains that would cause the number of ready
replicas to fall below the specified budget are blocked.
-->
<p>例如，如果你有一个三副本的 StatefulSet，
并设置了一个 <code>PodDisruptionBudget</code>，指定 <code>minAvailable: 2</code>。
如果所有的三个 Pod 均就绪，并且你并行地发出多个 drain 命令，
那么 <code>kubectl drain</code> 只会从 StatefulSet 中逐出一个 Pod，
因为 Kubernetes 会遵守 PodDisruptionBudget 并确保在任何时候只有一个 Pod 不可用
（最多不可用 Pod 个数的计算方法：<code>replicas - minAvailable</code>）。
任何会导致就绪副本数量低于指定预算的清空操作都将被阻止。</p>
<!-- 
## The Eviction API

If you prefer not to use [kubectl drain](/docs/reference/generated/kubectl/kubectl-commands/#drain) (such as
to avoid calling to an external command, or to get finer control over the pod
eviction process), you can also programmatically cause evictions using the
eviction API.
For more information, see [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/).
-->
<h2 id="the-eviction-api">驱逐 API</h2>
<p>如果你不喜欢使用
<a href="/docs/reference/generated/kubectl/kubectl-commands/#drain">kubectl drain</a>
（比如避免调用外部命令，或者更细化地控制 pod 驱逐过程），
你也可以用驱逐 API 通过编程的方式达到驱逐的效果。
更多信息，请参阅 <a href="/zh/docs/concepts/scheduling-eviction/api-eviction/">API 发起的驱逐</a>。</p>
<h2 id="what-s-next">What's next</h2>
<!-- 
* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).
-->
<ul>
<li>执行<a href="/zh/docs/tasks/run-application/configure-pdb/">配置 PDB</a>中的各个步骤，
保护你的应用</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9585dc0efb0450fd68728e7511754717">27 - 开发云控制器管理器</h1>
    
	<!--
reviewers:
- luxas
- thockin
- wlan0
title: Developing Cloud Controller Manager
content_type: concept
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code>
</div>


<!--
title: Cloud Controller Manager
id: cloud-controller-manager
date: 2018-04-12
full_link: /docs/concepts/architecture/cloud-controller/
short_description: >
  Control plane component that integrates Kubernetes with third-party cloud providers.

aka: 
tags:
- core-object
- architecture
- operation
-->
<!--
 A Kubernetes <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.
-->
<p>云控制器管理器是指嵌入特定云的控制逻辑的
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>组件。
云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上，
并将与该云平台交互的组件同与你的集群交互的组件分离开来。</p>
<!--
By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.
-->
<p>通过分离 Kubernetes 和底层云基础设置之间的互操作性逻辑，
云控制器管理器组件使云提供商能够以不同于 Kubernetes 主项目的
步调发布新特征。</p>
<!-- body -->
<!--
## Background

Since cloud providers develop and release at a different pace compared to the Kubernetes project, abstracting the provider-specific code to the `cloud-controller-manager` binary allows cloud vendors to evolve independently from the core Kubernetes code.
-->
<h2 id="背景">背景</h2>
<p>由于云驱动的开发和发布与 Kubernetes 项目本身步调不同，将特定于云环境的代码抽象到
<code>cloud-controller-manager</code> 二进制组件有助于云厂商独立于 Kubernetes
核心代码推进其驱动开发。</p>
<!--
The Kubernetes project provides skeleton cloud-controller-manager code with Go interfaces to allow you (or your cloud provider) to plug in your own implementations. This means that a cloud provider can implement a cloud-controller-manager by importing packages from Kubernetes core; each cloudprovider will register their own code by calling `cloudprovider.RegisterCloudProvider` to update a global variable of available cloud providers.
-->
<p>Kubernetes 项目提供 cloud-controller-manager 的框架代码，其中包含 Go 语言的接口，
便于你（或者你的云驱动提供者）接驳你自己的实现。这意味着每个云驱动可以通过从
Kubernetes 核心代码导入软件包来实现一个 cloud-controller-manager；
每个云驱动会通过调用 <code>cloudprovider.RegisterCloudProvider</code> 接口来注册其自身实现代码，
从而更新一个用来记录可用云驱动的全局变量。</p>
<!--
## Developing
-->
<h2 id="开发">开发</h2>
<h3 id="树外-out-of-tree">树外（Out of Tree）</h3>
<!--
To build an out-of-tree cloud-controller-manager for your cloud, follow these steps:
-->
<p>要为你的云环境构建一个树外（Out-of-Tree）云控制器管理器：</p>
<!--
1. Create a go package with an implementation that satisfies [cloudprovider.Interface](https://github.com/kubernetes/cloud-provider/blob/master/cloud.go).
2. Use [main.go in cloud-controller-manager](https://github.com/kubernetes/kubernetes/blob/master/cmd/cloud-controller-manager/main.go) from Kubernetes core as a template for your main.go. As mentioned above, the only difference should be the cloud package that will be imported.
3. Import your cloud package in `main.go`, ensure your package has an `init` block to run [cloudprovider.RegisterCloudProvider](https://github.com/kubernetes/cloud-provider/blob/master/plugins.go).
-->
<ol>
<li>使用满足 <a href="https://github.com/kubernetes/cloud-provider/blob/master/cloud.go"><code>cloudprovider.Interface</code></a>
接口的实现来创建一个 Go 语言包。</li>
<li>使用来自 Kubernetes 核心代码库的
<a href="https://github.com/kubernetes/kubernetes/blob/master/cmd/cloud-controller-manager/main.go">cloud-controller-manager 中的 main.go</a>
作为 <code>main.go</code> 的模板。如上所述，唯一的区别应该是将导入的云包不同。</li>
<li>在 <code>main.go</code> 中导入你的云包，确保你的包有一个 <code>init</code> 块来运行
<a href="https://github.com/kubernetes/cloud-provider/blob/master/plugins.go"><code>cloudprovider.RegisterCloudProvider</code></a>。</li>
</ol>
<!--
Many cloud providers publish their controller manager code as open source. If you are creating
a new cloud-controller-manager from scratch, you could take an existing out-of-tree cloud
controller manager as your starting point.
-->
<p>很多云驱动都将其控制器管理器代码以开源代码的形式公开。
如果你在开发一个新的 cloud-controller-manager，你可以选择某个树外（Out-of-Tree）
云控制器管理器作为出发点。</p>
<h3 id="树内-in-tree">树内（In Tree）</h3>
<!--
For in-tree cloud providers, you can run the in-tree cloud controller manager as a <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a> in your cluster. See [Cloud Controller Manager Administration](/docs/tasks/administer-cluster/running-cloud-controller/) for more details.
-->
<p>对于树内（In-Tree）驱动，你可以将树内云控制器管理器作为集群中的
<a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a> 来运行。
有关详细信息，请参阅<a href="/zh/docs/tasks/administer-cluster/running-cloud-controller/">云控制器管理器管理</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-00733cc3747eb3f5fe1c9e0439262967">28 - 开启服务拓扑</h1>
    
	<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code>
</div>


<!--
This feature, specifically the alpha `topologyKeys` field, is deprecated since
Kubernetes v1.21.
[Topology Aware Hints](/docs/concepts/services-networking/topology-aware-hints/),
introduced in Kubernetes v1.21, provide similar functionality.
-->
<p>这项功能，特别是 Alpha 状态的 <code>topologyKeys</code> 字段，在 kubernetes v1.21 中已经弃用。
在 kubernetes v1.21 加入的<a href="/zh/docs/concepts/services-networking/topology-aware-hints/">拓扑感知提示</a>
提供了类似的功能。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

Your Kubernetes server must be at or later than version 1.17.
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
_Service Topology_ enables a <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a> to route traffic based upon the Node
topology of the cluster. For example, a service can specify that traffic be
preferentially routed to endpoints that are on the same Node as the client, or
in the same availability zone.
-->
<p><em>服务拓扑（Service Topology）</em> 使 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务'>服务</a>
能够根据集群中的 Node 拓扑来路由流量。
比如，服务可以指定将流量优先路由到与客户端位于同一节点或者同一可用区域的端点上。</p>
<h2 id="before-you-begin-1">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

Your Kubernetes server must be at or later than version 1.17.
 To check the version, enter <code>kubectl version</code>.
</p>
<!--
The following prerequisites are needed in order to enable topology aware service
routing:

   * Kubernetes v1.17 or later
   * Configure <a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a> to run in iptables mode or IPVS mode
-->
<p>需要下面列的先决条件，才能启用拓扑感知的服务路由：</p>
<ul>
<li>Kubernetes 1.17 或更新版本</li>
<li>配置 <a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a> 以 iptables 或者 IPVS 模式运行</li>
</ul>
<!-- steps -->
<!--
## Enable Service Topology






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code>
</div>



To enable service topology, enable the `ServiceTopology`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for all Kubernetes components:
-->
<h2 id="启用服务拓扑">启用服务拓扑</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code>
</div>


<p>要启用服务拓扑功能，需要为所有 Kubernetes 组件启用 <code>ServiceTopology</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>：</p>
<pre><code>--feature-gates=&quot;ServiceTopology=true`
</code></pre><h2 id="what-s-next">What's next</h2>
<!--
* Read about [Topology Aware Hints](/docs/concepts/services-networking/topology-aware-hints/), the replacement for the `topologyKeys` field.
* Read about [EndpointSlices](/docs/concepts/services-networking/endpoint-slices/)
* Read about the [Service Topology](/docs/concepts/services-networking/service-topology/) concept
* Read [Connecting Applications with Services](/docs/concepts/services-networking/connect-applications-service/)
-->
<ul>
<li>阅读<a href="/zh/docs/concepts/services-networking/topology-aware-hints/">拓扑感知提示</a>，该技术是用来替换 <code>topologyKeys</code> 字段的。</li>
<li>阅读<a href="/zh/docs/concepts/services-networking/endpoint-slices">端点切片</a></li>
<li>阅读<a href="/zh/docs/concepts/services-networking/service-topology">服务拓扑</a>概念</li>
<li>阅读<a href="/zh/docs/concepts/services-networking/connect-applications-service/">通过服务来连接应用</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7127e6b7344b315b30b1ce8c4d8bfc55">29 - 控制节点上的 CPU 管理策略</h1>
    
	<!--
title: Control CPU Management Policies on the Node
reviewers:
- sjenning
- ConnorDoyle
- balajismaniam
content_type: task
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.12 [beta]</code>
</div>


<!--
Kubernetes keeps many aspects of how pods execute on nodes abstracted
from the user. This is by design.  However, some workloads require
stronger guarantees in terms of latency and/or performance in order to operate
acceptably. The kubelet provides methods to enable more complex workload
placement policies while keeping the abstraction free from explicit placement
directives.
-->
<p>按照设计，Kubernetes 对 pod 执行相关的很多方面进行了抽象，使得用户不必关心。
然而，为了正常运行，有些工作负载要求在延迟和/或性能方面有更强的保证。
为此，kubelet 提供方法来实现更复杂的负载放置策略，同时保持抽象，避免显式的放置指令。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## CPU Management Policies

By default, the kubelet uses [CFS quota](https://en.wikipedia.org/wiki/Completely_Fair_Scheduler)
to enforce pod CPU limits.  When the node runs many CPU-bound pods,
the workload can move to different CPU cores depending on
whether the pod is throttled and which CPU cores are available at
scheduling time.  Many workloads are not sensitive to this migration and thus
work fine without any intervention.
-->
<h2 id="cpu-管理策略">CPU 管理策略</h2>
<p>默认情况下，kubelet 使用 <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">CFS 配额</a>
来执行 Pod 的 CPU 约束。
当节点上运行了很多 CPU 密集的 Pod 时，工作负载可能会迁移到不同的 CPU 核，
这取决于调度时 Pod 是否被扼制，以及哪些 CPU 核是可用的。
许多工作负载对这种迁移不敏感，因此无需任何干预即可正常工作。</p>
<!--
However, in workloads where CPU cache affinity and scheduling latency
significantly affect workload performance, the kubelet allows alternative CPU
management policies to determine some placement preferences on the node.
-->
<p>然而，有些工作负载的性能明显地受到 CPU 缓存亲和性以及调度延迟的影响。
对此，kubelet 提供了可选的 CPU 管理策略，来确定节点上的一些分配偏好。</p>
<!--
### Configuration

The CPU Manager policy is set with the `-cpu-manager-policy` kubelet
option. There are two supported policies:
-->
<h3 id="配置">配置</h3>
<p>CPU 管理策略通过 kubelet 参数 <code>--cpu-manager-policy</code> 来指定。支持两种策略：</p>
<!--
* `none`: the default, which represents the existing scheduling behavior.
* `static`: allows pods with certain resource characteristics to be
  granted increased CPU affinity and exclusivity on the node.
-->
<ul>
<li><code>none</code>: 默认策略，表示现有的调度行为。</li>
<li><code>static</code>: 允许为节点上具有某些资源特征的 pod 赋予增强的 CPU 亲和性和独占性。</li>
</ul>
<!--
The CPU manager periodically writes resource updates through the CRI in
order to reconcile in-memory CPU assignments with cgroupfs. The reconcile
frequency is set through a new Kubelet configuration value
`-cpu-manager-reconcile-period`. If not specified, it defaults to the same
duration as `-node-status-update-frequency`.
-->
<p>CPU 管理器定期通过 CRI 写入资源更新，以保证内存中 CPU 分配与 cgroupfs 一致。
同步频率通过新增的 Kubelet 配置参数 <code>--cpu-manager-reconcile-period</code> 来设置。
如果不指定，默认与 <code>--node-status-update-frequency</code> 的周期相同。</p>
<!--
The behavior of the static policy can be fine-tuned using the `--cpu-manager-policy-options` flag.
The flag takes a comma-separated list of `key=value` policy options.
-->
<p>Static 策略的行为可以使用 <code>--cpu-manager-policy-options</code> 参数来微调。
该参数采用一个逗号分隔的 <code>key=value</code> 策略选项列表。</p>
<!--
### None policy

The `none` policy explicitly enables the existing default CPU
affinity scheme, providing no affinity beyond what the OS scheduler does
automatically.  Limits on CPU usage for
[Guaranteed pods](/docs/tasks/configure-pod-container/quality-service-pod/) and
[Burstable pods](/docs/tasks/configure-pod-container/quality-service-pod/)
are enforced using CFS quota.
-->
<h3 id="none-策略">none 策略</h3>
<p><code>none</code> 策略显式地启用现有的默认 CPU 亲和方案，不提供操作系统调度器默认行为之外的亲和性策略。
通过 CFS 配额来实现 <a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">Guaranteed pods</a>
和 <a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">Burstable pods</a>
的 CPU 使用限制。</p>
<!--
### Static policy

The `static` policy allows containers in `Guaranteed` pods with integer CPU
`requests` access to exclusive CPUs on the node. This exclusivity is enforced
using the [cpuset cgroup controller](https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt).
-->
<h3 id="static-策略">static 策略</h3>
<p><code>static</code> 策略针对具有整数型 CPU <code>requests</code> 的 <code>Guaranteed</code> Pod ，它允许该类 Pod
中的容器访问节点上的独占 CPU 资源。这种独占性是使用
<a href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt">cpuset cgroup 控制器</a> 来实现的。</p>
<!--
System services such as the container runtime and the kubelet itself can continue to run on these exclusive CPUs.  The exclusivity only extends to other pods.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 诸如容器运行时和 kubelet 本身的系统服务可以继续在这些独占 CPU 上运行。独占性仅针对其他 Pod。
</div>
<!--
CPU Manager doesn't support offlining and onlining of
CPUs at runtime. Also, if the set of online CPUs changes on the node,
the node must be drained and CPU manager manually reset by deleting the
state file `cpu_manager_state` in the kubelet root directory.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> CPU 管理器不支持运行时下线和上线 CPUs。此外，如果节点上的在线 CPUs 集合发生变化，
则必须驱逐节点上的 Pod，并通过删除 kubelet 根目录中的状态文件 <code>cpu_manager_state</code>
来手动重置 CPU 管理器。
</div>
<!--
This policy manages a shared pool of CPUs that initially contains all CPUs in the
node. The amount of exclusively allocatable CPUs is equal to the total
number of CPUs in the node minus any CPU reservations by the kubelet `--kube-reserved` or
`--system-reserved` options. From 1.17, the CPU reservation list can be specified
explicitly by kubelet `--reserved-cpus` option. The explicit CPU list specified by
`--reserved-cpus` takes precedence over the CPU reservation specified by
`--kube-reserved` and `--system-reserved`. CPUs reserved by these options are taken, in
integer quantity, from the initial shared pool in ascending order by physical
core ID.  This shared pool is the set of CPUs on which any containers in
`BestEffort` and `Burstable` pods run. Containers in `Guaranteed` pods with fractional
CPU `requests` also run on CPUs in the shared pool. Only containers that are
both part of a `Guaranteed` pod and have integer CPU `requests` are assigned
exclusive CPUs.
--->
<p>该策略管理一个共享 CPU 资源池，最初，该资源池包含节点上所有的 CPU 资源。可用
的独占性 CPU 资源数量等于节点的 CPU 总量减去通过 <code>--kube-reserved</code> 或 <code>--system-reserved</code> 参数保留的 CPU 。从1.17版本开始，CPU保留列表可以通过 kublet 的 '--reserved-cpus' 参数显式地设置。
通过 '--reserved-cpus' 指定的显式CPU列表优先于使用 '--kube-reserved' 和 '--system-reserved' 参数指定的保留CPU。 通过这些参数预留的 CPU 是以整数方式，按物理内
核 ID 升序从初始共享池获取的。 共享池是 <code>BestEffort</code> 和 <code>Burstable</code> pod 运行
的 CPU 集合。<code>Guaranteed</code> pod 中的容器，如果声明了非整数值的 CPU <code>requests</code> ，也将运行在共享池的 CPU 上。只有 <code>Guaranteed</code> pod 中，指定了整数型 CPU <code>requests</code> 的容器，才会被分配独占 CPU 资源。</p>
<!--
The kubelet requires a CPU reservation greater than zero be made
using either `--kube-reserved` and/or `--system-reserved`  or `--reserved-cpus` when the static
policy is enabled. This is because zero CPU reservation would allow the shared
pool to become empty.
--->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 当启用 static 策略时，要求使用 <code>--kube-reserved</code> 和/或 <code>--system-reserved</code> 或
<code>--reserved-cpus</code> 来保证预留的 CPU 值大于零。
这是因为零预留 CPU 值可能使得共享池变空。
</div>
<!--
As `Guaranteed` pods whose containers fit the requirements for being statically
assigned are scheduled to the node, CPUs are removed from the shared pool and
placed in the cpuset for the container. CFS quota is not used to bound
the CPU usage of these containers as their usage is bound by the scheduling domain
itself. In others words, the number of CPUs in the container cpuset is equal to the integer
CPU `limit` specified in the pod spec. This static assignment increases CPU
affinity and decreases context switches due to throttling for the CPU-bound
workload.

Consider the containers in the following pod specs:
-->
<p>当 <code>Guaranteed</code> Pod 调度到节点上时，如果其容器符合静态分配要求，
相应的 CPU 会被从共享池中移除，并放置到容器的 cpuset 中。
因为这些容器所使用的 CPU 受到调度域本身的限制，所以不需要使用 CFS 配额来进行 CPU 的绑定。
换言之，容器 cpuset  中的 CPU 数量与 Pod 规约中指定的整数型 CPU <code>limit</code> 相等。
这种静态分配增强了 CPU 亲和性，减少了 CPU 密集的工作负载在节流时引起的上下文切换。</p>
<p>考虑以下 Pod 规格的容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `BestEffort` QoS class because no resource `requests` or
`limits` are specified. It runs in the shared pool.
-->
<p>该 Pod 属于 <code>BestEffort</code> QoS 类型，因为其未指定 <code>requests</code> 或 <code>limits</code> 值。
所以该容器运行在共享 CPU 池中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Burstable` QoS class because resource `requests` do not
equal `limits` and the `cpu` quantity is not specified. It runs in the shared
pool.
-->
<p>该 Pod 属于 <code>Burstable</code> QoS 类型，因为其资源 <code>requests</code> 不等于 <code>limits</code>，且未指定 <code>cpu</code> 数量。
所以该容器运行在共享 CPU 池中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Burstable` QoS class because resource `requests` do not
equal `limits`. It runs in the shared pool.
-->
<p>该 pod 属于 <code>Burstable</code> QoS 类型，因为其资源 <code>requests</code> 不等于 <code>limits</code>。
所以该容器运行在共享 CPU 池中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Guaranteed` QoS class because `requests` are equal to `limits`.
And the container's resource limit for the CPU resource is an integer greater than
or equal to one. The `nginx` container is granted 2 exclusive CPUs.
-->
<p>该 Pod 属于 <code>Guaranteed</code> QoS 类型，因为其 <code>requests</code> 值与 <code>limits</code>相等。
同时，容器对 CPU 资源的限制值是一个大于或等于 1 的整数值。
所以，该 <code>nginx</code> 容器被赋予 2 个独占 CPU。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1.5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1.5&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Guaranteed` QoS class because `requests` are equal to `limits`.
But the container's resource limit for the CPU resource is a fraction. It runs in
the shared pool.
-->
<p>该 Pod 属于 <code>Guaranteed</code> QoS 类型，因为其 <code>requests</code> 值与 <code>limits</code>相等。
但是容器对 CPU 资源的限制值是一个小数。所以该容器运行在共享 CPU 池中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Guaranteed` QoS class because only `limits` are specified
and `requests` are set equal to `limits` when not explicitly specified. And the
container's resource limit for the CPU resource is an integer greater than or
equal to one. The `nginx` container is granted 2 exclusive CPUs.
-->
<p>该 Pod 属于 <code>Guaranteed</code> QoS 类型，因其指定了 <code>limits</code> 值，同时当未显式指定时，
<code>requests</code> 值被设置为与 <code>limits</code> 值相等。
同时，容器对 CPU 资源的限制值是一个大于或等于 1 的整数值。
所以，该 <code>nginx</code> 容器被赋予 2 个独占 CPU。</p>
<!--
#### Static policy options

If the `full-pcpus-only` policy option is specified, the static policy will always allocate full physical cores.
You can enable this option by adding `full-pcups-only=true` to the CPUManager policy options.
-->
<h4 id="static-策略选项">Static 策略选项</h4>
<p>如果使用 <code>full-pcpus-only</code> 策略选项，static 策略总是会分配完整的物理核心。
你可以通过在 CPUManager 策略选项里加上 <code>full-pcups-only=true</code> 来启用该选项。</p>
<!--
By default, without this option, the static policy allocates CPUs using a topology-aware best-fit allocation.
On SMT enabled systems, the policy can allocate individual virtual cores, which correspond to hardware threads.
This can lead to different containers sharing the same physical cores; this behaviour in turn contributes
to the [noisy neighbours problem](https://en.wikipedia.org/wiki/Cloud_computing_issues#Performance_interference_and_noisy_neighbors).
-->
<p>默认情况下，如果不使用该选项，static 策略会使用拓扑感知最适合的分配方法来分配 CPU。
在启用了 SMT 的系统上，此策略所分配是与硬件线程对应的、独立的虚拟核。
这会导致不同的容器共享相同的物理核心，该行为进而会导致
<a href="https://en.wikipedia.org/wiki/Cloud_computing_issues#Performance_interference_and_noisy_neighbors">吵闹的邻居问题</a>。</p>
<!--
With the option enabled, the pod will be admitted by the kubelet only if the CPU request of all its containers
can be fulfilled by allocating full physical cores.
If the pod does not pass the admission, it will be put in Failed state with the message `SMTAlignmentError`.
-->
<p>启用该选项之后，只有当一个 Pod 里所有容器的 CPU 请求都能够分配到完整的物理核心时，kubelet 才会接受该 Pod。
如果 Pod 没有被准入，它会被置于 Failed 状态，错误消息是 <code>SMTAlignmentError</code>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8060aed5bf1172fa62199a4c306a4cd1">30 - 控制节点上的拓扑管理策略</h1>
    
	<!--
title: Control Topology Management Policies on a node
reviewers:
- ConnorDoyle
- klueska
- lmdaly
- nolancon
- bg-chun
content_type: task
min-kubernetes-server-version: v1.18
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<!--
An increasing number of systems leverage a combination of CPUs and hardware accelerators to support latency-critical execution and high-throughput parallel computation. These include workloads in fields such as telecommunications, scientific computing, machine learning, financial services and data analytics. Such hybrid systems comprise a high performance environment.
-->
<p>越来越多的系统利用 CPU 和硬件加速器的组合来支持对延迟要求较高的任务和高吞吐量的并行计算。
这类负载包括电信、科学计算、机器学习、金融服务和数据分析等。
此类混合系统即用于构造这些高性能环境。</p>
<!--
In order to extract the best performance, optimizations related to CPU isolation, memory and device locality are required. However, in Kubernetes, these optimizations are handled by a disjoint set of components.
-->
<p>为了获得最佳性能，需要进行与 CPU 隔离、内存和设备局部性有关的优化。
但是，在 Kubernetes 中，这些优化由各自独立的组件集合来处理。</p>
<!--
_Topology Manager_ is a Kubelet component that aims to co-ordinate the set of components that are responsible for these optimizations.
-->
<p><em>拓扑管理器（Topology Manager）</em> 是一个 kubelet 的一部分，旨在协调负责这些优化的一组组件。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

Your Kubernetes server must be at or later than version v1.18.
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## How Topology Manager Works
-->
<h2 id="拓扑管理器如何工作">拓扑管理器如何工作</h2>
<!--
Prior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make resource allocation decisions independently of each other.
This can result in undesirable allocations on multiple-socketed systems, performance/latency sensitive applications will suffer due to these undesirable allocations. 
 Undesirable in this case meaning for example, CPUs and devices being allocated from different NUMA Nodes thus, incurring additional latency.
-->
<p>在引入拓扑管理器之前， Kubernetes 中的 CPU 和设备管理器相互独立地做出资源分配决策。
这可能会导致在多处理系统上出现并非期望的资源分配；由于这些与期望相左的分配，对性能或延迟敏感的应用将受到影响。
这里的不符合期望意指，例如， CPU 和设备是从不同的 NUMA 节点分配的，因此会导致额外的延迟。</p>
<!--
The Topology Manager is a Kubelet component, which acts as a source of truth so that other Kubelet components can make topology aligned resource allocation choices.
-->
<p>拓扑管理器是一个 Kubelet 组件，扮演信息源的角色，以便其他 Kubelet 组件可以做出与拓扑结构相对应的资源分配决定。</p>
<!--
The Topology Manager provides an interface for components, called *Hint Providers*, to send and receive topology information. Topology Manager has a set of node level policies which are explained below.
-->
<p>拓扑管理器为组件提供了一个称为 <em>建议供应者（Hint Providers）</em> 的接口，以发送和接收拓扑信息。
拓扑管理器具有一组节点级策略，具体说明如下。</p>
<!--
The Topology manager receives Topology information from the *Hint Providers* as a bitmask denoting NUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform a set of operations on the hints provided and converge on the hint determined by the policy to give the optimal result, if an undesirable hint is stored the preferred field for the hint will be set to false. In the current policies preferred is the narrowest preferred mask.
The selected hint is stored as part of the Topology Manager. Depending on the policy configured the pod can be accepted or rejected from the node based on the selected hint.
The hint is then stored in the Topology Manager for use by the *Hint Providers* when making the resource allocation decisions.
-->
<p>拓扑管理器从 <em>建议提供者</em> 接收拓扑信息，作为表示可用的 NUMA 节点和首选分配指示的位掩码。
拓扑管理器策略对所提供的建议执行一组操作，并根据策略对提示进行约减以得到最优解；如果存储了与预期不符的建议，则该建议的优选字段将被设置为 false。
在当前策略中，首选的是最窄的优选掩码。
所选建议将被存储为拓扑管理器的一部分。
取决于所配置的策略，所选建议可用来决定节点接受或拒绝 Pod 。
之后，建议会被存储在拓扑管理器中，供 <em>建议提供者</em> 进行资源分配决策时使用。</p>
<!--
### Enable the Topology Manager feature

Support for the Topology Manager requires `TopologyManager` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be enabled. It is enabled by default starting with Kubernetes 1.18.
-->
<h3 id="启用拓扑管理器功能特性">启用拓扑管理器功能特性</h3>
<p>对拓扑管理器的支持要求启用 <code>TopologyManager</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
从 Kubernetes 1.18 版本开始，这一特性默认是启用的。</p>
<!--
### Topology Manager Scopes and Policies

The Topology Manager currently:
 - Aligns Pods of all QoS classes.
 - Aligns the requested resources that Hint Provider provides topology hints for.
-->
<h3 id="拓扑管理器作用域和策略">拓扑管理器作用域和策略</h3>
<p>拓扑管理器目前：</p>
<ul>
<li>对所有 QoS 类的 Pod 执行对齐操作</li>
<li>针对建议提供者所提供的拓扑建议，对请求的资源进行对齐</li>
</ul>
<!--
If these conditions are met, the Topology Manager will align the requested resources.

In order to customise how this alignment is carried out, the Topology Manager provides two distinct knobs: `scope` and `policy`.
-->
<p>如果满足这些条件，则拓扑管理器将对齐请求的资源。</p>
<p>为了定制如何进行对齐，拓扑管理器提供了两种不同的方式：<code>scope</code> 和 <code>policy</code>。</p>
<!--
The `scope` defines the granularity at which you would like resource alignment to be performed (e.g. at the `pod` or `container` level). And the `policy` defines the actual strategy used to carry out the alignment (e.g. `best-effort`, `restricted`, `single-numa-node`, etc.).

Details on the various `scopes` and `policies` available today can be found below.
-->
<p><code>scope</code> 定义了资源对齐时你所希望使用的粒度（例如，是在 <code>pod</code> 还是 <code>container</code> 级别）。
<code>policy</code> 定义了对齐时实际使用的策略（例如，<code>best-effort</code>、<code>restricted</code>、<code>single-numa-node</code> 等等）。</p>
<p>可以在下文找到现今可用的各种 <code>scopes</code> 和 <code>policies</code> 的具体信息。</p>
<!--
To align CPU resources with other requested resources in a Pod Spec, the CPU Manager should be enabled and proper CPU Manager policy should be configured on a Node. See [control CPU Management Policies](/docs/tasks/administer-cluster/cpu-management-policies/).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 为了将 Pod 规约中的 CPU 资源与其他请求资源对齐，CPU 管理器需要被启用并且
节点上应配置了适当的 CPU 管理器策略。
参看<a href="/zh/docs/tasks/administer-cluster/cpu-management-policies/">控制 CPU 管理策略</a>.
</div>
<!--
To align memory (and hugepages) resources with other requested resources in a Pod Spec, the Memory Manager should be enabled and proper Memory Manager policy should be configured on a Node. Examine [Memory Manager](/docs/tasks/administer-cluster/memory-manager/) documentation.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 为了将 Pod 规约中的 memory（和 hugepages）资源与所请求的其他资源对齐，需要启用内存管理器，
并且在节点配置适当的内存管理器策略。查看<a href="/zh/docs/tasks/administer-cluster/memory-manager/">内存管理器</a>
文档。
</div>
<!--
### Topology Manager Scopes

The Topology Manager can deal with the alignment of resources in a couple of distinct scopes:

* `container` (default)
* `pod`

Either option can be selected at a time of the kubelet startup, with `--topology-manager-scope` flag.
-->
<h3 id="拓扑管理器作用域">拓扑管理器作用域</h3>
<p>拓扑管理器可以在以下不同的作用域内进行资源对齐：</p>
<ul>
<li><code>container</code> （默认）</li>
<li><code>pod</code></li>
</ul>
<p>在 kubelet 启动时，可以使用 <code>--topology-manager-scope</code> 标志来选择其中任一选项。</p>
<!--
### container scope

The `container` scope is used by default.
-->
<h3 id="容器作用域">容器作用域</h3>
<p>默认使用的是 <code>container</code> 作用域。</p>
<!--
Within this scope, the Topology Manager performs a number of sequential resource alignments, i.e., for each container (in a pod) a separate alignment is computed. In other words, there is no notion of grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect, the Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.
-->
<p>在该作用域内，拓扑管理器依次进行一系列的资源对齐，
也就是，对每一个容器（包含在一个 Pod 里）计算单独的对齐。
换句话说，在该特定的作用域内，没有根据特定的 NUMA 节点集来把容器分组的概念。
实际上，拓扑管理器会把单个容器任意地对齐到 NUMA 节点上。</p>
<!--
The notion of grouping the containers was endorsed and implemented on purpose in the following scope, for example the `pod` scope.
-->
<p>容器分组的概念是在以下的作用域内特别实现的，也就是 <code>pod</code> 作用域。</p>
<!--
### pod scope

To select the `pod` scope, start the kubelet with the command line option `--topology-manager-scope=pod`.
-->
<h3 id="pod-作用域">Pod 作用域</h3>
<p>使用命令行选项 <code>--topology-manager-scope=pod</code> 来启动 kubelet，就可以选择 <code>pod</code> 作用域。</p>
<!--
This scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the Topology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers) to either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the alignments produced by the Topology Manager on different occasions:
-->
<p>该作用域允许把一个 Pod 里的所有容器作为一个分组，分配到一个共同的 NUMA 节点集。
也就是，拓扑管理器会把一个 Pod 当成一个整体，
并且试图把整个 Pod（所有容器）分配到一个单个的 NUMA 节点或者一个共同的 NUMA 节点集。
以下的例子说明了拓扑管理器在不同的场景下使用的对齐方式：</p>
<!--
* all containers can be and are allocated to a single NUMA node;
* all containers can be and are allocated to a shared set of NUMA nodes.
-->
<ul>
<li>所有容器可以被分配到一个单一的 NUMA 节点；</li>
<li>所有容器可以被分配到一个共享的 NUMA 节点集。</li>
</ul>
<!--
The total amount of particular resource demanded for the entire pod is calculated according to [effective requests/limits](/docs/concepts/workloads/pods/init-containers/#resources) formula, and thus, this total value is equal to the maximum of:
* the sum of all app container requests,
* the maximum of init container requests,
for a resource.
-->
<p>整个 Pod 所请求的某种资源总量是根据
<a href="/zh/docs/concepts/workloads/pods/init-containers/#resources">有效 request/limit</a>
公式来计算的，
因此，对某一种资源而言，该总量等于以下数值中的最大值：</p>
<ul>
<li>所有应用容器请求之和；</li>
<li>初始容器请求的最大值。</li>
</ul>
<!--
Using the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically valuable for workloads that are latency sensitive or for high-throughput applications that perform IPC. By combining both options, you are able to place all containers in a pod onto a single NUMA node; hence, the inter-NUMA communication overhead can be eliminated for that pod.
-->
<p><code>pod</code> 作用域与 <code>single-numa-node</code> 拓扑管理器策略一起使用，
对于延时敏感的工作负载，或者对于进行 IPC 的高吞吐量应用程序，都是特别有价值的。
把这两个选项组合起来，你可以把一个 Pod 里的所有容器都放到一个单个的 NUMA 节点，
使得该 Pod 消除了 NUMA 之间的通信开销。</p>
<!--
In the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes is present among possible allocations. Reconsider the example above:
-->
<p>在 <code>single-numa-node</code> 策略下，只有当可能的分配方案中存在合适的 NUMA 节点集时，Pod 才会被接受。
重新考虑上述的例子：</p>
<!--
* a set containing only a single NUMA node - it leads to pod being admitted,
* whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one NUMA node, two or more NUMA nodes are required to satisfy the allocation).
-->
<ul>
<li>节点集只包含单个 NUMA 节点时，Pod 就会被接受，</li>
<li>然而，节点集包含多个 NUMA 节点时，Pod 就会被拒绝
（因为满足该分配方案需要两个或以上的 NUMA 节点，而不是单个 NUMA 节点）。</li>
</ul>
<!--
To recap, Topology Manager first computes a set of NUMA nodes and then tests it against Topology Manager policy, which either leads to the rejection or admission of the pod.
-->
<p>简要地说，拓扑管理器首先计算出 NUMA 节点集，然后使用拓扑管理器策略来测试该集合，
从而决定拒绝或者接受 Pod。</p>
<!--
### Topology Manager Policies
-->
<h3 id="拓扑管理器策略">拓扑管理器策略</h3>
<!--
Topology Manager supports four allocation policies. You can set a policy via a Kubelet flag, `--topology-manager-policy`.
There are four supported policies:

* `none` (default)
* `best-effort`
* `restricted`
* `single-numa-node`
-->
<p>拓扑管理器支持四种分配策略。
你可以通过 Kubelet 标志 <code>--topology-manager-policy</code> 设置策略。
所支持的策略有四种：</p>
<ul>
<li><code>none</code> (默认)</li>
<li><code>best-effort</code></li>
<li><code>restricted</code></li>
<li><code>single-numa-node</code></li>
</ul>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> If Topology Manager is configured with the <strong>pod</strong> scope, the container, which is considered by the policy, is reflecting requirements of the entire pod, and thus each container from the pod will result with <strong>the same</strong> topology alignment decision.
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果拓扑管理器配置使用 <strong>Pod</strong> 作用域，
那么在策略考量一个容器时，该容器反映的是整个 Pod 的要求，
于是该 Pod 里的每个容器都会得到 <strong>相同的</strong> 拓扑对齐决定。
</div>
<!--
### none policy {#policy-none}

This is the default policy and does not perform any topology alignment.
-->
<h3 id="policy-none">none 策略</h3>
<p>这是默认策略，不执行任何拓扑对齐。</p>
<!--
### best-effort policy {#policy-best-effort}

For each container in a Guaranteed Pod, kubelet, with `best-effort` topology 
management policy, calls each Hint Provider to discover their resource availability.
Using this information, the Topology Manager stores the 
preferred NUMA Node affinity for that container. If the affinity is not preferred, 
Topology Manager will store this and admit the pod to the node anyway.
-->
<h3 id="policy-best-effort">best-effort 策略</h3>
<p>对于 Guaranteed 类的 Pod 中的每个容器，具有 <code>best-effort</code> 拓扑管理策略的
kubelet 将调用每个建议提供者以确定资源可用性。
使用此信息，拓扑管理器存储该容器的首选 NUMA 节点亲和性。
如果亲和性不是首选，则拓扑管理器将存储该亲和性，并且无论如何都将  pod 接纳到该节点。</p>
<!--
The *Hint Providers* can then use this information when making the 
resource allocation decision.
-->
<p>之后 <em>建议提供者</em> 可以在进行资源分配决策时使用这个信息。</p>
<!--
### restricted policy {#policy-restricted}

For each container in a Guaranteed Pod, kubelet, with `restricted` topology 
management policy, calls each Hint Provider to discover their resource availability.
Using this information, the Topology Manager stores the 
preferred NUMA Node affinity for that container. If the affinity is not preferred, 
Topology Manager will reject this pod from the node. This will result in a pod in a `Terminated` state with a pod admission failure.
-->
<h3 id="policy-restricted">restricted 策略</h3>
<p>对于 Guaranteed 类 Pod 中的每个容器， 配置了 <code>restricted</code> 拓扑管理策略的 kubelet
调用每个建议提供者以确定其资源可用性。。
使用此信息，拓扑管理器存储该容器的首选 NUMA 节点亲和性。
如果亲和性不是首选，则拓扑管理器将从节点中拒绝此 Pod 。
这将导致 Pod 处于 <code>Terminated</code> 状态，且 Pod 无法被节点接纳。</p>
<!--
Once the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to reschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeploy of the pod.
An external control loop could be also implemented to trigger a redeployment of pods that have the `Topology Affinity` error.
-->
<p>一旦 Pod 处于 <code>Terminated</code> 状态，Kubernetes 调度器将不会尝试重新调度该 Pod。
建议使用 ReplicaSet 或者 Deployment 来重新部署 Pod。
还可以通过实现外部控制环，以启动对具有 <code>Topology Affinity</code> 错误的 Pod 的重新部署。</p>
<!--
If the pod is admitted, the *Hint Providers* can then use this information when making the 
resource allocation decision.
-->
<p>如果 Pod 被允许运行在某节点，则 <em>建议提供者</em> 可以在做出资源分配决定时使用此信息。</p>
<!--
### single-numa-node policy {#policy-single-numa-node}

For each container in a Guaranteed Pod, kubelet, with `single-numa-node` topology 
management policy, calls each Hint Provider to discover their resource availability.
Using this information, the Topology Manager determines if a single NUMA Node affinity is possible.
If it is, Topology Manager will store this and the *Hint Providers* can then use this information when making the 
resource allocation decision.
If, however, this is not possible then the Topology Manager will reject the pod from the node. This will result in a pod in a `Terminated` state with a pod admission failure.
-->
<h3 id="policy-single-numa-node">single-numa-node 策略</h3>
<p>对于 Guaranteed 类 Pod 中的每个容器， 配置了 <code>single-numa-nodde</code> 拓扑管理策略的
kubelet 调用每个建议提供者以确定其资源可用性。
使用此信息，拓扑管理器确定单 NUMA 节点亲和性是否可能。
如果是这样，则拓扑管理器将存储此信息，然后 <em>建议提供者</em> 可以在做出资源分配决定时使用此信息。
如果不可能，则拓扑管理器将拒绝 Pod 运行于该节点。
这将导致 Pod 处于 <code>Terminated</code> 状态，且 Pod 无法被节点接受。</p>
<!--
Once the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to reschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeploy of the Pod.
An external control loop could be also implemented to trigger a redeployment of pods that have the `Topology Affinity` error.
-->
<p>一旦 Pod 处于 <code>Terminated</code> 状态，Kubernetes 调度器将不会尝试重新调度该 Pod。
建议使用 ReplicaSet 或者 Deployment 来重新部署 Pod。
还可以通过实现外部控制环，以触发具有 <code>Topology Affinity</code> 错误的 Pod 的重新部署。</p>
<!--
### Pod Interactions with Topology Manager Policies

Consider the containers in the following pod specs:
-->
<h3 id="pod-与拓扑管理器策略的交互">Pod 与拓扑管理器策略的交互</h3>
<p>考虑以下 pod 规范中的容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `BestEffort` QoS class because no resource `requests` or
`limits` are specified.
-->
<p>该 Pod 以 <code>BestEffort</code> QoS 类运行，因为没有指定资源 <code>requests</code> 或 <code>limits</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Burstable` QoS class because requests are less than limits.
-->
<p>由于 requests 数少于 limits，因此该 Pod 以 <code>Burstable</code> QoS 类运行。</p>
<!--
If the selected policy is anything other than `none`, Topology Manager would consider these Pod specifications. The Topology Manager would consult the Hint Providers to get topology hints. In the case of the `static`, the CPU Manager policy would return default topology hint, because these Pods do not have explicity request CPU resources.
-->
<p>如果选择的策略是 <code>none</code> 以外的任何其他策略，拓扑管理器都会评估这些 Pod 的规范。
拓扑管理器会咨询建议提供者，获得拓扑建议。
若策略为 <code>static</code>，则 CPU 管理器策略会返回默认的拓扑建议，因为这些 Pod
并没有显式地请求 CPU 资源。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod with integer CPU request runs in the `Guaranteed` QoS class because
`requests` are equal to `limits`.
-->
<p>此 Pod 以 <code>Guaranteed</code> QoS 类运行，因为其 <code>requests</code> 值等于 <code>limits</code> 值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/deviceA</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/deviceB</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/deviceA</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/deviceB</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `BestEffort` QoS class because there are no CPU and memory requests.
-->
<p>因为未指定 CPU 和内存请求，所以 Pod 以 <code>BestEffort</code> QoS 类运行。</p>
<!--
The Topology Manager would consider both of the above pods. The Topology Manager would consult the Hint Providers, which are CPU and Device Manager to get topology hints for the pods. 

In the case of the `Guaranteed` pod with integer request, the `static` CPU Manager policy would return hints relating to the CPU request and the Device Manager would send back hints for the requested device.
-->
<p>拓扑管理器将考虑以上两个 Pod。拓扑管理器将咨询建议提供者即 CPU 和设备管理器，以获取 Pod 的拓扑提示。
对于 <code>Guaranteed</code> 类的 CPU 请求数为整数的 Pod，<code>static</code> CPU 管理器策略将返回与 CPU 请求有关的提示，
而设备管理器将返回有关所请求设备的提示。</p>
<!--
In the case of the `Guaranteed` pod with sharing CPU request, the `static` CPU Manager policy would return default topology hint as there is no exclusive CPU request and the Device Manager would send back hints for the requested device.

In the above two cases of the `Guaranteed` pod, the `none` CPU Manager policy would return default topology hint.
-->
<p>对于 <code>Guaranteed</code> 类的 CPU 请求可共享的 Pod，<code>static</code> CPU
管理器策略将返回默认的拓扑提示，因为没有排他性的 CPU 请求；而设备管理器
则针对所请求的设备返回有关提示。</p>
<p>在上述两种 <code>Guaranteed</code> Pod 的情况中，<code>none</code> CPU 管理器策略会返回默认的拓扑提示。</p>
<!--
In the case of the `BestEffort` pod, the `static` CPU Manager policy would send back the default topology hint as there is no CPU request and the Device Manager would send back the hints for each of the requested devices.
-->
<p>对于 <code>BestEffort</code> Pod，由于没有 CPU 请求，<code>static</code> CPU 管理器策略将发送默认提示，
而设备管理器将为每个请求的设备发送提示。</p>
<!--
Using this information the Topology Manager calculates the optimal hint for the pod and stores this information, which will be used by the Hint Providers when they are making their resource assignments.
-->
<p>基于此信息，拓扑管理器将为 Pod 计算最佳提示并存储该信息，并且供
提示提供程序在进行资源分配时使用。</p>
<!--
### Known Limitations

1. The maximum number of NUMA nodes that Topology Manager allows is 8. With more than 8 NUMA nodes there will be a state explosion when trying to enumerate the possible NUMA affinities and generating their hints.

2. The scheduler is not topology-aware, so it is possible to be scheduled on a node and then fail on the node due to the Topology Manager.
-->
<h3 id="已知的局限性">已知的局限性</h3>
<ol>
<li>拓扑管理器所能处理的最大 NUMA 节点个数是 8。若 NUMA 节点数超过 8，
枚举可能的 NUMA 亲和性并为之生成提示时会发生状态爆炸。</li>
<li>调度器不是拓扑感知的，所以有可能一个 Pod 被调度到一个节点之后，会因为拓扑管理器的缘故在该节点上启动失败。</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2bffd7f3571cdd609bd97fb2e1bdb2fe">31 - 改变默认 StorageClass</h1>
    
	<!-- overview -->
<!--
This page shows how to change the default Storage Class that is used to
provision volumes for PersistentVolumeClaims that have no special requirements.
-->
<p>本文展示了如何改变默认的 Storage Class，它用于为没有特殊需求的 PersistentVolumeClaims 配置 volumes。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Why change the default storage class?

Depending on the installation method, your Kubernetes cluster may be deployed with
an existing StorageClass that is marked as default. This default StorageClass
is then used to dynamically provision storage for PersistentVolumeClaims
that do not require any specific storage class. See
[PersistentVolumeClaim documentation](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)
for details.
-->
<h2 id="为什么要改变默认存储类">为什么要改变默认存储类？</h2>
<p>取决于安装模式，你的 Kubernetes 集群可能和一个被标记为默认的已有 StorageClass 一起部署。
这个默认的 StorageClass 以后将被用于动态的为没有特定存储类需求的 PersistentVolumeClaims
配置存储。更多细节请查看
<a href="/zh/docs/concepts/storage/persistent-volumes/#perspersistentvolumeclaims">PersistentVolumeClaim 文档</a>。</p>
<!--
The pre-installed default StorageClass may not fit well with your expected workload;
for example, it might provision storage that is too expensive. If this is the case,
you can either change the default StorageClass or disable it completely to avoid
dynamic provisioning of storage.
-->
<p>预先安装的默认 StorageClass 可能不能很好的适应你期望的工作负载；例如，它配置的存储可能太过昂贵。
如果是这样的话，你可以改变默认 StorageClass，或者完全禁用它以防止动态配置存储。</p>
<!--
Deleting the default StorageClass may not work, as it may be re-created
automatically by the addon manager running in your cluster. Please consult the docs for your installation
for details about addon manager and how to disable individual addons.
-->
<p>删除默认 StorageClass 可能行不通，因为它可能会被你集群中的扩展管理器自动重建。
请查阅你的安装文档中关于扩展管理器的细节，以及如何禁用单个扩展。</p>
<!--
## Changing the default StorageClass
-->
<h2 id="改变默认-storageclass">改变默认 StorageClass</h2>
<!--
1. List the StorageClasses in your cluster: 
-->
<ol>
<li>
<p>列出你的集群中的 StorageClasses：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get storageclass
</code></pre></div><p>输出类似这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">NAME                 PROVISIONER               AGE
standard <span style="color:#666">(</span>default<span style="color:#666">)</span>   kubernetes.io/gce-pd      1d
gold                 kubernetes.io/gce-pd      1d
</code></pre></div><p>默认 StorageClass 以 <code>(default)</code> 标记。</p>
</li>
</ol>
<!--
1. Mark the default StorageClass as non-default:
-->
<ol start="2">
<li>
<p>标记默认 StorageClass  非默认：</p>
<!--
The default StorageClass has an annotation
`storageclass.kubernetes.io/is-default-class` set to `true`. Any other value
or absence of the annotation is interpreted as `false`.

To mark a StorageClass as non-default, you need to change its value to `false`:
-->
<p>默认 StorageClass 的注解 <code>storageclass.kubernetes.io/is-default-class</code> 设置为 <code>true</code>。
注解的其它任意值或者缺省值将被解释为 <code>false</code>。</p>
<p>要标记一个 StorageClass 为非默认的，你需要改变它的值为 <code>false</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl patch storageclass standard -p <span style="color:#b44">&#39;{&#34;metadata&#34;: {&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;false&#34;}}}&#39;</span>
</code></pre></div><!--
where `standard` is the name of your chosen StorageClass.
-->
<p>这里的 <code>standard</code> 是你选择的 StorageClass 的名字。</p>
</li>
</ol>
<!--
1. Mark a StorageClass as default:
-->
<ol start="3">
<li>
<p>标记一个 StorageClass 为默认的：</p>
<!--
Similar to the previous step, you need to add/set the annotation
`storageclass.kubernetes.io/is-default-class=true`.
-->
<p>和前面的步骤类似，你需要添加/设置注解 <code>storageclass.kubernetes.io/is-default-class=true</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl patch storageclass &lt;your-class-name&gt; -p <span style="color:#b44">&#39;{&#34;metadata&#34;: {&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;true&#34;}}}&#39;</span>
</code></pre></div><!--
Please note that at most one StorageClass can be marked as default. If two
or more of them are marked as default, a `PersistentVolumeClaim` without
`storageClassName` explicitly specified cannot be created.
-->
<p>请注意，最多只能有一个 StorageClass 能够被标记为默认。
如果它们中有两个或多个被标记为默认，Kubernetes 将忽略这个注解，
也就是它将表现为没有默认 StorageClass。</p>
</li>
</ol>
<!--
1. Verify that your chosen StorageClass is default:
-->
<ol start="4">
<li>
<p>验证你选用的 StorageClass 为默认的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get storageclass
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似这样：</p>
<pre><code>NAME             PROVISIONER               AGE
standard         kubernetes.io/gce-pd      1d
gold (default)   kubernetes.io/gce-pd      1d
</code></pre></li>
</ol>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [PersistentVolumes](/docs/concepts/storage/persistent-volumes/).
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fbc9136f53eccd6eb8c80f4bbea3b8f4">32 - 更改 PersistentVolume 的回收策略</h1>
    
	<!-- overview -->
<!--
This page shows how to change the reclaim policy of a Kubernetes
PersistentVolume.
-->
<p>本文展示了如何更改 Kubernetes PersistentVolume 的回收策略。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Why change reclaim policy of a PersistentVolume

PersistentVolumes can have various reclaim policies, including "Retain",
"Recycle", and "Delete". For dynamically provisioned PersistentVolumes,
the default reclaim policy is "Delete". This means that a dynamically provisioned
volume is automatically deleted when a user deletes the corresponding
PersistentVolumeClaim. This automatic behavior might be inappropriate if the volume
contains precious data. In that case, it is more appropriate to use the "Retain"
policy. With the "Retain" policy, if a user deletes a PersistentVolumeClaim,
the corresponding PersistentVolume will not be deleted. Instead, it is moved to the
Released phase, where all of its data can be manually recovered.
-->
<h2 id="为什么要更改-persistentvolume-的回收策略">为什么要更改 PersistentVolume 的回收策略</h2>
<p>PersistentVolumes 可以有多种回收策略，包括 &quot;Retain&quot;、&quot;Recycle&quot; 和  &quot;Delete&quot;。
对于动态配置的 PersistentVolumes 来说，默认回收策略为 &quot;Delete&quot;。
这表示当用户删除对应的 PersistentVolumeClaim 时，动态配置的 volume 将被自动删除。
如果 volume 包含重要数据时，这种自动行为可能是不合适的。
那种情况下，更适合使用 &quot;Retain&quot; 策略。
使用 &quot;Retain&quot; 时，如果用户删除 PersistentVolumeClaim，对应的 PersistentVolume 不会被删除。
相反，它将变为 Released 状态，表示所有的数据可以被手动恢复。</p>
<!--
## Changing the reclaim policy of a PersistentVolume
-->
<h2 id="更改-persistentvolume-的回收策略">更改 PersistentVolume 的回收策略</h2>
<!--
1. List the PersistentVolumes in your cluster:
-->
<ol>
<li>
<p>列出你集群中的 PersistentVolumes</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pv
</code></pre></div><p>输出类似于这样：</p>
<pre><code>NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                  REASON    AGE
pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1                   10s
pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2                   6s
pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim3                   3s
</code></pre><!--
This list also includes the name of the claims that are bound to each volume
for easier identification of dynamically provisioned volumes.
-->
<p>这个列表同样包含了绑定到每个卷的 claims 名称，以便更容易的识别动态配置的卷。</p>
</li>
</ol>
<!--
1. Choose one of your PersistentVolumes and change its reclaim policy:
-->
<ol start="2">
<li>
<p>选择你的 PersistentVolumes 中的一个并更改它的回收策略：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch pv &lt;your-pv-name&gt; -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;persistentVolumeReclaimPolicy&#34;:&#34;Retain&#34;}}&#39;</span>
</code></pre></div><!--
where `<your-pv-name>` is the name of your chosen PersistentVolume.
-->
<p>这里的 <code>&lt;your-pv-name&gt;</code> 是你选择的 PersistentVolume 的名字。</p>
<!--
On Windows, you must _double_ quote any JSONPath template that contains spaces
(not single quote as shown above for bash). This in turn means that you must
use a single quote or escaped double quote around any literals in the template. For example:
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>在 Windows 系统上，你必须对包含空格的 JSONPath 模板加双引号（而不是像上面
一样为 Bash 环境使用的单引号）。这也意味着你必须使用单引号或者转义的双引号
来处理模板中的字面值。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cmd" data-lang="cmd">kubectl patch pv &lt;your-pv-name&gt; -p <span style="color:#b44">&#34;{\&#34;</span>spec\<span style="color:#b44">&#34;:{\&#34;</span>persistentVolumeReclaimPolicy\<span style="color:#b44">&#34;:\&#34;</span>Retain\<span style="color:#b44">&#34;}}&#34;</span>
</code></pre></div>
</div>
</li>
</ol>
<!--
1. Verify that your chosen PersistentVolume has the right policy:
-->
<ol start="3">
<li>
<p>验证你选择的 PersistentVolume 拥有正确的策略：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pv
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于这样：</p>
<pre><code>NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                  REASON    AGE
pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1                   40s
pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2                   36s
pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Retain          Bound     default/claim3                   33s
</code></pre><!--
In the preceding output, you can see that the volume bound to claim
`default/claim3` has reclaim policy `Retain`. It will not be automatically
deleted when a user deletes claim `default/claim3`.
-->
<p>在前面的输出中，你可以看到绑定到申领 <code>default/claim3</code> 的卷的回收策略为 <code>Retain</code>。
当用户删除申领 <code>default/claim3</code> 时，它不会被自动删除。</p>
</li>
</ol>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [PersistentVolumes](/docs/concepts/storage/persistent-volumes/).
* Learn more about [PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims).
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a></li>
<li>进一步了解 <a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a></li>
</ul>
<h3 id="参考">参考</h3>
<ul>
<li><a href="/docs/api-reference/v1.23/#persistentvolume-v1-core">PersistentVolume</a></li>
<li><a href="/docs/api-reference/v1.23/#persistentvolumeclaim-v1-core">PersistentVolumeClaim</a></li>
<li>参阅 <a href="/docs/api-reference/v1.23/#persistentvolumeclaim-v1-core">PersistentVolumeSpec</a> 的 <code>persistentVolumeReclaimPolicy</code> 字段</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-966cd1cc69c69410d8698b3ac74abce2">33 - 自动扩缩集群 DNS 服务</h1>
    
	<!--
title: Autoscale the DNS Service in a Cluster
content_type: task
-->
<!-- overview -->
<!--
This page shows how to enable and configure autoscaling of the DNS service in a
Kubernetes cluster.
-->
<p>本页展示了如何在集群中启用和配置 DNS 服务的自动扩缩功能。</p>
<h2 id="before-you-begin">Before you begin</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</li>
</ul>
<!--
* This guide assumes your nodes use the AMD64 or Intel 64 CPU architecture

* Make sure the [DNS feature](/docs/concepts/services-networking/dns-pod-service/) itself is enabled.

* Kubernetes version 1.4.0 or later is recommended.
-->
<ul>
<li>
<p>本指南假设你的节点使用 AMD64 或 Intel 64 CPU 架构</p>
</li>
<li>
<p>确保已启用 <a href="/zh/docs/concepts/services-networking/dns-pod-service/">DNS 功能</a>本身。</p>
</li>
<li>
<p>建议使用 Kubernetes 1.4.0 或更高版本。</p>
</li>
</ul>
<!-- steps -->
<!--
## Determining whether DNS horizontal autoscaling is already enabled

List the <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployments'>Deployments</a>
in your cluster in the kube-system namespace:

```shell
kubectl get deployment --namespace=kube-system
```

The output is similar to this:

    NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
    ...
    dns-autoscaler        1         1         1            1           ...
    ...

If you see "dns-autoscaler" in the output, DNS horizontal autoscaling is
already enabled, and you can skip to
[Tuning autoscaling parameters](#tuning-autoscaling-parameters).
-->
<h2 id="determining-whether-dns-horizontal-autoscaling-is-already-enabled">确定是否 DNS 水平 水平自动扩缩特性已经启用</h2>
<p>在 kube-system 命名空间中列出集群中的 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployments'>Deployments</a> ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><p>输出类似如下这样：</p>
<pre><code>NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
...
dns-autoscaler        1         1         1            1           ...
...
</code></pre><p>如果在输出中看到 “dns-autoscaler”，说明 DNS 水平自动扩缩已经启用，可以跳到
<a href="#tuning-autoscaling-parameters">调优自动扩缩参数</a>。</p>
<!--
## Getting the name of your DNS Deployment {#find-scaling-target}

List the Deployments in your cluster in the kube-system namespace:

```shell
kubectl get deployment --namespace=kube-system
```

The output is similar to this:
-->
<h2 id="find-scaling-target">获取 DNS Deployment 的名称</h2>
<p>列出集群内 kube-system 名字空间中的 DNS Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment -l k8s-app<span style="color:#666">=</span>kube-dns --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><p>输出类似如下这样：</p>
<pre><code>NAME      READY   UP-TO-DATE   AVAILABLE   AGE
...
coredns   2/2     2            2           ...
...
</code></pre><!--
If you don't see a Deployment for DNS services, you can also look for it by name:
-->
<p>如果看不到 DNS 服务的 Deployment，你也可以通过名字来查找：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!--
and look for a deployment named `coredns` or `kube-dns`.
-->
<p>并在输出中寻找名称为 <code>coredns</code> 或 <code>kube-dns</code> 的 Deployment。</p>
<!--
Your scale target is:
-->
<p>你的扩缩目标为：</p>
<pre><code>Deployment/&lt;your-deployment-name&gt;
</code></pre><!--
where `<your-deployment-name>` is the name of your DNS Deployment. For example, if
your DNS Deployment name is coredns, your scale target is Deployment/coredns.
-->
<p>其中 <code>&lt;your-deployment-name&gt;</code> 是 DNS Deployment 的名称。
例如，如果你的 DNS Deployment 名称是 <code>coredns</code>，则你的扩展目标是 Deployment/coredns。</p>
<!--
CoreDNS is the default DNS service for Kubernetes. CoreDNS sets the label
`k8s-app=kube-dns` so that it can work in clusters that originally used
kube-dns.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> CoreDNS 是 Kubernetes 的默认 DNS 服务。CoreDNS 设置标签 <code>k8s-app=kube-dns</code>，
以便能够在原来使用 <code>kube-dns</code> 的集群中工作。
</div>
<!--
## Enabling DNS horizontal autoscaling      {#enablng-dns-horizontal-autoscaling}

In this section, you create a Deployment. The Pods in the Deployment run a
container based on the `cluster-proportional-autoscaler-amd64` image.

Create a file named `dns-horizontal-autoscaler.yaml` with this content:
-->
<h2 id="enablng-dns-horizontal-autoscaling">启用 DNS 水平自动扩缩  </h2>
<p>在本节，我们创建一个 Deployment。Deployment 中的 Pod 运行一个基于
<code>cluster-proportional-autoscaler-amd64</code> 镜像的容器。</p>
<p>创建文件 <code>dns-horizontal-autoscaler.yaml</code>，内容如下所示：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/dns/dns-horizontal-autoscaler.yaml" download="admin/dns/dns-horizontal-autoscaler.yaml"><code>admin/dns/dns-horizontal-autoscaler.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-dns-dns-horizontal-autoscaler-yaml')" title="Copy admin/dns/dns-horizontal-autoscaler.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-dns-dns-horizontal-autoscaler-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:kube-dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;nodes&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;list&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;watch&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;replicationcontrollers/scale&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;get&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;update&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;apps&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;deployments/scale&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;replicasets/scale&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;get&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;update&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 待以下 issue 修复后，请删除 Configmaps</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># kubernetes-incubator/cluster-proportional-autoscaler#16</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;configmaps&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;get&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;create&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRoleBinding<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:kube-dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">subjects</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">roleRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:kube-dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">priorityClassName</span>:<span style="color:#bbb"> </span>system-cluster-critical<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">seccompProfile</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>RuntimeDefault<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">supplementalGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#666">65534</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">fsGroup</span>:<span style="color:#bbb"> </span><span style="color:#666">65534</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kubernetes.io/os</span>:<span style="color:#bbb"> </span>linux<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/cpa/cluster-proportional-autoscaler:1.8.4<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- /cluster-proportional-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- --namespace=kube-system<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- --configmap=kube-dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># 应该保持目标与 cluster/addons/dns/kube-dns.yaml.base 同步</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span>- --target=&lt;SCALE_TARGET&gt;<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic">#当集群使用大节点（有更多核）时，“coresPerReplica”应该占主导地位。</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic">#如果使用小节点，“nodesPerReplica“ 应该占主导地位。</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span>- --default-params={&#34;linear&#34;:{&#34;coresPerReplica&#34;:256,&#34;nodesPerReplica&#34;:16,&#34;preventSinglePointFailure&#34;:true,&#34;includeUnschedulableNodes&#34;:true}}<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- --logtostderr=true<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- --v=2<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;CriticalAddonsOnly&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Exists&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">serviceAccountName</span>:<span style="color:#bbb"> </span>kube-dns-autoscaler<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the file, replace `<SCALE_TARGET>` with your scale target.

Go to the directory that contains your configuration file, and enter this
command to create the Deployment:
-->
<p>在文件中，将 <code>&lt;SCALE_TARGET&gt;</code> 替换成扩缩目标。</p>
<p>进入到包含配置文件的目录中，输入如下命令创建 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f dns-horizontal-autoscaler.yaml
</code></pre></div><!--
The output of a successful command is:
-->
<p>一个成功的命令输出是：</p>
<pre><code>deployment.apps/dns-autoscaler created
</code></pre><!--
DNS horizontal autoscaling is now enabled.
-->
<p>DNS 水平自动扩缩在已经启用了。</p>
<!--
## Tuning autoscaling parameters  {#tuning-autoscaling-parameters}

Verify that the dns-autoscaler <a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a> exists:
-->
<h2 id="tuning-autoscaling-parameters">调优自动扩缩参数  </h2>
<p>验证 dns-autoscaler <a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a> 是否存在：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get configmap --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME                  DATA      AGE
...
dns-autoscaler        1         ...
...
</code></pre><!--
Modify the data in the ConfigMap:
-->
<p>修改该 ConfigMap 中的数据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit configmap dns-autoscaler --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!--
Look for this line:
-->
<p>找到如下这行内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">linear</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;{&#34;coresPerReplica&#34;:256,&#34;min&#34;:1,&#34;nodesPerReplica&#34;:16}&#39;</span><span style="color:#bbb">
</span></code></pre></div><!--
Modify the fields according to your needs. The "min" field indicates the
minimal number of DNS backends. The actual number of backends number is
calculated using this equation:
-->
<p>根据需要修改对应的字段。“min” 字段表明 DNS 后端的最小数量。
实际后端的数量通过使用如下公式来计算：</p>
<pre><code>replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )
</code></pre><!--
Note that the values of both `coresPerReplica` and `nodesPerReplica` are
integers.

The idea is that when a cluster is using nodes that have many cores,
`coresPerReplica` dominates. When a cluster is using nodes that have fewer
cores, `nodesPerReplica` dominates.

There are other supported scaling patterns. For details, see
[cluster-proportional-autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler).
-->
<p>注意 <code>coresPerReplica</code> 和 <code>nodesPerReplica</code> 的值都是整数。</p>
<p>背后的思想是，当一个集群使用具有很多核心的节点时，由 <code>coresPerReplica</code> 来控制。
当一个集群使用具有较少核心的节点时，由 <code>nodesPerReplica</code> 来控制。</p>
<p>其它的扩缩模式也是支持的，详情查看
<a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster-proportional-autoscaler</a>。</p>
<!--
## Disable DNS horizontal autoscaling

There are a few options for tuning DNS horizontal autoscaling. Which option to
use depends on different conditions.
-->
<h2 id="禁用-dns-水平自动扩缩">禁用 DNS 水平自动扩缩</h2>
<p>有几个可供调优的 DNS 水平自动扩缩选项。具体使用哪个选项因环境而异。</p>
<!--
### Option 1: Scale down the dns-autoscaler deployment to 0 replicas

This option works for all situations. Enter this command:
-->
<h3 id="选项-1-缩容-dns-autoscaler-deployment-至-0-个副本">选项 1：缩容 dns-autoscaler Deployment 至 0 个副本</h3>
<p>该选项适用于所有场景。运行如下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale deployment --replicas<span style="color:#666">=</span><span style="color:#666">0</span> dns-autoscaler --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!-- The output is: -->
<p>输出如下所示：</p>
<pre><code>deployment.apps/dns-autoscaler scaled
</code></pre><!--
Verify that the replica count is zero:
-->
<p>验证当前副本数为 0：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!--
The output displays 0 in the DESIRED and CURRENT columns:
-->
<p>输出内容中，在 DESIRED 和 CURRENT 列显示为 0：</p>
<pre><code>NAME                                 DESIRED   CURRENT   READY   AGE
...
dns-autoscaler-6b59789fc8            0         0         0       ...
...
</code></pre><!--
### Option 2: Delete the dns-autoscaler deployment

This option works if dns-autoscaler is under your own control, which means
no one will re-create it:
-->
<h3 id="选项-2-删除-dns-autoscaler-deployment">选项 2：删除 dns-autoscaler Deployment</h3>
<p>如果 dns-autoscaler 为你所控制，也就说没有人会去重新创建它，可以选择此选项：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment dns-autoscaler --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!-- The output is:-->
<p>输出内容如下所示：</p>
<pre><code>deployment.apps &quot;dns-autoscaler&quot; deleted
</code></pre><!--
### Option 3: Delete the dns-autoscaler manifest file from the master node

This option works if dns-autoscaler is under control of the (deprecated)
[Addon Manager](https://git.k8s.io/kubernetes/cluster/addons/README.md),
and you have write access to the master node.
-->
<h3 id="选项-3-从主控节点删除-dns-autoscaler-清单文件">选项 3：从主控节点删除 dns-autoscaler 清单文件</h3>
<p>如果 dns-autoscaler 在<a href="https://git.k8s.io/kubernetes/cluster/addons/README.md">插件管理器</a>
的控制之下，并且具有操作 master 节点的写权限，可以使用此选项。</p>
<!--
Sign in to the master node and delete the corresponding manifest file.
The common path for this dns-autoscaler is:
-->
<p>登录到主控节点，删除对应的清单文件。
dns-autoscaler 对应的路径一般为：</p>
<pre><code>/etc/kubernetes/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
</code></pre><!--
After the manifest file is deleted, the Addon Manager will delete the
dns-autoscaler Deployment.
-->
<p>当清单文件被删除后，插件管理器将删除 dns-autoscaler Deployment。</p>
<!-- discussion -->
<!--
## Understanding how DNS horizontal autoscaling works

* The cluster-proportional-autoscaler application is deployed separately from
the DNS service.

* An autoscaler Pod runs a client that polls the Kubernetes API server for the
number of nodes and cores in the cluster.
-->
<h2 id="理解-dns-水平自动扩缩工作原理">理解 DNS 水平自动扩缩工作原理</h2>
<ul>
<li>
<p>cluster-proportional-autoscaler 应用独立于 DNS 服务部署。</p>
</li>
<li>
<p>autoscaler Pod 运行一个客户端，它通过轮询 Kubernetes API 服务器获取集群中节点和核心的数量。</p>
</li>
</ul>
<!--
* A desired replica count is calculated and applied to the DNS backends based on
the current schedulable nodes and cores and the given scaling parameters.

* The scaling parameters and data points are provided via a ConfigMap to the
autoscaler, and it refreshes its parameters table every poll interval to be up
to date with the latest desired scaling parameters.
-->
<ul>
<li>
<p>系统会基于当前可调度的节点个数、核心数以及所给的扩缩参数，计算期望的副本数并应用到 DNS 后端。</p>
</li>
<li>
<p>扩缩参数和数据点会基于一个 ConfigMap 来提供给 autoscaler，它会在每次轮询时刷新它的参数表，
以与最近期望的扩缩参数保持一致。</p>
</li>
</ul>
<!--
* Changes to the scaling parameters are allowed without rebuilding or restarting
the autoscaler Pod.

* The autoscaler provides a controller interface to support two control
patterns: *linear* and *ladder*.
-->
<ul>
<li>
<p>扩缩参数是可以被修改的，而且不需要重建或重启 autoscaler Pod。</p>
</li>
<li>
<p>autoscaler 提供了一个控制器接口来支持两种控制模式：<em>linear</em> 和 <em>ladder</em>。</p>
</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about [Guaranteed Scheduling For Critical Add-On Pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).
* Learn more about the
[implementation of cluster-proportional-autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler).

-->
<ul>
<li>阅读<a href="/zh/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">为关键插件 Pod 提供的调度保障</a></li>
<li>进一步了解 <a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster-proportional-autoscaler 实现</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3d0cd7d2f13d4759094f281504cf57b8">34 - 自定义 DNS 服务</h1>
    
	<!-- 
reviewers:
- bowei
- zihongz
title: Customizing DNS Service
content_type: task
min-kubernetes-server-version: v1.12
-->
<!-- overview -->
<!-- 
This page explains how to configure your DNS
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod(s)'>Pod(s)</a> and customize the
DNS resolution process in your cluster.
-->
<p>本页说明如何配置 DNS <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod(s)'>Pod(s)</a>，以及定制集群中 DNS 解析过程。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

Your Kubernetes server must be at or later than version v1.12.
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- 
Your cluster must be running the CoreDNS add-on.
[Migrating to CoreDNS](/docs/tasks/administer-cluster/coredns/#migrating-to-coredns)
explains how to use `kubeadm` to migrate from `kube-dns`.
-->
<p>你的集群必须运行 CoreDNS 插件。
文档<a href="/zh/docs/tasks/administer-cluster/coredns/#migrating-to-coredns">迁移到 CoreDNS</a>
解释了如何使用 <code>kubeadm</code> 从 <code>kube-dns</code> 迁移到 CoreDNS。</p>
<!-- steps -->
<!-- 
## Introduction 

DNS is a built-in Kubernetes service launched automatically
using the addon manager
[cluster add-on](http://releases.k8s.io/master/cluster/addons/README.md). 
-->
<h2 id="介绍">介绍</h2>
<p>DNS 是使用<a href="http://releases.k8s.io/master/cluster/addons/README.md">集群插件</a>
管理器自动启动的内置的 Kubernetes 服务。</p>
<!-- 
As of Kubernetes v1.12, CoreDNS is the recommended DNS Server, replacing kube-dns.  If your cluster
originally used kube-dns, you may still have `kube-dns` deployed rather than CoreDNS.
-->
<p>从 Kubernetes v1.12 开始，CoreDNS 是推荐的 DNS 服务器，取代了 kube-dns。 如果
你的集群原来使用 kube-dns，你可能部署的仍然是 <code>kube-dns</code> 而不是 CoreDNS。</p>
<!--
The CoreDNS Service is named `kube-dns` in the `metadata.name` field.  
This is so that there is greater interoperability with workloads that relied on the legacy `kube-dns` Service name to resolve addresses internal to the cluster. Using a Service named `kube-dns` abstracts away the implementation detail of which DNS provider is running behind that common name.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> CoreDNS 服务在其 <code>metadata.name</code> 字段被命名为 <code>kube-dns</code>。
这是为了能够与依靠传统 <code>kube-dns</code> 服务名称来解析集群内部地址的工作负载具有更好的互操作性。
使用 <code>kube-dns</code> 作为服务名称可以抽离共有名称之后运行的是哪个 DNS 提供程序这一实现细节。
</div>
<!--
If you are running CoreDNS as a Deployment, it will typically be exposed as a Kubernetes Service with a static IP address.
The kubelet passes DNS resolver information to each container with the `-cluster-dns=<dns-service-ip>` flag.
-->
<p>如果你在使用 Deployment 运行 CoreDNS，则该 Deployment 通常会向外暴露为一个具有
静态 IP 地址 Kubernetes 服务。
kubelet 使用 <code>--cluster-dns=&lt;DNS 服务 IP&gt;</code> 标志将 DNS 解析器的信息传递给每个容器。</p>
<!-- 
DNS names also need domains. You configure the local domain in the kubelet
with the flag `-cluster-domain=<default-local-domain>`. 
-->
<p>DNS 名称也需要域名。 你可在 kubelet 中使用 <code>--cluster-domain=&lt;默认本地域名&gt;</code>
标志配置本地域名。</p>
<!-- 
The DNS server supports forward lookups (A and AAAA records), port lookups (SRV records), reverse IP address lookups (PTR records),
and more. For more information, see [DNS for Services and Pods](/docs/concepts/services-networking/dns-pod-service/).
-->
<p>DNS 服务器支持正向查找（A 和 AAAA 记录）、端口发现（SRV 记录）、反向 IP 地址发现（PTR 记录）等。
更多信息，请参见<a href="/zh/docs/concepts/services-networking/dns-pod-service/">Pod 和 服务的 DNS</a>。</p>
<!-- 
If a Pod's `dnsPolicy` is set to "`default`", it inherits the name resolution
configuration from the node that the Pod runs on. The Pod's DNS resolution
should behave the same as the node.
But see [Known issues](/docs/tasks/administer-cluster/dns-debugging-resolution/#known-issues). 
-->
<p>如果 Pod 的 <code>dnsPolicy</code> 设置为 &quot;<code>default</code>&quot;，则它将从 Pod 运行所在节点继承名称解析配置。
Pod 的 DNS 解析行为应该与节点相同。
但请参阅<a href="/zh/docs/tasks/administer-cluster/dns-debugging-resolution/#known-issues">已知问题</a>。</p>
<!-- 
If you don't want this, or if you want a different DNS config for pods, you can
use the kubelet's `-resolv-conf` flag.  Set this flag to "" to prevent Pods from
inheriting DNS. Set it to a valid file path to specify a file other than
`/etc/resolv.conf` for DNS inheritance. 
-->
<p>如果你不想这样做，或者想要为 Pod 使用其他 DNS 配置，则可以
使用 kubelet 的 <code>--resolv-conf</code> 标志。 将此标志设置为 &quot;&quot; 可以避免 Pod 继承 DNS。
将其设置为有别于 <code>/etc/resolv.conf</code> 的有效文件路径可以设定 DNS 继承不同的配置。</p>
<h2 id="coredns">CoreDNS</h2>
<!-- 
CoreDNS is a general-purpose authoritative DNS server that can serve as cluster DNS, complying with the [dns specifications]
(https://github.com/kubernetes/dns/blob/master/docs/specification.md). 
-->
<p>CoreDNS 是通用的权威 DNS 服务器，可以用作集群 DNS，符合
<a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">DNS 规范</a>。</p>
<!-- 
### CoreDNS ConfigMap options 

CoreDNS is a DNS server that is modular and pluggable, and each plugin adds new functionality to CoreDNS. 
This can be configured by maintaining a [Corefile](https://coredns.io/2017/07/23/corefile-explained/), which is the CoreDNS
configuration file. A cluster administrator can modify the ConfigMap for the CoreDNS Corefile to change how service discovery works.  
-->
<h3 id="coredns-configmap-options">CoreDNS ConfigMap 选项 </h3>
<p>CoreDNS 是模块化且可插拔的 DNS 服务器，每个插件都为 CoreDNS 添加了新功能。
可以通过维护 <a href="https://coredns.io/2017/07/23/corefile-explained/">Corefile</a>，即 CoreDNS 配置文件，
来定制其行为。 集群管理员可以修改 CoreDNS Corefile 的 ConfigMap，以更改服务发现的工作方式。</p>
<!-- 
In Kubernetes, CoreDNS is installed with the following default Corefile configuration. 
-->
<p>在 Kubernetes 中，CoreDNS 安装时使用如下默认 Corefile 配置。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>coredns<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">Corefile</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    .:53 {
</span><span style="color:#b44;font-style:italic">        errors
</span><span style="color:#b44;font-style:italic">        health {
</span><span style="color:#b44;font-style:italic">            lameduck 5s
</span><span style="color:#b44;font-style:italic">        }
</span><span style="color:#b44;font-style:italic">        ready
</span><span style="color:#b44;font-style:italic">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span style="color:#b44;font-style:italic">           pods insecure
</span><span style="color:#b44;font-style:italic">           fallthrough in-addr.arpa ip6.arpa
</span><span style="color:#b44;font-style:italic">           ttl 30
</span><span style="color:#b44;font-style:italic">        }
</span><span style="color:#b44;font-style:italic">        prometheus :9153
</span><span style="color:#b44;font-style:italic">        forward . /etc/resolv.conf
</span><span style="color:#b44;font-style:italic">        cache 30
</span><span style="color:#b44;font-style:italic">        loop
</span><span style="color:#b44;font-style:italic">        reload
</span><span style="color:#b44;font-style:italic">        loadbalance
</span><span style="color:#b44;font-style:italic">    }</span><span style="color:#bbb">    
</span></code></pre></div><!-- 
The Corefile configuration includes the following [plugins](https://coredns.io/plugins/) of CoreDNS: 
-->
<p>Corefile 配置包括以下 CoreDNS <a href="https://coredns.io/plugins/">插件</a>：</p>
<!-- 
* [errors](https://coredns.io/plugins/errors/): Errors are logged to stdout.
* [health](https://coredns.io/plugins/health/): Health of CoreDNS is reported to http://localhost:8080/health.
* [kubernetes](https://coredns.io/plugins/kubernetes/): CoreDNS will reply to DNS queries based on IP of the services and pods of Kubernetes. You can find more details [here](https://coredns.io/plugins/kubernetes/).  
-->
<ul>
<li>
<p><a href="https://coredns.io/plugins/errors/">errors</a>：错误记录到标准输出。</p>
</li>
<li>
<p><a href="https://coredns.io/plugins/health/">health</a>：在 http://localhost:8080/health 处提供 CoreDNS 的健康报告。</p>
</li>
<li>
<p><a href="https://coredns.io/plugins/ready/">ready</a>：在端口 8181 上提供的一个 HTTP 末端，当所有能够
表达自身就绪的插件都已就绪时，在此末端返回 200 OK。</p>
</li>
<li>
<p><a href="https://coredns.io/plugins/kubernetes/">kubernetes</a>：CoreDNS 将基于 Kubernetes 的服务和 Pod 的
IP 答复 DNS 查询。你可以在 CoreDNS 网站阅读<a href="https://coredns.io/plugins/kubernetes/">更多细节</a>。
你可以使用 <code>ttl</code> 来定制响应的 TTL。默认值是 5 秒钟。TTL 的最小值可以是 0 秒钟，
最大值为 3600 秒。将 TTL 设置为 0 可以禁止对 DNS 记录进行缓存。</p>
<!-- 
The `pods insecure` option is provided for backward compatibility with kube-dns. You can use the
`pods verified` option, which returns an A record only if there exists a pod in same namespace
with matching IP. The `pods disabled` option can be used if you don't use pod records. 
-->
<p><code>pods insecure</code> 选项是为了与 kube-dns 向后兼容。你可以使用 <code>pods verified</code> 选项，该选项使得
仅在相同名称空间中存在具有匹配 IP 的 Pod 时才返回 A 记录。如果你不使用 Pod 记录，则可以使用
<code>pods disabled</code> 选项。</p>
</li>
</ul>
<!-- 
* [prometheus](https://coredns.io/plugins/prometheus/): Metrics of CoreDNS are available at http://localhost:9153/metrics in [Prometheus](https://prometheus.io/) format.
* [forward](https://coredns.io/plugins/forward/): Any queries that are not within the cluster domain of Kubernetes will be forwarded to predefined resolvers (/etc/resolv.conf).
* [cache](https://coredns.io/plugins/cache/): This enables a frontend cache.
* [loop](https://coredns.io/plugins/loop/): Detects simple forwarding loops and halts the CoreDNS process if a loop is found.
* [reload](https://coredns.io/plugins/reload): Allows automatic reload of a changed Corefile. After you edit the ConfigMap configuration, allow two minutes for your changes to take effect.
* [loadbalance](https://coredns.io/plugins/loadbalance): This is a round-robin DNS loadbalancer that randomizes the order of A, AAAA, and MX records in the answer. 
-->
<ul>
<li><a href="https://coredns.io/plugins/prometheus/">prometheus</a>：CoreDNS 的度量指标值以
<a href="https://prometheus.io/">Prometheus</a> 格式在 http://localhost:9153/metrics 上提供。</li>
<li><a href="https://coredns.io/plugins/forward/">forward</a>: 不在 Kubernetes 集群域内的任何查询都将转发到
预定义的解析器 (/etc/resolv.conf).</li>
<li><a href="https://coredns.io/plugins/cache/">cache</a>：启用前端缓存。</li>
<li><a href="https://coredns.io/plugins/loop/">loop</a>：检测到简单的转发环，如果发现死循环，则中止 CoreDNS 进程。</li>
<li><a href="https://coredns.io/plugins/reload">reload</a>：允许自动重新加载已更改的 Corefile。
编辑 ConfigMap 配置后，请等待两分钟，以使更改生效。</li>
<li><a href="https://coredns.io/plugins/loadbalance">loadbalance</a>：这是一个轮转式 DNS 负载均衡器，
它在应答中随机分配 A、AAAA 和 MX 记录的顺序。</li>
</ul>
<!-- 
You can modify the default CoreDNS behavior by modifying the ConfigMap. 
-->
<p>你可以通过修改 ConfigMap 来更改默认的 CoreDNS 行为。</p>
<!-- 
### Configuration of Stub-domain and upstream nameserver using CoreDNS 

CoreDNS has the ability to configure stubdomains and upstream nameservers using the [forward plugin](https://coredns.io/plugins/forward/).
-->
<h3 id="使用-coredns-配置存根域和上游域名服务器">使用 CoreDNS 配置存根域和上游域名服务器</h3>
<p>CoreDNS 能够使用 <a href="https://coredns.io/plugins/forward/">forward 插件</a>配置存根域和上游域名服务器。</p>
<!-- 
#### Example

If a cluster operator has a [Consul](https://www.consul.io/) domain server located at 10.150.0.1, and all Consul names have the suffix .consul.local. To configure it in CoreDNS, the cluster administrator creates the following stanza in the CoreDNS ConfigMap. 
-->
<h4 id="示例">示例</h4>
<p>如果集群操作员在 10.150.0.1 处运行了 <a href="https://www.consul.io/">Consul</a> 域服务器，
且所有 Consul 名称都带有后缀 <code>.consul.local</code>。要在 CoreDNS 中对其进行配置，
集群管理员可以在 CoreDNS 的 ConfigMap 中创建加入以下字段。</p>
<pre><code>consul.local:53 {
        errors
        cache 30
        forward . 10.150.0.1
    }
</code></pre><!-- 
To explicitly force all non-cluster DNS lookups to go through a specific nameserver at 172.16.0.1, point the  `forward` to the nameserver instead of `/etc/resolv.conf` 
-->
<p>要显式强制所有非集群 DNS 查找通过特定的域名服务器（位于 172.16.0.1），可将 <code>forward</code>
指向该域名服务器，而不是 <code>/etc/resolv.conf</code>。</p>
<pre><code>forward .  172.16.0.1
</code></pre><!-- 
The final ConfigMap along with the default `Corefile` configuration looks like: 
-->
<p>最终的包含默认的 <code>Corefile</code> 配置的 ConfigMap 如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>coredns<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">Corefile</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    .:53 {
</span><span style="color:#b44;font-style:italic">        errors
</span><span style="color:#b44;font-style:italic">        health
</span><span style="color:#b44;font-style:italic">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span style="color:#b44;font-style:italic">           pods insecure
</span><span style="color:#b44;font-style:italic">           fallthrough in-addr.arpa ip6.arpa
</span><span style="color:#b44;font-style:italic">        }
</span><span style="color:#b44;font-style:italic">        prometheus :9153
</span><span style="color:#b44;font-style:italic">        forward . 172.16.0.1
</span><span style="color:#b44;font-style:italic">        cache 30
</span><span style="color:#b44;font-style:italic">        loop
</span><span style="color:#b44;font-style:italic">        reload
</span><span style="color:#b44;font-style:italic">        loadbalance
</span><span style="color:#b44;font-style:italic">    }
</span><span style="color:#b44;font-style:italic">    consul.local:53 {
</span><span style="color:#b44;font-style:italic">        errors
</span><span style="color:#b44;font-style:italic">        cache 30
</span><span style="color:#b44;font-style:italic">        forward . 10.150.0.1
</span><span style="color:#b44;font-style:italic">    }</span><span style="color:#bbb">    
</span></code></pre></div><!-- 
The `kubeadm` supports automatic translation of the CoreDNS ConfigMap from the kube-dns ConfigMap.
-->
<p>工具 <code>kubeadm</code> 支持将 kube-dns ConfigMap 自动转换为 CoreDNS ConfigMap。</p>
<!--
While kube-dns accepts an FQDN for stubdomain and nameserver (eg: ns.foo.com), CoreDNS does not support this feature. 
During translation, all FQDN nameservers will be omitted from the CoreDNS config.*** 
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 尽管 kube-dns 接受 FQDN（例如：ns.foo.com）作为存根域和名字服务器，CoreDNS 不支持此功能。
转换期间，CoreDNS 配置中将忽略所有的 FQDN 域名服务器。
</div>
<!-- 
## CoreDNS configuration equivalent to kube-dns

CoreDNS supports the features of kube-dns and more.
A ConfigMap created for kube-dns to support `StubDomains`and `upstreamNameservers` translates to the `proxy` plugin in CoreDNS.

### Example

This example ConfigMap for kube-dns specifies stubdomains and upstreamnameservers: 
-->
<h2 id="coredns-配置等同于-kube-dns">CoreDNS 配置等同于 kube-dns</h2>
<p>CoreDNS 不仅仅提供 kube-dns 的功能。
为 kube-dns 创建的 ConfigMap 支持 <code>StubDomains</code> 和 <code>upstreamNameservers</code> 转换为 CoreDNS 中的 <code>forward</code> 插件。</p>
<h3 id="示例-1">示例</h3>
<p>用于 kubedns 的此示例 ConfigMap 描述了 stubdomains 和 upstreamnameservers：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">stubDomains</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>{<span style="color:#b44">&#34;abc.com&#34;</span><span style="color:#bbb"> </span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;1.2.3.4&#34;</span>],<span style="color:#bbb"> </span><span style="color:#b44">&#34;my.cluster.local&#34;</span><span style="color:#bbb"> </span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;2.3.4.5&#34;</span>]}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">upstreamNameservers</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>[<span style="color:#b44">&#34;8.8.8.8&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;8.8.4.4&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span></code></pre></div><!-- 
The equivalent configuration in CoreDNS creates a Corefile: 
-->
<p>CoreDNS 中的等效配置将创建一个 Corefile：</p>
<ul>
<li>
<p>针对 stubDomains:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">abc.com:53 {<span style="color:#bbb">
</span><span style="color:#bbb">     </span>errors<span style="color:#bbb">
</span><span style="color:#bbb">     </span>cache 30<span style="color:#bbb">
</span><span style="color:#bbb">     </span>proxy . 1.2.3.4<span style="color:#bbb">
</span><span style="color:#bbb"> </span>}<span style="color:#bbb">
</span><span style="color:#bbb"> </span>my.cluster.local:53 {<span style="color:#bbb">
</span><span style="color:#bbb">     </span>errors<span style="color:#bbb">
</span><span style="color:#bbb">     </span>cache 30<span style="color:#bbb">
</span><span style="color:#bbb">     </span>proxy . 2.3.4.5<span style="color:#bbb">
</span><span style="color:#bbb"> </span>}<span style="color:#bbb">
</span></code></pre></div></li>
</ul>
<!-- 
The complete Corefile with the default plugins: 
-->
<p>带有默认插件的完整 Corefile：</p>
<pre><code>.:53 {
    errors
    health
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
    }
    federation cluster.local {
       foo foo.feddomain.com
    }
    prometheus :9153
    forward .  8.8.8.8 8.8.4.4
    cache 30
}
abc.com:53 {
    errors
    cache 30
    forward . 1.2.3.4
}
my.cluster.local:53 {
    errors
    cache 30
    forward . 2.3.4.5
}
</code></pre><!-- 
## Migration to CoreDNS

To migrate from kube-dns to CoreDNS, [a detailed blog](https://coredns.io/2018/05/21/migration-from-kube-dns-to-coredns/) is available to help users adapt CoreDNS in place of kube-dns.
A cluster administrator can also migrate using [the deploy script](https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh).

-->
<h2 id="迁移到-coredns">迁移到 CoreDNS</h2>
<p>要从 kube-dns 迁移到 CoreDNS，<a href="https://coredns.io/2018/05/21/migration-from-kube-dns-to-coredns/">此博客</a>
提供了帮助用户将 kube-dns 替换为 CoreDNS。
集群管理员还可以使用<a href="https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh">部署脚本</a>
进行迁移。</p>
<h2 id="what-s-next">What's next</h2>
<!--
- Read [Debugging DNS Resolution](/docs/tasks/administer-cluster/dns-debugging-resolution/).
-->
<ul>
<li>阅读<a href="/zh/docs/tasks/administer-cluster/dns-debugging-resolution/">调试 DNS 解析</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8bcf4aeb5bbb6d6969a146e5ab97557b">35 - 调试 DNS 问题</h1>
    
	<!-- overview -->
<!--
This page provides hints on diagnosing DNS problems.
-->
<p>这篇文章提供了一些关于 DNS 问题诊断的方法。</p>
<!-- steps -->
<h2 id="before-you-begin">Before you begin</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- 
Your cluster must be configured to use the CoreDNS
<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='addon'>addon</a> or its precursor,
kube-dns.  
-->
<p>你的集群必须使用了 CoreDNS <a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='插件'>插件</a>
或者其前身，<code>kube-dns</code>。</p>


Your Kubernetes server must be at or later than version v1.6.
 To check the version, enter <code>kubectl version</code>.

<!--
### Create a simple Pod to use as a test environment



 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/dns/dnsutils.yaml" download="admin/dns/dnsutils.yaml"><code>admin/dns/dnsutils.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-dns-dnsutils-yaml')" title="Copy admin/dns/dnsutils.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-dns-dnsutils-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dnsutils<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dnsutils<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;3600&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>



<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> This example creates a pod in the <code>default</code> namespace. DNS name resolution for
services depends on the namespace of the pod. For more information, review
<a href="/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names">DNS for Services and Pods</a>.
</div>

Use that manifest to create a Pod:

```shell
kubectl create -f https://k8s.io/examples/admin/dns/busybox.yaml
pod/busybox created

kubectl get pods busybox
NAME      READY     STATUS    RESTARTS   AGE
busybox   1/1       Running   0          <some-time>
```
-->
<h3 id="创建一个简单的-pod-作为测试环境">创建一个简单的 Pod 作为测试环境</h3>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/dns/dnsutils.yaml" download="admin/dns/dnsutils.yaml"><code>admin/dns/dnsutils.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-dns-dnsutils-yaml')" title="Copy admin/dns/dnsutils.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-dns-dnsutils-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dnsutils<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dnsutils<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;3600&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 此示例在 <code>default</code> 命名空间创建 pod。 服务的 DNS 名字解析取决于 pod 的命名空间。 详细信息请查阅
<a href="/zh/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names">服务和 Pod 的 DNS</a>。
</div>
<p>使用上面的清单来创建一个 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml
</code></pre></div><pre><code>pod/dnsutils created
</code></pre><!--
…and verify its status:
-->
<p>验证其状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods dnsutils
</code></pre></div><pre><code>NAME      READY     STATUS    RESTARTS   AGE
dnsutils   1/1       Running   0          &lt;some-time&gt;
</code></pre><!--
Once that pod is running, you can exec `nslookup` in that environment.
If you see something like the following, DNS is working correctly.
-->
<p>一旦 Pod 处于运行状态，你就可以在该环境里执行 <code>nslookup</code>。
如果你看到类似下列的内容，则表示 DNS 是正常运行的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup kubernetes.default
</code></pre></div><pre><code>Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      kubernetes.default
Address 1: 10.0.0.1
</code></pre><!--
If the `nslookup` command fails, check the following:
-->
<p>如果 <code>nslookup</code> 命令执行失败，请检查下列内容：</p>
<!--
### Check the local DNS configuration first

Take a look inside the resolv.conf file.
(See [Customizing DNS Service](/docs/tasks/administer-cluster/dns-custom-nameservers) and
[Known issues](#known-issues) below for more information)
-->
<h3 id="先检查本地的-dns-配置">先检查本地的 DNS 配置</h3>
<p>查看 resolv.conf 文件的内容
（阅读<a href="/zh/docs/tasks/administer-cluster/dns-custom-nameservers/">定制 DNS 服务</a> 和
后文的<a href="#known-issues">已知问题</a> ，获取更多信息)</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -ti dnsutils -- cat /etc/resolv.conf
</code></pre></div><!--
Verify that the search path and name server are set up like the following
(note that search path may vary for different cloud providers):
-->
<p>验证 search 和 nameserver 的配置是否与下面的内容类似
（注意 search 根据不同的云提供商可能会有所不同)：</p>
<pre><code>search default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal
nameserver 10.0.0.10
options ndots:5
</code></pre><!--
Errors such as the following indicate a problem with the CoreDNS (or kube-dns)
add-on or with associated Services:
-->
<p>下列错误表示 CoreDNS （或 kube-dns）插件或者相关服务出现了问题：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup kubernetes.default
</code></pre></div><p>输出为：</p>
<pre><code>Server:    10.0.0.10
Address 1: 10.0.0.10

nslookup: can't resolve 'kubernetes.default'
</code></pre><p>或者</p>
<pre><code>Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'kubernetes.default'
</code></pre><!--
### Check if the DNS pod is running

Use the `kubectl get pods` command to verify that the DNS pod is running.
-->
<h3 id="check-if-the-dns-pod-is-running">检查 DNS Pod 是否运行  </h3>
<p>使用 <code>kubectl get pods</code> 命令来验证 DNS Pod 是否运行。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --namespace<span style="color:#666">=</span>kube-system -l k8s-app<span style="color:#666">=</span>kube-dns
</code></pre></div><pre><code>NAME                       READY     STATUS    RESTARTS   AGE
...
coredns-7b96bf9f76-5hsxb   1/1       Running   0           1h
coredns-7b96bf9f76-mvmmt   1/1       Running   0           1h
...
</code></pre><!--
The value for label `k8s-app` is `kube-dns` for both CoreDNS and kube-dns deployments.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 对于 CoreDNS 和 kube-dns 部署而言，标签 <code>k8s-app</code> 的值都应该是 <code>kube-dns</code>。
</div>
<!--
If you see that no CoreDNS pod is running or that the pod has failed/completed,
the DNS add-on may not be deployed by default in your current environment and you
will have to deploy it manually.
-->
<p>如果你发现没有 CoreDNS Pod 在运行，或者该 Pod 的状态是 failed 或者 completed，
那可能这个 DNS 插件在您当前的环境里并没有成功部署，你将需要手动去部署它。</p>
<!--
### Check for Errors in the DNS pod

Use `kubectl logs` command to see logs for the DNS containers.
-->
<h3 id="check-for-errors-in-the-dns-pod">检查 DNS Pod 里的错误   </h3>
<p>使用 <code>kubectl logs</code> 命令来查看 DNS 容器的日志信息。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs --namespace<span style="color:#666">=</span>kube-system -l k8s-app<span style="color:#666">=</span>kube-dns
</code></pre></div><!--
Here is an example of a healthy CoreDNS log:
-->
<p>下列是一个正常运行的 CoreDNS 日志信息：</p>
<pre><code>.:53
2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2
2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.2
linux/amd64, go1.10.3, 2e322f6
2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c
</code></pre><!--
See if there are any suspicious or unexpected messages in the logs.
-->
<p>查看是否日志中有一些可疑的或者意外的消息。</p>
<!--
### Is DNS service up?

Verify that the DNS service is up by using the `kubectl get service` command.
-->
<h3 id="is-dns-service-up">检查是否启用了 DNS 服务  </h3>
<p>使用 <code>kubectl get service</code> 命令来检查 DNS 服务是否已经启用。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><pre><code>NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
...
kube-dns     ClusterIP   10.0.0.10      &lt;none&gt;        53/UDP,53/TCP        1h
...
</code></pre><!--
The service name is `kube-dns` for both CoreDNS and kube-dns deployments.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 不管是 CoreDNS 还是 kube-dns，这个服务的名字都会是 <code>kube-dns</code> 。
</div>
<!--
If you have created the Service or in the case it should be created by default
but it does not appear, see
[debugging Services](/docs/tasks/debug-application-cluster/debug-service/) for
more information.
-->
<p>如果你已经创建了 DNS 服务，或者该服务应该是默认自动创建的但是它并没有出现，
请阅读<a href="/zh/docs/tasks/debug-application-cluster/debug-service/">调试服务</a>
来获取更多信息。</p>
<!--
### Are DNS endpoints exposed?

You can verify that DNS endpoints are exposed by using the `kubectl get endpoints`
command.
-->
<h3 id="are-dns-endpoints-exposed">DNS 的端点公开了吗？   </h3>
<p>你可以使用 <code>kubectl get endpoints</code> 命令来验证 DNS 的端点是否公开了。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get ep kube-dns --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><pre><code>NAME       ENDPOINTS                       AGE
kube-dns   10.180.3.17:53,10.180.3.17:53    1h
</code></pre><!--
If you do not see the endpoints, see endpoints section in the
[debugging services](/docs/tasks/debug-application-cluster/debug-service/) documentation.

For additional Kubernetes DNS examples, see the
[cluster-dns examples](https://github.com/kubernetes/examples/tree/master/staging/cluster-dns)
in the Kubernetes GitHub repository.
-->
<p>如果你没看到对应的端点，请阅读
<a href="/zh/docs/tasks/debug-application-cluster/debug-service/">调试服务</a>的端点部分。</p>
<p>若需要了解更多的 Kubernetes DNS 例子，请在 Kubernetes GitHub 仓库里查看
<a href="https://github.com/kubernetes/examples/tree/master/staging/cluster-dns">cluster-dns 示例</a>。</p>
<!--
### Are DNS queries being received/processed?

You can verify if queries are being received by CoreDNS by adding the `log` plugin to the CoreDNS configuration (aka Corefile).
The CoreDNS Corefile is held in a ConfigMap named `coredns`. To edit it, use the command ...
-->
<h3 id="are-dns-queries-bing-received-processed">DNS 查询有被接收或者执行吗？  </h3>
<p>你可以通过给 CoreDNS 的配置文件（也叫 Corefile）添加 <code>log</code> 插件来检查查询是否被正确接收。
CoreDNS 的 Corefile 被保存在一个叫 <code>coredns</code> 的 ConfigMap 里，使用下列命令来编辑它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl -n kube-system edit configmap coredns
</code></pre></div><!--
Then add `log` in the Corefile section per the example below.
-->
<p>然后按下面的例子给 Corefile 添加 <code>log</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>coredns<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">Corefile</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    .:53 {
</span><span style="color:#b44;font-style:italic">        log
</span><span style="color:#b44;font-style:italic">        errors
</span><span style="color:#b44;font-style:italic">        health
</span><span style="color:#b44;font-style:italic">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span style="color:#b44;font-style:italic">          pods insecure
</span><span style="color:#b44;font-style:italic">          upstream
</span><span style="color:#b44;font-style:italic">          fallthrough in-addr.arpa ip6.arpa
</span><span style="color:#b44;font-style:italic">        }
</span><span style="color:#b44;font-style:italic">        prometheus :9153
</span><span style="color:#b44;font-style:italic">        forward . /etc/resolv.conf
</span><span style="color:#b44;font-style:italic">        cache 30
</span><span style="color:#b44;font-style:italic">        loop
</span><span style="color:#b44;font-style:italic">        reload
</span><span style="color:#b44;font-style:italic">        loadbalance
</span><span style="color:#b44;font-style:italic">    }</span><span style="color:#bbb">    
</span></code></pre></div><!--
After saving the changes, it may take up to minute or two for Kubernetes to propagate these changes to the CoreDNS pods.
-->
<p>保存这些更改后，你可能会需要等待一到两分钟让 Kubernetes 把这些更改应用到
CoreDNS 的 Pod 里。</p>
<!--
Next, make some queries and view the logs per the sections above in this document. If CoreDNS pods are receiving the queries, you should see them in the logs.

Here is an example of a query in the log.
-->
<p>接下来，发起一些查询并依照前文所述查看日志信息，如果 CoreDNS 的 Pod 接收到这些查询，
你将可以在日志信息里看到它们。</p>
<p>下面是日志信息里的查询例子：</p>
<pre><code>.:53
2018/08/15 14:37:15 [INFO] CoreDNS-1.2.0
2018/08/15 14:37:15 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.0
linux/amd64, go1.10.3, 2e322f6
2018/09/07 15:29:04 [INFO] plugin/reload: Running configuration MD5 = 162475cdf272d8aa601e6fe67a6ad42f
2018/09/07 15:29:04 [INFO] Reloading complete
172.17.0.18:41675 - [07/Sep/2018:15:29:11 +0000] 59925 &quot;A IN kubernetes.default.svc.cluster.local. udp 54 false 512&quot; NOERROR qr,aa,rd,ra 106 0.000066649s

</code></pre><!--
### Are you in the right namespace for the service?

DNS queries that don't specify a namespace are limited to the pod's 
namespace. 

If the namespace of the pod and service differ, the DNS query must include 
the namespace of the service.

This query is limited to the pod's namespace:
```shell
kubectl exec -i -t dnsutils -- nslookup <service-name>
```
-->
<h3 id="你的服务在正确的命名空间中吗">你的服务在正确的命名空间中吗？</h3>
<p>未指定命名空间的 DNS 查询仅作用于 pod 所在的命名空间。</p>
<p>如果 pod 和服务的命名空间不相同，则 DNS 查询必须指定服务所在的命名空间。</p>
<p>该查询仅限于 pod 所在的名称空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup &lt;service-name&gt;
</code></pre></div><!--
This query specifies the namespace:
```shell
kubectl exec -i -t dnsutils -- nslookup <service-name>.<namespace>
```
-->
<p>指定命名空间的查询：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup &lt;service-name&gt;.&lt;namespace&gt;
</code></pre></div><!--
To learn more about name resolution, see 
[DNS for Services and Pods](/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names). 
-->
<p>要进一步了解名字解析，请查看
<a href="/zh/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names">服务和 Pod 的 DNS</a>。</p>
<!--
## Known issues

Some Linux distributions (e.g. Ubuntu), use a local DNS resolver by default (systemd-resolved).
Systemd-resolved moves and replaces `/etc/resolv.conf` with a stub file that can cause a fatal forwarding
loop when resolving names in upstream servers. This can be fixed manually by using kubelet's `-resolv-conf` flag
to point to the correct `resolv.conf` (With `systemd-resolved`, this is `/run/systemd/resolve/resolv.conf`).
kubeadm automatically detects `systemd-resolved`, and adjusts the kubelet flags accordingly.
-->
<h2 id="known-issues">已知问题</h2>
<p>有些 Linux 发行版本（比如 Ubuntu）默认使用一个本地的 DNS 解析器（systemd-resolved）。
<code>systemd-resolved</code> 会用一个存根文件（Stub File）来覆盖 <code>/etc/resolv.conf</code> 内容，
从而可能在上游服务器中解析域名产生转发环（forwarding loop）。 这个问题可以通过手动指定
kubelet 的 <code>--resolv-conf</code> 标志为正确的 <code>resolv.conf</code>（如果是 <code>systemd-resolved</code>，
则这个文件路径为 <code>/run/systemd/resolve/resolv.conf</code>）来解决。
kubeadm 会自动检测 <code>systemd-resolved</code> 并对应的更改 kubelet 的命令行标志。</p>
<!--
Kubernetes installs do not configure the nodes' `resolv.conf` files to use the
cluster DNS by default, because that process is inherently distribution-specific.
This should probably be implemented eventually.
-->
<p>Kubernetes 的安装并不会默认配置节点的 <code>resolv.conf</code> 文件来使用集群的 DNS 服务，因为这个配置对于不同的发行版本是不一样的。这个问题应该迟早会被解决的。</p>
<!--
Linux's libc (a.k.a. glibc) has a limit for the DNS `nameserver` records to 3
by default. What's more, for the glibc versions which are older than
glibc-2.17-222 ([the new versions update see this
issue](https://access.redhat.com/solutions/58028)), the allowed number of DNS
`search` records has been limited to 6 ([see this bug from
2005](https://bugzilla.redhat.com/show_bug.cgi?id=168253)). Kubernetes needs
to consume 1 `nameserver` record and 3 `search` records. This means that if a
local installation already uses 3 `nameserver`s or uses more than 3 `search`es
while your glibc version is in the affected list, some of those settings will
be lost. To work around the DNS `nameserver` records limit, the node can run
`dnsmasq`, which will provide more `nameserver` entries. You can also use
kubelet's `--resolv-conf` flag. To fix the DNS `search` records limit,
consider upgrading your linux distribution or upgrading to an unaffected
version of glibc.
-->
<p>Linux 的 libc 限制 <code>nameserver</code> 只能有三个记录。不仅如此，对于 glibc-2.17-222
之前的版本（<a href="https://access.redhat.com/solutions/58028">参见此 Issue 了解新版本的更新</a>），<code>search</code> 的记录不能超过 6 个
（ <a href="https://bugzilla.redhat.com/show_bug.cgi?id=168253">详情请查阅这个 2005 年的 bug</a>）。
Kubernetes 需要占用一个 <code>nameserver</code> 记录和三个<code>search</code>记录。
这意味着如果一个本地的安装已经使用了三个 <code>nameserver</code> 或者使用了超过三个
<code>search</code> 记录，而你的 glibc 版本也在有问题的版本列表中，那么有些配置很可能会丢失。
为了绕过 DNS <code>nameserver</code> 个数限制，节点可以运行 <code>dnsmasq</code>，以提供更多的
<code>nameserver</code> 记录。你也可以使用kubelet 的 <code>--resolv-conf</code> 标志来解决这个问题。
要想修复 DNS <code>search</code> 记录个数限制问题，可以考虑升级你的 Linux 发行版本，或者
升级 glibc 到一个不再受此困扰的版本。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
With [Expanded DNS Configuration](/docs/concepts/services-networking/dns-pod-service/#expanded-dns-configuration),
Kubernetes allows more DNS `search` records.
-->
<p>使用<a href="/zh/docs/concepts/services-networking/dns-pod-service/#expanded-dns-configuration">扩展 DNS 设置</a>，
Kubernetes 允许更多的 <code>search</code> 记录。
</div>
<!--
If you are using Alpine version 3.3 or earlier as your base image, DNS may not
work properly owing to a known issue with Alpine.
Check [here](https://github.com/kubernetes/kubernetes/issues/30215)
for more information.
-->
<p>如果你使用 Alpine  3.3 或更早版本作为你的基础镜像，DNS 可能会由于 Alpine 中
一个已知的问题导致无法正常工作。
请查看<a href="https://github.com/kubernetes/kubernetes/issues/30215">这里</a>获取更多信息。</p>
<h2 id="what-s-next">What's next</h2>
<!--
- [Autoscaling the DNS Service in a Cluster](/docs/tasks/administer-cluster/dns-horizontal-autoscaling/).
- [DNS for Services and Pods](/docs/concepts/services-networking/dns-pod-service/)
-->
<ul>
<li>参阅<a href="/zh/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">自动扩缩集群中的 DNS 服务</a>.</li>
<li>阅读<a href="/zh/docs/concepts/services-networking/dns-pod-service/">服务和 Pod 的 DNS</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a24171610b6ea75a142cb9c8c7882390">36 - 迁移多副本的控制面以使用云控制器管理器</h1>
    
	<!--
reviewers:
- jpbetz
- cheftako
title: "Migrate Replicated Control Plane To Use Cloud Controller Manager"
linkTitle: "Migrate Replicated Control Plane To Use Cloud Controller Manager"
content_type: task
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
title: Cloud Controller Manager
id: cloud-controller-manager
date: 2018-04-12
full_link: /docs/concepts/architecture/cloud-controller/
short_description: >
  Control plane component that integrates Kubernetes with third-party cloud providers.

aka: 
tags:
- core-object
- architecture
- operation
-->
<!--
 A Kubernetes <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.
-->
<p>云控制器管理器是指嵌入特定云的控制逻辑的
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>组件。
云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上，
并将与该云平台交互的组件同与你的集群交互的组件分离开来。</p>
<!--
By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.
-->
<p>通过分离 Kubernetes 和底层云基础设置之间的互操作性逻辑，
云控制器管理器组件使云提供商能够以不同于 Kubernetes 主项目的
步调发布新特征。</p>
<!--
## Background

As part of the [cloud provider extraction effort](https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/),
all cloud specific controllers must be moved out of the `kube-controller-manager`. 
All existing clusters that run cloud controllers in the `kube-controller-manager` must migrate to instead run the controllers in a cloud provider specific `cloud-controller-manager`.

Leader Migration provides a mechanism in which HA clusters can safely migrate "cloud specific" controllers between 
the `kube-controller-manager` and the `cloud-controller-manager` via a shared resource lock between the two components while upgrading the replicated control plane. 
For a single-node control plane, or if unavailability of controller managers can be tolerated during the upgrade, Leader Migration is not needed and this guide can be ignored.
-->
<h2 id="背景">背景</h2>
<p>作为<a href="https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/">云驱动提取工作</a>
的一部分，所有特定于云的控制器都必须移出 <code>kube-controller-manager</code>。
所有在 <code>kube-controller-manager</code> 中运行云控制器的现有集群必须迁移到特定于云厂商的
<code>cloud-controller-manager</code> 中运行这些控制器。</p>
<p>领导者迁移（Leader Migration）提供了一种机制，使得 HA 集群可以通过这两个组件之间共享资源锁，
在升级多副本的控制平面时，安全地将“特定于云”的控制器从 <code>kube-controller-manager</code> 迁移到
<code>cloud-controller-manager</code>。
对于单节点控制平面，或者在升级过程中可以容忍控制器管理器不可用的情况，则不需要领导者迁移，
亦可以忽略本指南。</p>
<!--
Leader Migration can be enabled by setting `--enable-leader-migration` on `kube-controller-manager` or `cloud-controller-manager`.
Leader Migration only applies during the upgrade and can be safely disabled or left enabled after the upgrade is complete.

This guide walks you through the manual process of upgrading the control plane from `kube-controller-manager` with 
built-in cloud provider to running both `kube-controller-manager` and `cloud-controller-manager`. 
If you use a tool to administrator the cluster, please refer to the documentation of the tool and the cloud provider for more details.
-->
<p>领导者迁移可以通过在 <code>kube-controller-manager</code> 或 <code>cloud-controller-manager</code> 上设置
<code>--enable-leader-migration</code> 来启用。
领导者迁移仅在升级期间适用，并且在升级完成后可以安全地禁用或保持启用状态。</p>
<p>本指南将引导你手动将控制平面从内置的云驱动的 <code>kube-controller-manager</code> 升级为
同时运行 <code>kube-controller-manager</code> 和 <code>cloud-controller-manager</code>。
如果使用某种工具来管理群集，请参阅对应工具和云驱动的文档以获取更多详细信息。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
It is assumed that the control plane is running Kubernetes version N and to be upgraded to version N + 1. 
Although it is possible to migrate within the same version, ideally the migration should be performed as part of a upgrade so that changes of configuration can be aligned to each release. 
The exact versions of N and N + 1 depend on each cloud provider. For example, if a cloud provider builds a `cloud-controller-manager` to work with Kubernetes 1.22, then N can be 1.21 and N + 1 can be 1.22.

The control plane nodes should run `kube-controller-manager` with Leader Election enabled through `--leader-elect=true`. 
As of version N, an in-tree cloud privider must be set with `--cloud-provider` flag and `cloud-controller-manager` should not yet be deployed.
-->
<p>假定控制平面正在运行 Kubernetes 版本 N，要升级到版本 N+1。
尽管可以在同一版本内进行迁移，但理想情况下，迁移应作为升级的一部分执行，
以便可以配置的变更可以与发布版本变化对应起来。
N 和 N+1 的确切版本值取决于各个云厂商。例如，如果云厂商构建了一个可与 Kubernetes 1.22
配合使用的 <code>cloud-controller-manager</code>，则 N 可以为 1.21，N+1 可以为 1.22。</p>
<p>控制平面节点应运行 <code>kube-controller-manager</code>，并通过 <code>--leader-elect=true</code> 启用领导者选举。
在版本 N 中，树内云驱动必须设置 <code>--cloud-provider</code> 标志，而且 <code>cloud-controller-manager</code>
应该尚未部署。</p>
<!--
The out-of-tree cloud provider must have built a `cloud-controller-manager` with Leader Migration implementation. 
If the cloud provider imports `k8s.io/cloud-provider` and `k8s.io/controller-manager` of version v0.21.0 or later, Leader Migration will be available.
However, for version before v0.22.0, Leader Migration is alpha and requires feature gate `ControllerManagerLeaderMigration` to be enabled.

This guide assumes that kubelet of each control plane node starts `kube-controller-manager` 
and `cloud-controller-manager` as static pods defined by their manifests. 
If the components run in a different setting, please adjust the steps accordingly.

For authorization, this guide assumes that the cluster uses RBAC. 
If another authorization mode grants permissions to `kube-controller-manager` and `cloud-controller-manager` components, 
please grant the needed access in a way that matches the mode.
-->
<p>树外云驱动必须已经构建了一个实现了领导者迁移的 <code>cloud-controller-manager</code>。
如果云驱动导入了 v0.21.0 或更高版本的 <code>k8s.io/cloud-provider</code> 和 <code>k8s.io/controller-manager</code>，
则可以进行领导者迁移。
但是，对 v0.22.0 以下的版本，领导者迁移是一项 Alpha 阶段功能，需要启用特性门控
<code>ControllerManagerLeaderMigration</code>。</p>
<p>本指南假定每个控制平面节点的 kubelet 以静态 Pod 的形式启动 <code>kube-controller-manager</code>
和 <code>cloud-controller-manager</code>，静态 Pod 的定义在清单文件中。
如果组件以其他设置运行，请相应地调整这里的步骤。</p>
<p>关于鉴权，本指南假定集群使用 RBAC。如果其他鉴权模式授予 <code>kube-controller-manager</code>
和 <code>cloud-controller-manager</code> 组件权限，请以与该模式匹配的方式授予所需的访问权限。</p>
<!-- steps -->
<!--
### Grant access to Migration Lease

The default permissions of the controller manager allow only accesses to their main Lease.
In order for the migration to work, accesses to another Lease are required.

You can grant `kube-controller-manager` full access to the leases API by modifying 
the `system::leader-locking-kube-controller-manager` role. 
This task guide assumes that the name of the migration lease is `cloud-provider-extraction-migration`.

`kubectl patch -n kube-system role 'system::leader-locking-kube-controller-manager' -p '{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}' --type=merge`

Do the same to the `system::leader-locking-cloud-controller-manager` role.

`kubectl patch -n kube-system role 'system::leader-locking-cloud-controller-manager' -p '{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}' --type=merge`
-->
<h3 id="授予访问迁移租约的权限">授予访问迁移租约的权限</h3>
<p>控制器管理器的默认权限仅允许访问其主租约（Lease）对象。为了使迁移正常进行，
需要授权它访问其他 Lease 对象。</p>
<p>你可以通过修改 <code>system::leader-locking-kube-controller-manager</code> 角色来授予
<code>kube-controller-manager</code> 对 Lease API 的完全访问权限。
本任务指南假定迁移 Lease 的名称为 <code>cloud-provider-extraction-migration</code>。</p>
<p><code>kubectl patch -n kube-system role 'system::leader-locking-kube-controller-manager' -p '{&quot;rules&quot;: [ {&quot;apiGroups&quot;:[ &quot;coordination.k8s.io&quot;], &quot;resources&quot;: [&quot;leases&quot;], &quot;resourceNames&quot;: [&quot;cloud-provider-extraction-migration&quot;], &quot;verbs&quot;: [&quot;create&quot;, &quot;list&quot;, &quot;get&quot;, &quot;update&quot;] } ]}' --type=merge</code></p>
<p>对 <code>system::leader-locking-cloud-controller-manager</code> 角色执行相同的操作。</p>
<p><code>kubectl patch -n kube-system role 'system::leader-locking-cloud-controller-manager' -p '{&quot;rules&quot;: [ {&quot;apiGroups&quot;:[ &quot;coordination.k8s.io&quot;], &quot;resources&quot;: [&quot;leases&quot;], &quot;resourceNames&quot;: [&quot;cloud-provider-extraction-migration&quot;], &quot;verbs&quot;: [&quot;create&quot;, &quot;list&quot;, &quot;get&quot;, &quot;update&quot;] } ]}' --type=merge</code></p>
<!--
### Initial Leader Migration configuration

Leader Migration optionally takes a configuration file representing the state of controller-to-manager assignment. At this moment, with in-tree cloud provider, `kube-controller-manager` runs `route`, `service`, and `cloud-node-lifecycle`. The following example configuration shows the assignment.

Leader Migration can be enabled without a configuration. Please see [Default Configuration](#default-configuration) for details.
-->
<h3 id="初始领导者迁移配置">初始领导者迁移配置</h3>
<p>领导者迁移可以选择使用一个表示如何将控制器分配给不同管理器的配置文件。
目前，对于树内云驱动，<code>kube-controller-manager</code> 运行 <code>route</code>、<code>service</code> 和
<code>cloud-node-lifecycle</code>。以下示例配置显示的是这种分配。</p>
<p>领导者迁移可以不指定配置的情况下启用。请参阅<a href="#default-configuration">默认配置</a>
以获取更多详细信息。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LeaderMigrationConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>controllermanager.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">leaderName</span>:<span style="color:#bbb"> </span>cloud-provider-extraction-migration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resourceLock</span>:<span style="color:#bbb"> </span>leases<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">controllerLeaders</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>route<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>kube-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>kube-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-node-lifecycle<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>kube-controller-manager<span style="color:#bbb">
</span></code></pre></div><!--
On each control plane node, save the content to `/etc/leadermigration.conf`, 
and update the manifest of `kube-controller-manager` so that the file is mounted inside the container at the same location. 
Also, update the same manifest to add the following arguments:

- `--enable-leader-migration` to enable Leader Migration on the controller manager
- `--leader-migration-config=/etc/leadermigration.conf` to set configuration file

Restart `kube-controller-manager` on each node. At this moment, `kube-controller-manager` has leader migration enabled and is ready for the migration.
-->
<p>在每个控制平面节点上，请将如上内容保存到 <code>/etc/leadermigration.conf</code> 中，
并更新 <code>kube-controller-manager</code> 清单，以便将文件挂载到容器内的同一位置。
另外，请更新同一清单，添加以下参数：</p>
<ul>
<li><code>--enable-leader-migration</code> 在控制器管理器上启用领导者迁移</li>
<li><code>--leader-migration-config=/etc/leadermigration.conf</code> 设置配置文件</li>
</ul>
<p>在每个节点上重新启动 <code>kube-controller-manager</code>。这时，<code>kube-controller-manager</code>
已启用领导者迁移，为迁移准备就绪。</p>
<!--
### Deploy Cloud Controller Manager

In version N + 1, the desired state of controller-to-manager assignment can be represented by a new configuration file, shown as follows. 
Please note `component` field of each `controllerLeaders` changing from `kube-controller-manager` to `cloud-controller-manager`.
-->
<h3 id="部署云控制器管理器">部署云控制器管理器</h3>
<p>在版本 N+1 中，如何将控制器分配给不同管理器的预期分配状态可以由新的配置文件表示，
如下所示。请注意，各个 <code>controllerLeaders</code> 的 <code>component</code> 字段从 <code>kube-controller-manager</code>
更改为 <code>cloud-controller-manager</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LeaderMigrationConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>controllermanager.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">leaderName</span>:<span style="color:#bbb"> </span>cloud-provider-extraction-migration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resourceLock</span>:<span style="color:#bbb"> </span>leases<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">controllerLeaders</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>route<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-node-lifecycle<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></code></pre></div><!--
When creating control plane nodes of version N + 1, the content should be deploy to `/etc/leadermigration.conf`. 
The manifest of `cloud-controller-manager` should be updated to mount the configuration file in 
the same manner as `kube-controller-manager` of version N. Similarly, add `--feature-gates=ControllerManagerLeaderMigration=true`,
`--enable-leader-migration`, and `--leader-migration-config=/etc/leadermigration.conf` to the arguments of `cloud-controller-manager`.

Create a new control plane node of version N + 1 with the updated `cloud-controller-manager` manifest, 
and with the `--cloud-provider` flag unset for `kube-controller-manager`.
`kube-controller-manager` of version N + 1 MUST NOT have Leader Migration enabled because, 
with an external cloud provider, it does not run the migrated controllers anymore and thus it is not involved in the migration.

Please refer to [Cloud Controller Manager Administration](/docs/tasks/administer-cluster/running-cloud-controller/)
for more detail on how to deploy `cloud-controller-manager`.
-->
<p>当创建版本 N+1 的控制平面节点时，应将如上内容写入到 <code>/etc/leadermigration.conf</code>。
你需要更新 <code>cloud-controller-manager</code> 的清单，以与版本 N 的 <code>kube-controller-manager</code>
相同的方式挂载配置文件。
类似地，添加 <code>--feature-gates=ControllerManagerLeaderMigration=true</code>、<code>--enable-leader-migration</code>
和 <code>--leader-migration-config=/etc/leadermigration.conf</code> 到 <code>cloud-controller-manager</code>
的参数中。</p>
<p>使用已更新的 <code>cloud-controller-manager</code> 清单创建一个新的 N+1 版本的控制平面节点，
同时确保没有设置 <code>kube-controller-manager</code> 的 <code>--cloud-provider</code> 标志。
版本为 N+1 的 <code>kube-controller-manager</code> 不能启用领导者迁移，
因为在使用外部云驱动的情况下，它不再运行已迁移的控制器，因此不参与迁移。</p>
<p>请参阅<a href="/zh/docs/tasks/administer-cluster/running-cloud-controller/">云控制器管理器管理</a>
了解有关如何部署 <code>cloud-controller-manager</code> 的更多细节。</p>
<!--
### Upgrade Control Plane

The control plane now contains nodes of both version N and N + 1. 
The nodes of version N run `kube-controller-manager` only, 
and these of version N + 1 run both `kube-controller-manager` and `cloud-controller-manager`. 
The migrated controllers, as specified in the configuration, are running under either `kube-controller-manager` of 
version N or `cloud-controller-manager` of version N + 1 depending on which controller manager holds the migration lease.
No controller will ever be running under both controller managers at any time.

In a rolling manner, create a new control plane node of version N + 1 and bring down one of version N + 1 until the control plane contains only nodes of version N + 1.
If a rollback from version N + 1 to N is required, add nodes of version N with Leader Migration enabled for `kube-controller-manager` back to the control plane, replacing one of version N + 1 each time until there are only nodes of version N.
-->
<h3 id="升级控制平面">升级控制平面</h3>
<p>现在，控制平面同时包含 N 和 N+1 版本的节点。
版本 N 的节点仅运行 <code>kube-controller-manager</code>，而版本 N+1 的节点同时运行
<code>kube-controller-manager</code> 和 <code>cloud-controller-manager</code>。
根据配置所指定，已迁移的控制器在版本 N 的 <code>kube-controller-manager</code> 或版本
N+1 的 <code>cloud-controller-manager</code> 下运行，具体取决于哪个控制器管理器拥有迁移租约对象。
任何时候都不会有同一个控制器在两个控制器管理器下运行。</p>
<p>以滚动的方式创建一个新的版本为 N+1 的控制平面节点，并将版本 N 中的一个关闭，
直到控制平面仅包含版本为 N+1 的节点。
如果需要从 N+1 版本回滚到 N 版本，则将 <code>kube-controller-manager</code> 启用了领导者迁移的、
且版本为 N 的节点添加回控制平面，每次替换 N+1 版本中的一个，直到只有版本 N 的节点为止。</p>
<!--
### (Optional) Disable Leader Migration {#disable-leader-migration}

Now that the control plane has been upgraded to run both `kube-controller-manager` and `cloud-controller-manager` of version N + 1, 
Leader Migration has finished its job and can be safely disabled to save one Lease resource. 
It is safe to re-enable Leader Migration for the rollback in the future.

In a rolling manager, update manifest of `cloud-controller-manager` to unset both 
`--enable-leader-migration` and `--leader-migration-config=` flag, 
also remove the mount of `/etc/leadermigration.conf`, and finally remove `/etc/leadermigration.conf`. 
To re-enable Leader Migration, recreate the configuration file and add its mount and the flags that enable Leader Migration back to `cloud-controller-manager`.
-->
<h3 id="disable-leader-migration">（可选）禁用领导者迁移</h3>
<p>现在，控制平面已经完成升级，同时运行版本 N+1 的 <code>kube-controller-manager</code>
和 <code>cloud-controller-manager</code>。领导者迁移的任务已经结束，可以被安全地禁用以节省一个
Lease 资源。在将来可以安全地重新启用领导者迁移，以完成回滚。</p>
<p>在滚动管理器中，更新 <code>cloud-controller-manager</code> 的清单以同时取消设置
<code>--enable-leader-migration</code> 和 <code>--leader-migration-config=</code> 标志，并删除
<code>/etc/leadermigration.conf</code> 的挂载，最后删除 <code>/etc/leadermigration.conf</code>。
要重新启用领导者迁移，请重新创建配置文件，并将其挂载和启用领导者迁移的标志添加回到
<code>cloud-controller-manager</code>。</p>
<!--
### Default Configuration

Starting Kubernetes 1.22, Leader Migration provides a default configuration suitable for the default controller-to-manager assignment.
The default configuration can be enabled by setting `--enable-leader-migration` but without `--leader-migration-config=`.

For `kube-controller-manager` and `cloud-controller-manager`, if there are no flags that enable any in-tree cloud provider or change ownership of controllers, the default configuration can be used to avoid manual creation of the configuration file.
-->
<h3 id="default-configuration">默认配置</h3>
<p>从 Kubernetes 1.22 开始，领导者迁移提供了一个默认配置，它适用于控制器与管理器间默认的分配关系。
可以通过设置 <code>--enable-leader-migration</code>，但不设置 <code>--leader-migration-config=</code>
来启用默认配置。</p>
<p>对于 <code>kube-controller-manager</code> 和 <code>cloud-controller-manager</code>，如果没有用参数来启用树内云驱动或者改变控制器属主，
则可以使用默认配置来避免手动创建配置文件。</p>
<h2 id="what-s-next">What's next</h2>
<!--
- Read the [Controller Manager Leader Migration](https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider/2436-controller-manager-leader-migration) enhancement proposal
-->
<ul>
<li>阅读<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider/2436-controller-manager-leader-migration">领导者迁移控制器管理器</a>
改进建议提案。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1e966f5d0540bbee0876f9d0d08d54dc">37 - 通过名字空间共享集群</h1>
    
	<!--
reviewers:
- derekwaynecarr
- janetkuo
title: Share a Cluster with Namespaces
content_type: task
-->
<!-- overview -->
<!--
This page shows how to view, work in, and delete <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespaces'>namespaces</a>. The page also shows how to use Kubernetes namespaces to subdivide your cluster.
-->
<p>本页展示如何查看、使用和删除<a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>。
本页同时展示如何使用 Kubernetes 名字空间来划分集群。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
* Have an [existing Kubernetes cluster](/docs/setup/).
* You have a basic understanding of Kubernetes <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>, <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a>, and <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployments'>Deployments</a>.
-->
<ul>
<li>你已拥有一个<a href="/zh/docs/setup/">配置好的 Kubernetes 集群</a>。</li>
<li>你已对 Kubernetes 的 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> ,
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a> , 和
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployments'>Deployments</a> 有基本理解。</li>
</ul>
<!-- steps -->
<!-- ## Viewing namespaces -->
<h2 id="查看名字空间">查看名字空间</h2>
<!-- 1. List the current namespaces in a cluster using: -->
<ol>
<li>列出集群中现有的名字空间：</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces
</code></pre></div><pre><code>NAME          STATUS    AGE
default       Active    11d
kube-system   Active    11d
kube-public   Active    11d
</code></pre><!-- Kubernetes starts with three initial namespaces: -->
<p>初始状态下，Kubernetes 具有三个名字空间：</p>
<!--
* `default` The default namespace for objects with no other namespace
* `kube-system` The namespace for objects created by the Kubernetes system
* `kube-public` This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement. -->
<ul>
<li><code>default</code> 无名字空间对象的默认名字空间</li>
<li><code>kube-system</code> 由 Kubernetes 系统创建的对象的名字空间</li>
<li><code>kube-public</code> 自动创建且被所有用户可读的名字空间（包括未经身份认证的）。此名字空间通常在某些资源在整个集群中可见且可公开读取时被集群使用。此名字空间的公共方面只是一个约定，而不是一个必要条件。</li>
</ul>
<!-- You can also get the summary of a specific namespace using: -->
<p>你还可以通过下列命令获取特定名字空间的摘要：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces &lt;name&gt;
</code></pre></div><!-- Or you can get detailed information with: -->
<p>或用下面的命令获取详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe namespaces &lt;name&gt;
</code></pre></div><pre><code>Name:           default
Labels:         &lt;none&gt;
Annotations:    &lt;none&gt;
Status:         Active

No resource quota.

Resource Limits
 Type       Resource    Min Max Default
 ----               --------    --- --- ---
 Container          cpu         -   -   100m
</code></pre><!-- Note that these details show both resource quota (if present) as well as resource limit ranges. -->
<p>请注意，这些详情同时显示了资源配额（如果存在）以及资源限制区间。</p>
<!-- Resource quota tracks aggregate usage of resources in the *Namespace* and allows cluster operators
to define *Hard* resource usage limits that a *Namespace* may consume. -->
<p>资源配额跟踪并聚合 <em>Namespace</em> 中资源的使用情况，并允许集群运营者定义 <em>Namespace</em> 可能消耗的 <em>Hard</em> 资源使用限制。</p>
<!-- A limit range defines min/max constraints on the amount of resources a single entity can consume in
a *Namespace*.

See [Admission control: Limit Range](https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md) -->
<p>限制区间定义了单个实体在一个 <em>Namespace</em> 中可使用的最小/最大资源量约束。</p>
<p>参阅 <a href="https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md">准入控制: 限制区间</a></p>
<!--
A namespace can be in one of two phases:

* `Active` the namespace is in use
* `Terminating` the namespace is being deleted, and can not be used for new objects

For more details, see [Namespace](/docs/reference/kubernetes-api/cluster-resources/namespace-v1/)
in the API reference.
 -->
<p>名字空间可以处于下列两个阶段中的一个:</p>
<ul>
<li><code>Active</code> 名字空间正在被使用中</li>
<li><code>Terminating</code> 名字空间正在被删除，且不能被用于新对象。</li>
</ul>
<p>更多细节，参阅 API 参考中的<a href="/docs/reference/kubernetes-api/cluster-resources/namespace-v1/">命名空间</a>。</p>
<!-- ## Creating a new namespace -->
<h2 id="创建名字空间">创建名字空间</h2>
<!--
Avoid creating namespace with prefix `kube-`, since it is reserved for Kubernetes system namespaces.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 避免使用前缀 <code>kube-</code> 创建名字空间，因为它是为 Kubernetes 系统名字空间保留的。
</div>
<!-- 1. Create a new YAML file called `my-namespace.yaml` with the contents: -->
<ol>
<li>
<p>新建一个名为 <code>my-namespace.yaml</code> 的 YAML 文件，并写入下列内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Namespace<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>&lt;insert-namespace-name-here&gt;<span style="color:#bbb">
</span></code></pre></div><!-- Then run: -->
<p>然后运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./my-namespace.yaml
</code></pre></div></li>
</ol>
<!--
2. Alternatively, you can create namespace using below command:
-->
<ol start="2">
<li>
<p>或者，你可以使用下面的命令创建名字空间：</p>
<pre><code>kubectl create namespace &lt;insert-namespace-name-here&gt;
</code></pre></li>
</ol>
<!--
The name of your namespace must be a valid
[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).
-->
<p>请注意，名字空间的名称必须是一个合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-label-names">DNS 标签</a>。</p>
<!--
There's an optional field `finalizers`, which allows observables to purge resources whenever the namespace is deleted. Keep in mind that if you specify a nonexistent finalizer, the namespace will be created but will get stuck in the `Terminating` state if the user tries to delete it.

More information on `finalizers` can be found in the namespace [design doc](https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#finalizers).
-->
<p>可选字段 <code>finalizers</code> 允许观察者们在名字空间被删除时清除资源。记住如果指定了一个不存在的终结器，名字空间仍会被创建，但如果用户试图删除它，它将陷入 <code>Terminating</code> 状态。</p>
<p>更多有关 <code>finalizers</code> 的信息请查阅 <a href="https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#finalizers">设计文档</a> 中名字空间部分。</p>
<!-- ## Deleting a namespace -->
<h2 id="删除名字空间">删除名字空间</h2>
<!--
Delete a namespace with
-->
<p>删除名字空间使用命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespaces &lt;insert-some-namespace-name&gt;
</code></pre></div><!-- This deletes _everything_ under the namespace! -->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 这会删除名字空间下的 <em>所有内容</em> ！
</div>


<!-- This delete is asynchronous, so for a time you will see the namespace in the `Terminating` state. -->
<p>删除是异步的，所以有一段时间你会看到名字空间处于 <code>Terminating</code> 状态。</p>
<!--
## Subdividing your cluster using Kubernetes namespaces
-->
<h2 id="使用-kubernetes-名字空间细分你的集群">使用 Kubernetes 名字空间细分你的集群</h2>
<!--
1. Understand the default namespace

   By default, a Kubernetes cluster will instantiate a default namespace when provisioning
   the cluster to hold the default set of Pods, Services, and Deployments used by the cluster.
-->
<ol>
<li>
<p>理解 default 名字空间</p>
<p>默认情况下，Kubernetes 集群会在配置集群时实例化一个 default 名字空间，用以存放集群所使用的默认
Pods、Services 和 Deployments 集合。</p>
<!--
Assuming you have a fresh cluster, you can introspect the available namespace's by doing the following:
-->
<p>假设你有一个新的集群，你可以通过执行以下操作来内省可用的名字空间</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces
</code></pre></div><pre><code>NAME      STATUS    AGE
default   Active    13m
</code></pre></li>
</ol>
<!--
2. Create new namespaces
-->
<ol start="2">
<li>
<p>创建新的名字空间</p>
<!--
For this exercise, we will create two additional Kubernetes namespaces to hold our content.
-->
<p>在本练习中，我们将创建两个额外的 Kubernetes 名字空间来保存我们的内容。</p>
<!--
In a scenario where an organization is using a shared Kubernetes cluster for development and
production use cases:
-->
<p>在某组织使用共享的 Kubernetes 集群进行开发和生产的场景中：</p>
<!--
The development team would like to maintain a space in the cluster where they can
get a view on the list of Pods, Services, and Deployments
they use to build and run their application.  In this space, Kubernetes resources come
and go, and the restrictions on who can or cannot modify resources
are relaxed to enable agile development.
-->
<p>开发团队希望在集群中维护一个空间，以便他们可以查看用于构建和运行其应用程序的 Pods、Services
和 Deployments 列表。在这个空间里，Kubernetes 资源被自由地加入或移除，
对谁能够或不能修改资源的限制被放宽，以实现敏捷开发。</p>
<!--
The operations team would like to maintain a space in the cluster where they can enforce
strict procedures on who can or cannot manipulate the set of
Pods, Services, and Deployments that run the production site.
-->
<p>运维团队希望在集群中维护一个空间，以便他们可以强制实施一些严格的规程，
对谁可以或不可以操作运行生产站点的 Pods、Services 和 Deployments 集合进行控制。</p>
<!--
One pattern this organization could follow is to partition the Kubernetes cluster into
two namespaces: `development` and `production`.
-->
<p>该组织可以遵循的一种模式是将 Kubernetes 集群划分为两个名字空间：development 和 production。</p>
<!-- Let's create two new namespaces to hold our work. -->
<p>让我们创建两个新的名字空间来保存我们的工作。</p>
<!-- Create the `development` namespace using kubectl. -->
<p>使用 kubectl 创建 <code>development</code> 名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
</code></pre></div><!-- And then let's create the `production` namespace using kubectl. -->
<p>让我们使用 kubectl 创建 <code>production</code> 名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
</code></pre></div><!-- To be sure things are right, list all of the namespaces in our cluster. -->
<p>为了确保一切正常，列出集群中的所有名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces --show-labels
</code></pre></div><pre><code>NAME          STATUS    AGE       LABELS
default       Active    32m       &lt;none&gt;
development   Active    29s       name=development
production    Active    23s       name=production
</code></pre></li>
</ol>
<!-- 3. Create pods in each namespace -->
<ol start="3">
<li>
<p>在每个名字空间中创建 pod</p>
<!--
A Kubernetes namespace provides the scope for Pods, Services, and Deployments in the cluster.

Users interacting with one namespace do not see the content in another namespace.
-->
<p>Kubernetes 名字空间为集群中的 Pods、Services 和 Deployments 提供了作用域。</p>
<p>与一个名字空间交互的用户不会看到另一个名字空间中的内容。</p>
<!-- To demonstrate this, let's spin up a simple Deployment and Pods in the `development` namespace. -->
<p>为了演示这一点，让我们在 <code>development</code> 名字空间中启动一个简单的 Deployment 和 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment snowflake --image<span style="color:#666">=</span>k8s.gcr.io/serve_hostname -n<span style="color:#666">=</span>development
kubectl scale deployment snowflake --replicas<span style="color:#666">=</span><span style="color:#666">2</span> -n<span style="color:#666">=</span>development
</code></pre></div><!--
We have created a deployment whose replica size is 2 that is running the pod
called `snowflake` with a basic container that serves the hostname.
-->
<p>我们创建了一个副本个数为 2 的 Deployment，运行名为 <code>snowflake</code> 的
Pod，其中包含一个负责提供主机名的基本容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment -n<span style="color:#666">=</span>development
</code></pre></div><pre><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
snowflake    2/2     2            2           2m
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>snowflake -n<span style="color:#666">=</span>development
</code></pre></div><pre><code>NAME                         READY     STATUS    RESTARTS   AGE
snowflake-3968820950-9dgr8   1/1       Running   0          2m
snowflake-3968820950-vgc4n   1/1       Running   0          2m
</code></pre><!--
And this is great, developers are able to do what they want, and they do not have to worry about affecting content in the `production` namespace.

Let's switch to the `production` namespace and show how resources in one namespace are hidden from the other.

The `production` namespace should be empty, and the following commands should return nothing.
-->
<p>看起来还不错，开发人员能够做他们想做的事，而且他们不必担心会影响到
<code>production</code> 名字空间下面的内容。</p>
<p>让我们切换到 <code>production</code> 名字空间，展示一下一个名字空间中的资源是如何对
另一个名字空间隐藏的。</p>
<p>名字空间 <code>production</code> 应该是空的，下面的命令应该不会返回任何东西。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment -n<span style="color:#666">=</span>production
kubectl get pods -n<span style="color:#666">=</span>production
</code></pre></div><!--
Production likes to run cattle, so let's create some cattle pods.
-->
<p>生产环境下一般以养牛的方式运行负载，所以让我们创建一些 Cattle（牛）Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment cattle --image<span style="color:#666">=</span>k8s.gcr.io/serve_hostname -n<span style="color:#666">=</span>production
kubectl scale deployment cattle --replicas<span style="color:#666">=</span><span style="color:#666">5</span> -n<span style="color:#666">=</span>production

kubectl get deployment -n<span style="color:#666">=</span>production
</code></pre></div><pre><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
cattle       5/5     5            5           10s
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>cattle -n<span style="color:#666">=</span>production
</code></pre></div><pre><code>NAME                      READY     STATUS    RESTARTS   AGE
cattle-2263376956-41xy6   1/1       Running   0          34s
cattle-2263376956-kw466   1/1       Running   0          34s
cattle-2263376956-n4v97   1/1       Running   0          34s
cattle-2263376956-p5p3i   1/1       Running   0          34s
cattle-2263376956-sxpth   1/1       Running   0          34s
</code></pre></li>
</ol>
<!--
At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.
-->
<p>此时，应该很清楚的展示了用户在一个名字空间中创建的资源对另一个名字空间是隐藏的。</p>
<!--
As the policy support in Kubernetes evolves, we will extend this scenario to show how you can provide different
authorization rules for each namespace.
-->
<p>随着 Kubernetes 中的策略支持的发展，我们将扩展此场景，以展示如何为每个名字空间提供不同的授权规则。</p>
<!-- discussion -->
<!--
## Understanding the motivation for using namespaces
-->
<h2 id="理解使用名字空间的动机">理解使用名字空间的动机</h2>
<!--
A single cluster should be able to satisfy the needs of multiple users or groups of users (henceforth a 'user community').
-->
<p>单个集群应该能满足多个用户及用户组的需求（以下称为 “用户社区”）。</p>
<!-- Kubernetes _namespaces_ help different projects, teams, or customers to share a Kubernetes cluster. -->
<p>Kubernetes <em>名字空间</em> 帮助不同的项目、团队或客户去共享 Kubernetes 集群。</p>
<!--
It does this by providing the following:

1. A scope for [Names](/docs/concepts/overview/working-with-objects/names/).
2. A mechanism to attach authorization and policy to a subsection of the cluster.
-->
<p>名字空间通过以下方式实现这点：</p>
<ol>
<li>为<a href="/zh/docs/concepts/overview/working-with-objects/names/">名字</a>设置作用域.</li>
<li>为集群中的部分资源关联鉴权和策略的机制。</li>
</ol>
<!--
Use of multiple namespaces is optional.
-->
<p>使用多个名字空间是可选的。</p>
<!--
Each user community wants to be able to work in isolation from other communities.
-->
<p>每个用户社区都希望能够与其他社区隔离开展工作。</p>
<!--
Each user community has its own:

1. resources (pods, services, replication controllers, etc.)
2. policies (who can or cannot perform actions in their community)
3. constraints (this community is allowed this much quota, etc.)
-->
<p>每个用户社区都有自己的：</p>
<ol>
<li>资源（pods、服务、 副本控制器等等）</li>
<li>策略（谁能或不能在他们的社区里执行操作）</li>
<li>约束（该社区允许多少配额，等等）</li>
</ol>
<!--
A cluster operator may create a Namespace for each unique user community.
-->
<p>集群运营者可以为每个唯一用户社区创建名字空间。</p>
<!--
The Namespace provides a unique scope for:

1. named resources (to avoid basic naming collisions)
2. delegated management authority to trusted users
3. ability to limit community resource consumption
-->
<p>名字空间为下列内容提供唯一的作用域：</p>
<ol>
<li>命名资源（避免基本的命名冲突）</li>
<li>将管理权限委派给可信用户</li>
<li>限制社区资源消耗的能力</li>
</ol>
<!--
Use cases include:

1.  As a cluster operator, I want to support multiple user communities on a single cluster.
2.  As a cluster operator, I want to delegate authority to partitions of the cluster to trusted users
    in those communities.
3.  As a cluster operator, I want to limit the amount of resources each community can consume in order
    to limit the impact to other communities using the cluster.
4.  As a cluster user, I want to interact with resources that are pertinent to my user community in
    isolation of what other user communities are doing on the cluster.
-->
<p>用例包括:</p>
<ol>
<li>作为集群运营者, 我希望能在单个集群上支持多个用户社区。</li>
<li>作为集群运营者，我希望将集群分区的权限委派给这些社区中的受信任用户。</li>
<li>作为集群运营者，我希望能限定每个用户社区可使用的资源量，以限制对使用同一集群的其他用户社区的影响。</li>
<li>作为群集用户，我希望与我的用户社区相关的资源进行交互，而与其他用户社区在该集群上执行的操作无关。</li>
</ol>
<!--
## Understanding namespaces and DNS
-->
<h2 id="理解名字空间和-dns">理解名字空间和 DNS</h2>
<!--
When you create a [Service](/docs/concepts/services-networking/service/), it creates a corresponding [DNS entry](/docs/concepts/services-networking/dns-pod-service/).
This entry is of the form `<service-name>.<namespace-name>.svc.cluster.local`, which means
that if a container uses `<service-name>` it will resolve to the service which
is local to a namespace.  This is useful for using the same configuration across
multiple namespaces such as Development, Staging and Production.  If you want to reach
across namespaces, you need to use the fully qualified domain name (FQDN).
-->
<p>当你创建<a href="/zh/docs/concepts/services-networking/service/">服务</a>时，Kubernetes
会创建相应的 <a href="/zh/docs/concepts/services-networking/dns-pod-service/">DNS 条目</a>。
此条目的格式为 <code>&lt;服务名称&gt;.&lt;名字空间名称&gt;.svc.cluster.local</code>。
这意味着如果容器使用 <code>&lt;服务名称&gt;</code>，它将解析为名字空间本地的服务。
这对于在多个名字空间（如开发、暂存和生产）中使用相同的配置非常有用。
如果要跨名字空间访问，则需要使用完全限定的域名（FQDN）。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [setting the namespace preference](/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-preference).
* Learn more about [setting the namespace for a request](/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-for-a-request)
* See [namespaces design](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/namespaces.md).
-->
<ul>
<li>进一步了解<a href="/zh/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-preference">设置名字空间偏好</a></li>
<li>进一步了解<a href="/zh/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-for-a-request">设置请求的名字空间</a></li>
<li>参阅<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/namespaces.md">名字空间的设计文档</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f58763cc9447491b6c40f939a02d441d">38 - 通过配置文件设置 Kubelet 参数</h1>
    
	<!--
reviewers:
- mtaufen
- dawnchen
title: Set Kubelet parameters via a config file
content_type: task
--->
<!-- overview -->
<!--
A subset of the Kubelet's configuration parameters may be
set via an on-disk config file, as a substitute for command-line flags.
--->
<p>通过保存在硬盘的配置文件设置 kubelet 的部分配置参数，这可以作为命令行参数的替代。</p>
<!--
Providing parameters via a config file is the recommended approach because
it simplifies node deployment and configuration management.
--->
<p>建议通过配置文件的方式提供参数，因为这样可以简化节点部署和配置管理。</p>
<!-- steps -->
<!--
## Create the config file

The subset of the Kubelet's configuration that can be configured via a file
is defined by the
[`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/)
struct.
-->
<h2 id="创建配置文件">创建配置文件</h2>
<p><a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a> 结构体定义了可以通过文件配置的 Kubelet 配置子集，</p>
<!--
The configuration file must be a JSON or YAML representation of the parameters
in this struct. Make sure the Kubelet has read permissions on the file.

Here is an example of what this file might look like:
-->
<p>配置文件必须是这个结构体中参数的 JSON 或 YAML 表现形式。
确保 kubelet 可以读取该文件。</p>
<p>下面是一个 Kubelet 配置文件示例：</p>
<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: &quot;192.168.0.8&quot;,
port: 20250,
serializeImagePulls: false,
evictionHard:
    memory.available:  &quot;200Mi&quot;
</code></pre><!--
In the example, the Kubelet is configured to serve on IP address 192.168.0.8 and port 20250, pull images in parallel,
and evict Pods when available memory drops below 200Mi.
All other Kubelet configuration values are left at their built-in defaults, unless overridden
by flags. Command line flags which target the same value as a config file will override that value.
-->
<p>在这个示例中, Kubelet 被设置为在地址 192.168.0.8 端口 20250 上提供服务，以并行方式拖拽镜像，
当可用内存低于 200Mi 时, kubelet 将会开始驱逐 Pods。
没有声明的其余配置项都将使用默认值，除非使用命令行参数来重载。
命令行中的参数将会覆盖配置文件中的对应值。</p>
<!--
## Start a Kubelet process configured via the config file

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> If you use kubeadm to initialize your cluster, use the kubelet-config while creating your cluster with <code>kubeadmin init</code>.
See <a href="/docs/setup/production-environment/tools/kubeadm/kubelet-integration/">configuring kubelet using kubeadm</a> for details.
</div>

Start the Kubelet with the `--config` flag set to the path of the Kubelet's config file.
The Kubelet will then load its config from this file.
--->
<h2 id="启动通过配置文件配置的-kubelet-进程">启动通过配置文件配置的 Kubelet 进程</h2>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果你使用 kubeadm 初始化你的集群，在使用 <code>kubeadmin init</code> 创建你的集群的时候请使用 kubelet-config。
更多细节请阅读<a href="/zh/docs/setup/production-environment/tools/kubeadm/kubelet-integration/">使用 kubeadm 配置 kubelet</a>
</div>
<p>启动 Kubelet 需要将 <code>--config</code> 参数设置为 Kubelet 配置文件的路径。Kubelet 将从此文件加载其配置。</p>
<!--
Note that command line flags which target the same value as a config file will override that value.
This helps ensure backwards compatibility with the command-line API.
-->
<p>请注意，命令行参数与配置文件有相同的值时，就会覆盖配置文件中的该值。
这有助于确保命令行 API 的向后兼容性。</p>
<!--
Note that relative file paths in the Kubelet config file are resolved relative to the
location of the Kubelet config file, whereas relative paths in command line flags are resolved
relative to the Kubelet's current working directory.
-->
<p>请注意，kubelet 配置文件中的相对文件路径是相对于 kubelet 配置文件的位置解析的，
而命令行参数中的相对路径是相对于 kubelet 的当前工作目录解析的。</p>
<!--
Note that some default values differ between command-line flags and the Kubelet config file.
If `--config` is provided and the values are not specified via the command line, the
defaults for the `KubeletConfiguration` version apply.
In the above example, this version is `kubelet.config.k8s.io/v1beta1`.
--->
<p>请注意，命令行参数和 Kubelet 配置文件的某些默认值不同。
如果设置了 <code>--config</code>，并且没有通过命令行指定值，则 <code>KubeletConfiguration</code>
版本的默认值生效。在上面的例子中，version 是 <code>kubelet.config.k8s.io/v1beta1</code>。</p>
<!-- discussion -->
<h2 id="what-s-next">What's next</h2>
<!--
- Learn more about kubelet configuration by checking the
  [`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/)
  reference.
--->
<ul>
<li>参阅 <a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>
进一步学习 kubelet 的配置。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5e59f5575dce11fdaed640afdbeedfc1">39 - 配置 API 对象配额</h1>
    
	<!--
title: Configure Quotas for API Objects
content_type: task
-->
<!-- overview -->
<!--
This page shows how to configure quotas for API objects, including
PersistentVolumeClaims and Services. A quota restricts the number of
objects, of a particular type, that can be created in a namespace.
You specify quotas in a
[ResourceQuota](/docs/reference/generated/kubernetes-api/v1.23/#resourcequota-v1-core)
object.
-->
<p>本文讨论如何为 API 对象配置配额，包括 PersistentVolumeClaim 和 Service。
配额限制了可以在命名空间中创建的特定类型对象的数量。
你可以在 <a href="/docs/reference/generated/kubernetes-api/v1.23/#resourcequota-v1-core">ResourceQuota</a> 对象中指定配额。</p>
<h2 id="before-you-begin">Before you begin</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间以便本例中创建的资源和集群中的其余部分相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace quota-object-example
</code></pre></div><!--
## Create a ResourceQuota

Here is the configuration file for a ResourceQuota object:
-->
<h2 id="创建-resourcequota">创建 ResourceQuota</h2>
<p>下面是一个 ResourceQuota 对象的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-objects.yaml" download="admin/resource/quota-objects.yaml"><code>admin/resource/quota-objects.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-objects-yaml')" title="Copy admin/resource/quota-objects.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-objects-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>object-quota-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.loadbalancers</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.nodeports</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the ResourceQuota:
-->
<p>创建 ResourceQuota：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects.yaml --namespace<span style="color:#666">=</span>quota-object-example
</code></pre></div><!--
View detailed information about the ResourceQuota:
-->
<p>查看 ResourceQuota 的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get resourcequota object-quota-demo --namespace<span style="color:#666">=</span>quota-object-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows that in the quota-object-example namespace, there can be at most
one PersistentVolumeClaim, at most two Services of type LoadBalancer, and no Services
of type NodePort.
-->
<p>输出结果表明在 quota-object-example 命名空间中，至多只能有一个 PersistentVolumeClaim，
最多两个 LoadBalancer 类型的服务，不能有 NodePort 类型的服务。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.loadbalancers</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.nodeports</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">used</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.loadbalancers</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.nodeports</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
## Create a PersistentVolumeClaim

Here is the configuration file for a PersistentVolumeClaim object:
-->
<h2 id="创建-persistentvolumeclaim">创建 PersistentVolumeClaim</h2>
<p>下面是一个 PersistentVolumeClaim 对象的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-objects-pvc.yaml" download="admin/resource/quota-objects-pvc.yaml"><code>admin/resource/quota-objects-pvc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-objects-pvc-yaml')" title="Copy admin/resource/quota-objects-pvc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-objects-pvc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pvc-quota-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>manual<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>3Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the PersistentVolumeClaim:
-->
<p>创建 PersistentVolumeClaim：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects-pvc.yaml --namespace<span style="color:#666">=</span>quota-object-example
</code></pre></div><!--
Verify that the PersistentVolumeClaim was created:
-->
<p>确认已创建完 PersistentVolumeClaim：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get persistentvolumeclaims --namespace<span style="color:#666">=</span>quota-object-example
</code></pre></div><!--
The output shows that the PersistentVolumeClaim exists and has status Pending:
-->
<p>输出信息表明 PersistentVolumeClaim 存在并且处于 Pending 状态：</p>
<pre><code>NAME             STATUS
pvc-quota-demo   Pending
</code></pre><!--
## Attempt to create a second PersistentVolumeClaim

Here is the configuration file for a second PersistentVolumeClaim:
-->
<h2 id="尝试创建第二个-persistentvolumeclaim">尝试创建第二个 PersistentVolumeClaim</h2>
<p>下面是第二个 PersistentVolumeClaim 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-objects-pvc-2.yaml" download="admin/resource/quota-objects-pvc-2.yaml"><code>admin/resource/quota-objects-pvc-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-objects-pvc-2-yaml')" title="Copy admin/resource/quota-objects-pvc-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-objects-pvc-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pvc-quota-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>manual<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>4Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Attempt to create the second PersistentVolumeClaim:
-->
<p>尝试创建第二个 PersistentVolumeClaim：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/admin/resource/quota-objects-pvc-2.yaml --namespace<span style="color:#666">=</span>quota-object-example
</code></pre></div><!--
The output shows that the second PersistentVolumeClaim was not created,
because it would have exceeded the quota for the namespace.
-->
<p>输出信息表明第二个 PersistentVolumeClaim 没有创建成功，因为这会超出命名空间的配额。</p>
<pre><code>persistentvolumeclaims &quot;pvc-quota-demo-2&quot; is forbidden:
exceeded quota: object-quota-demo, requested: persistentvolumeclaims=1,
used: persistentvolumeclaims=1, limited: persistentvolumeclaims=1
</code></pre><!--
## Notes

These are the strings used to identify API resources that can be constrained
by quotas:
-->
<h2 id="说明">说明</h2>
<p>下面这些字符串可被用来标识那些能被配额限制的 API 资源：</p>
<table>
<tr><th>字符串</th><th>API 对象</th></tr>
<tr><td>"pods"</td><td>Pod</td></tr>
<tr><td>"services"</td><td>Service</td></tr>
<tr><td>"replicationcontrollers"</td><td>ReplicationController</td></tr>
<tr><td>"resourcequotas"</td><td>ResourceQuota</td></tr>
<tr><td>"secrets"</td><td>Secret</td></tr>
<tr><td>"configmaps"</td><td>ConfigMap</td></tr>
<tr><td>"persistentvolumeclaims"</td><td>PersistentVolumeClaim</td></tr>
<tr><td>"services.nodeports"</td><td>NodePort 类型的 Service</td></tr>
<tr><td>"services.loadbalancers"</td><td>LoadBalancer 类型的 Service</td></tr>
</table>
<!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace quota-object-example
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)

* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)

* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)

* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)

* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)

* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认的内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">为命名空间配置默认的 CPU 请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存的最小和最大限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 的最小和最大限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置 CPU 和内存配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)

* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)

* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a02f35804917d7a269c38d7e2c475005">40 - 限制存储消耗</h1>
    
	<!--
title: Limit Storage Consumption
content_type: task
-->
<!-- overview -->
<!--
This example demonstrates how to limit the amount of storage consumed in a namespace
-->
<p>此示例演示了如何限制一个名字空间中的存储使用量。</p>
<!--
The following resources are used in the demonstration: [ResourceQuota](/docs/concepts/policy/resource-quotas/),
[LimitRange](/docs/tasks/administer-cluster/memory-default-namespace/),
and [PersistentVolumeClaim](/docs/concepts/storage/persistent-volumes/).
-->
<p>演示中用到了以下资源：<a href="/zh/docs/concepts/policy/resource-quotas/">ResourceQuota</a>，
<a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">LimitRange</a> 和
<a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaim</a>。</p>
<h2 id="before-you-begin">Before you begin</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</li>
</ul>
<!-- steps -->
<!--
## Scenario: Limiting Storage Consumption
-->
<h2 id="场景-限制存储消耗">场景：限制存储消耗</h2>
<!--
The cluster-admin is operating a cluster on behalf of a user population and the admin wants to control
how much storage a single namespace can consume in order to control cost.
-->
<p>集群管理员代表用户群操作集群，管理员希望控制单个名称空间可以消耗多少存储空间以控制成本。</p>
<!--
The admin would like to limit:
-->
<p>管理员想要限制：</p>
<!--
1. The number of persistent volume claims in a namespace
2. The amount of storage each claim can request
3. The amount of cumulative storage the namespace can have
-->
<ol>
<li>名字空间中持久卷申领（persistent volume claims）的数量</li>
<li>每个申领（claim）可以请求的存储量</li>
<li>名字空间可以具有的累计存储量</li>
</ol>
<!--
## LimitRange to limit requests for storage
-->
<h2 id="使用-limitrange-限制存储请求">使用 LimitRange 限制存储请求</h2>
<!--
Adding a `LimitRange` to a namespace enforces storage request sizes to a minimum and maximum. Storage is requested via `PersistentVolumeClaim`. The admission controller that enforces limit ranges will reject any PVC that is above or below the values set by the admin.
-->
<p>将 <code>LimitRange</code> 添加到名字空间会为存储请求大小强制设置最小值和最大值。
存储是通过 <code>PersistentVolumeClaim</code> 来发起请求的。
执行限制范围控制的准入控制器会拒绝任何高于或低于管理员所设阈值的 PVC。</p>
<!--
In this example, a PVC requesting 10Gi of storage would be rejected because it exceeds the 2Gi max.
-->
<p>在此示例中，请求 10Gi 存储的 PVC 将被拒绝，因为它超过了最大 2Gi。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>storagelimits<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">max</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>2Gi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">min</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></code></pre></div><!--
Minimum storage requests are used when the underlying storage provider requires certain minimums. For example,
AWS EBS volumes have a 1Gi minimum requirement.
-->
<p>当底层存储提供程序需要某些最小值时，将会用到所设置最小存储请求值。
例如，AWS EBS volumes 的最低要求为 1Gi。</p>
<!--
## StorageQuota to limit PVC count and cumulative storage capacity
-->
<h2 id="使用-storagequota-限制-pvc-数目和累计存储容量">使用 StorageQuota 限制 PVC 数目和累计存储容量</h2>
<!--
Admins can limit the number of PVCs in a namespace as well as the cumulative capacity of those PVCs. New PVCs that exceed
either maximum value will be rejected.
-->
<p>管理员可以限制某个名字空间中的 PVCs 个数以及这些 PVCs 的累计容量。
新 PVCs 请求如果超过任一上限值将被拒绝。</p>
<!--
In this example, a 6th PVC in the namespace would be rejected because it exceeds the maximum count of 5. Alternatively,
a 5Gi maximum quota when combined with the 2Gi max limit above, cannot have 3 PVCs where each has 2Gi. That would be 6Gi requested
 for a namespace capped at 5Gi.
-->
<p>在此示例中，名字空间中的第 6 个 PVC 将被拒绝，因为它超过了最大计数 5。
或者，当与上面的 2Gi 最大容量限制结合在一起时，意味着 5Gi 的最大配额
不能支持 3 个都是 2Gi 的 PVC。
后者实际上是向名字空间请求 6Gi 容量，而该命令空间已经设置上限为 5Gi。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>storagequota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests.storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5Gi&#34;</span><span style="color:#bbb">
</span></code></pre></div><!-- discussion -->
<!--
## Summary

A limit range can put a ceiling on how much storage is requested while a resource quota can effectively cap the storage consumed by a namespace through claim counts and cumulative storage capacity. The allows a cluster-admin to plan their
cluster's storage budget without risk of any one project going over their allotment.
-->
<h2 id="小结">小结</h2>
<p>限制范围对象可以用来设置可请求的存储量上限，而资源配额对象则可以通过申领计数和
累计存储容量有效地限制名字空间耗用的存储量。
这两种机制使得集群管理员能够规划其集群存储预算而不会发生任一项目超量分配的风险。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-6b4e7ca6586f448c8533a120c29bdd25">41 - 静态加密 Secret 数据</h1>
    
	<!--
reviewers:
- smarterclayton
title: Encrypting Secret Data at Rest
content_type: task
-->
<!-- overview -->
<!--
This page shows how to enable and configure encryption of secret data at rest.
-->
<p>本文展示如何启用和配置静态 Secret 数据的加密</p>
<h2 id="before-you-begin">Before you begin</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
建议在至少有两个节点的集群上运行本教程，且这些节点不作为控制平面主机。
如果你还没有集群，你可以通过 <a href="https://minikube.sigs.k8s.io/docs/tutorials/multi_node/">Minikube</a>
构建一个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
cluster, you can create one by using
[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 To check the version, enter <code>kubectl version</code>.
</li>
</ul>
<!--
* etcd v3 or later is required
-->
<ul>
<li>需要 etcd v3 或者更高版本</li>
</ul>
<!-- steps -->
<!--
## Configuration and determining whether encryption at rest is already enabled

The `kube-apiserver` process accepts an argument `-experimental-encryption-provider-config`
that controls how API data is encrypted in etcd. An example configuration
is provided below.

## Understanding the encryption at rest configuration.
-->
<h2 id="配置并确定是否已启用静态数据加密">配置并确定是否已启用静态数据加密</h2>
<p><code>kube-apiserver</code> 的参数 <code>--experimental-encryption-provider-config</code> 控制 API 数据在 etcd 中的加密方式。
下面提供一个配置示例。</p>
<h2 id="理解静态数据加密">理解静态数据加密</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">aesgcm</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key2<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>dGhpcyBpcyBwYXNzd29yZA==<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">aescbc</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key2<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>dGhpcyBpcyBwYXNzd29yZA==<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">secretbox</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=<span style="color:#bbb">
</span></code></pre></div><!--
Each `resources` array item is a separate config and contains a complete configuration. The
`resources.resources` field is an array of Kubernetes resource names (`resource` or `resource.group`)
that should be encrypted. The `providers` array is an ordered list of the possible encryption
providers. Only one provider type may be specified per entry (`identity` or `aescbc` may be provided,
but not both in the same item).
-->
<p>每个 <code>resources</code> 数组项目是一个单独的完整的配置。
<code>resources.resources</code> 字段是要加密的 Kubernetes 资源名称（<code>resource</code> 或 <code>resource.group</code>）的数组。
<code>providers</code> 数组是可能的加密 provider 的有序列表。
每个条目只能指定一个 provider 类型（可以是 <code>identity</code> 或 <code>aescbc</code>，但不能在同一个项目中同时指定）。</p>
<!--
The first provider in the list is used to encrypt resources going into storage. When reading
resources from storage each provider that matches the stored data attempts to decrypt the data in
order. If no provider can read the stored data due to a mismatch in format or secret key, an error
is returned which prevents clients from accessing that resource.
-->
<p>列表中的第一个 provider 用于加密进入存储的资源。
当从存储器读取资源时，与存储的数据匹配的所有 provider 将按顺序尝试解密数据。
如果由于格式或密钥不匹配而导致没有 provider 能够读取存储的数据，则会返回一个错误，以防止客户端访问该资源。</p>
<!--
**IMPORTANT:** If any resource is not readable via the encryption config (because keys were changed),
the only recourse is to delete that key from the underlying etcd directly. Calls that attempt to
read that resource will fail until it is deleted or a valid decryption key is provided.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <strong>重要：</strong> 如果通过加密配置无法读取资源（因为密钥已更改），唯一的方法是直接从底层 etcd 中删除该密钥。
任何尝试读取资源的调用将会失败，直到它被删除或提供有效的解密密钥。
</div>

<h3 id="providers">Providers:</h3>
<!--
Name | Encryption | Strength | Speed | Key Length | Other Considerations
-----|------------|----------|-------|------------|---------------------
`identity` | None | N/A | N/A | N/A | Resources written as-is without encryption. When set as the first provider, the resource will be decrypted as new values are written.
`secretbox` | XSalsa20 and Poly1305 | Strong | Faster | 32-byte | A newer standard and may not be considered acceptable in environments that require high levels of review.
`aesgcm` | AES-GCM with random nonce | Must be rotated every 200k writes | Fastest | 16, 24, or 32-byte | Is not recommended for use except when an automated key rotation scheme is implemented.
`aescbc` | AES-CBC with PKCS#7 padding | Weak | Fast | 32-byte | Not recommended due to CBC's vulnerability to padding oracle attacks.
`kms` | Uses envelope encryption scheme: Data is encrypted by data encryption keys (DEKs) using AES-CBC with PKCS#7 padding, DEKs are encrypted by key encryption keys (KEKs) according to configuration in Key Management Service (KMS) | Strongest | Fast | 32-bytes |  The recommended choice for using a third party tool for key management. Simplifies key rotation, with a new DEK generated for each encryption, and KEK rotation controlled by the user. [Configure the KMS provider](/docs/tasks/administer-cluster/kms-provider/)

Each provider supports multiple keys - the keys are tried in order for decryption, and if the provider
is the first provider, the first key is used for encryption.
-->





<table><caption style="display: none;">Kubernetes 静态数据加密的 Providers</caption>
<thead>
<tr>
<th>名称</th>
<th>加密类型</th>
<th>强度</th>
<th>速度</th>
<th>密钥长度</th>
<th>其它事项</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>identity</code></td>
<td>无</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>不加密写入的资源。当设置为第一个 provider 时，资源将在新值写入时被解密。</td>
</tr>
<tr>
<td><code>secretbox</code></td>
<td>XSalsa20 和 Poly1305</td>
<td>强</td>
<td>更快</td>
<td>32字节</td>
<td>较新的标准，在需要高度评审的环境中可能不被接受。</td>
</tr>
<tr>
<td><code>aesgcm</code></td>
<td>带有随机数的 AES-GCM</td>
<td>必须每 200k 写入一次</td>
<td>最快</td>
<td>16, 24 或者 32字节</td>
<td>建议不要使用，除非实施了自动密钥循环方案。</td>
</tr>
<tr>
<td><code>aescbc</code></td>
<td>填充 PKCS#7 的 AES-CBC</td>
<td>弱</td>
<td>快</td>
<td>32字节</td>
<td>由于 CBC 容易受到密文填塞攻击（Padding Oracle Attack），不推荐使用。</td>
</tr>
<tr>
<td><code>kms</code></td>
<td>使用信封加密方案：数据使用带有 PKCS#7 填充的 AES-CBC 通过数据加密密钥（DEK）加密，DEK 根据 Key Management Service（KMS）中的配置通过密钥加密密钥（Key Encryption Keys，KEK）加密</td>
<td>最强</td>
<td>快</td>
<td>32字节</td>
<td>建议使用第三方工具进行密钥管理。为每个加密生成新的 DEK，并由用户控制 KEK 轮换来简化密钥轮换。<a href="/zh/docs/tasks/administer-cluster/kms-provider/">配置 KMS 提供程序</a></td>
</tr>
</tbody>
</table>
<p>每个 provider 都支持多个密钥 - 在解密时会按顺序使用密钥，如果是第一个 provider，则第一个密钥用于加密。</p>
<!--
__Storing the raw encryption key in the EncryptionConfig only moderately improves your security posture, compared to no encryption.
Please use `kms` provider for additional security.__ By default, the `identity` provider is used to protect secrets in etcd, which
provides no encryption. `EncryptionConfiguration` was introduced to encrypt secrets locally, with a locally managed key.
-->
<p><strong>在 EncryptionConfig 中保存原始的加密密钥与不加密相比只会略微地提升安全级别。
请使用 <code>kms</code> 驱动以获得更强的安全性。</strong>
默认情况下，<code>identity</code> 驱动被用来对 etcd 中的 Secret 提供保护，
而这个驱动不提供加密能力。
<code>EncryptionConfiguration</code> 的引入是为了能够使用本地管理的密钥来在本地加密 Secret 数据。</p>
<!--
Encrypting secrets with a locally managed key protects against an etcd compromise, but it fails to protect against a host compromise.
Since the encryption keys are stored on the host in the EncryptionConfig YAML file, a skilled attacker can access that file and
extract the encryption keys.
-->
<p>使用本地管理的密钥来加密 Secret 能够保护数据免受 etcd 破坏的影响，不过无法针对
主机被侵入提供防护。
这是因为加密的密钥保存在主机上的 EncryptionConfig YAML 文件中，有经验的入侵者
仍能访问该文件并从中提取出加密密钥。</p>
<!--
Envelope encryption creates dependence on a separate key, not stored in Kubernetes. In this case, an attacker would need to compromise etcd, the kubeapi-server, and the third-party KMS provider to retrieve the plaintext values, providing a higher level of security than locally-stored encryption keys.
-->
<p>封套加密（Envelope Encryption）引入了对独立密钥的依赖，而这个密钥并不保存在 Kubernetes 中。
在这种情况下下，入侵者需要攻破 etcd、kube-apiserver 和第三方的 KMS
驱动才能获得明文数据，因而这种方案提供了比本地保存加密密钥更高的安全级别。</p>
<!--
## Encrypting your data

Create a new encryption config file:
-->
<h2 id="加密你的数据">加密你的数据</h2>
<p>创建一个新的加密配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">aescbc</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div><!--
To create a new secret perform the following steps:

1. Generate a 32 byte random key and base64 encode it. If you're on Linux or Mac OS X, run the following command:
-->
<p>遵循如下步骤来创建一个新的 secret：</p>
<ol>
<li>
<p>生成一个 32 字节的随机密钥并进行 base64 编码。如果你在 Linux 或 Mac OS X 上，请运行以下命令：</p>
<pre><code>head -c 32 /dev/urandom | base64
</code></pre></li>
</ol>
<!--
2. Place that value in the secret field.
3. Set the `--experimental-encryption-provider-config` flag on the `kube-apiserver` to point to the location of the config file.
4. Restart your API server.

**IMPORTANT:** Your config file contains keys that can decrypt content in etcd, so you must properly restrict permissions on your masters so only the user who runs the kube-apiserver can read it.
-->
<ol start="2">
<li>将这个值放入到 secret 字段中。</li>
<li>设置 <code>kube-apiserver</code> 的 <code>--experimental-encryption-provider-config</code> 参数，将其指向
配置文件所在位置。</li>
<li>重启你的 API server。</li>
</ol>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 你的配置文件包含可以解密 etcd 内容的密钥，因此你必须正确限制主控节点的访问权限，
以便只有能运行 kube-apiserver 的用户才能读取它。
</div>
<!--
## Verifying that data is encrypted

Data is encrypted when written to etcd. After restarting your `kube-apiserver`, any newly created or
updated secret should be encrypted when stored. To check, you can use the `etcdctl` command line
program to retrieve the contents of your secret.

1. Create a new secret called `secret1` in the `default` namespace:
-->
<h2 id="验证数据已被加密">验证数据已被加密</h2>
<p>数据在写入 etcd 时会被加密。重新启动你的 <code>kube-apiserver</code> 后，任何新创建或更新的密码在存储时都应该被加密。
如果想要检查，你可以使用 <code>etcdctl</code> 命令行程序来检索你的加密内容。</p>
<ol>
<li>
<p>创建一个新的 secret，名称为 <code>secret1</code>，命名空间为 <code>default</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic secret1 -n default --from-literal<span style="color:#666">=</span><span style="color:#b8860b">mykey</span><span style="color:#666">=</span>mydata
</code></pre></div></li>
</ol>
<!--
2. Using the etcdctl commandline, read that secret out of etcd:
-->
<ol start="2">
<li>
<p>使用 etcdctl 命令行，从 etcd 中读取 secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl get /registry/secrets/default/secret1 <span style="color:#666">[</span>...<span style="color:#666">]</span> | hexdump -C
</code></pre></div><!--
where `[...]` must be the additional arguments for connecting to the etcd server.
-->
<p>这里的 <code>[...]</code> 是用来连接 etcd 服务的额外参数。</p>
</li>
</ol>
<!--
3. Verify the stored secret is prefixed with `k8s:enc:aescbc:v1:` which indicates the `aescbc` provider has encrypted the resulting data.
4. Verify the secret is correctly decrypted when retrieved via the API:
-->
<ol start="3">
<li>
<p>验证存储的密钥前缀是否为 <code>k8s:enc:aescbc:v1:</code>，这表明 <code>aescbc</code> provider 已加密结果数据。</p>
</li>
<li>
<p>通过 API 检索，验证 secret 是否被正确解密：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe secret secret1 -n default
</code></pre></div><!--
should match `mykey: mydata`, mydata is encoded, check [decoding a secret](/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret) to
completely decode the secret.
-->
<p>其输出应该是 <code>mykey: bXlkYXRh</code>，<code>mydata</code> 数据是被加密过的，请参阅
<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret">解密 Secret</a>
了解如何完全解码 Secret 内容。</p>
</li>
</ol>
<!--
## Ensure all secrets are encrypted

Since secrets are encrypted on write, performing an update on a secret will encrypt that content.
-->
<h2 id="确保所有-secret-都被加密">确保所有 Secret 都被加密</h2>
<p>由于 Secret 是在写入时被加密，因此对 Secret 执行更新也会加密该内容。</p>
<pre><code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</code></pre><!--
The command above reads all secrets and then updates them to apply server side encryption.
-->
<p>上面的命令读取所有 Secret，然后使用服务端加密来更新其内容。</p>
<!--
If an error occurs due to a conflicting write, retry the command.
For larger clusters, you may wish to subdivide the secrets by namespace or script an update.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果由于冲突写入而发生错误，请重试该命令。
对于较大的集群，你可能希望通过命名空间或更新脚本来对 Secret 进行划分。
</div>
<!--
## Rotating a decryption key

Changing the secret without incurring downtime requires a multi step operation, especially in
the presence of a highly available deployment where multiple `kube-apiserver` processes are running.

1. Generate a new key and add it as the second key entry for the current provider on all servers
2. Restart all `kube-apiserver` processes to ensure each server can decrypt using the new key
3. Make the new key the first entry in the `keys` array so that it is used for encryption in the config
4. Restart all `kube-apiserver` processes to ensure each server now encrypts using the new key
5. Run `kubectl get secrets -all-namespaces -o json | kubectl replace -f -` to encrypt all existing secrets with the new key
6. Remove the old decryption key from the config after you back up etcd with the new key in use and update all secrets

With a single `kube-apiserver`, step 2 may be skipped.
-->
<h2 id="轮换解密密钥">轮换解密密钥</h2>
<p>在不发生停机的情况下更改 Secret 需要多步操作，特别是在有多个 <code>kube-apiserver</code> 进程正在运行的
高可用环境中。</p>
<ol>
<li>生成一个新密钥并将其添加为所有服务器上当前提供程序的第二个密钥条目</li>
<li>重新启动所有 <code>kube-apiserver</code> 进程以确保每台服务器都可以使用新密钥进行解密</li>
<li>将新密钥设置为 <code>keys</code> 数组中的第一个条目，以便在配置中使用其进行加密</li>
<li>重新启动所有 <code>kube-apiserver</code> 进程以确保每个服务器现在都使用新密钥进行加密</li>
<li>运行 <code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -</code> 以用新密钥加密所有现有的秘密</li>
<li>在使用新密钥备份 etcd 后，从配置中删除旧的解密密钥并更新所有密钥</li>
</ol>
<p>如果只有一个 <code>kube-apiserver</code>，第 2 步可能可以忽略。</p>
<!--
## Decrypting all data

To disable encryption at rest place the `identity` provider as the first entry in the config:
-->
<h2 id="解密所有数据">解密所有数据</h2>
<p>要禁用 rest 加密，请将 <code>identity</code> provider 作为配置中的第一个条目：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">aescbc</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style="color:#bbb">
</span></code></pre></div><!--
and restart all `kube-apiserver` processes. Then run
-->
<p>并重新启动所有 <code>kube-apiserver</code> 进程。然后运行：</p>
<pre><code>kubectl get secrets -all-namespaces -o json | kubectl replace -f -`
</code></pre><!--
to force all secrets to be decrypted.
-->
<p>以强制解密所有 secret。</p>


</div>



    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      
      <a class="text-white" href="/zh/docs/home/">主页</a>
      
      <a class="text-white" href="/zh/blog/">博客</a>
      
      <a class="text-white" href="/zh/training/">培训</a>
      
      <a class="text-white" href="/zh/partners/">合作伙伴</a>
      
      <a class="text-white" href="/zh/community/">社区</a>
      
      <a class="text-white" href="/zh/case-studies/">案例分析</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" href="https://discuss.kubernetes.io">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/kubernetesio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://youtube.com/kubernetescommunity">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/kubernetes/kubernetes">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://slack.k8s.io">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Contribute" aria-label="Contribute">
    <a class="text-white" target="_blank" href="https://git.k8s.io/community/contributors/guide">
      <i class="fas fa-edit"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" href="https://stackoverflow.com/questions/tagged/kubernetes">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2024 The Kubernetes Authors | Documentation Distributed under <a href="https://git.k8s.io/website/LICENSE" class="light-text">CC BY 4.0</a></small>
        <br/>
        <small class="text-white">Copyright &copy; 2024 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage" class="light-text">Trademark Usage page</a></small>
        <br/>
        <small class="text-white">ICP license: 京ICP备17074266号-3</small>
        
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>










<script src="/js/main.js"></script>






  </body>
</html>
