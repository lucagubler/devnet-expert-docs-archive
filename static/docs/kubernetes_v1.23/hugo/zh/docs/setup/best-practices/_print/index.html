<!doctype html>
<html lang="zh" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>


<link rel="alternate" hreflang="en" href="http://localhost:1313/docs/setup/best-practices/">
<link rel="alternate" hreflang="ko" href="http://localhost:1313/ko/docs/setup/best-practices/">
<link rel="alternate" hreflang="ja" href="http://localhost:1313/ja/docs/setup/best-practices/">
<link rel="alternate" hreflang="id" href="http://localhost:1313/id/docs/setup/best-practices/">
<link rel="alternate" hreflang="uk" href="http://localhost:1313/uk/docs/setup/best-practices/">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.87.0" />
<link rel="canonical" type="text/html" href="http://localhost:1313/zh/docs/setup/best-practices/">
<link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="apple-touch-icon" href="/images/kubernetes-192x192.png">
<title>最佳实践 | Kubernetes</title><meta property="og:title" content="最佳实践" />
<meta property="og:description" content="生产级别的容器编排系统" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/zh/docs/setup/best-practices/" /><meta property="og:site_name" content="Kubernetes" />

<meta itemprop="name" content="最佳实践">
<meta itemprop="description" content="生产级别的容器编排系统"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="最佳实践"/>
<meta name="twitter:description" content="生产级别的容器编排系统"/>






<link href="/scss/main.css" rel="stylesheet">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "http://localhost:1313/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href="/css/feature-states.css">



<meta name="description" content="">
<meta property="og:description" content="">
<meta name="twitter:description" content="">
<meta property="og:url" content="http://localhost:1313/zh/docs/setup/best-practices/">
<meta property="og:title" content="最佳实践">
<meta name="twitter:title" content="最佳实践">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">

<script src="/js/script.js"></script>


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/zh/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/zh/docs/" >文档</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/blog/" >Kubernetes 博客</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/training/" >培训</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/partners/" >合作伙伴</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/community/" >社区</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/case-studies/" >案例分析</a>
			</li>
			
			
			
			<li class="nav-item dropdown">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/releases">Release Information</a>
	
	<a class="dropdown-item" href="https://kubernetes.io/zh/docs/setup/best-practices/">v1.23</a>
	
	<a class="dropdown-item" href="https://v1-22.docs.kubernetes.io/zh/docs/setup/best-practices/">v1.22</a>
	
	<a class="dropdown-item" href="https://v1-21.docs.kubernetes.io/zh/docs/setup/best-practices/">v1.21</a>
	
	<a class="dropdown-item" href="https://v1-20.docs.kubernetes.io/zh/docs/setup/best-practices/">v1.20</a>
	
	<a class="dropdown-item" href="https://v1-19.docs.kubernetes.io/zh/docs/setup/best-practices/">v1.19</a>
	
</div>
			</li>
			
			
			<li class="nav-item dropdown">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/setup/best-practices/">English</a>
	
	<a class="dropdown-item" href="/ko/docs/setup/best-practices/">한국어 Korean</a>
	
	<a class="dropdown-item" href="/ja/docs/setup/best-practices/">日本語 Japanese</a>
	
	<a class="dropdown-item" href="/id/docs/setup/best-practices/">Bahasa Indonesia</a>
	
	<a class="dropdown-item" href="/uk/docs/setup/best-practices/">Українська</a>
	
</div>

			</li>
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/zh/docs/setup/best-practices/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">最佳实践</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-970615c97499e3651fd3a98e0387cefc">运行于多可用区环境</a></li>


    
  
    
    
	
<li>2: <a href="#pg-c797ee17120176c685455db89ae091a9">大规模集群的注意事项</a></li>


    
  
    
    
	
<li>3: <a href="#pg-f89867de1d34943f1524f67a241f5cc9">校验节点设置</a></li>


    
  
    
    
	
<li>4: <a href="#pg-0394f813094b7a35058dffe5b8bacd20">PKI 证书和要求</a></li>


    
  
    
    
	
<li>5: <a href="#pg-92a61cf5b0575aa3500f7665b68127d1">强制实施 Pod 安全性标准</a></li>


    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-970615c97499e3651fd3a98e0387cefc">1 - 运行于多可用区环境</h1>
    
	<!--
reviewers:
- jlowdermilk
- justinsb
- quinton-hoole
title: Running in multiple zones
weight: 10
content_type: concept
-->
<!-- overview -->
<!--
This page describes running a cluster across multiple zones.
-->
<p>本页描述如何跨多个区（Zone）中运行集群。</p>
<!-- body -->
<!--
## Background

Kubernetes is designed so that a single Kubernetes cluster can run
across multiple failure zones, typically where these zones fit within
a logical grouping called a _region_. Major cloud providers define a region
as a set of failure zones (also called _availability zones_) that provide
a consistent set of features: within a region, each zone offers the same
APIs and services.

Typical cloud architectures aim to minimize the chance that a failure in
one zone also impairs services in another zone.
-->
<h2 id="背景">背景</h2>
<p>Kubernetes 从设计上允许同一个 Kubernetes 集群跨多个失效区来运行，
通常这些区位于某个称作 <em>区域（region）</em> 逻辑分组中。
主要的云提供商都将区域定义为一组失效区的集合（也称作 <em>可用区（Availability Zones）</em>），
能够提供一组一致的功能特性：每个区域内，各个可用区提供相同的 API 和服务。</p>
<p>典型的云体系结构都会尝试降低某个区中的失效影响到其他区中服务的概率。</p>
<!--
## Control plane behavior

All [control plane components](/docs/concepts/overview/components/#control-plane-components)
support running as a pool of interchangeable resources, replicated per
component.
-->
<h2 id="control-plane-behavior">控制面行为  </h2>
<p>所有的<a href="/zh/docs/concepts/overview/components/#control-plane-components">控制面组件</a>
都支持以一组可相互替换的资源池的形式来运行，每个组件都有多个副本。</p>
<!--
When you deploy a cluster control plane, place replicas of
control plane components across multiple failure zones. If availability is
an important concern, select at least three failure zones and replicate
each individual control plane component (API server, scheduler, etcd,
cluster controller manager) across at least three failure zones.
If you are running a cloud controller manager then you should
also replicate this across all the failure zones you selected.
-->
<p>当你部署集群控制面时，应将控制面组件的副本跨多个失效区来部署。
如果可用性是一个很重要的指标，应该选择至少三个失效区，并将每个
控制面组件（API 服务器、调度器、etcd、控制器管理器）复制多个副本，
跨至少三个失效区来部署。如果你在运行云控制器管理器，则也应该将
该组件跨所选的三个失效区来部署。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Kubernetes does not provide cross-zone resilience for the API server
endpoints. You can use various techniques to improve availability for
the cluster API server, including DNS round-robin, SRV records, or
a third-party load balancing solution with health checking.
-->
<p>Kubernetes 并不会为 API 服务器端点提供跨失效区的弹性。
你可以为集群 API 服务器使用多种技术来提升其可用性，包括使用
DNS 轮转、SRV 记录或者带健康检查的第三方负载均衡解决方案等等。
</div>
<!--
## Node behavior

Kubernetes automatically spreads the Pods for
workload resources (such as <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>
or <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>)
across different nodes in a cluster. This spreading helps
reduce the impact of failures.
-->
<h2 id="node-behavior">节点行为  </h2>
<p>Kubernetes 自动为负载资源（如<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>
或 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>)）
跨集群中不同节点来部署其 Pods。
这种分布逻辑有助于降低失效带来的影响。</p>
<!--
When nodes start up, the kubelet on each node automatically adds
<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='labels'>labels</a> to the Node object
that represents that specific kubelet in the Kubernetes API.
These labels can include
[zone information](/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
-->
<p>节点启动时，每个节点上的 kubelet 会向 Kubernetes API 中代表该 kubelet 的 Node 对象
添加 <a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>。
这些标签可能包含<a href="/zh/docs/reference/labels-annotations-taints/#topologykubernetesiozone">区信息</a>。</p>
<!--
If your cluster spans multiple zones or regions, you can use node labels
in conjunction with
[Pod topology spread constraints](/docs/concepts/workloads/pods/pod-topology-spread-constraints/)
to control how Pods are spread across your cluster among fault domains:
regions, zones, and even specific nodes.
These hints enable the
<a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='scheduler'>scheduler</a> to place
Pods for better expected availability, reducing the risk that a correlated
failure affects your whole workload.
-->
<p>如果你的集群跨了多个可用区或者地理区域，你可以使用节点标签，结合
<a href="/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/">Pod 拓扑分布约束</a>
来控制如何在你的集群中多个失效域之间分布 Pods。这里的失效域可以是
地理区域、可用区甚至是特定节点。
这些提示信息使得<a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='调度器'>调度器</a>
能够更好地分布 Pods，以实现更好的可用性，降低因为某种失效给整个工作负载
带来的风险。</p>
<!--
For example, you can set a constraint to make sure that the
3 replicas of a StatefulSet are all running in different zones to each
other, whenever that is feasible. You can define this declaratively
without explicitly defining which availability zones are in use for
each workload.
-->
<p>例如，你可以设置一种约束，确保某个 StatefulSet 中的三个副本都运行在
不同的可用区中，只要其他条件允许。你可以通过声明的方式来定义这种约束，
而不需要显式指定每个工作负载使用哪些可用区。</p>
<!--
### Distributing nodes across zones

Kubernetes' core does not create nodes for you; you need to do that yourself,
or use a tool such as the [Cluster API](https://cluster-api.sigs.k8s.io/) to
manage nodes on your behalf.

Using tools such as the Cluster API you can define sets of machines to run as
worker nodes for your cluster across multiple failure domains, and rules to
automatically heal the cluster in case of whole-zone service disruption.
-->
<h3 id="distributing-nodes-across-zones">跨多个区分布节点</h3>
<p>Kubernetes 的核心逻辑并不会帮你创建节点，你需要自行完成此操作，或者使用
类似 <a href="https://cluster-api.sigs.k8s.io/">Cluster API</a> 这类工具来替你管理节点。</p>
<!--
Using tools such as the Cluster API you can define sets of machines to run as
worker nodes for your cluster across multiple failure domains, and rules to
automatically heal the cluster in case of whole-zone service disruption.
-->
<p>使用类似 Cluster API 这类工具，你可以跨多个失效域来定义一组用做你的集群
工作节点的机器，以及当整个区的服务出现中断时如何自动治愈集群的策略。</p>
<!--
## Manual zone assignment for Pods

You can apply [node selector constraints](/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
to Pods that you create, as well as to Pod templates in workload resources
such as Deployment, StatefulSet, or Job.
-->
<h2 id="为-pods-手动指定区">为 Pods 手动指定区</h2>
<!--
You can apply [node selector constraints](/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
to Pods that you create, as well as to Pod templates in workload resources
such as Deployment, StatefulSet, or Job.
-->
<p>你可以应用<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector">节点选择算符约束</a>
到你所创建的 Pods 上，或者为 Deployment、StatefulSet 或 Job 这类工作负载资源
中的 Pod 模板设置此类约束。</p>
<!--
## Storage access for zones

When persistent volumes are created, the `PersistentVolumeLabel`
[admission controller](/docs/reference/access-authn-authz/admission-controllers/)
automatically adds zone labels to any PersistentVolumes that are linked to a specific
zone. The <a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='scheduler'>scheduler</a> then ensures,
through its `NoVolumeZoneConflict` predicate, that pods which claim a given PersistentVolume
are only placed into the same zone as that volume.
-->
<h2 id="跨区的存储访问">跨区的存储访问</h2>
<p>当创建持久卷时，<code>PersistentVolumeLabel</code>
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制器</a>
会自动向那些链接到特定区的 PersistentVolume 添加区标签。
<a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='调度器'>调度器</a>通过其
<code>NoVolumeZoneConflict</code> 断言确保申领给定 PersistentVolume 的 Pods 只会
被调度到该卷所在的可用区。</p>
<!--
You can specify a <a class='glossary-tooltip' title='StorageClass 是管理员用来描述可用的不同存储类型的一种方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/storage-classes/' target='_blank' aria-label='StorageClass'>StorageClass</a>
for PersistentVolumeClaims that specifies the failure domains (zones) that the
storage in that class may use.
To learn about configuring a StorageClass that is aware of failure domains or zones,
see [Allowed topologies](/docs/concepts/storage/storage-classes/#allowed-topologies).
-->
<p>你可以为 PersistentVolumeClaim 指定<a class='glossary-tooltip' title='StorageClass 是管理员用来描述可用的不同存储类型的一种方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/storage-classes/' target='_blank' aria-label='StorageClass'>StorageClass</a>
以设置该类中的存储可以使用的失效域（区）。
要了解如何配置能够感知失效域或区的 StorageClass，请参阅
<a href="/zh/docs/concepts/storage/storage-classes/#allowed-topologies">可用的拓扑逻辑</a>。</p>
<!--
## Networking

By itself, Kubernetes does not include zone-aware networking. You can use a
[network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)
to configure cluster networking, and that network solution might have zone-specific
elements. For example, if your cloud provider supports Services with
`type=LoadBalancer`, the load balancer might only send traffic to Pods running in the
same zone as the load balancer element processing a given connection.
Check your cloud provider's documentation for details.
-->
<h2 id="networking">网络 </h2>
<p>Kubernetes 自身不提供与可用区相关的联网配置。
你可以使用<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件</a>
来配置集群的联网，该网络解决方案可能拥有一些与可用区相关的元素。
例如，如果你的云提供商支持 <code>type=LoadBalancer</code> 的 Service，则负载均衡器
可能仅会将请求流量发送到运行在负责处理给定连接的负载均衡器组件所在的区。
请查阅云提供商的文档了解详细信息。</p>
<!--
For custom or on-premises deployments, similar considerations apply.
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> and
<a class='glossary-tooltip' title='Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/ingress/' target='_blank' aria-label='Ingress'>Ingress</a> behavior, including handling
of different failure zones, does vary depending on exactly how your cluster is set up.
-->
<p>对于自定义的或本地集群部署，也可以考虑这些因素
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>
<a class='glossary-tooltip' title='Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/ingress/' target='_blank' aria-label='Ingress'>Ingress</a> 的行为，
包括处理不同失效区的方法，在很大程度上取决于你的集群是如何搭建的。</p>
<!--
## Fault recovery

When you set up your cluster, you might also need to consider whether and how
your setup can restore service if all the failure zones in a region go
off-line at the same time. For example, do you rely on there being at least
one node able to run Pods in a zone?  
Make sure that any cluster-critical repair work does not rely
on there being at least one healthy node in your cluster. For example: if all nodes
are unhealthy, you might need to run a repair Job with a special
<a class='glossary-tooltip' title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='toleration'>toleration</a> so that the repair
can complete enough to bring at least one node into service.

Kubernetes doesn't come with an answer for this challenge; however, it's
something to consider.
-->
<h2 id="fault-recovery">失效恢复   </h2>
<p>在搭建集群时，你可能需要考虑当某区域中的所有失效区都同时掉线时，是否以及如何
恢复服务。例如，你是否要求在某个区中至少有一个节点能够运行 Pod？
请确保任何对集群很关键的修复工作都不要指望集群中至少有一个健康节点。
例如：当所有节点都不健康时，你可能需要运行某个修复性的 Job，
该 Job 要设置特定的<a class='glossary-tooltip' title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='容忍度'>容忍度</a>
以便修复操作能够至少将一个节点恢复为可用状态。</p>
<p>Kubernetes 对这类问题没有现成的解决方案；不过这也是要考虑的因素之一。</p>
<h2 id="what-s-next">What's next</h2>
<!--
To learn how the scheduler places Pods in a cluster, honoring the configured constraints,
visit [Scheduling and Eviction](/docs/concepts/scheduling-eviction/).
-->
<p>要了解调度器如何在集群中放置 Pods 并遵从所配置的约束，可参阅
<a href="/zh/docs/concepts/scheduling-eviction/">调度与驱逐</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c797ee17120176c685455db89ae091a9">2 - 大规模集群的注意事项</h1>
    
	<!-- 
reviewers:
- davidopp
- lavalamp
title: Considerations for large clusters
weight: 20
-->
<!--
A cluster is a set of <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='nodes'>nodes</a> (physical
or virtual machines) running Kubernetes agents, managed by the
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>.
Kubernetes v1.23 supports clusters with up to 5000 nodes. More specifically,
Kubernetes is designed to accommodate configurations that meet *all* of the following criteria:
-->
<p>集群是运行 Kubernetes 代理的、
由<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>管理的一组
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>（物理机或虚拟机）。
Kubernetes v1.23 支持的最大节点数为 5000。
更具体地说，Kubernetes旨在适应满足以下<em>所有</em>标准的配置：</p>
<!--
* No more than 110 pods per node
* No more than 5000 nodes
* No more than 150000 total pods
* No more than 300000 total containers
-->
<ul>
<li>每个节点的 Pod 数量不超过 110</li>
<li>节点数不超过 5000</li>
<li>Pod 总数不超过 150000</li>
<li>容器总数不超过 300000</li>
</ul>
<!-- 
You can scale your cluster by adding or removing nodes. The way you do this depends
on how your cluster is deployed.
-->
<p>你可以通过添加或删除节点来扩展集群。集群扩缩的方式取决于集群的部署方式。</p>
<!--  
## Cloud provider resource quotas {#quota-issues}

To avoid running into cloud provider quota issues, when creating a cluster with many nodes,
consider:
* Requesting a quota increase for cloud resources such as:
    * Computer instances
    * CPUs
    * Storage volumes
    * In-use IP addresses
    * Packet filtering rule sets
    * Number of load balancers
    * Network subnets
    * Log streams
* Gating the cluster scaling actions to brings up new nodes in batches, with a pause
  between batches, because some cloud providers rate limit the creation of new instances.
-->
<h2 id="quota-issues">云供应商资源配额</h2>
<p>为避免遇到云供应商配额问题，在创建具有大规模节点的集群时，请考虑以下事项：</p>
<ul>
<li>请求增加云资源的配额，例如：
<ul>
<li>计算实例</li>
<li>CPUs</li>
<li>存储卷</li>
<li>使用中的 IP 地址</li>
<li>数据包过滤规则集</li>
<li>负载均衡数量</li>
<li>网络子网</li>
<li>日志流</li>
</ul>
</li>
<li>由于某些云供应商限制了创建新实例的速度，因此通过分批启动新节点来控制集群扩展操作，并在各批之间有一个暂停。</li>
</ul>
<!--  
## Control plane components

For a large cluster, you need a control plane with sufficient compute and other
resources.

Typically you would run one or two control plane instances per failure zone,
scaling those instances vertically first and then scaling horizontally after reaching
the point of falling returns to (vertical) scale.
-->
<h2 id="控制面组件">控制面组件</h2>
<p>对于大型集群，你需要一个具有足够计算能力和其他资源的控制平面。</p>
<p>通常，你将在每个故障区域运行一个或两个控制平面实例，
先垂直缩放这些实例，然后在到达下降点（垂直）后再水平缩放。</p>
<!-- 
You should run at least one instance per failure zone to provide fault-tolerance. Kubernetes
nodes do not automatically steer traffic towards control-plane endpoints that are in the
same failure zone; however, your cloud provider might have its own mechanisms to do this.

For example, using a managed load balancer, you configure the load balancer to send traffic
that originates from the kubelet and Pods in failure zone _A_, and direct that traffic only
to the control plane hosts that are also in zone _A_. If a single control-plane host or
endpoint failure zone _A_ goes offline, that means that all the control-plane traffic for
nodes in zone _A_ is now being sent between zones. Running multiple control plane hosts in
each zone makes that outcome less likely.
-->
<p>你应该在每个故障区域至少应运行一个实例，以提供容错能力。
Kubernetes 节点不会自动将流量引向相同故障区域中的控制平面端点。
但是，你的云供应商可能有自己的机制来执行此操作。</p>
<p>例如，使用托管的负载均衡器时，你可以配置负载均衡器发送源自故障区域 <em>A</em> 中的 kubelet 和 Pod 的流量，
并将该流量仅定向到也位于区域 <em>A</em> 中的控制平面主机。
如果单个控制平面主机或端点故障区域 <em>A</em> 脱机，则意味着区域 <em>A</em> 中的节点的所有控制平面流量现在都在区域之间发送。
在每个区域中运行多个控制平面主机能降低出现这种结果的可能性。</p>
<!--
### etcd storage
-->
<h3 id="etcd-存储">etcd 存储</h3>
<!--
To improve performance of large clusters, you can store Event objects in a separate
dedicated etcd instance.
-->
<p>为了提高大规模集群的性能，你可以将事件对象存储在单独的专用 etcd 实例中。</p>
<!--
When creating a cluster, you can (using custom tooling):

* start and configure additional etcd instance
* configure the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a> to use it for storing events
-->
<p>在创建集群时，你可以（使用自定义工具）：</p>
<ul>
<li>启动并配置额外的 etcd 实例</li>
<li>配置 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>，将它用于存储事件</li>
</ul>
<!--
See [Operating etcd clusters for Kubernetes](/docs/tasks/administer-cluster/configure-upgrade-etcd/) and
[Set up a High Availability etcd cluster with kubeadm](/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)
for details on configuring and managing etcd for a large cluster.
-->
<p>有关为大型集群配置和管理 etcd 的详细信息，请参阅
<a href="/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/">为 Kubernetes 运行 etcd 集群</a>
和使用 <a href="/zh/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">kubeadm 创建一个高可用 etcd 集群</a>。</p>
<!--
### Addon Resources
-->
<h3 id="addon-resources">插件资源  </h3>
<!--
Kubernetes [resource limits](/docs/concepts/configuration/manage-resources-containers/)
help to minimize the impact of memory leaks and other ways that pods and containers can
impact on other components. These resource limits apply to
<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='addon'>addon</a> resources just as they apply to application workloads.

  For example, you can set CPU and memory limits for a logging component:
-->
<p>Kubernetes <a href="/zh/docs/concepts/configuration/manage-resources-containers/">资源限制</a>
有助于最大程度地减少内存泄漏的影响以及 Pod 和容器可能对其他组件的其他方式的影响。
这些资源限制适用于<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='插件'>插件</a>资源，
就像它们适用于应用程序工作负载一样。</p>
<p>例如，你可以对日志组件设置 CPU 和内存限制</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-cloud-logging<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>fluent/fluentd-kubernetes-daemonset:v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>100m<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span></code></pre></div><!-- 
Addons' default limits are typically based on data collected from experience running
each addon on small or medium Kubernetes clusters. When running on large
clusters, addons often consume more of some resources than their default limits.
If a large cluster is deployed without adjusting these values, the addon(s)
may continuously get killed because they keep hitting the memory limit.
Alternatively, the addon may run but with poor performance due to CPU time
slice restrictions.
-->
<p>插件的默认限制通常基于从中小规模 Kubernetes 集群上运行每个插件的经验收集的数据。
插件在大规模集群上运行时，某些资源消耗常常比其默认限制更多。
如果在不调整这些值的情况下部署了大规模集群，则插件可能会不断被杀死，因为它们不断达到内存限制。
或者，插件可能会运行，但由于 CPU 时间片的限制而导致性能不佳。</p>
<!--  
To avoid running into cluster addon resource issues, when creating a cluster with
many nodes, consider the following:

* Some addons scale vertically - there is one replica of the addon for the cluster
  or serving a whole failure zone. For these addons, increase requests and limits
  as you scale out your cluster.
* Many addons scale horizontally - you add capacity by running more pods - but with
  a very large cluster you may also need to raise CPU or memory limits slightly.
  The VerticalPodAutoscaler can run in _recommender_ mode to provide suggested
  figures for requests and limits.
* Some addons run as one copy per node, controlled by a <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>: for example, a node-level log aggregator. Similar to
  the case with horizontally-scaled addons, you may also need to raise CPU or memory
  limits slightly.
-->
<p>为避免遇到集群插件资源问题，在创建大规模集群时，请考虑以下事项：</p>
<ul>
<li>部分垂直扩展插件 —— 总有一个插件副本服务于整个集群或服务于整个故障区域。
对于这些附加组件，请在扩大集群时加大资源请求和资源限制。</li>
<li>许多水平扩展插件 —— 你可以通过运行更多的 Pod 来增加容量——但是在大规模集群下，
可能还需要稍微提高 CPU 或内存限制。
VerticalPodAutoscaler 可以在 <em>recommender</em> 模式下运行，
以提供有关请求和限制的建议数字。</li>
<li>一些插件在每个节点上运行一个副本，并由 DaemonSet 控制：
例如，节点级日志聚合器。与水平扩展插件的情况类似，
你可能还需要稍微提高 CPU 或内存限制。</li>
</ul>
<!-- 
## What's next

`VerticalPodAutoscaler` is a custom resource that you can deploy into your cluster
to help you manage resource requests and limits for pods.  
Visit [Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme)
to learn more about `VerticalPodAutoscaler` and how you can use it to scale cluster
components, including cluster-critical addons.

The [cluster autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme)
integrates with a number of cloud providers to help you run the right number of
nodes for the level of resource demand in your cluster.
-->
<h2 id="what-s-next">What's next</h2>
<p><code>VerticalPodAutoscaler</code> 是一种自定义资源，你可以将其部署到集群中，帮助你管理资源请求和 Pod 的限制。
访问 <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme">Vertical Pod Autoscaler</a>
以了解有关 <code>VerticalPodAutoscaler</code> 的更多信息，
以及如何使用它来扩展集群组件（包括对集群至关重要的插件）的信息。</p>
<p><a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme">集群自动扩缩器</a>
与许多云供应商集成在一起，帮助你在你的集群中，按照资源需求级别运行正确数量的节点。</p>
<!-- 
The [addon resizer](https://github.com/kubernetes/autoscaler/tree/master/addon-resizer#readme)
helps you in resizing the addons automatically as your cluster's scale changes.
-->
<p><a href="https://github.com/kubernetes/autoscaler/tree/master/addon-resizer#readme">addon resizer</a>
可帮助你在集群规模变化时自动调整插件的大小。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f89867de1d34943f1524f67a241f5cc9">3 - 校验节点设置</h1>
    
	<!--
---
reviewers:
- Random-Liu
title: Validate node setup
weight: 30
---
-->
<nav id="TableOfContents">
  <ul>
    <li><a href="#节点一致性测试">节点一致性测试</a></li>
    <li><a href="#节点的前提条件">节点的前提条件</a></li>
    <li><a href="#运行节点一致性测试">运行节点一致性测试</a></li>
    <li><a href="#针对其他硬件体系结构运行节点一致性测试">针对其他硬件体系结构运行节点一致性测试</a></li>
    <li><a href="#运行特定的测试">运行特定的测试</a></li>
    <li><a href="#注意">注意</a></li>
  </ul>
</nav>
<!--
## Node Conformance Test
-->
<h2 id="节点一致性测试">节点一致性测试</h2>
<!--
*Node conformance test* is a containerized test framework that provides a system
verification and functionality test for a node. 
-->
<p><em>节点一致性测试</em> 是一个容器化的测试框架，提供了针对节点的系统验证和功能测试。</p>
<!--
The test validates whether the node meets the minimum requirements for Kubernetes; a node that passes the testis qualified to join a Kubernetes cluster.
-->
<p>该测试主要检测节点是否满足 Kubernetes 的最低要求，通过检测的节点有资格加入 Kubernetes 集群。</p>
<!--
## Node Prerequisite
-->
<h2 id="节点的前提条件">节点的前提条件</h2>
<!--
To run node conformance test, a node must satisfy the same prerequisites as astandard Kubernetes node. At a minimum, the node should have the following daemons installed:
-->
<p>要运行节点一致性测试，节点必须满足与标准 Kubernetes 节点相同的前提条件。节点至少应安装以下守护程序：</p>
<!--
* Container Runtime (Docker)
* Kubelet
-->
<ul>
<li>容器运行时 (Docker)</li>
<li>Kubelet</li>
</ul>
<!--
## Running Node Conformance Test
-->
<h2 id="运行节点一致性测试">运行节点一致性测试</h2>
<!--
To run the node conformance test, perform the following steps:
-->
<p>要运行节点一致性测试，请执行以下步骤：</p>
<!--
1. Work out the value of the `--kubeconfig` option for the kubelet; for example:
   `--kubeconfig=/var/lib/kubelet/config.yaml`.
    Because the test framework starts a local control plane to test the kubelet,
    use `http://localhost:8080` as the URL of the API server.
    There are some other kubelet command line parameters you may want to use:
  * `--pod-cidr`: If you are using `kubenet`, you should specify an arbitrary CIDR
    to Kubelet, for example `--pod-cidr=10.180.0.0/24`.
  * `--cloud-provider`: If you are using `--cloud-provider=gce`, you should
    remove the flag to run the test.
-->
<ol>
<li>得出 kubelet 的 <code>--kubeconfig</code> 的值；例如：<code>--kubeconfig=/var/lib/kubelet/config.yaml</code>.
由于测试框架启动了本地控制平面来测试 kubelet， 因此使用 <code>http://localhost:8080</code>
作为API 服务器的 URL。
一些其他的 kubelet 命令行参数可能会被用到：
<ul>
<li><code>--pod-cidr</code>： 如果使用 <code>kubenet</code>， 需要为 Kubelet 任意指定一个 CIDR，
例如 <code>--pod-cidr=10.180.0.0/24</code>。</li>
<li><code>--cloud-provider</code>： 如果使用 <code>--cloud-provider=gce</code>，需要移除这个参数
来运行测试。</li>
</ul>
</li>
</ol>
<!--
2. Run the node conformance test with command:
-->
<ol start="2">
<li>
<p>使用以下命令运行节点一致性测试：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># $CONFIG_DIR 是您 Kubelet 的 pod manifest 路径。</span>
<span style="color:#080;font-style:italic"># $LOG_DIR 是测试的输出路径。</span>
sudo docker run -it --rm --privileged --net<span style="color:#666">=</span>host <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -v /:/rootfs -v <span style="color:#b8860b">$CONFIG_DIR</span>:<span style="color:#b8860b">$CONFIG_DIR</span> -v <span style="color:#b8860b">$LOG_DIR</span>:/var/result <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  k8s.gcr.io/node-test:0.2
</code></pre></div></li>
</ol>
<!--
## Running Node Conformance Test for Other Architectures
-->
<h2 id="针对其他硬件体系结构运行节点一致性测试">针对其他硬件体系结构运行节点一致性测试</h2>
<!--
Kubernetes also provides node conformance test docker images for other architectures:
-->
<p>Kubernetes 也为其他硬件体系结构的系统提供了节点一致性测试的 Docker 镜像：</p>
<!--
| Arch  |      Image      |      |
| ----- | :-------------: | ---- |
| amd64 | node-test-amd64 |      |
| arm   |  node-test-arm  |      |
| arm64 | node-test-arm64 |      |
-->
<table>
<thead>
<tr>
<th>架构</th>
<th style="text-align:center">镜像</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>amd64</td>
<td style="text-align:center">node-test-amd64</td>
<td></td>
</tr>
<tr>
<td>arm</td>
<td style="text-align:center">node-test-arm</td>
<td></td>
</tr>
<tr>
<td>arm64</td>
<td style="text-align:center">node-test-arm64</td>
<td></td>
</tr>
</tbody>
</table>
<!--
## Running Selected Test
-->
<h2 id="运行特定的测试">运行特定的测试</h2>
<!--
To run specific tests, overwrite the environment variable `FOCUS` with theregular expression of tests you want to run.
-->
<p>要运行特定测试，请使用您希望运行的测试的特定表达式覆盖环境变量 <code>FOCUS</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo docker run -it --rm --privileged --net<span style="color:#666">=</span>host <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   -v /:/rootfs:ro -v <span style="color:#b8860b">$CONFIG_DIR</span>:<span style="color:#b8860b">$CONFIG_DIR</span> -v <span style="color:#b8860b">$LOG_DIR</span>:/var/result <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   -e <span style="color:#b8860b">FOCUS</span><span style="color:#666">=</span>MirrorPod <span style="color:#b62;font-weight:bold">\ </span><span style="color:#080;font-style:italic"># Only run MirrorPod test</span>
k8s.gcr.io/node-test:0.2
</code></pre></div><!--
To skip specific tests, overwrite the environment variable `SKIP` with theregular expression of tests you want to skip.
-->
<p>要跳过特定的测试，请使用您希望跳过的测试的常规表达式覆盖环境变量 <code>SKIP</code>。</p>
<!--
```shell
sudo docker run -it --rm --privileged --net=host \
  -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \
  -e SKIP=MirrorPod \ # Run all conformance tests but skip MirrorPod test
k8s.gcr.io/node-test:0.2
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo docker run -it --rm --privileged --net<span style="color:#666">=</span>host <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -v /:/rootfs:ro -v <span style="color:#b8860b">$CONFIG_DIR</span>:<span style="color:#b8860b">$CONFIG_DIR</span> -v <span style="color:#b8860b">$LOG_DIR</span>:/var/result <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -e <span style="color:#b8860b">SKIP</span><span style="color:#666">=</span>MirrorPod <span style="color:#b62;font-weight:bold">\ </span><span style="color:#080;font-style:italic"># 运行除 MirrorPod 测试外的所有一致性测试内容</span>
k8s.gcr.io/node-test:0.2
</code></pre></div><!--
Node conformance test is a containerized version of [node e2e test](https://github.com/kubernetes/community/blob/main/contributors/devel/e2e-node-tests.md).
-->
<p>节点一致性测试是<a href="https://github.com/kubernetes/community/blob/main/contributors/devel/e2e-node-tests.md">节点端到端测试</a>的容器化版本。</p>
<!--
By default, it runs all conformance tests.
-->
<p>默认情况下，它会运行所有一致性测试。</p>
<!--
Theoretically, you can run any node e2e test if you configure the container andmount required volumes properly. But **it is strongly recommended to only run conformance test**, because it requires much more complex configuration to run non-conformance test.
-->
<p>理论上，只要合理地配置容器和挂载所需的卷，就可以运行任何的节点端到端测试用例。 但是这里<strong>强烈建议只运行一致性测试</strong>，因为运行非一致性测试需要很多复杂的配置。</p>
<!--
## Caveats
-->
<h2 id="注意">注意</h2>
<!--
* The test leaves some docker images on the node, including the node conformance
  test image and images of containers used in the functionality
  test.
* The test leaves dead containers on the node. These containers are created
  during the functionality test.
-->
<ul>
<li>测试会在节点上遗留一些 Docker 镜像， 包括节点一致性测试本身的镜像和功能测试相关的镜像。</li>
<li>测试会在节点上遗留一些死的容器。这些容器是在功能测试的过程中创建的。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0394f813094b7a35058dffe5b8bacd20">4 - PKI 证书和要求</h1>
    
	<!--
title: PKI certificates and requirements
reviewers:
- sig-cluster-lifecycle
content_type: concept
weight: 40
-->
<!-- overview -->
<!--
Kubernetes requires PKI certificates for authentication over TLS.
If you install Kubernetes with [kubeadm](/docs/reference/setup-tools/kubeadm/), the certificates that your cluster requires are automatically generated.
You can also generate your own certificates - for example, to keep your private keys more secure by not storing them on the API server.
This page explains the certificates that your cluster requires.
-->
<p>Kubernetes 需要 PKI 证书才能进行基于 TLS 的身份验证。如果你是使用
<a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm</a> 安装的 Kubernetes，
则会自动生成集群所需的证书。你还可以生成自己的证书。
例如，不将私钥存储在 API 服务器上，可以让私钥更加安全。此页面说明了集群必需的证书。</p>
<!-- body -->
<!--
## How certificates are used by your cluster

Kubernetes requires PKI for the following operations:
-->
<h2 id="集群是如何使用证书的">集群是如何使用证书的</h2>
<p>Kubernetes 需要 PKI 才能执行以下操作：</p>
<!--
* Client certificates for the kubelet to authenticate to the API server
* Server certificate for the API server endpoint
* Client certificates for administrators of the cluster to authenticate to the API server
* Client certificates for the API server to talk to the kubelets
* Client certificate for the API server to talk to etcd
* Client certificate/kubeconfig for the controller manager to talk to the API server
* Client certificate/kubeconfig for the scheduler to talk to the API server.
* Client and server certificates for the [front-proxy](/docs/tasks/extend-kubernetes/configure-aggregation-layer/)
-->
<ul>
<li>Kubelet 的客户端证书，用于 API 服务器身份验证</li>
<li>API 服务器端点的证书</li>
<li>集群管理员的客户端证书，用于 API 服务器身份认证</li>
<li>API 服务器的客户端证书，用于和 Kubelet 的会话</li>
<li>API 服务器的客户端证书，用于和 etcd 的会话</li>
<li>控制器管理器的客户端证书/kubeconfig，用于和 API 服务器的会话</li>
<li>调度器的客户端证书/kubeconfig，用于和 API 服务器的会话</li>
<li><a href="/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/">前端代理</a> 的客户端及服务端证书</li>
</ul>
<!--
`front-proxy` certificates are required only if you run kube-proxy to support [an extension API server](/docs/tasks/access-kubernetes-api/setup-extension-api-server/).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 只有当你运行 kube-proxy 并要支持
<a href="/zh/docs/tasks/extend-kubernetes/setup-extension-api-server/">扩展 API 服务器</a>
时，才需要 <code>front-proxy</code> 证书
</div>
<!--
etcd also implements mutual TLS to authenticate clients and peers.
-->
<p>etcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证。</p>
<!--
## Where certificates are stored

If you install Kubernetes with kubeadm, certificates are stored in `/etc/kubernetes/pki`. All paths in this documentation are relative to that directory.
-->
<h2 id="证书存放的位置">证书存放的位置</h2>
<p>如果你是通过 kubeadm 安装的 Kubernetes，所有证书都存放在 <code>/etc/kubernetes/pki</code> 目录下。本文所有相关的路径都是基于该路径的相对路径。</p>
<!--
## Configure certificates manually

If you don't want kubeadm to generate the required certificates, you can create them in either of the following ways.
-->
<h2 id="手动配置证书">手动配置证书</h2>
<p>如果你不想通过 kubeadm 生成这些必需的证书，你可以通过下面两种方式之一来手动创建他们。</p>
<!--
### Single root CA

You can create a single root CA, controlled by an administrator. This root CA can then create multiple intermediate CAs, and delegate all further creation to Kubernetes itself.
-->
<h3 id="单根-ca">单根 CA</h3>
<p>你可以创建一个单根 CA，由管理员控制器它。该根 CA 可以创建多个中间 CA，并将所有进一步的创建委托给 Kubernetes。</p>
<!--
Required CAs:

| 路径                    | 默认 CN                    | 描述                             |
|------------------------|---------------------------|----------------------------------|
| ca.crt,key             | kubernetes-ca             | Kubernetes general CA            |
| etcd/ca.crt,key        | etcd-ca                   | For all etcd-related functions   |
| front-proxy-ca.crt,key | kubernetes-front-proxy-ca | For the [front-end proxy](/docs/tasks/extend-kubernetes/configure-aggregation-layer/) |

On top of the above CAs, it is also necessary to get a public/private key pair for service account management, `sa.key` and `sa.pub`.
-->
<p>需要这些 CA：</p>
<table>
<thead>
<tr>
<th>路径</th>
<th>默认 CN</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>ca.crt,key</td>
<td>kubernetes-ca</td>
<td>Kubernetes 通用 CA</td>
</tr>
<tr>
<td>etcd/ca.crt,key</td>
<td>etcd-ca</td>
<td>与 etcd 相关的所有功能</td>
</tr>
<tr>
<td>front-proxy-ca.crt,key</td>
<td>kubernetes-front-proxy-ca</td>
<td>用于 <a href="/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/">前端代理</a></td>
</tr>
</tbody>
</table>
<p>上面的 CA 之外，还需要获取用于服务账户管理的密钥对，也就是 <code>sa.key</code> 和 <code>sa.pub</code>。</p>
<!--
### All certificates

If you don't wish to copy the CA private keys to your cluster, you can generate all certificates yourself.

Required certificates:
-->
<h3 id="所有的证书">所有的证书</h3>
<p>如果你不想将 CA 的私钥拷贝至你的集群中，你也可以自己生成全部的证书。</p>
<p>需要这些证书：</p>
<table>
<thead>
<tr>
<th>默认 CN</th>
<th>父级 CA</th>
<th>O (位于 Subject 中)</th>
<th>类型</th>
<th>主机 (SAN)</th>
</tr>
</thead>
<tbody>
<tr>
<td>kube-etcd</td>
<td>etcd-ca</td>
<td></td>
<td>server, client</td>
<td><code>localhost</code>, <code>127.0.0.1</code></td>
</tr>
<tr>
<td>kube-etcd-peer</td>
<td>etcd-ca</td>
<td></td>
<td>server, client</td>
<td><code>&lt;hostname&gt;</code>, <code>&lt;Host_IP&gt;</code>, <code>localhost</code>, <code>127.0.0.1</code></td>
</tr>
<tr>
<td>kube-etcd-healthcheck-client</td>
<td>etcd-ca</td>
<td></td>
<td>client</td>
<td></td>
</tr>
<tr>
<td>kube-apiserver-etcd-client</td>
<td>etcd-ca</td>
<td>system:masters</td>
<td>client</td>
<td></td>
</tr>
<tr>
<td>kube-apiserver</td>
<td>kubernetes-ca</td>
<td></td>
<td>server</td>
<td><code>&lt;hostname&gt;</code>, <code>&lt;Host_IP&gt;</code>, <code>&lt;advertise_IP&gt;</code>, <code>[1]</code></td>
</tr>
<tr>
<td>kube-apiserver-kubelet-client</td>
<td>kubernetes-ca</td>
<td>system:masters</td>
<td>client</td>
<td></td>
</tr>
<tr>
<td>front-proxy-client</td>
<td>kubernetes-front-proxy-ca</td>
<td></td>
<td>client</td>
<td></td>
</tr>
</tbody>
</table>
<!--
[1]: any other IP or DNS name you contact your cluster on (as used by [kubeadm](/docs/reference/setup-tools/kubeadm/) the load balancer stable IP and/or DNS name, `kubernetes`, `kubernetes.default`, `kubernetes.default.svc`,
`kubernetes.default.svc.cluster`, `kubernetes.default.svc.cluster.local`)

where `kind` maps to one or more of the [x509 key usage](https://godoc.org/k8s.io/api/certificates/v1beta1#KeyUsage) types:
-->
<p>[1]: 用来连接到集群的不同 IP 或 DNS 名
（就像 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm</a> 为负载均衡所使用的固定
IP 或 DNS 名，<code>kubernetes</code>、<code>kubernetes.default</code>、<code>kubernetes.default.svc</code>、
<code>kubernetes.default.svc.cluster</code>、<code>kubernetes.default.svc.cluster.local</code>）。</p>
<p>其中，<code>kind</code> 对应一种或多种类型的 <a href="https://godoc.org/k8s.io/api/certificates/v1beta1#KeyUsage">x509 密钥用途</a>：</p>
<!--
| kind   | Key usage                                                                       |
|--------|---------------------------------------------------------------------------------|
| server | digital signature, key encipherment, server auth                                |
| client | digital signature, key encipherment, client auth                                |
-->
<table>
<thead>
<tr>
<th>kind</th>
<th>密钥用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>server</td>
<td>数字签名、密钥加密、服务端认证</td>
</tr>
<tr>
<td>client</td>
<td>数字签名、密钥加密、客户端认证</td>
</tr>
</tbody>
</table>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Hosts/SAN listed above are the recommended ones for getting a working cluster; if required by a specific setup, it is possible to add additional SANs on all the server certificates.
-->
<p>上面列出的 Hosts/SAN 是推荐的配置方式；如果需要特殊安装，则可以在所有服务器证书上添加其他 SAN。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
For kubeadm users only:

* The scenario where you are copying to your cluster CA certificates without private keys is referred as external CA in the kubeadm documentation.
* If you are comparing the above list with a kubeadm generated PKI, please be aware that `kube-etcd`, `kube-etcd-peer` and `kube-etcd-healthcheck-client` certificates
  are not generated in case of external etcd.
-->
<p>对于 kubeadm 用户：</p>
<ul>
<li>不使用私钥，将证书复制到集群 CA 的方案，在 kubeadm 文档中将这种方案称为外部 CA。</li>
<li>如果将以上列表与 kubeadm 生成的 PKI 进行比较，你会注意到，如果使用外部 etcd，则不会生成 <code>kube-etcd</code>、<code>kube-etcd-peer</code> 和 <code>kube-etcd-healthcheck-client</code> 证书。</li>
</ul>

</div>
<!--
### Certificate paths

Certificates should be placed in a recommended path (as used by [kubeadm](/docs/reference/setup-tools/kubeadm/)). Paths should be specified using the given argument regardless of location.
-->
<h3 id="证书路径">证书路径</h3>
<p>证书应放置在建议的路径中（以便 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm</a>使用）。无论使用什么位置，都应使用给定的参数指定路径。</p>
<table>
<thead>
<tr>
<th>默认 CN</th>
<th>建议的密钥路径</th>
<th>建议的证书路径</th>
<th>命令</th>
<th>密钥参数</th>
<th>证书参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>etcd-ca</td>
<td>etcd/ca.key</td>
<td>etcd/ca.crt</td>
<td>kube-apiserver</td>
<td></td>
<td>--etcd-cafile</td>
</tr>
<tr>
<td>kube-apiserver-etcd-client</td>
<td>apiserver-etcd-client.key</td>
<td>apiserver-etcd-client.crt</td>
<td>kube-apiserver</td>
<td>--etcd-keyfile</td>
<td>--etcd-certfile</td>
</tr>
<tr>
<td>kubernetes-ca</td>
<td>ca.key</td>
<td>ca.crt</td>
<td>kube-apiserver</td>
<td></td>
<td>--client-ca-file</td>
</tr>
<tr>
<td>kubernetes-ca</td>
<td>ca.key</td>
<td>ca.crt</td>
<td>kube-controller-manager</td>
<td>--cluster-signing-key-file</td>
<td>--client-ca-file, --root-ca-file, --cluster-signing-cert-file</td>
</tr>
<tr>
<td>kube-apiserver</td>
<td>apiserver.key</td>
<td>apiserver.crt</td>
<td>kube-apiserver</td>
<td>--tls-private-key-file</td>
<td>--tls-cert-file</td>
</tr>
<tr>
<td>kube-apiserver-kubelet-client</td>
<td>apiserver-kubelet-client.key</td>
<td>apiserver-kubelet-client.crt</td>
<td>kube-apiserver</td>
<td>--kubelet-client-key</td>
<td>--kubelet-client-certificate</td>
</tr>
<tr>
<td>front-proxy-ca</td>
<td>front-proxy-ca.key</td>
<td>front-proxy-ca.crt</td>
<td>kube-apiserver</td>
<td></td>
<td>--requestheader-client-ca-file</td>
</tr>
<tr>
<td>front-proxy-ca</td>
<td>front-proxy-ca.key</td>
<td>front-proxy-ca.crt</td>
<td>kube-controller-manager</td>
<td></td>
<td>--requestheader-client-ca-file</td>
</tr>
<tr>
<td>front-proxy-client</td>
<td>front-proxy-client.key</td>
<td>front-proxy-client.crt</td>
<td>kube-apiserver</td>
<td>--proxy-client-key-file</td>
<td>--proxy-client-cert-file</td>
</tr>
<tr>
<td>etcd-ca</td>
<td>etcd/ca.key</td>
<td>etcd/ca.crt</td>
<td>etcd</td>
<td></td>
<td>--trusted-ca-file, --peer-trusted-ca-file</td>
</tr>
<tr>
<td>kube-etcd</td>
<td>etcd/server.key</td>
<td>etcd/server.crt</td>
<td>etcd</td>
<td>--key-file</td>
<td>--cert-file</td>
</tr>
<tr>
<td>kube-etcd-peer</td>
<td>etcd/peer.key</td>
<td>etcd/peer.crt</td>
<td>etcd</td>
<td>--peer-key-file</td>
<td>--peer-cert-file</td>
</tr>
<tr>
<td>etcd-ca</td>
<td></td>
<td>etcd/ca.crt</td>
<td>etcdctl</td>
<td></td>
<td>--cacert</td>
</tr>
<tr>
<td>kube-etcd-healthcheck-client</td>
<td>etcd/healthcheck-client.key</td>
<td>etcd/healthcheck-client.crt</td>
<td>etcdctl</td>
<td>--key</td>
<td>--cert</td>
</tr>
</tbody>
</table>
<!--
Same considerations apply for the service account key pair:
-->
<p>注意事项同样适用于服务帐户密钥对：</p>
<table>
<thead>
<tr>
<th>私钥路径</th>
<th>公钥路径</th>
<th>命令</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>sa.key</td>
<td></td>
<td>kube-controller-manager</td>
<td>--service-account-private-key-file</td>
</tr>
<tr>
<td></td>
<td>sa.pub</td>
<td>kube-apiserver</td>
<td>--service-account-key-file</td>
</tr>
</tbody>
</table>
<!--
## Configure certificates for user accounts

You must manually configure these administrator account and service accounts:
-->
<h2 id="为用户帐户配置证书">为用户帐户配置证书</h2>
<p>你必须手动配置以下管理员帐户和服务帐户：</p>
<table>
<thead>
<tr>
<th>文件名</th>
<th>凭据名称</th>
<th>默认 CN</th>
<th>O (位于 Subject 中)</th>
</tr>
</thead>
<tbody>
<tr>
<td>admin.conf</td>
<td>default-admin</td>
<td>kubernetes-admin</td>
<td>system:masters</td>
</tr>
<tr>
<td>kubelet.conf</td>
<td>default-auth</td>
<td>system:node:<code>&lt;nodeName&gt;</code> （参阅注释）</td>
<td>system:nodes</td>
</tr>
<tr>
<td>controller-manager.conf</td>
<td>default-controller-manager</td>
<td>system:kube-controller-manager</td>
<td></td>
</tr>
<tr>
<td>scheduler.conf</td>
<td>default-scheduler</td>
<td>system:kube-scheduler</td>
<td></td>
</tr>
</tbody>
</table>
<!--
The value of `<nodeName>` for `kubelet.conf` **must** match precisely the value of the node name provided by the kubelet as it registers with the apiserver. For further details, read the [Node Authorization](/docs/reference/access-authn-authz/node/).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <code>kubelet.conf</code> 中 <code>&lt;nodeName&gt;</code> 的值 <strong>必须</strong> 与 kubelet 向 apiserver 注册时提供的节点名称的值完全匹配。
有关更多详细信息，请阅读<a href="/zh/docs/reference/access-authn-authz/node/">节点授权</a>。
</div>
<!--
1. For each config, generate an x509 cert/key pair with the given CN and O.

1. Run `kubectl` as follows for each config:
-->
<ol>
<li>
<p>对于每个配置，请都使用给定的 CN 和 O 生成 x509 证书/密钥偶对。</p>
</li>
<li>
<p>为每个配置运行下面的 <code>kubectl</code> 命令：</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>&lt;filename&gt; kubectl config set-cluster default-cluster --server<span style="color:#666">=</span>https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs
<span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>&lt;filename&gt; kubectl config set-credentials &lt;credential-name&gt; --client-key &lt;path-to-key&gt;.pem --client-certificate &lt;path-to-cert&gt;.pem --embed-certs
<span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>&lt;filename&gt; kubectl config set-context default-system --cluster default-cluster --user &lt;credential-name&gt;
<span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>&lt;filename&gt; kubectl config use-context default-system
</code></pre></div><!--
These files are used as follows:

| filename                | command                 | comment                                                               |
|-------------------------|-------------------------|-----------------------------------------------------------------------|
| admin.conf              | kubectl                 | Configures administrator user for the cluster                                      |
| kubelet.conf            | kubelet                 | One required for each node in the cluster.                            |
| controller-manager.conf | kube-controller-manager | Must be added to manifest in `manifests/kube-controller-manager.yaml` |
| scheduler.conf          | kube-scheduler          | Must be added to manifest in `manifests/kube-scheduler.yaml`          |
-->
<p>这些文件用途如下：</p>
<table>
<thead>
<tr>
<th>文件名</th>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>admin.conf</td>
<td>kubectl</td>
<td>配置集群的管理员</td>
</tr>
<tr>
<td>kubelet.conf</td>
<td>kubelet</td>
<td>集群中的每个节点都需要一份</td>
</tr>
<tr>
<td>controller-manager.conf</td>
<td>kube-controller-manager</td>
<td>必需添加到 <code>manifests/kube-controller-manager.yaml</code> 清单中</td>
</tr>
<tr>
<td>scheduler.conf</td>
<td>kube-scheduler</td>
<td>必需添加到 <code>manifests/kube-scheduler.yaml</code> 清单中</td>
</tr>
</tbody>
</table>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-92a61cf5b0575aa3500f7665b68127d1">5 - 强制实施 Pod 安全性标准</h1>
    
	<!--
reviewers:
- tallclair
- liggitt
title: Enforcing Pod Security Standards
weight: 40
-->
<!-- overview -->
<!--
This page provides an overview of best practices when it comes to enforcing
[Pod Security Standards](/docs/concepts/security/pod-security-standards).
-->
<p>本页提供实施 <a href="/zh/docs/concepts/security/pod-security-standards">Pod 安全标准（Pod Security Standards）</a>
时的一些最佳实践。</p>
<!-- body -->
<!--
## Using the built-in Pod Security Admission Controller
-->
<h2 id="使用内置的-pod-安全性准入控制器">使用内置的 Pod 安全性准入控制器</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!--
The [Pod Security Admission Controller](/docs/reference/access-authn-authz/admission-controllers/#podsecurity)
intends to replace the deprecated PodSecurityPolicies. 
-->
<p><a href="/zh/docs/reference/access-authn-authz/admission-controllers/#podsecurity">Pod 安全性准入控制器</a>
尝试替换已被废弃的 PodSecurityPolicies。</p>
<!--
### Configure all cluster namespaces
-->
<h3 id="configure-all-cluster-namespaces">配置所有集群名字空间   </h3>
<!--
Namespaces that lack any configuration at all should be considered significant gaps in your cluster
security model. We recommend taking the time to analyze the types of workloads occurring in each
namespace, and by referencing the Pod Security Standards, decide on an appropriate level for
each of them. Unlabeled namespaces should only indicate that they've yet to be evaluated.
-->
<p>完全未经配置的名字空间应该被视为集群安全模型中的重大缺陷。
我们建议花一些时间来分析在每个名字空间中执行的负载的类型，
并通过引用 Pod 安全性标准来确定每个负载的合适级别。
未设置标签的名字空间应该视为尚未被评估。</p>
<!--
In the scenario that all workloads in all namespaces have the same security requirements,
we provide an [example](/docs/concepts/security/pod-security-admission/#applying-to-all-namespaces)
that illustrates how the PodSecurity labels can be applied in bulk.
-->
<p>针对所有名字空间中的所有负载都具有相同的安全性需求的场景，
我们提供了一个<a href="/zh/docs/concepts/security/pod-security-admission/#applying-to-all-namespaces">示例</a>
用来展示如何批量应用 Pod 安全性标签。</p>
<!--
### Embrace the principle of least privilege

In an ideal world, every pod in every namespace would meet the requirements of the `restricted`
policy. However, this is not possible nor practical, as some workloads will require elevated
privileges for legitimate reasons.
-->
<h3 id="拥抱最小特权原则">拥抱最小特权原则</h3>
<p>在一个理想环境中，每个名字空间中的每个 Pod 都会满足 <code>restricted</code> 策略的需求。
不过，这既不可能也不现实，某些负载会因为合理的原因而需要特权上的提升。</p>
<!--
- Namespaces allowing `privileged` workloads should establish and enforce appropriate access controls.
- For workloads running in those permissive namespaces, maintain documentation about their unique
  security requirements. If at all possible, consider how those requirements could be further
  constrained.
-->
<ul>
<li>允许 <code>privileged</code> 负载的名字空间需要建立并实施适当的访问控制机制。</li>
<li>对于运行在特权宽松的名字空间中的负载，需要维护其独特安全性需求的文档。
如果可能的话，要考虑如何进一步约束这些需求。</li>
</ul>
<!--
### Adopt a multi-mode strategy

The `audit` and `warn` modes of the Pod Security Standards admission controller make it easy to
collect important security insights about your pods without breaking existing workloads.
-->
<h3 id="采用多种模式的策略">采用多种模式的策略</h3>
<p>Pod 安全性标准准入控制器的 <code>audit</code> 和 <code>warn</code> 模式（mode）
能够在不影响现有负载的前提下，让该控制器更方便地收集关于 Pod 的重要的安全信息。</p>
<!--
It is good practice to enable these modes for all namespaces, setting them to the _desired_ level
and version you would eventually like to `enforce`. The warnings and audit annotations generated in
this phase can guide you toward that state. If you expect workload authors to make changes to fit
within the desired level, enable the `warn` mode. If you expect to use audit logs to monitor/drive
changes to fit within the desired level, enable the `audit` mode.
-->
<p>针对所有名字空间启用这些模式是一种好的实践，将它们设置为你最终打算 <code>enforce</code> 的
<em>期望的</em> 级别和版本。这一阶段中所生成的警告和审计注解信息可以帮助你到达这一状态。
如果你期望负载的作者能够作出变更以便适应期望的级别，可以启用 <code>warn</code> 模式。
如果你希望使用审计日志了监控和驱动变更，以便负载能够适应期望的级别，可以启用 <code>audit</code> 模式。</p>
<!--
When you have the `enforce` mode set to your desired value, these modes can still be useful in a
few different ways:

- By setting `warn` to the same level as `enforce`, clients will receive warnings when attempting
  to create Pods (or resources that have Pod templates) that do not pass validation. This will help
  them update those resources to become compliant.
- In Namespaces that pin `enforce` to a specific non-latest version, setting the `audit` and `warn`
  modes to the same level as `enforce`, but to the `latest` version, gives visibility into settings
  that were allowed by previous versions but are not allowed per current best practices.
-->
<p>当你将 <code>enforce</code> 模式设置为期望的取值时，这些模式在不同的场合下仍然是有用的：</p>
<ul>
<li>通过将 <code>warn</code> 设置为 <code>enforce</code> 相同的级别，客户可以在尝试创建无法通过合法检查的 Pod
（或者包含 Pod 模板的资源）时收到警告信息。这些信息会帮助于更新资源使其合规。</li>
<li>在将 <code>enforce</code> 锁定到特定的非最新版本的名字空间中，将 <code>audit</code> 和 <code>warn</code>
模式设置为 <code>enforce</code> 一样的级别而非 <code>latest</code> 版本，
这样可以方便看到之前版本所允许但当前最佳实践中被禁止的设置。</li>
</ul>
<!--
## Third-party alternatives
-->
<h2 id="third-party-alternatives">第三方替代方案    </h2>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<!--
Other alternatives for enforcing security profiles are being developed in the Kubernetes
ecosystem:
-->
<p>Kubernetes 生态系统中也有一些其他强制实施安全设置的替代方案处于开发状态中：</p>
<ul>
<li><a href="https://github.com/kubewarden">Kubewarden</a>.</li>
<li><a href="https://kyverno.io/policies/">Kyverno</a>.</li>
<li><a href="https://github.com/open-policy-agent/gatekeeper">OPA Gatekeeper</a>.</li>
</ul>
<!--
The decision to go with a _built-in_ solution (e.g. PodSecurity admission controller) versus a
third-party tool is entirely dependent on your own situation. When evaluating any solution,
trust of your supply chain is crucial. Ultimately, using _any_ of the aforementioned approaches
will be better than doing nothing.
-->
<p>采用 <em>内置的</em> 方案（例如 PodSecurity 准入控制器）还是第三方工具，
这一决策完全取决于你自己的情况。在评估任何解决方案时，对供应链的信任都是至关重要的。
最终，使用前述方案中的 <em>任何</em> 一种都好过放任自流。</p>

</div>



    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      
      <a class="text-white" href="/zh/docs/home/">主页</a>
      
      <a class="text-white" href="/zh/blog/">博客</a>
      
      <a class="text-white" href="/zh/training/">培训</a>
      
      <a class="text-white" href="/zh/partners/">合作伙伴</a>
      
      <a class="text-white" href="/zh/community/">社区</a>
      
      <a class="text-white" href="/zh/case-studies/">案例分析</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" href="https://discuss.kubernetes.io">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/kubernetesio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://youtube.com/kubernetescommunity">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/kubernetes/kubernetes">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://slack.k8s.io">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Contribute" aria-label="Contribute">
    <a class="text-white" target="_blank" href="https://git.k8s.io/community/contributors/guide">
      <i class="fas fa-edit"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" href="https://stackoverflow.com/questions/tagged/kubernetes">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2024 The Kubernetes Authors | Documentation Distributed under <a href="https://git.k8s.io/website/LICENSE" class="light-text">CC BY 4.0</a></small>
        <br/>
        <small class="text-white">Copyright &copy; 2024 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage" class="light-text">Trademark Usage page</a></small>
        <br/>
        <small class="text-white">ICP license: 京ICP备17074266号-3</small>
        
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>










<script src="/js/main.js"></script>






  </body>
</html>
