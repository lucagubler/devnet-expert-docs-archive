<!doctype html>
<html lang="zh" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>


<link rel="alternate" hreflang="en" href="http://localhost:1313/docs/setup/">
<link rel="alternate" hreflang="ko" href="http://localhost:1313/ko/docs/setup/">
<link rel="alternate" hreflang="ja" href="http://localhost:1313/ja/docs/setup/">
<link rel="alternate" hreflang="fr" href="http://localhost:1313/fr/docs/setup/">
<link rel="alternate" hreflang="de" href="http://localhost:1313/de/docs/setup/">
<link rel="alternate" hreflang="pt-br" href="http://localhost:1313/pt-br/docs/setup/">
<link rel="alternate" hreflang="es" href="http://localhost:1313/es/docs/setup/">
<link rel="alternate" hreflang="id" href="http://localhost:1313/id/docs/setup/">
<link rel="alternate" hreflang="ru" href="http://localhost:1313/ru/docs/setup/">
<link rel="alternate" hreflang="pl" href="http://localhost:1313/pl/docs/setup/">
<link rel="alternate" hreflang="uk" href="http://localhost:1313/uk/docs/setup/">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.87.0" />
<link rel="canonical" type="text/html" href="http://localhost:1313/zh/docs/setup/">
<link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="apple-touch-icon" href="/images/kubernetes-192x192.png">
<title>入门 | Kubernetes</title><meta property="og:title" content="入门" />
<meta property="og:description" content="生产级别的容器编排系统" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/zh/docs/setup/" /><meta property="og:site_name" content="Kubernetes" />

<meta itemprop="name" content="入门">
<meta itemprop="description" content="生产级别的容器编排系统"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="入门"/>
<meta name="twitter:description" content="生产级别的容器编排系统"/>






<link href="/scss/main.css" rel="stylesheet">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "http://localhost:1313/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href="/css/feature-states.css">



<meta name="description" content="本节列出了设置和运行 Kubernetes 的不同方法。
安装 Kubernetes 时，请根据以下条件选择安装类型：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。
可以下载 Kubernetes，在本地机器、云或你自己的数据中心上部署 Kubernetes 集群。 如果你不想自己管理 Kubernetes 集群，则可以选择托管服务，包括经过认证的平台。 在各种云和裸机环境中，还有其他标准化和定制的解决方案。
学习环境 如果正打算学习 Kubernetes，请使用 Kubernetes 社区支持 或生态系统中的工具在本地计算机上设置 Kubernetes 集群。 请参阅安装工具。
生产环境 在评估生产环境的解决方案时， 请考虑要自己管理 Kubernetes 集群（或相关抽象）的哪些方面，将哪些托付给提供商。
对于你自己管理的集群，官方支持的用于部署 Kubernetes 的工具是 kubeadm。
control plane to run on Linux. Within your cluster you can run applications on Linux or other operating systems, including Windows. - Learn to [set up clusters with Windows nodes](/docs/setup/production-environment/windows/) -- What&#39;s next  下载 Kubernetes 下载并安装工具，包括 kubectl 在内 为新集群选择容器运行时 了解集群设置的最佳实践  Kubernetes 的设计是让其控制平面在 Linux 上运行的。 在集群中，你可以在 Linux 或其他操作系统（包括 Windows）上运行应用程序。">
<meta property="og:description" content="本节列出了设置和运行 Kubernetes 的不同方法。
安装 Kubernetes 时，请根据以下条件选择安装类型：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。
可以下载 Kubernetes，在本地机器、云或你自己的数据中心上部署 Kubernetes 集群。 如果你不想自己管理 Kubernetes 集群，则可以选择托管服务，包括经过认证的平台。 在各种云和裸机环境中，还有其他标准化和定制的解决方案。
学习环境 如果正打算学习 Kubernetes，请使用 Kubernetes 社区支持 或生态系统中的工具在本地计算机上设置 Kubernetes 集群。 请参阅安装工具。
生产环境 在评估生产环境的解决方案时， 请考虑要自己管理 Kubernetes 集群（或相关抽象）的哪些方面，将哪些托付给提供商。
对于你自己管理的集群，官方支持的用于部署 Kubernetes 的工具是 kubeadm。
control plane to run on Linux. Within your cluster you can run applications on Linux or other operating systems, including Windows. - Learn to [set up clusters with Windows nodes](/docs/setup/production-environment/windows/) -- What&#39;s next  下载 Kubernetes 下载并安装工具，包括 kubectl 在内 为新集群选择容器运行时 了解集群设置的最佳实践  Kubernetes 的设计是让其控制平面在 Linux 上运行的。 在集群中，你可以在 Linux 或其他操作系统（包括 Windows）上运行应用程序。">
<meta name="twitter:description" content="本节列出了设置和运行 Kubernetes 的不同方法。
安装 Kubernetes 时，请根据以下条件选择安装类型：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。
可以下载 Kubernetes，在本地机器、云或你自己的数据中心上部署 Kubernetes 集群。 如果你不想自己管理 Kubernetes 集群，则可以选择托管服务，包括经过认证的平台。 在各种云和裸机环境中，还有其他标准化和定制的解决方案。
学习环境 如果正打算学习 Kubernetes，请使用 Kubernetes 社区支持 或生态系统中的工具在本地计算机上设置 Kubernetes 集群。 请参阅安装工具。
生产环境 在评估生产环境的解决方案时， 请考虑要自己管理 Kubernetes 集群（或相关抽象）的哪些方面，将哪些托付给提供商。
对于你自己管理的集群，官方支持的用于部署 Kubernetes 的工具是 kubeadm。
control plane to run on Linux. Within your cluster you can run applications on Linux or other operating systems, including Windows. - Learn to [set up clusters with Windows nodes](/docs/setup/production-environment/windows/) -- What&#39;s next  下载 Kubernetes 下载并安装工具，包括 kubectl 在内 为新集群选择容器运行时 了解集群设置的最佳实践  Kubernetes 的设计是让其控制平面在 Linux 上运行的。 在集群中，你可以在 Linux 或其他操作系统（包括 Windows）上运行应用程序。">
<meta property="og:url" content="http://localhost:1313/zh/docs/setup/">
<meta property="og:title" content="入门">
<meta name="twitter:title" content="入门">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">

<script src="/js/script.js"></script>


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/zh/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/zh/docs/" >文档</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/blog/" >Kubernetes 博客</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/training/" >培训</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/partners/" >合作伙伴</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/community/" >社区</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/case-studies/" >案例分析</a>
			</li>
			
			
			
			<li class="nav-item dropdown">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/releases">Release Information</a>
	
	<a class="dropdown-item" href="https://kubernetes.io/zh/docs/setup/">v1.23</a>
	
	<a class="dropdown-item" href="https://v1-22.docs.kubernetes.io/zh/docs/setup/">v1.22</a>
	
	<a class="dropdown-item" href="https://v1-21.docs.kubernetes.io/zh/docs/setup/">v1.21</a>
	
	<a class="dropdown-item" href="https://v1-20.docs.kubernetes.io/zh/docs/setup/">v1.20</a>
	
	<a class="dropdown-item" href="https://v1-19.docs.kubernetes.io/zh/docs/setup/">v1.19</a>
	
</div>
			</li>
			
			
			<li class="nav-item dropdown">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/setup/">English</a>
	
	<a class="dropdown-item" href="/ko/docs/setup/">한국어 Korean</a>
	
	<a class="dropdown-item" href="/ja/docs/setup/">日本語 Japanese</a>
	
	<a class="dropdown-item" href="/fr/docs/setup/">Français</a>
	
	<a class="dropdown-item" href="/de/docs/setup/">Deutsch</a>
	
	<a class="dropdown-item" href="/pt-br/docs/setup/">Português</a>
	
	<a class="dropdown-item" href="/es/docs/setup/">Español</a>
	
	<a class="dropdown-item" href="/id/docs/setup/">Bahasa Indonesia</a>
	
	<a class="dropdown-item" href="/ru/docs/setup/">Русский</a>
	
	<a class="dropdown-item" href="/pl/docs/setup/">Polski</a>
	
	<a class="dropdown-item" href="/uk/docs/setup/">Українська</a>
	
</div>

			</li>
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/zh/docs/setup/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">入门</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-0b597086a9d1382f86abadcfeab657d6">学习环境</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>2: <a href="#pg-4e14853fdaa3bd273f31a60112b9b5ac">生产环境</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1: <a href="#pg-a77d3feb6e6d9978f32fa14622642e9a">容器运行时</a></li>


    
  
    
    
	
<li>2.2: <a href="#pg-d2f55eefe7222b7c637875af9c3ec199">Turnkey 云解决方案</a></li>


    
  
    
    
	
<li>2.3: <a href="#pg-00e1646f68aeb89f9722cf6f6cfcad94">使用部署工具安装 Kubernetes</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.3.1: <a href="#pg-a16f59f325a17cdeed324d5c889f7f73">使用 kubeadm 引导集群</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.3.1.1: <a href="#pg-29e59491dd6118b23072dfe9ebb93323">安装 kubeadm</a></li>


    
  
    
    
	
<li>2.3.1.2: <a href="#pg-c3689df4b0c61a998e79d91a865aa244">对 kubeadm 进行故障排查</a></li>


    
  
    
    
	
<li>2.3.1.3: <a href="#pg-134ed1f6142a98e6ac681a1ba4920e53">使用 kubeadm 创建集群</a></li>


    
  
    
    
	
<li>2.3.1.4: <a href="#pg-4c656c5eda3e1c06ad1aedebdc04a211">使用 kubeadm API 定制组件</a></li>


    
  
    
    
	
<li>2.3.1.5: <a href="#pg-015edbc7cc688d31b1d1edce7c186135">高可用拓扑选项</a></li>


    
  
    
    
	
<li>2.3.1.6: <a href="#pg-3941d5c3409342219bf7e03128b8ecb6">利用 kubeadm 创建高可用集群</a></li>


    
  
    
    
	
<li>2.3.1.7: <a href="#pg-8160424c22d24f7d2d63c521e107dbf8">使用 kubeadm 创建一个高可用 etcd 集群</a></li>


    
  
    
    
	
<li>2.3.1.8: <a href="#pg-07709e71de6b4ac2573041c31213dbeb">使用 kubeadm 配置集群中的每个 kubelet</a></li>


    
  
    
    
	
<li>2.3.1.9: <a href="#pg-df2f3f20d404ebe2b03fcda1fcee50e7">使用 kubeadm 支持双协议栈</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.3.2: <a href="#pg-478acca1934b6d89a0bc00fb25bfe5b6">使用 Kops 安装 Kubernetes</a></li>


    
  
    
    
	
<li>2.3.3: <a href="#pg-f8b4964187fe973644e06ee629eff1de">使用 Kubespray 安装 Kubernetes</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.4: <a href="#pg-acce7e24090fea04715a7a516ba3e69b">Windows Kubernetes</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.4.1: <a href="#pg-a307d413f1f7430fced233023087e2a1">Kubernetes 对 Windows 的支持</a></li>


    
  
    
    
	
<li>2.4.2: <a href="#pg-3a51e66c5de55f9093a8dc55742006d3">Kubernetes 中 Windows 容器的调度指南</a></li>


    
  

    </ul>
    
  

    </ul>
    
  
    
    
	
<li>3: <a href="#pg-84b6491601d6a2b3da4cd5a105c866ba">最佳实践</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.1: <a href="#pg-970615c97499e3651fd3a98e0387cefc">运行于多可用区环境</a></li>


    
  
    
    
	
<li>3.2: <a href="#pg-c797ee17120176c685455db89ae091a9">大规模集群的注意事项</a></li>


    
  
    
    
	
<li>3.3: <a href="#pg-f89867de1d34943f1524f67a241f5cc9">校验节点设置</a></li>


    
  
    
    
	
<li>3.4: <a href="#pg-0394f813094b7a35058dffe5b8bacd20">PKI 证书和要求</a></li>


    
  
    
    
	
<li>3.5: <a href="#pg-92a61cf5b0575aa3500f7665b68127d1">强制实施 Pod 安全性标准</a></li>


    
  

    </ul>
    
  

    </ul>


<div class="content">
      <!--
reviewers:
- brendandburns
- erictune
- mikedanese
title: Getting started
main_menu: true
weight: 20
content_type: concept
no_list: true
card:
  name: setup
  weight: 20
  anchors:
  - anchor: "#learning-environment"
    title: Learning environment
  - anchor: "#production-environment"
    title: Production environment  
-->
<!-- overview -->
<!--
This section lists the different ways to set up and run Kubernetes.
-->
<p>本节列出了设置和运行 Kubernetes 的不同方法。</p>
<!--
When you install Kubernetes, choose an installation type based on: ease of maintenance, security,
control, available resources, and expertise required to operate and manage a cluster.
-->
<p>安装 Kubernetes 时，请根据以下条件选择安装类型：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。</p>
<!--
You can [download Kubernetes](/releases/download/) to deploy a Kubernetes cluster
on a local machine, into the cloud, or for your own datacenter.

If you don't want to manage a Kubernetes cluster yourself, you could pick a managed service, including
[certified platforms](/docs/setup/production-environment/turnkey-solutions/).
There are also other standardized and custom solutions across a wide range of cloud and
bare metal environments.
-->
<p>可以<a href="/releases/download/">下载 Kubernetes</a>，在本地机器、云或你自己的数据中心上部署 Kubernetes 集群。
如果你不想自己管理 Kubernetes 集群，则可以选择托管服务，包括<a href="/zh/docs/setup/production-environment/turnkey-solutions/">经过认证的平台</a>。
在各种云和裸机环境中，还有其他标准化和定制的解决方案。</p>
<!-- body -->
<!--
## Learning environment
-->
<h2 id="学习环境">学习环境</h2>
<!--
If you're learning Kubernetes, use the tools supported by the Kubernetes community,
or tools in the ecosystem to set up a Kubernetes cluster on a local machine.
See [Install tools](/docs/tasks/tools/).
-->
<p>如果正打算学习 Kubernetes，请使用 Kubernetes 社区支持
或生态系统中的工具在本地计算机上设置 Kubernetes 集群。
请参阅<a href="/zh/docs/tasks/tools/">安装工具</a>。</p>
<!--
## Production environment
-->
<h2 id="生产环境">生产环境</h2>
<!--
When evaluating a solution for a
[production environment](/docs/setup/production-environment/), consider which aspects of
operating a Kubernetes cluster (or _abstractions_) you want to manage yourself and which you
prefer to hand off to a provider.

For a cluster you're managing yourself, the officially supported tool
for deploying Kubernetes is [kubeadm](/docs/setup/production-environment/tools/kubeadm/).
-->
<p>在评估<a href="/zh/docs/setup/production-environment/">生产环境</a>的解决方案时，
请考虑要自己管理 Kubernetes 集群（或相关抽象）的哪些方面，将哪些托付给提供商。</p>
<p>对于你自己管理的集群，官方支持的用于部署 Kubernetes 的工具是
<a href="/zh/docs/setup/production-environment/tools/kubeadm/">kubeadm</a>。</p>
<!--
## What's next

- [Download Kubernetes](/releases/download/)
- Download and [install tools](/docs/tasks/tools/) including `kubectl`
- Select a [container runtime](/docs/setup/production-environment/container-runtimes/) for your new cluster
- Learn about [best practices](/docs/setup/best-practices/) for cluster setup

Kubernetes is designed for its <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> to
run on Linux. Within your cluster you can run applications on Linux or other operating systems, including
Windows.
- Learn to [set up clusters with Windows nodes](/docs/setup/production-environment/windows/)
-->
<h2 id="what-s-next">What's next</h2>
<ul>
<li><a href="/releases/download/">下载 Kubernetes</a></li>
<li>下载并<a href="/zh/docs/tasks/tools/">安装工具</a>，包括 kubectl 在内</li>
<li>为新集群选择<a href="/zh/docs/setup/production-environment/container-runtimes/">容器运行时</a></li>
<li>了解集群设置的<a href="/zh/docs/setup/best-practices/">最佳实践</a></li>
</ul>
<p>Kubernetes 的设计是让其<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>在 Linux 上运行的。
在集群中，你可以在 Linux 或其他操作系统（包括 Windows）上运行应用程序。</p>
<ul>
<li>学习<a href="/zh/docs/setup/production-environment/windows/">配置包含 Windows 节点的集群</a></li>
</ul>

</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0b597086a9d1382f86abadcfeab657d6">1 - 学习环境</h1>
    
	<!--
{{/* There is a Netlify redirect from this page to /docs/tasks/tools/ */}}
{{/* This page content only exists to provide a navigation stub */}}
{{/* and to protect in case that redirect is one day removed. */}}

{{/* If you're localizing this page, you only need to copy the front matter */}}
{{/* and add a redirect into "/static/_redirects", for YOUR localization. */}}
-->

</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4e14853fdaa3bd273f31a60112b9b5ac">2 - 生产环境</h1>
    
	<!-- overview -->
<!--
A production-quality Kubernetes cluster requires planning and preparation.
If your Kubernetes cluster is to run critical workloads, it must be configured to be resilient.
This page explains steps you can take to set up a production-ready cluster,
or to promote an existing cluster for production use.
If you're already familiar with production setup and want the links, skip to
[What's next](#what-s-next).
-->
<p>生产质量的 Kubernetes 集群需要规划和准备。
如果你的 Kubernetes 集群是用来运行关键负载的，该集群必须被配置为弹性的（Resilient）。
本页面阐述你在安装生产就绪的集群或将现有集群升级为生产用途时可以遵循的步骤。
如果你已经熟悉生产环境安装，因此只关注一些链接，则可以跳到<a href="#what-s-next">接下来</a>节。</p>
<!-- body -->
<!--
## Production considerations

Typically, a production Kubernetes cluster environment has more requirements than a
personal learning, development, or test environment Kubernetes. A production environment may require
secure access by many users, consistent availability, and the resources to adapt
to changing demands.
-->
<h2 id="production-considerations">生产环境考量 </h2>
<p>通常，一个生产用 Kubernetes 集群环境与个人学习、开发或测试环境所使用的
Kubernetes 相比有更多的需求。生产环境可能需要被很多用户安全地访问，需要
提供一致的可用性，以及能够与需求变化相适配的资源。</p>
<!--
As you decide where you want your production Kubernetes environment to live
(on premises or in a cloud) and the amount of management you want to take
on or hand to others, consider how your requirements for a Kubernetes cluster
are influenced by the following issues:
-->
<p>在你决定在何处运行你的生产用 Kubernetes 环境（在本地或者在云端），以及
你希望承担或交由他人承担的管理工作量时，需要考察以下因素如何影响你对
Kubernetes 集群的需求：</p>
<!--
- *Availability*: A single-machine Kubernetes [learning environment](/docs/setup/#learning-environment)
has a single point of failure. Creating a highly available cluster means considering:
  - Separating the control plane from the worker nodes.
  - Replicating the control plane components on multiple nodes.
  - Load balancing traffic to the cluster’s <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a>.
  - Having enough worker nodes available, or able to quickly become available, as changing workloads warrant it.
-->
<ul>
<li><em>可用性</em>：一个单机的 Kubernetes <a href="/zh/docs/setup/#%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83">学习环境</a>
具有单点失效特点。创建高可用的集群则意味着需要考虑：
<ul>
<li>将控制面与工作节点分开</li>
<li>在多个节点上提供控制面组件的副本</li>
<li>为针对集群的 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>
的流量提供负载均衡</li>
<li>随着负载的合理需要，提供足够的可用的（或者能够迅速变为可用的）工作节点</li>
</ul>
</li>
</ul>
<!--
- *Scale*: If you expect your production Kubernetes environment to receive a stable amount of
demand, you might be able to set up for the capacity you need and be done. However,
if you expect demand to grow over time or change dramatically based on things like
season or special events, you need to plan how to scale to relieve increased
pressure from more requests to the control plane and worker nodes or scale down to reduce unused
resources.
-->
<ul>
<li><em>规模</em>：如果你预期你的生产用 Kubernetes 环境要承受固定量的请求，
你可能可以针对所需要的容量来一次性完成安装。
不过，如果你预期服务请求会随着时间增长，或者因为类似季节或者特殊事件的
原因而发生剧烈变化，你就需要规划如何处理请求上升时对控制面和工作节点
的压力，或者如何缩减集群规模以减少未使用资源的消耗。</li>
</ul>
<!--
- *Security and access management*: You have full admin privileges on your own
Kubernetes learning cluster. But shared clusters with important workloads, and
more than one or two users, require a more refined approach to who and what can
access cluster resources. You can use role-based access control
([RBAC](/docs/reference/access-authn-authz/rbac/)) and other
security mechanisms to make sure that users and workloads can get access to the
resources they need, while keeping workloads, and the cluster itself, secure.
You can set limits on the resources that users and workloads can access
by managing [policies](/docs/concepts/policy/) and
[container resources](/docs/concepts/configuration/manage-resources-containers/).
-->
<ul>
<li><em>安全性与访问管理</em>：在你自己的学习环境 Kubernetes 集群上，你拥有完全的管理员特权。
但是针对运行着重要工作负载的共享集群，用户账户不止一两个时，就需要更细粒度
的方案来确定谁或者哪些主体可以访问集群资源。
你可以使用基于角色的访问控制（<a href="/zh/docs/reference/access-authn-authz/rbac/">RBAC</a>）
和其他安全机制来确保用户和负载能够访问到所需要的资源，同时确保工作负载及集群
自身仍然是安全的。
你可以通过管理<a href="/zh/docs/concets/policy/">策略</a>和
<a href="/zh/docs/concepts/configuration/manage-resources-containers">容器资源</a>来
针对用户和工作负载所可访问的资源设置约束，</li>
</ul>
<!--
Before building a Kubernetes production environment on your own, consider
handing off some or all of this job to 
[Turnkey Cloud Solutions](/docs/setup/production-environment/turnkey-solutions/) 
providers or other [Kubernetes Partners](https://kubernetes.io/partners/).
Options include:
-->
<p>在自行构造 Kubernetes 生产环境之前，请考虑将这一任务的部分或者全部交给
<a href="/zh/docs/setup/production-environment/turnkey-solutions">云方案承包服务</a>
提供商或者其他 <a href="https://kubernetes.io/partners/">Kubernetes 合作伙伴</a>。
选项有：</p>
<!--
- *Serverless*: Just run workloads on third-party equipment without managing
a cluster at all. You will be charged for things like CPU usage, memory, and
disk requests.
- *Managed control plane*: Let the provider manage the scale and availability
of the cluster's control plane, as well as handle patches and upgrades.
- *Managed worker nodes*: Configure pools of nodes to meet your needs,
then the provider makes sure those nodes are available and ready to implement
upgrades when needed.
- *Integration*: There are providers that integrate Kubernetes with other
services you may need, such as storage, container registries, authentication
methods, and development tools.
-->
<ul>
<li><em>无服务</em>：仅是在第三方设备上运行负载，完全不必管理集群本身。你需要为
CPU 用量、内存和磁盘请求等付费。</li>
<li><em>托管控制面</em>：让供应商决定集群控制面的规模和可用性，并负责打补丁和升级等操作。</li>
<li><em>托管工作节点</em>：配置一个节点池来满足你的需要，由供应商来确保节点始终可用，
并在需要的时候完成升级。</li>
<li><em>集成</em>：有一些供应商能够将 Kubernetes 与一些你可能需要的其他服务集成，
这类服务包括存储、容器镜像仓库、身份认证方法以及开发工具等。</li>
</ul>
<!--
Whether you build a production Kubernetes cluster yourself or work with
partners, review the following sections to evaluate your needs as they relate
to your cluster’s *control plane*, *worker nodes*, *user access*, and
*workload resources*.
-->
<p>无论你是自行构造一个生产用 Kubernetes 集群还是与合作伙伴一起协作，请审阅
下面章节以评估你的需求，因为这关系到你的集群的 <em>控制面</em>、<em>工作节点</em>、
<em>用户访问</em> 以及 <em>负载资源</em>。</p>
<!--
## Production cluster setup

In a production-quality Kubernetes cluster, the control plane manages the
cluster from services that can be spread across multiple computers
in different ways. Each worker node, however, represents a single entity that
is configured to run Kubernetes pods.
-->
<h2 id="production-cluster-setup">生产用集群安装 </h2>
<p>在生产质量的 Kubernetes 集群中，控制面用不同的方式来管理集群和可以
分布到多个计算机上的服务。每个工作节点则代表的是一个可配置来运行
Kubernetes Pods 的实体。</p>
<!--
### Production control plane

The simplest Kubernetes cluster has the entire control plane and worker node
services running on the same machine. You can grow that environment by adding
worker nodes, as reflected in the diagram illustrated in
[Kubernetes Components](/docs/concepts/overview/components/).
If the cluster is meant to be available for a short period of time, or can be
discarded if something goes seriously wrong, this might meet your needs.
-->
<h3 id="production-control-plane">生产用控制面 </h3>
<p>最简单的 Kubernetes 集群中，整个控制面和工作节点服务都运行在同一台机器上。
你可以通过添加工作节点来提升环境能力，正如
<a href="/zh/docs/concepts/overview/components/">Kubernetes 组件</a>示意图所示。
如果只需要集群在很短的一段时间内可用，或者可以在某些事物出现严重问题时直接丢弃，
这种配置可能符合你的需要。</p>
<!--
If you need a more permanent, highly available cluster, however, you should
consider ways of extending the control plane. By design, one-machine control
plane services running on a single machine are not highly available.
If keeping the cluster up and running
and ensuring that it can be repaired if something goes wrong is important,
consider these steps:
-->
<p>如果你需要一个更为持久的、高可用的集群，那么你就需要考虑扩展控制面的方式。
根据设计，运行在一台机器上的单机控制面服务不是高可用的。
如果保持集群处于运行状态并且需要确保在出现问题时能够被修复这点很重要，
可以考虑以下步骤：</p>
<!--
- *Choose deployment tools*: You can deploy a control plane using tools such
as kubeadm, kops, and kubespray. See
[Installing Kubernetes with deployment tools](/docs/setup/production-environment/tools/)
to learn tips for production-quality deployments using each of those deployment
methods. Different [Container Runtimes](/docs/setup/production-environment/container-runtimes/)
are available to use with your deployments.
-->
<ul>
<li><em>选择部署工具</em>：你可以使用类似 kubeadm、kops 和 kubespray 这类工具来部署控制面。
参阅<a href="/zh/docs/setup/production-environment/tools/">使用部署工具安装 Kubernetes</a>
以了解使用这类部署方法来完成生产就绪部署的技巧。
存在不同的<a href="/zh/docs/setup/production-environment/container-runtimes/">容器运行时</a>
可供你的部署采用。</li>
</ul>
<!--
- *Manage certificates*: Secure communications between control plane services
are implemented using certificates. Certificates are automatically generated
during deployment or you can generate them using your own certificate authority.
See [PKI certificates and requirements](/docs/setup/best-practices/certificates/) for details.
-->
<ul>
<li><em>管理证书</em>：控制面服务之间的安全通信是通过证书来完成的。证书是在部署期间
自动生成的，或者你也可以使用你自己的证书机构来生成它们。
参阅 <a href="/zh/docs/setup/best-practices/certificates/">PKI 证书和需求</a>了解细节。</li>
</ul>
<!--
- *Configure load balancer for apiserver*: Configure a load balancer
to distribute external API requests to the apiserver service instances running on different nodes. See 
[Create an External Load Balancer](/docs/tasks/access-application-cluster/create-external-load-balancer/)
for details.
-->
<ul>
<li><em>为 API 服务器配置负载均衡</em>：配置负载均衡器来将外部的 API 请求散布给运行在
不同节点上的 API 服务实例。参阅
<a href="/zh/docs/access-application-cluster/create-external-load-balancer/">创建外部负载均衡器</a>
了解细节。</li>
</ul>
<!--
- *Separate and backup etcd service*: The etcd services can either run on the
same machines as other control plane services or run on separate machines, for
extra security and availability. Because etcd stores cluster configuration data,
backing up the etcd database should be done regularly to ensure that you can
repair that database if needed.
See the [etcd FAQ](https://etcd.io/docs/v3.4/faq/) for details on configuring and using etcd.
See [Operating etcd clusters for Kubernetes](/docs/tasks/administer-cluster/configure-upgrade-etcd/)
and [Set up a High Availability etcd cluster with kubeadm](/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)
for details.
-->
<ul>
<li><em>分离并备份 etcd 服务</em>：etcd 服务可以运行于其他控制面服务所在的机器上，
也可以运行在不同的机器上以获得更好的安全性和可用性。
因为 etcd 存储着集群的配置数据，应该经常性地对 etcd 数据库进行备份，
以确保在需要的时候你可以修复该数据库。与配置和使用 etcd 相关的细节可参阅
<a href="/https://etcd.io/docs/v3.4/faq/">etcd FAQ</a>。
更多的细节可参阅<a href="/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/">为 Kubernetes 运维 etcd 集群</a>
和<a href="/zh/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">使用 kubeadm 配置高可用的 etcd 集群</a>。</li>
</ul>
<!--
- *Create multiple control plane systems*: For high availability, the
control plane should not be limited to a single machine. If the control plane
services are run by an init service (such as systemd), each service should run on at
least three machines. However, running control plane services as pods in
Kubernetes ensures that the replicated number of services that you request
will always be available.
The scheduler should be fault tolerant,
but not highly available. Some deployment tools set up [Raft](https://raft.github.io/)
consensus algorithm to do leader election of Kubernetes services. If the
primary goes away, another service elects itself and take over. 
-->
<ul>
<li><em>创建多控制面系统</em>：为了实现高可用性，控制面不应被限制在一台机器上。
如果控制面服务是使用某 init 服务（例如 systemd）来运行的，每个服务应该
至少运行在三台机器上。不过，将控制面作为服务运行在 Kubernetes Pods
中可以确保你所请求的个数的服务始终保持可用。
调度器应该是可容错的，但不是高可用的。
某些部署工具会安装 <a href="https://raft.github.io/">Raft</a> 票选算法来对 Kubernetes
服务执行领导者选举。如果主节点消失，另一个服务会被选中并接手相应服务。</li>
</ul>
<!--
- *Span multiple zones*: If keeping your cluster available at all times is
critical, consider creating a cluster that runs across multiple data centers,
referred to as zones in cloud environments. Groups of zones are referred to as regions.
By spreading a cluster across
multiple zones in the same region, it can improve the chances that your
cluster will continue to function even if one zone becomes unavailable.
See [Running in multiple zones](/docs/setup/best-practices/multiple-zones/) for details.
-->
<ul>
<li><em>跨多个可用区</em>：如果保持你的集群一直可用这点非常重要，可以考虑创建一个跨
多个数据中心的集群；在云环境中，这些数据中心被视为可用区。
若干个可用区在一起可构成地理区域。
通过将集群分散到同一区域中的多个可用区内，即使某个可用区不可用，整个集群
能够继续工作的机会也大大增加。
更多的细节可参阅<a href="/zh/docs/setup/best-practices/multiple-zones/">跨多个可用区运行</a>。</li>
</ul>
<!--
- *Manage on-going features*: If you plan to keep your cluster over time,
there are tasks you need to do to maintain its health and security. For example,
if you installed with kubeadm, there are instructions to help you with
[Certificate Management](/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/)
and [Upgrading kubeadm clusters](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/).
See [Administer a Cluster](/docs/tasks/administer-cluster/)
for a longer list of Kubernetes administrative tasks.
-->
<ul>
<li><em>管理演进中的特性</em>：如果你计划长时间保留你的集群，就需要执行一些维护其
健康和安全的任务。例如，如果你采用 kubeadm 安装的集群，则有一些可以帮助你完成
<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">证书管理</a>
和<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade">升级 kubeadm 集群</a>
的指令。
参见<a href="/zh/docs/tasks/administer-cluster">管理集群</a>了解一个 Kubernetes
管理任务的较长列表。</li>
</ul>
<!--
To learn about available options when you run control plane services, see
[kube-apiserver](/docs/reference/command-line-tools-reference/kube-apiserver/),
[kube-controller-manager](/docs/reference/command-line-tools-reference/kube-controller-manager/),
and [kube-scheduler](/docs/reference/command-line-tools-reference/kube-scheduler/)
component pages. For highly available control plane examples, see
[Options for Highly Available topology](/docs/setup/production-environment/tools/kubeadm/ha-topology/),
[Creating Highly Available clusters with kubeadm](/docs/setup/production-environment/tools/kubeadm/high-availability/),
and [Operating etcd clusters for Kubernetes](/docs/tasks/administer-cluster/configure-upgrade-etcd/).
See [Backing up an etcd cluster](/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster)
for information on making an etcd backup plan.
-->
<p>要了解运行控制面服务时可使用的选项，可参阅
<a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a>、
<a href="/zh/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a> 和
<a href="/zh/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler</a>
组件参考页面。
如要了解高可用控制面的例子，可参阅
<a href="/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/">高可用拓扑结构选项</a>、
<a href="/zh/docs/setup/production-environment/tools/kubeadm/high-availability/">使用 kubeadm 创建高可用集群</a> 以及<a href="/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/">为 Kubernetes 运维 etcd 集群</a>。
关于制定 etcd 备份计划，可参阅
<a href="/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster">对 etcd 集群执行备份</a>。</p>
<!--
### Production worker nodes

Production-quality workloads need to be resilient and anything they rely
on needs to be resilient (such as CoreDNS). Whether you manage your own
control plane or have a cloud provider do it for you, you still need to
consider how you want to manage your worker nodes (also referred to
simply as *nodes*).  
-->
<h3 id="生产用工作节点">生产用工作节点</h3>
<p>生产质量的工作负载需要是弹性的；它们所依赖的其他组件（例如 CoreDNS）也需要是弹性的。
无论你是自行管理控制面还是让云供应商来管理，你都需要考虑如何管理工作节点
（有时也简称为<em>节点</em>）。</p>
<!--
- *Configure nodes*: Nodes can be physical or virtual machines. If you want to
create and manage your own nodes, you can install a supported operating system,
then add and run the appropriate
[Node services](/docs/concepts/overview/components/#node-components). Consider:
-->
<ul>
<li><em>配置节点</em>：节点可以是物理机或者虚拟机。如果你希望自行创建和管理节点，
你可以安装一个受支持的操作系统，之后添加并运行合适的
<a href="/zh/docs/concepts/overview/components/#node-components">节点服务</a>。
考虑：
<!--
- The demands of your workloads when you set up nodes by having appropriate memory, CPU, and disk speed and storage capacity available.
- Whether generic computer systems will do or you have workloads that need GPU processors, Windows nodes, or VM isolation.
-->
<ul>
<li>在安装节点时要通过配置适当的内存、CPU 和磁盘速度、存储容量来满足
你的负载的需求。</li>
<li>是否通用的计算机系统即足够，还是你有负载需要使用 GPU 处理器、Windows 节点
或者 VM 隔离。</li>
</ul>
</li>
</ul>
<!--
- *Validate nodes*: See [Valid node setup](/docs/setup/best-practices/node-conformance/)
for information on how to ensure that a node meets the requirements to join
a Kubernetes cluster.
-->
<ul>
<li><em>验证节点</em>：参阅<a href="/zh/docs/setup/best-practices/node-conformance/">验证节点配置</a>
以了解如何确保节点满足加入到 Kubernetes 集群的需求。</li>
</ul>
<!--
- *Add nodes to the cluster*: If you are managing your own cluster you can
add nodes by setting up your own machines and either adding them manually or
having them register themselves to the cluster’s apiserver. See the
[Nodes](/docs/concepts/architecture/nodes/) section for information on how to set up Kubernetes to add nodes in these ways.
-->
<ul>
<li><em>添加节点到集群中</em>：如果你自行管理你的集群，你可以通过安装配置你的机器，
之后或者手动加入集群，或者让它们自动注册到集群的 API 服务器。参阅
<a href="/zh/docs/concepts/architecture/nodes/">节点</a>节，了解如何配置 Kubernetes
以便以这些方式来添加节点。</li>
</ul>
<!--
- *Add Windows nodes to the cluster*: Kubernetes offers support for Windows
worker nodes, allowing you to run workloads implemented in Windows containers. See
[Windows in Kubernetes](/docs/setup/production-environment/windows/) for details.
-->
<ul>
<li><em>向集群中添加 Windows 节点</em>：Kubernetes 提供对 Windows 工作节点的支持；
这使得你可以运行实现于 Windows 容器内的工作负载。参阅
<a href="/zh/docs/setup/production-environment/windows/">Kubernetes 中的 Windows</a>
了解进一步的详细信息。</li>
</ul>
<!--
- *Scale nodes*: Have a plan for expanding the capacity your cluster will
eventually need. See [Considerations for large clusters](/docs/setup/best-practices/cluster-large/)
to help determine how many nodes you need, based on the number of pods and
containers you need to run. If you are managing nodes yourself, this can mean
purchasing and installing your own physical equipment.
-->
<ul>
<li><em>扩缩节点</em>：制定一个扩充集群容量的规划，你的集群最终会需要这一能力。
参阅<a href="/zh/docs/setup/best-practices/cluster-large/">大规模集群考察事项</a>
以确定你所需要的节点数；这一规模是基于你要运行的 Pod 和容器个数来确定的。
如果你自行管理集群节点，这可能意味着要购买和安装你自己的物理设备。</li>
</ul>
<!--
- *Autoscale nodes*: Most cloud providers support
[Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme)
to replace unhealthy nodes or grow and shrink the number of nodes as demand requires. See the
[Frequently Asked Questions](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md)
for how the autoscaler works and
[Deployment](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment)
for how it is implemented by different cloud providers. For on-premises, there
are some virtualization platforms that can be scripted to spin up new nodes
based on demand.
-->
<ul>
<li><em>节点自动扩缩容</em>：大多数云供应商支持
<a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme">集群自动扩缩器（Cluster Autoscaler）</a>
以便替换不健康的节点、根据需求来增加或缩减节点个数。参阅
<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md">常见问题</a>
了解自动扩缩器的工作方式，并参阅
<a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment">Deployment</a>
了解不同云供应商是如何实现集群自动扩缩器的。
对于本地集群，有一些虚拟化平台可以通过脚本来控制按需启动新节点。</li>
</ul>
<!--
- *Set up node health checks*: For important workloads, you want to make sure
that the nodes and pods running on those nodes are healthy. Using the
[Node Problem Detector](/docs/tasks/debug/debug-cluster/monitor-node-health/)
daemon, you can ensure your nodes are healthy.
-->
<ul>
<li><em>安装节点健康检查</em>：对于重要的工作负载，你会希望确保节点以及在节点上
运行的 Pod 处于健康状态。通过使用
<a href="/zh/docs/tasks/debug/debug-cluster/monitor-node-health/">Node Problem Detector</a>，
你可以确保你的节点是健康的。</li>
</ul>
<!--
## Production user management

In production, you may be moving from a model where you or a small group of
people are accessing the cluster to where there may potentially be dozens or
hundreds of people. In a learning environment or platform prototype, you might have a single
administrative account for everything you do. In production, you will want
more accounts with different levels of access to different namespaces.
-->
<h3 id="生产级用户环境">生产级用户环境</h3>
<p>在生产环境中，情况可能不再是你或者一小组人在访问集群，而是几十
上百人需要访问集群。在学习环境或者平台原型环境中，你可能具有一个
可以执行任何操作的管理账号。在生产环境中，你可需要对不同名字空间
具有不同访问权限级别的很多账号。</p>
<!--
Taking on a production-quality cluster means deciding how you
want to selectively allow access by other users. In particular, you need to
select strategies for validating the identities of those who try to access your
cluster (authentication) and deciding if they have permissions to do what they
are asking (authorization):
-->
<p>建立一个生产级别的集群意味着你需要决定如何有选择地允许其他用户访问集群。
具体而言，你需要选择验证尝试访问集群的人的身份标识（身份认证），并确定
他们是否被许可执行他们所请求的操作（鉴权）：</p>
<!--
- *Authentication*: The apiserver can authenticate users using client
certificates, bearer tokens, an authenticating proxy, or HTTP basic auth.
You can choose which authentication methods you want to use.
Using plugins, the apiserver can leverage your organization’s existing
authentication methods, such as LDAP or Kerberos. See
[Authentication](/docs/reference/access-authn-authz/authentication/)
for a description of these different methods of authenticating Kubernetes users.
-->
<ul>
<li><em>认证（Authentication）</em>：API 服务器可以使用客户端证书、持有者令牌、身份
认证代理或者 HTTP 基本认证机制来完成身份认证操作。
你可以选择你要使用的认证方法。通过使用插件，API 服务器可以充分利用你所在
组织的现有身份认证方法，例如 LDAP 或者 Kerberos。
关于认证 Kubernetes 用户身份的不同方法的描述，可参阅
<a href="/zh/docs/reference/access-authn-authz/authentication/">身份认证</a>。</li>
</ul>
<!--
- *Authorization*: When you set out to authorize your regular users, you will probably choose between RBAC and ABAC authorization. See [Authorization Overview](/docs/reference/access-authn-authz/authorization/) to review different modes for authorizing user accounts (as well as service account access to your cluster):
-->
<ul>
<li><em>鉴权（Authorization）</em>：当你准备为一般用户执行权限判定时，你可能会需要
在 RBAC 和 ABAC 鉴权机制之间做出选择。参阅
<a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权概述</a>，了解
对用户账户（以及访问你的集群的服务账户）执行鉴权的不同模式。
<!--
- *Role-based access control* ([RBAC](/docs/reference/access-authn-authz/rbac/)): Lets you assign access to your cluster by allowing specific sets of permissions to authenticated users. Permissions can be assigned for a specific namespace (Role) or across the entire cluster (ClusterRole). Then using RoleBindings and ClusterRoleBindings, those permissions can be attached to particular users.
-->
<ul>
<li><em>基于角色的访问控制</em>（<a href="/zh/docs/reference/access-authn-authz/rbac/">RBAC</a>）：
让你通过为通过身份认证的用户授权特定的许可集合来控制集群访问。
访问许可可以针对某特定名字空间（Role）或者针对整个集群（ClusterRole）。
通过使用 RoleBinding 和 ClusterRoleBinding 对象，这些访问许可可以被
关联到特定的用户身上。</li>
</ul>
<!--
- *Attribute-based access control* ([ABAC](/docs/reference/access-authn-authz/abac/)): Lets you create policies based on resource attributes in the cluster and will allow or deny access based on those attributes. Each line of a policy file identifies versioning properties (apiVersion and kind) and a map of spec properties to match the subject (user or group), resource property, non-resource property (/version or /apis), and readonly. See [Examples](/docs/reference/access-authn-authz/abac/#examples) for details.
-->
<ul>
<li><em>基于属性的访问控制</em>（<a href="/zh/docs/reference/access-authn-authz/abac/">ABAC</a>）：
让你能够基于集群中资源的属性来创建访问控制策略，基于对应的属性来决定
允许还是拒绝访问。策略文件的每一行都给出版本属性（apiVersion 和 kind）
以及一个规约属性的映射，用来匹配主体（用户或组）、资源属性、非资源属性
（/version 或 /apis）和只读属性。
参阅<a href="/zh/docs/reference/access-authn-authz/abac/#examples">示例</a>以了解细节。</li>
</ul>
</li>
</ul>
<!--
As someone setting up authentication and authorization on your production Kubernetes cluster, here are some things to consider:
-->
<p>作为在你的生产用 Kubernetes 集群中安装身份认证和鉴权机制的负责人，
要考虑的事情如下：</p>
<!--
- *Set the authorization mode*: When the Kubernetes API server
([kube-apiserver](/docs/reference/command-line-tools-reference/kube-apiserver/))
starts, the supported authentication modes must be set using the *--authorization-mode*
flag. For example, that flag in the *kube-adminserver.yaml* file (in */etc/kubernetes/manifests*)
could be set to Node,RBAC. This would allow Node and RBAC authorization for authenticated requests.
-->
<ul>
<li><em>设置鉴权模式</em>：当 Kubernetes API 服务器
（<a href="/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a>）
启动时，所支持的鉴权模式必须使用 <code>--authorization-mode</code> 标志配置。
例如，<code>kube-apiserver.yaml</code>（位于 <code>/etc/kubernetes/manifests</code> 下）中对应的
标志可以设置为 <code>Node,RBAC</code>。这样就会针对已完成身份认证的请求执行 Node 和 RBAC
鉴权。</li>
</ul>
<!--
- *Create user certificates and role bindings (RBAC)*: If you are using RBAC
authorization, users can create a CertificateSigningRequest (CSR) that can be
signed by the cluster CA. Then you can bind Roles and ClusterRoles to each user.
See [Certificate Signing Requests](/docs/reference/access-authn-authz/certificate-signing-requests/)
for details.
-->
<ul>
<li><em>创建用户证书和角色绑定（RBAC）</em>：如果你在使用 RBAC 鉴权，用户可以创建
由集群 CA 签名的 CertificateSigningRequest（CSR）。接下来你就可以将 Role
和 ClusterRole 绑定到每个用户身上。
参阅<a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/">证书签名请求</a>
了解细节。</li>
</ul>
<!--
- *Create policies that combine attributes (ABAC)*: If you are using ABAC
authorization, you can assign combinations of attributes to form policies to
authorize selected users or groups to access particular resources (such as a
pod), namespace, or apiGroup. For more information, see
[Examples](/docs/reference/access-authn-authz/abac/#examples).
-->
<ul>
<li><em>创建组合属性的策略（ABAC）</em>：如果你在使用 ABAC 鉴权，你可以设置属性组合
以构造策略对所选用户或用户组执行鉴权，判定他们是否可访问特定的资源
（例如 Pod）、名字空间或者 apiGroup。进一步的详细信息可参阅
<a href="/zh/docs/reference/access-authn-authz/abac/#examples">示例</a>。</li>
</ul>
<!--
- *Consider Admission Controllers*: Additional forms of authorization for
requests that can come in through the API server include
[Webhook Token Authentication](/docs/reference/access-authn-authz/authentication/#webhook-token-authentication).
Webhooks and other special authorization types need to be enabled by adding
[Admission Controllers](/docs/reference/access-authn-authz/admission-controllers/)
to the API server.
-->
<ul>
<li><em>考虑准入控制器</em>：针对指向 API 服务器的请求的其他鉴权形式还包括
<a href="/zh/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">Webhook 令牌认证</a>。
Webhook 和其他特殊的鉴权类型需要通过向 API 服务器添加
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制器</a>
来启用。</li>
</ul>
<!--
## Set limits on workload resources

Demands from production workloads can cause pressure both inside and outside
of the Kubernetes control plane. Consider these items when setting up for the
needs of your cluster's workloads:
-->
<h2 id="set-limits-on-workload-resources">为负载资源设置约束 </h2>
<p>生产环境负载的需求可能对 Kubernetes 的控制面内外造成压力。
在针对你的集群的负载执行配置时，要考虑以下条目：</p>
<!--
- *Set namespace limits*: Set per-namespace quotas on things like memory and CPU. See
[Manage Memory, CPU, and API Resources](/docs/tasks/administer-cluster/manage-resources/)
for details. You can also set
[Hierarchical Namespaces](/blog/2020/08/14/introducing-hierarchical-namespaces/)
for inheriting limits.
-->
<ul>
<li><em>设置名字空间限制</em>：为每个名字空间的内存和 CPU 设置配额。
参阅<a href="/zh/docs/tasks/administer-cluster/manage-resources/">管理内存、CPU 和 API 资源</a>
以了解细节。你也可以设置
<a href="/blog/2020/08/14/introducing-hierarchical-namespaces/">层次化名字空间</a>
来继承这类约束。</li>
</ul>
<!--
- *Prepare for DNS demand*: If you expect workloads to massively scale up,
your DNS service must be ready to scale up as well. See
[Autoscale the DNS service in a Cluster](/docs/tasks/administer-cluster/dns-horizontal-autoscaling/).
-->
<ul>
<li><em>为 DNS 请求做准备</em>：如果你希望工作负载能够完成大规模扩展，你的 DNS 服务
也必须能够扩大规模。参阅
<a href="/zh/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">自动扩缩集群中 DNS 服务</a>。</li>
</ul>
<!--
- *Create additional service accounts*: User accounts determine what users can
do on a cluster, while a service account defines pod access within a particular
namespace. By default, a pod takes on the default service account from its namespace.
See [Managing Service Accounts](/docs/reference/access-authn-authz/service-accounts-admin/)
for information on creating a new service account. For example, you might want to:
-->
<ul>
<li><em>创建额外的服务账户</em>：用户账户决定用户可以在集群上执行的操作，服务账号则定义的
是在特定名字空间中 Pod 的访问权限。
默认情况下，Pod 使用所在名字空间中的 default 服务账号。
参阅<a href="/zh/docs/reference/access-authn-authz/service-accounts-admin/">管理服务账号</a>
以了解如何创建新的服务账号。例如，你可能需要：
<!--
- Add secrets that a pod could use to pull images from a particular container registry. See [Configure Service Accounts for Pods](/docs/tasks/configure-pod-container/configure-service-account/) for an example.
- Assign RBAC permissions to a service account. See [ServiceAccount permissions](/docs/reference/access-authn-authz/rbac/#service-account-permissions) for details.
-->
<ul>
<li>为 Pod 添加 Secret，以便 Pod 能够从某特定的容器镜像仓库拉取镜像。
参阅<a href="/zh/docs/tasks/configure-pod-container/configure-service-account/">为 Pod 配置服务账号</a>
以获得示例。</li>
<li>为服务账号设置 RBAC 访问许可。参阅
<a href="/zh/docs/reference/access-authn-authz/rbac/#service-account-permissions">服务账号访问许可</a>
了解细节。</li>
</ul>
</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
- Decide if you want to build your own production Kubernetes or obtain one from
available [Turnkey Cloud Solutions](/docs/setup/production-environment/turnkey-solutions/)
or [Kubernetes Partners](https://kubernetes.io/partners/).
- If you choose to build your own cluster, plan how you want to
handle [certificates](/docs/setup/best-practices/certificates/)
and set up high availability for features such as
[etcd](/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)
and the
[API server](/docs/setup/production-environment/tools/kubeadm/ha-topology/).
-->
<ul>
<li>决定你是想自行构造自己的生产用 Kubernetes 还是从某可用的
<a href="/zh/docs/setup/production-environment/turnkey-solutions/">云服务外包厂商</a>
或 <a href="https://kubernetes.io/partners/">Kubernetes 合作伙伴</a>获得集群。</li>
<li>如果你决定自行构造集群，则需要规划如何处理
<a href="/zh/docs/setup/best-practices/certificates/">证书</a>
并为类似
<a href="/zh/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">etcd</a>
和
<a href="/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/">API 服务器</a>
这些功能组件配置高可用能力。</li>
</ul>
<!--
- Choose from [kubeadm](/docs/setup/production-environment/tools/kubeadm/), [kops](/docs/setup/production-environment/tools/kops/) or [Kubespray](/docs/setup/production-environment/tools/kubespray/)
deployment methods.
-->
<ul>
<li>选择使用 <a href="/zh/docs/setup/production-environment/tools/kubeadm/">kubeadm</a>、
<a href="/zh/docs/setup/production-environment/tools/kops/">kops</a> 或
<a href="/zh/docs/setup/production-environment/tools/kubespray/">Kubespray</a>
作为部署方法。</li>
</ul>
<!--
- Configure user management by determining your
[Authentication](/docs/reference/access-authn-authz/authentication/) and
[Authorization](/docs/reference/access-authn-authz/authorization/) methods.
-->
<ul>
<li>通过决定<a href="/zh/docs/reference/access-authn-authz/authentication/">身份认证</a>和
<a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权</a>方法来配置用户管理。</li>
</ul>
<!--
- Prepare for application workloads by setting up
[resource limits](/docs/tasks/administer-cluster/manage-resources/),
[DNS autoscaling](/docs/tasks/administer-cluster/dns-horizontal-autoscaling/)
and [service accounts](/docs/reference/access-authn-authz/service-accounts-admin/).
-->
<ul>
<li>通过配置<a href="/zh/docs/tasks/administer-cluster/manage-resources/">资源限制</a>、
<a href="/zh/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">DNS 自动扩缩</a>
和<a href="/zh/docs/reference/access-authn-authz/service-accounts-admin/">服务账号</a>
来为应用负载作准备。</li>
</ul>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a77d3feb6e6d9978f32fa14622642e9a">2.1 - 容器运行时</h1>
    
	<!--
reviewers:
- vincepri
- bart0sh
title: Container runtimes
content_type: concept
weight: 20
-->
<!-- overview -->
<!-- 
You need to install a
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>
into each node in the cluster so that Pods can run there. This page outlines
what is involved and describes related tasks for setting up nodes.
 -->
<p>你需要在集群内每个节点上安装一个
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>
以使 Pod 可以运行在上面。本文概述了所涉及的内容并描述了与节点设置相关的任务。</p>
<!-- body -->
<!-- 
This page lists details for using several common container runtimes with
Kubernetes, on Linux:
 -->
<p>本文列出了在 Linux 上结合 Kubernetes 使用的几种通用容器运行时的详细信息：</p>
<ul>
<li><a href="#containerd">containerd</a></li>
<li><a href="#cri-o">CRI-O</a></li>
<li><a href="#docker">Docker</a></li>
</ul>
<!-- 
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> For other operating systems, look for documentation specific to your platform.
</div>
 -->
<p>提示：对于其他操作系统，请查阅特定于你所使用平台的相关文档。</p>
<!--
## Cgroup drivers
-->
<h2 id="cgroup-驱动程序">Cgroup 驱动程序</h2>
<!--
Control groups are used to constrain resources that are allocated to processes.

When [systemd](https://www.freedesktop.org/wiki/Software/systemd/) is chosen as the init
system for a Linux distribution, the init process generates and consumes a root control group
(`cgroup`) and acts as a cgroup manager.
Systemd has a tight integration with cgroups and allocates a cgroup per systemd unit. It's possible
to configure your container runtime and the kubelet to use `cgroupfs`. Using `cgroupfs` alongside
systemd means that there will be two different cgroup managers.
-->
<p>控制组用来约束分配给进程的资源。</p>
<p>当某个 Linux 系统发行版使用 <a href="https://www.freedesktop.org/wiki/Software/systemd/">systemd</a>
作为其初始化系统时，初始化进程会生成并使用一个 root 控制组 (<code>cgroup</code>), 并充当 cgroup 管理器。
Systemd 与 cgroup 集成紧密，并将为每个 systemd 单元分配一个 cgroup。
你也可以配置容器运行时和 kubelet 使用 <code>cgroupfs</code>。
连同 systemd 一起使用 <code>cgroupfs</code> 意味着将有两个不同的 cgroup 管理器。</p>
<!--
A single cgroup manager simplifies the view of what resources are being allocated
and will by default have a more consistent view of the available and in-use resources.
When there are two cgroup managers on a system, you end up with two views of those resources.
In the field, people have reported cases where nodes that are configured to use `cgroupfs`
for the kubelet and Docker, but `systemd` for the rest of the processes, become unstable under
resource pressure.
-->
<p>单个 cgroup 管理器将简化分配资源的视图，并且默认情况下将对可用资源和使用
中的资源具有更一致的视图。
当有两个管理器共存于一个系统中时，最终将对这些资源产生两种视图。
在此领域人们已经报告过一些案例，某些节点配置让 kubelet 和 docker 使用
<code>cgroupfs</code>，而节点上运行的其余进程则使用 systemd; 这类节点在资源压力下
会变得不稳定。</p>
<!--
Changing the settings such that your container runtime and kubelet use `systemd` as the cgroup driver
stabilized the system. To configure this for Docker, set `native.cgroupdriver=systemd`.
-->
<p>更改设置，令容器运行时和 kubelet 使用 <code>systemd</code> 作为 cgroup 驱动，以此使系统更为稳定。
对于 Docker, 设置 <code>native.cgroupdriver=systemd</code> 选项。</p>
<!--
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p>Changing the cgroup driver of a Node that has joined a cluster is a sensitive operation.
If the kubelet has created Pods using the semantics of one cgroup driver, changing the container
runtime to another cgroup driver can cause errors when trying to re-create the Pod sandbox
for such existing Pods. Restarting the kubelet may not solve such errors.</p>
<p>If you have automation that makes it feasible, replace the node with another using the updated
configuration, or reinstall it using automation.</p>

</div>

-->
<p>注意：更改已加入集群的节点的 cgroup 驱动是一项敏感的操作。
如果 kubelet 已经使用某 cgroup 驱动的语义创建了 pod，更改运行时以使用
别的 cgroup 驱动，当为现有 Pods 重新创建 PodSandbox 时会产生错误。
重启 kubelet 也可能无法解决此类问题。
如果你有切实可行的自动化方案，使用其他已更新配置的节点来替换该节点，
或者使用自动化方案来重新安装。</p>
<!--
## Cgroup v2

Cgroup v2 is the next version of the cgroup Linux API.  Differently than cgroup v1, there is a single
hierarchy instead of a different one for each controller.
-->
<h2 id="cgroup-v2">Cgroup v2</h2>
<p>Cgroup v2 是 cgroup Linux API 的下一个版本。与 cgroup v1 不同的是，
Cgroup v2 只有一个层次结构，而不是每个控制器有一个不同的层次结构。</p>
<!--
The new version offers several improvements over cgroup v1, some of these improvements are:

- cleaner and easier to use API
- safe sub-tree delegation to containers
- newer features like Pressure Stall Information
-->
<p>新版本对 cgroup v1 进行了多项改进，其中一些改进是：</p>
<ul>
<li>更简洁、更易于使用的 API</li>
<li>可将安全子树委派给容器</li>
<li>更新的功能，如压力失速信息（Pressure Stall Information）</li>
</ul>
<!--
Even if the kernel supports a hybrid configuration where some controllers are managed by cgroup v1
and some others by cgroup v2, Kubernetes supports only the same cgroup version to manage all the
controllers.

If systemd doesn't use cgroup v2 by default, you can configure the system to use it by adding
`systemd.unified_cgroup_hierarchy=1` to the kernel command line.
-->
<p>尽管内核支持混合配置，即其中一些控制器由 cgroup v1 管理，另一些由 cgroup v2 管理，
Kubernetes 仅支持使用同一 cgroup 版本来管理所有控制器。</p>
<p>如果 systemd 默认不使用 cgroup v2，你可以通过在内核命令行中添加
<code>systemd.unified_cgroup_hierarchy=1</code> 来配置系统去使用它。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># dnf install -y grubby &amp;&amp; \</span>
  sudo grubby <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --update-kernel<span style="color:#666">=</span>ALL <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --args<span style="color:#666">=</span><span style="color:#b44">&#34;systemd.unified_cgroup_hierarchy=1&#34;</span>
</code></pre></div><!--
To apply the configuration, it is necessary to reboot the node.

There should not be any noticeable difference in the user experience when switching to cgroup v2, unless
users are accessing the cgroup file system directly, either on the node or from within the containers.

In order to use it, cgroup v2 must be supported by the CRI runtime as well.
-->
<p>要应用配置，必须重新启动节点。</p>
<p>切换到 cgroup v2 时，用户体验不应有任何明显差异，
除非用户直接在节点上或在容器内访问 cgroup 文件系统。
为了使用它，CRI 运行时也必须支持 cgroup v2。</p>
<!-- 
### Migrating to the `systemd` driver in kubeadm managed clusters
-->
<h3 id="将-kubeadm-托管的集群迁移到-systemd-驱动">将 kubeadm 托管的集群迁移到 <code>systemd</code> 驱动</h3>
<!-- 
Follow this [Migration guide](/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver)
if you wish to migrate to the `systemd` cgroup driver in existing kubeadm managed clusters.
-->
<p>如果你想迁移到现有 kubeadm 托管集群中的 <code>systemd</code> cgroup 驱动程序，
遵循此<a href="/zh/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver">迁移指南</a>。</p>
<!-- 
## Container runtimes
 -->
<h2 id="容器运行时">容器运行时</h2>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<h3 id="containerd">containerd</h3>
<!--
This section contains the necessary steps to use containerd as CRI runtime.

Use the following commands to install Containerd on your system:

Install and configure prerequisites:
-->
<p>本节包含使用 containerd 作为 CRI 运行时的必要步骤。</p>
<p>使用以下命令在系统上安装 Containerd：</p>
<p>安装和配置的先决条件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span style="color:#b44">overlay
</span><span style="color:#b44">br_netfilter
</span><span style="color:#b44">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

<span style="color:#080;font-style:italic"># 设置必需的 sysctl 参数，这些参数在重新启动后仍然存在。</span>
cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style="color:#b44">net.bridge.bridge-nf-call-iptables  = 1
</span><span style="color:#b44">net.ipv4.ip_forward                 = 1
</span><span style="color:#b44">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 应用 sysctl 参数而无需重新启动</span>
sudo sysctl --system
</code></pre></div><!--
Install containerd:
-->
<p>安装 containerd:</p>
<ul class="nav nav-tabs" id="tab-cri-containerd-installation" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tab-cri-containerd-installation-0" role="tab" aria-controls="tab-cri-containerd-installation-0" aria-selected="true">Linux</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-cri-containerd-installation-1" role="tab" aria-controls="tab-cri-containerd-installation-1">Windows (PowerShell)</a></li></ul>
<div class="tab-content" id="tab-cri-containerd-installation"><div id="tab-cri-containerd-installation-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tab-cri-containerd-installation-0">

<p><!--
1. Install the `containerd.io` package from the official Docker repositories.
   Instructions for setting up the Docker repository for your respective Linux distribution and
   installing the `containerd.io` package can be found at
   [Install Docker Engine](https://docs.docker.com/engine/install/#server).
-->
<ol>
<li>从官方Docker仓库安装 <code>containerd.io</code> 软件包。可以在
<a href="https://docs.docker.com/engine/install/#server">安装 Docker 引擎</a>
中找到有关为各自的 Linux 发行版设置 Docker 存储库和安装 <code>containerd.io</code>
软件包的说明。</li>
</ol>
<!--
2. Configure containerd:
-->
<ol start="2">
<li>
<p>配置 containerd：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
</code></pre></div></li>
</ol>
<!--
3. Restart containerd:
-->
<ol start="3">
<li>
<p>重新启动 containerd:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl restart containerd
</code></pre></div></li>
</ol>
</div>
  <div id="tab-cri-containerd-installation-1" class="tab-pane" role="tabpanel" aria-labelledby="tab-cri-containerd-installation-1">

<p><!--
Start a Powershell session, set `$Version` to the desired version (ex: `$Version=1.4.3`),
and then run the following commands:
-->
<p>启动 Powershell 会话，将 <code>$Version</code> 设置为所需的版本（例如：<code>$Version=1.4.3</code>），
然后运行以下命令：</p>
<!--
1. Download containerd:
-->
<ol>
<li>
<p>下载 containerd：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">curl.exe -L https<span style="">:</span>//github.com/containerd/containerd/releases/download/v<span style="color:#b8860b">$Version</span>/containerd-<span style="color:#b8860b">$Version</span>-windows-amd64.tar.gz -o <span style="color:#a2f">containerd-windows</span>-amd64.tar.gz
tar.exe xvf .\<span style="color:#a2f">containerd-windows</span>-amd64.tar.gz
</code></pre></div></li>
</ol>
<!--
2. Extract and configure:
-->
<ol start="2">
<li>
<p>提取并配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">Copy-Item</span> -Path <span style="color:#b44">&#34;.\bin\&#34;</span> -Destination <span style="color:#b44">&#34;$Env:ProgramFiles\containerd&#34;</span> -Recurse -Force
<span style="color:#a2f">cd </span><span style="color:#b8860b">$Env:ProgramFiles</span>\containerd\
.\containerd.exe config <span style="color:#a2f;font-weight:bold">default</span> | <span style="color:#a2f">Out-File</span> config.toml -Encoding ascii

<span style="color:#080;font-style:italic"># 检查配置。根据你的配置，可能需要调整：</span>
<span style="color:#080;font-style:italic"># - sandbox_image (Kubernetes pause 镜像)</span>
<span style="color:#080;font-style:italic"># - cni bin_dir 和 conf_dir 位置</span>
<span style="color:#a2f">Get-Content</span> config.toml

<span style="color:#080;font-style:italic"># (可选 - 不过强烈建议) 禁止 Windows Defender 扫描 containerd</span>
<span style="color:#a2f">Add-MpPreference</span> -ExclusionProcess <span style="color:#b44">&#34;$Env:ProgramFiles\containerd\containerd.exe&#34;</span>
</code></pre></div></li>
</ol>
<!--
3. Start containerd:
-->
<ol start="3">
<li>
<p>启动 containerd:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">.\containerd.exe --register-service
<span style="color:#a2f">Start-Service</span> containerd
</code></pre></div></li>
</ol>
</div></div>

<!-- 
#### Using the `systemd` cgroup driver {#containerd-systemd}
--> 
<h4 id="containerd-systemd">使用 <code>systemd</code> cgroup 驱动程序</h4>
<!-- 
To use the `systemd` cgroup driver in `/etc/containerd/config.toml` with `runc`, set
-->
<p>结合 <code>runc</code> 使用 <code>systemd</code> cgroup 驱动，在 <code>/etc/containerd/config.toml</code> 中设置</p>
<pre><code>[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]
  ...
  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
    SystemdCgroup = true
</code></pre><!--
If you apply this change make sure to restart containerd again:
-->
<p>如果您应用此更改，请确保再次重新启动 containerd：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl restart containerd
</code></pre></div><!--
When using kubeadm, manually configure the
[cgroup driver for kubelet](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-control-plane-node).
-->
<p>当使用 kubeadm 时，请手动配置
<a href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-control-plane-node">kubelet 的 cgroup 驱动</a>.</p>
<h3 id="cri-o">CRI-O</h3>
<!--
This section contains the necessary steps to install CRI-O as a container runtime.

Use the following commands to install CRI-O on your system:
-->
<p>本节包含安装 CRI-O 作为容器运行时的必要步骤。</p>
<p>使用以下命令在系统中安装 CRI-O：</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The CRI-O major and minor versions must match the Kubernetes major and minor versions.
For more information, see the [CRI-O compatibility matrix](https://github.com/cri-o/cri-o#compatibility-matrix-cri-o--kubernetes).
-->
<p>CRI-O 的主要以及次要版本必须与 Kubernetes 的主要和次要版本相匹配。
更多信息请查阅
<a href="https://github.com/cri-o/cri-o#compatibility-matrix-cri-o--kubernetes">CRI-O 兼容性列表</a>。
</div>
<!--
Install and configure prerequisites:
-->
<p>安装并配置前置环境：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">
<span style="color:#080;font-style:italic"># 创建 .conf 文件以在启动时加载模块</span>
cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/modules-load.d/crio.conf
</span><span style="color:#b44">overlay
</span><span style="color:#b44">br_netfilter
</span><span style="color:#b44">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

<span style="color:#080;font-style:italic"># 配置 sysctl 参数，这些配置在重启之后仍然起作用</span>
cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style="color:#b44">net.bridge.bridge-nf-call-iptables  = 1
</span><span style="color:#b44">net.ipv4.ip_forward                 = 1
</span><span style="color:#b44">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#b44">EOF</span>

sudo sysctl --system
</code></pre></div><ul class="nav nav-tabs" id="tab-cri-cri-o-installation" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tab-cri-cri-o-installation-0" role="tab" aria-controls="tab-cri-cri-o-installation-0" aria-selected="true">Debian</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-cri-cri-o-installation-1" role="tab" aria-controls="tab-cri-cri-o-installation-1">Ubuntu</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-cri-cri-o-installation-2" role="tab" aria-controls="tab-cri-cri-o-installation-2">CentOS</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-cri-cri-o-installation-3" role="tab" aria-controls="tab-cri-cri-o-installation-3">openSUSE Tumbleweed</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tab-cri-cri-o-installation-4" role="tab" aria-controls="tab-cri-cri-o-installation-4">Fedora</a></li></ul>
<div class="tab-content" id="tab-cri-cri-o-installation"><div id="tab-cri-cri-o-installation-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tab-cri-cri-o-installation-0">

<p><!-- 
To install CRI-O on the following operating systems, set the environment variable `OS`
to the appropriate value from the following table:

| Operating system | `$OS`             |
| ---------------- | ----------------- |
| Debian Unstable  | `Debian_Unstable` |
| Debian Testing   | `Debian_Testing`  |

<br />
Then, set `$VERSION` to the CRI-O version that matches your Kubernetes version.
For instance, if you want to install CRI-O 1.20, set `VERSION=1.20`.
You can pin your installation to a specific release.
To install version 1.20.0, set `VERSION=1.20:1.20.0`.
<br />

Then run
-->
<p>在下列操作系统上安装 CRI-O, 使用下表中合适的值设置环境变量 <code>OS</code>:</p>
<table>
<thead>
<tr>
<th>操作系统</th>
<th><code>$OS</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Debian Unstable</td>
<td><code>Debian_Unstable</code></td>
</tr>
<tr>
<td>Debian Testing</td>
<td><code>Debian_Testing</code></td>
</tr>
</tbody>
</table>
<p><br />
然后，将 <code>$VERSION</code> 设置为与你的 Kubernetes 相匹配的 CRI-O 版本。
例如，如果你要安装 CRI-O 1.20, 请设置 <code>VERSION=1.20</code>.
你也可以安装一个特定的发行版本。
例如要安装 1.20.0 版本，设置 <code>VERSION=1.20.0:1.20.0</code>.
<br /></p>
<p>然后执行</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
</span><span style="color:#b44">deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /
</span><span style="color:#b44">EOF</span>
cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list
</span><span style="color:#b44">deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /
</span><span style="color:#b44">EOF</span>

curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style="color:#b8860b">$VERSION</span>/<span style="color:#b8860b">$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -
curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style="color:#b8860b">$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -

sudo apt-get update
sudo apt-get install cri-o cri-o-runc
</code></pre></div></div>
  <div id="tab-cri-cri-o-installation-1" class="tab-pane" role="tabpanel" aria-labelledby="tab-cri-cri-o-installation-1">

<p><!-- 
To install on the following operating systems, set the environment variable `OS`
to the appropriate field in the following table:

| Operating system | `$OS`             |
| ---------------- | ----------------- |
| Ubuntu 20.04     | `xUbuntu_20.04`   |
| Ubuntu 19.10     | `xUbuntu_19.10`   |
| Ubuntu 19.04     | `xUbuntu_19.04`   |
| Ubuntu 18.04     | `xUbuntu_18.04`   |

<br />
Then, set `$VERSION` to the CRI-O version that matches your Kubernetes version.
For instance, if you want to install CRI-O 1.20, set `VERSION=1.20`.
You can pin your installation to a specific release.
To install version 1.20.0, set `VERSION=1.20:1.20.0`.
<br />

Then run
-->
<p>在下列操作系统上安装 CRI-O, 使用下表中合适的值设置环境变量 <code>OS</code>:</p>
<table>
<thead>
<tr>
<th>操作系统</th>
<th><code>$OS</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Ubuntu 20.04</td>
<td><code>xUbuntu_20.04</code></td>
</tr>
<tr>
<td>Ubuntu 19.10</td>
<td><code>xUbuntu_19.10</code></td>
</tr>
<tr>
<td>Ubuntu 19.04</td>
<td><code>xUbuntu_19.04</code></td>
</tr>
<tr>
<td>Ubuntu 18.04</td>
<td><code>xUbuntu_18.04</code></td>
</tr>
</tbody>
</table>
<p><br />
然后，将 <code>$VERSION</code> 设置为与你的 Kubernetes 相匹配的 CRI-O 版本。
例如，如果你要安装 CRI-O 1.20, 请设置 <code>VERSION=1.20</code>.
你也可以安装一个特定的发行版本。
例如要安装 1.20.0 版本，设置 <code>VERSION=1.20:1.20.0</code>.
<br /></p>
<p>然后执行</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
</span><span style="color:#b44">deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /
</span><span style="color:#b44">EOF</span>
cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list
</span><span style="color:#b44">deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /
</span><span style="color:#b44">EOF</span>

curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style="color:#b8860b">$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -
curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style="color:#b8860b">$VERSION</span>/<span style="color:#b8860b">$OS</span>/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers-cri-o.gpg add -

sudo apt-get update
sudo apt-get install cri-o cri-o-runc
</code></pre></div></div>
  <div id="tab-cri-cri-o-installation-2" class="tab-pane" role="tabpanel" aria-labelledby="tab-cri-cri-o-installation-2">

<p><!-- 
To install on the following operating systems, set the environment variable `OS`
to the appropriate field in the following table:

| Operating system | `$OS`             |
| ---------------- | ----------------- |
| Centos 8         | `CentOS_8`        |
| Centos 8 Stream  | `CentOS_8_Stream` |
| Centos 7         | `CentOS_7`        |

<br />
Then, set `$VERSION` to the CRI-O version that matches your Kubernetes version.
For instance, if you want to install CRI-O 1.20, set `VERSION=1.20`.
You can pin your installation to a specific release.
To install version 1.20.0, set `VERSION=1.20:1.20.0`.
<br />

Then run
-->
<p>在下列操作系统上安装 CRI-O, 使用下表中合适的值设置环境变量 <code>OS</code>:</p>
<table>
<thead>
<tr>
<th>操作系统</th>
<th><code>$OS</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Centos 8</td>
<td><code>CentOS_8</code></td>
</tr>
<tr>
<td>Centos 8 Stream</td>
<td><code>CentOS_8_Stream</code></td>
</tr>
<tr>
<td>Centos 7</td>
<td><code>CentOS_7</code></td>
</tr>
</tbody>
</table>
<p><br />
然后，将 <code>$VERSION</code> 设置为与你的 Kubernetes 相匹配的 CRI-O 版本。
例如，如果你要安装 CRI-O 1.20, 请设置 <code>VERSION=1.20</code>.
你也可以安装一个特定的发行版本。
例如要安装 1.20.0 版本，设置 <code>VERSION=1.20:1.20.0</code>.
<br /></p>
<p>然后执行</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style="color:#b8860b">$OS</span>/devel:kubic:libcontainers:stable.repo
sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:<span style="color:#b8860b">$VERSION</span>.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style="color:#b8860b">$VERSION</span>/<span style="color:#b8860b">$OS</span>/devel:kubic:libcontainers:stable:cri-o:<span style="color:#b8860b">$VERSION</span>.repo
sudo yum install cri-o
</code></pre></div></div>
  <div id="tab-cri-cri-o-installation-3" class="tab-pane" role="tabpanel" aria-labelledby="tab-cri-cri-o-installation-3">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo zypper install cri-o
</code></pre></div></div>
  <div id="tab-cri-cri-o-installation-4" class="tab-pane" role="tabpanel" aria-labelledby="tab-cri-cri-o-installation-4">

<p><!-- 
Set `$VERSION` to the CRI-O version that matches your Kubernetes version.
For instance, if you want to install CRI-O 1.20, `VERSION=1.20`.

You can find available versions with:
```shell
sudo dnf module list cri-o
```
CRI-O does not support pinning to specific releases on Fedora.

Then run
-->
<p>将 <code>$VERSION</code> 设置为与你的 Kubernetes 相匹配的 CRI-O 版本。
例如，如果要安装 CRI-O 1.20，请设置 <code>VERSION=1.20</code>。
你可以用下列命令查找可用的版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo dnf module list cri-o
</code></pre></div><p>CRI-O 不支持在 Fedora 上固定到特定的版本。</p>
<p>然后执行</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo dnf module <span style="color:#a2f">enable</span> cri-o:<span style="color:#b8860b">$VERSION</span>
sudo dnf install cri-o --now
</code></pre></div></div></div>

<!-- 
Start CRI-O:
-->
<p>启动 CRI-O：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl daemon-reload
sudo systemctl <span style="color:#a2f">enable</span> crio --now
</code></pre></div><!--
Refer to the [CRI-O installation guide](https://github.com/cri-o/cri-o/blob/master/install.md)
for more information.
 -->
<p>参阅<a href="https://github.com/cri-o/cri-o/blob/master/install.md">CRI-O 安装指南</a>
了解进一步的详细信息。</p>
<!--
#### cgroup driver

CRI-O uses the systemd cgroup driver per default. To switch to the `cgroupfs`
cgroup driver, either edit `/etc/crio/crio.conf` or place a drop-in
configuration in `/etc/crio/crio.conf.d/02-cgroup-manager.conf`, for example:
-->
<h4 id="cgroup-驱动">cgroup 驱动</h4>
<p>默认情况下，CRI-O 使用 systemd cgroup 驱动程序。要切换到 <code>cgroupfs</code>
驱动程序，或者编辑 <code>/ etc / crio / crio.conf</code> 或放置一个插件
在 <code>/etc/crio/crio.conf.d/02-cgroup-manager.conf</code> 中的配置，例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-toml" data-lang="toml">[crio.runtime]
conmon_cgroup = <span style="color:#b44">&#34;pod&#34;</span>
cgroup_manager = <span style="color:#b44">&#34;cgroupfs&#34;</span>
</code></pre></div><!-- 
Please also note the changed `conmon_cgroup`, which has to be set to the value
`pod` when using CRI-O with `cgroupfs`. It is generally necessary to keep the
cgroup driver configuration of the kubelet (usually done via kubeadm) and CRI-O
in sync.
 -->
<p>另请注意更改后的 <code>conmon_cgroup</code>，将 CRI-O 与 <code>cgroupfs</code> 一起使用时，
必须将其设置为 <code>pod</code>。通常有必要保持 kubelet 的 cgroup 驱动程序配置
（通常透过 kubeadm 完成）和 CRI-O 一致。</p>
<h3 id="docker">Docker</h3>
<!--
1. On each of your nodes, install the Docker for your Linux distribution as per
   [Install Docker Engine](https://docs.docker.com/engine/install/#server).
   You can find the latest validated version of Docker in this
   [dependencies](https://git.k8s.io/kubernetes/build/dependencies.yaml) file.
 -->
<ol>
<li>在每个节点上，根据<a href="https://docs.docker.com/engine/install/#server">安装 Docker 引擎</a>
为你的 Linux 发行版安装 Docker。
你可以在此文件中找到最新的经过验证的 Docker 版本
<a href="https://git.k8s.io/kubernetes/build/dependencies.yaml">依赖关系</a>。</li>
</ol>
<!--
2. Configure the Docker daemon, in particular to use systemd for the management of the container’s cgroups.
 -->
<ol start="2">
<li>
<p>配置 Docker 守护程序，尤其是使用 systemd 来管理容器的 cgroup。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo mkdir /etc/docker
cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/docker/daemon.json
</span><span style="color:#b44">{
</span><span style="color:#b44">  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span><span style="color:#b44">  &#34;log-driver&#34;: &#34;json-file&#34;,
</span><span style="color:#b44">  &#34;log-opts&#34;: {
</span><span style="color:#b44">    &#34;max-size&#34;: &#34;100m&#34;
</span><span style="color:#b44">  },
</span><span style="color:#b44">  &#34;storage-driver&#34;: &#34;overlay2&#34;
</span><span style="color:#b44">}
</span><span style="color:#b44">EOF</span>
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
   `overlay2` is the preferred storage driver for systems running Linux kernel version 4.0 or higher,
   or RHEL or CentOS using version 3.10.0-514 and above.
   -->
<p>对于运行 Linux 内核版本 4.0 或更高版本，或使用 3.10.0-51 及更高版本的 RHEL
或 CentOS 的系统，<code>overlay2</code>是首选的存储驱动程序。
</div>
</li>
</ol>
<!--
3. Restart Docker and enable on boot:
-->
<ol start="3">
<li>
<p>重新启动 Docker 并在启动时启用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl <span style="color:#a2f">enable</span> docker
sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre></div></li>
</ol>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
For more information refer to
  - [Configure the Docker daemon](https://docs.docker.com/config/daemon/)
  - [Control Docker with systemd](https://docs.docker.com/config/daemon/systemd/)
-->
<p>有关更多信息，请参阅</p>
<ul>
<li><a href="https://docs.docker.com/config/daemon/">配置 Docker 守护程序</a></li>
<li><a href="https://docs.docker.com/config/daemon/systemd/">使用 systemd 控制 Docker</a></li>
</ul>

</div>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d2f55eefe7222b7c637875af9c3ec199">2.2 - Turnkey 云解决方案</h1>
    
	<!-- 
---
title: Turnkey Cloud Solutions
content_type: concept
weight: 30
---
-->
<!-- overview -->
<!-- 
This page provides a list of Kubernetes certified solution providers. From each
provider page, you can learn how to install and setup production
ready clusters.
-->
<p>本页列示 Kubernetes 认证解决方案供应商。
在每一个供应商分页，你可以学习如何安装和设置生产就绪的集群。</p>
<!-- body -->





<script>
function updateLandscapeSource(button,shouldUpdateFragment) {
  console.log({button: button,shouldUpdateFragment: shouldUpdateFragment});
  try {
    if(shouldUpdateFragment) {
      window.location.hash = "#"+button.id;
      
    } else {
      var landscapeElements = document.querySelectorAll("#landscape");
      let categories=button.dataset.landscapeTypes;
      let link = "https://landscape.cncf.io/card-mode?category="+encodeURIComponent(categories)+"&grouping=category&embed=yes";
      landscapeElements[0].src = link;
    }
  }
  catch(err) {
    console.log({message: "error handling Landscape switch", error: err})
  }
}


document.addEventListener("DOMContentLoaded", function () {
  let hashChangeHandler = () => {
    if (window.location.hash) {
      let selectedTriggerElements = document.querySelectorAll(".landscape-trigger"+window.location.hash);
      if (selectedTriggerElements.length == 1) {
        landscapeSource = selectedTriggerElements[0];
        console.log("Updating Landscape source based on fragment:", window
          .location
          .hash
          .substring(1));
        updateLandscapeSource(landscapeSource,false);
      }
    }
  }
  var landscapeTriggerElements = document.querySelectorAll(".landscape-trigger");
  landscapeTriggerElements.forEach(element => {
    element.onclick = function() {
        updateLandscapeSource(element,true);
    };
  });
  var landscapeDefaultElements = document.querySelectorAll(".landscape-trigger.landscape-default");
  if (landscapeDefaultElements.length == 1) {
    let defaultLandscapeSource = landscapeDefaultElements[0];
    updateLandscapeSource(defaultLandscapeSource,false);
  }
  window.addEventListener("hashchange", hashChangeHandler, false);
   
  hashChangeHandler();
});
</script><div id="frameHolder">
  
  <iframe frameborder="0" id="landscape" scrolling="no" src="https://landscape.cncf.io/card-mode?category=certified-kubernetes-hosted&grouping=category&embed=yes" style="width: 1px; min-width: 100%"></iframe>
  
  <script src="https://landscape.cncf.io/iframeResizer.js"></script>
</div>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-00e1646f68aeb89f9722cf6f6cfcad94">2.3 - 使用部署工具安装 Kubernetes</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-a16f59f325a17cdeed324d5c889f7f73">2.3.1 - 使用 kubeadm 引导集群</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-29e59491dd6118b23072dfe9ebb93323">2.3.1.1 - 安装 kubeadm</h1>
    
	<!--
title: Installing kubeadm
content_type: task
weight: 10
card:
  name: setup
  weight: 20
  title: Install the kubeadm setup tool
-->
<!-- overview -->
<!--
<img src="https://raw.githubusercontent.com/kubernetes/kubeadm/master/logos/stacked/color/kubeadm-stacked-color.png" align="right" width="150px">This page shows how to install the `kubeadm` toolbox.
For information on how to create a cluster with kubeadm once you have performed this installation process, see the [Using kubeadm to Create a Cluster](/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/) page.
-->
<p><img src="/images/kubeadm-stacked-color.png" align="right" width="150px">本页面显示如何安装 <code>kubeadm</code> 工具箱。
有关在执行此安装过程后如何使用 kubeadm 创建集群的信息，请参见
<a href="/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">使用 kubeadm 创建集群</a> 页面。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
* A compatible Linux host. The Kubernetes project provides generic instructions for Linux distributions based on Debian and Red Hat, and those distributions without a package manager.
* 2 GB or more of RAM per machine (any less will leave little room for your apps)
* 2 CPUs or more
* Full network connectivity between all machines in the cluster (public or private network is fine)
* Unique hostname, MAC address, and product_uuid for every node. See [here](#verify-mac-address) for more details.
* Certain ports are open on your machines. See [here](#check-required-ports) for more details.
* Swap disabled. You **MUST** disable swap in order for the kubelet to work properly.
-->
<ul>
<li>一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux
发行版以及一些不提供包管理器的发行版提供通用的指令</li>
<li>每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)</li>
<li>2 CPU 核或更多</li>
<li>集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)</li>
<li>节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见<a href="#verify-mac-address">这里</a>了解更多详细信息。</li>
<li>开启机器上的某些端口。请参见<a href="#check-required-ports">这里</a> 了解更多详细信息。</li>
<li>禁用交换分区。为了保证 kubelet 正常工作，你 <strong>必须</strong> 禁用交换分区。</li>
</ul>
<!-- steps -->
<!--
## Verify the MAC address and product_uuid are unique for every node

* You can get the MAC address of the network interfaces using the command `ip link` or `ifconfig -a`
* The product_uuid can be checked by using the command `sudo cat /sys/class/dmi/id/product_uuid`

It is very likely that hardware devices will have unique addresses, although some virtual machines may have
identical values. Kubernetes uses these values to uniquely identify the nodes in the cluster.
If these values are not unique to each node, the installation process
may [fail](https://github.com/kubernetes/kubeadm/issues/31).
-->
<h2 id="verify-mac-address">确保每个节点上 MAC 地址和 product_uuid 的唯一性   </h2>
<ul>
<li>你可以使用命令 <code>ip link</code> 或 <code>ifconfig -a</code> 来获取网络接口的 MAC 地址</li>
<li>可以使用 <code>sudo cat /sys/class/dmi/id/product_uuid</code> 命令对 product_uuid 校验</li>
</ul>
<p>一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。
Kubernetes 使用这些值来唯一确定集群中的节点。
如果这些值在每个节点上不唯一，可能会导致安装
<a href="https://github.com/kubernetes/kubeadm/issues/31">失败</a>。</p>
<!--
## Check network adapters

If you have more than one network adapter, and your Kubernetes components are not reachable on the default
route, we recommend you add IP route(s) so Kubernetes cluster addresses go via the appropriate adapter.
-->
<h2 id="检查网络适配器">检查网络适配器</h2>
<p>如果你有一个以上的网络适配器，同时你的 Kubernetes 组件通过默认路由不可达，我们建议你预先添加 IP 路由规则，这样 Kubernetes 集群就可以通过对应的适配器完成连接。</p>
<!--
## Letting iptables see bridged traffic

Make sure that the `br_netfilter` module is loaded. This can be done by running `lsmod | grep br_netfilter`. To load it explicitly call `sudo modprobe br_netfilter`.

As a requirement for your Linux Node's iptables to correctly see bridged traffic, you should ensure `net.bridge.bridge-nf-call-iptables` is set to 1 in your `sysctl` config, e.g.
-->
<h2 id="允许-iptables-检查桥接流量">允许 iptables 检查桥接流量</h2>
<p>确保 <code>br_netfilter</code> 模块被加载。这一操作可以通过运行 <code>lsmod | grep br_netfilter</code>
来完成。若要显式加载该模块，可执行 <code>sudo modprobe br_netfilter</code>。</p>
<p>为了让你的 Linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的
<code>sysctl</code> 配置中将 <code>net.bridge.bridge-nf-call-iptables</code> 设置为 1。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span><span style="color:#b44">br_netfilter
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span><span style="color:#b44">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#b44">net.bridge.bridge-nf-call-iptables = 1
</span><span style="color:#b44">EOF</span>
sudo sysctl --system
</code></pre></div><!--
For more details please see the [Network Plugin Requirements](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements) page.
-->
<p>更多的相关细节可查看<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements">网络插件需求</a>页面。</p>
<!--
## Check required ports
These
[required ports](/docs/reference/ports-and-protocols/)
need to be open in order for Kubernetes components to communicate with each other. You can use tools like netcat to check if a port is open. For example:
-->
<h2 id="check-required-ports">检查所需端口</h2>
<p>启用这些<a href="/zh/docs/reference/ports-and-protocols/">必要的端口</a>后才能使 Kubernetes 的各组件相互通信。可以使用 netcat 之类的工具来检查端口是否启用，例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">nc 127.0.0.1 <span style="color:#666">6443</span>
</code></pre></div><!--
The pod network plugin you use (see below) may also require certain ports to be
open. Since this differs with each pod network plugin, please see the
documentation for the plugins about what port(s) those need.
-->
<p>你使用的 Pod 网络插件 (详见后续章节) 也可能需要开启某些特定端口。由于各个 Pod 网络插件的功能都有所不同，
请参阅他们各自文档中对端口的要求。</p>
<!--
## Installing runtime {#installing-runtime}

To run containers in Pods, Kubernetes uses a
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>.
-->
<h2 id="installing-runtime">安装 runtime</h2>
<p>为了在 Pod 中运行容器，Kubernetes 使用
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时（Container Runtime）'>容器运行时（Container Runtime）</a>。</p>
<ul class="nav nav-tabs" id="container-runtimes" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#container-runtimes-0" role="tab" aria-controls="container-runtimes-0" aria-selected="true">Linux 节点</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#container-runtimes-1" role="tab" aria-controls="container-runtimes-1">其它操作系统</a></li></ul>
<div class="tab-content" id="container-runtimes"><div id="container-runtimes-0" class="tab-pane show active" role="tabpanel" aria-labelledby="container-runtimes-0">

<p><!--
By default, Kubernetes uses the
<a class='glossary-tooltip' title='一组与 kubelet 集成的容器运行时 API' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/components/#container-runtime' target='_blank' aria-label='Container Runtime Interface'>Container Runtime Interface</a> (CRI)
to interface with your chosen container runtime.

If you don't specify a runtime, kubeadm automatically tries to detect an installed
container runtime by scanning through a list of well known Unix domain sockets.
The following table lists container runtimes that kubeadm looks for, and their associated socket paths:

| Runtime    | Domain Socket                   |
|------------|---------------------------------|
| Docker     | /var/run/dockershim.sock            |
| containerd | /run/containerd/containerd.sock |
| CRI-O      | /var/run/crio/crio.sock         |
-->
<p>默认情况下，Kubernetes 使用
<a class='glossary-tooltip' title='一组与 kubelet 集成的容器运行时 API' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/components/#container-runtime' target='_blank' aria-label='容器运行时接口（Container Runtime Interface，CRI）'>容器运行时接口（Container Runtime Interface，CRI）</a>
来与你所选择的容器运行时交互。</p>
<p>如果你不指定运行时，则 kubeadm 会自动尝试检测到系统上已经安装的运行时，
方法是扫描一组众所周知的 Unix 域套接字。
下面的表格列举了一些 kubeadm 查找的容器运行时及其对应的套接字路径：</p>
<table>
<thead>
<tr>
<th>运行时</th>
<th>域套接字</th>
</tr>
</thead>
<tbody>
<tr>
<td>Docker Engine</td>
<td><code>/var/run/dockershim.sock</code></td>
</tr>
<tr>
<td>containerd</td>
<td><code>/run/containerd/containerd.sock</code></td>
</tr>
<tr>
<td>CRI-O</td>
<td><code>/var/run/crio/crio.sock</code></td>
</tr>
</tbody>
</table>
<!--
<br />
If both Docker Engine and containerd are detected, kubeadm will give precedence to Docker Engine. This is
needed because Docker 18.09 ships with containerd and both are detectable even if you only
installed Docker.
**If any other two or more runtimes are detected, kubeadm exits with an error.**

The kubelet can integrate with Docker Engine using the deprecated `dockershim` adapter (the dockershim is part of the kubelet itself).

See [container runtimes](/docs/setup/production-environment/container-runtimes/)
for more information.
-->
<br/>
如果同时检测到 Docker Engine 和 containerd，kubeadm 将优先考虑 Docker Engine。
这是必然的，因为 Docker 18.09 附带了 containerd 并且两者都是可以检测到的，
即使你仅安装了 Docker。
**如果检测到其他两个或多个运行时，kubeadm 输出错误信息并退出。**
<p>kubelet 可以使用已弃用的 dockershim 适配器与 Docker Engine 集成（dockershim 是 kubelet 本身的一部分）。</p>
<p>参阅<a href="/zh/docs/setup/production-environment/container-runtimes/">容器运行时</a>
以了解更多信息。</p>
</div>
  <div id="container-runtimes-1" class="tab-pane" role="tabpanel" aria-labelledby="container-runtimes-1">

<p><!--
By default, kubeadm uses <a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a> as the container runtime.
The kubelet can integrate with Docker Engine using the deprecated `dockershim` adapter (the dockershim is part of the kubelet itself).

See [container runtimes](/docs/setup/production-environment/container-runtimes/)
for more information.
-->
<p>默认情况下， kubeadm 使用 <a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a> 作为容器运行时。
kubelet 可以使用已弃用的 dockershim 适配器与 Docker Engine 集成（dockershim 是 kubelet 本身的一部分）。
参阅<a href="/zh/docs/setup/production-environment/container-runtimes/">容器运行时</a>
以了解更多信息。</p>
</div></div>

<!--
## Installing kubeadm, kubelet and kubectl

You will install these packages on all of your machines:

* `kubeadm`: the command to bootstrap the cluster.

* `kubelet`: the component that runs on all of the machines in your cluster
    and does things like starting pods and containers.

* `kubectl`: the command line util to talk to your cluster.

kubeadm **will not** install or manage `kubelet` or `kubectl` for you, so you will
need to ensure they match the version of the Kubernetes control plane you want
kubeadm to install for you. If you do not, there is a risk of a version skew occurring that
can lead to unexpected, buggy behaviour. However, _one_ minor version skew between the
kubelet and the control plane is supported, but the kubelet version may never exceed the API
server version. For example, kubelets running 1.7.0 should be fully compatible with a 1.8.0 API server,
but not vice versa.

For information about installing `kubectl`, see [Install and set up kubectl](/docs/tasks/tools/).
-->
<h2 id="安装-kubeadm-kubelet-和-kubectl">安装 kubeadm、kubelet 和 kubectl</h2>
<p>你需要在每台机器上安装以下的软件包：</p>
<ul>
<li>
<p><code>kubeadm</code>：用来初始化集群的指令。</p>
</li>
<li>
<p><code>kubelet</code>：在集群中的每个节点上用来启动 Pod 和容器等。</p>
</li>
<li>
<p><code>kubectl</code>：用来与集群通信的命令行工具。</p>
</li>
</ul>
<p>kubeadm <strong>不能</strong> 帮你安装或者管理 <code>kubelet</code> 或 <code>kubectl</code>，所以你需要
确保它们与通过 kubeadm 安装的控制平面的版本相匹配。
如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。
然而，控制平面与 kubelet 间的相差一个次要版本不一致是支持的，但 kubelet
的版本不可以超过 API 服务器的版本。
例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。</p>
<p>有关安装 <code>kubectl</code> 的信息，请参阅<a href="/zh/docs/tasks/tools/">安装和设置 kubectl</a>文档。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
These instructions exclude all Kubernetes packages from any system upgrades.
This is because kubeadm and Kubernetes require
[special attention to upgrade](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-14/).
-->
<p>这些指南不包括系统升级时使用的所有 Kubernetes 程序包。这是因为 kubeadm 和 Kubernetes
有<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">特殊的升级注意事项</a>。
</div>


<!--
For more information on version skews, see:

* Kubernetes [version and version-skew policy](/docs/setup/release/version-skew-policy/)
* Kubeadm-specific [version skew policy](/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy)
-->
<p>关于版本偏差的更多信息，请参阅以下文档：</p>
<ul>
<li>Kubernetes <a href="/zh/docs/setup/release/version-skew-policy/">版本与版本间的偏差策略</a></li>
<li>Kubeadm 特定的<a href="/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy">版本偏差策略</a></li>
</ul>
<ul class="nav nav-tabs" id="k8s-install" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-0" role="tab" aria-controls="k8s-install-0" aria-selected="true">基于 Debian 的发行版</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-1" role="tab" aria-controls="k8s-install-1">基于 Red Hat 的发行版</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-2" role="tab" aria-controls="k8s-install-2">无包管理器的情况</a></li></ul>
<div class="tab-content" id="k8s-install"><div id="k8s-install-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-0">

<p><!--
1. Update the `apt` package index and install packages needed to use the Kubernetes `apt` repository:
-->
<ol>
<li>
<p>更新 <code>apt</code> 包索引并安装使用 Kubernetes <code>apt</code> 仓库所需要的包：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
</code></pre></div></li>
</ol>
<!--
2. Download the Google Cloud public signing key:
-->
<ol start="2">
<li>
<p>下载 Google Cloud 公开签名秘钥：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
</code></pre></div></li>
</ol>
<!--
3. Add the Kubernetes `apt` repository:
-->
<ol start="3">
<li>
<p>添加 Kubernetes <code>apt</code> 仓库：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">echo</span> <span style="color:#b44">&#34;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> | sudo tee /etc/apt/sources.list.d/kubernetes.list
</code></pre></div></li>
</ol>
<!--
4. Update `apt` package index, install kubelet, kubeadm and kubectl, and pin their version:
-->
<ol start="4">
<li>
<p>更新 <code>apt</code> 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre></div></li>
</ol>
</div>
  <div id="k8s-install-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#b44">&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
</span><span style="color:#b44">[kubernetes]
</span><span style="color:#b44">name=Kubernetes
</span><span style="color:#b44">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
</span><span style="color:#b44">enabled=1
</span><span style="color:#b44">gpgcheck=1
</span><span style="color:#b44">repo_gpgcheck=1
</span><span style="color:#b44">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style="color:#b44">exclude=kubelet kubeadm kubectl
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 将 SELinux 设置为 permissive 模式（相当于将其禁用）</span>
sudo setenforce <span style="color:#666">0</span>
sudo sed -i <span style="color:#b44">&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config

sudo yum install -y kubelet kubeadm kubectl --disableexcludes<span style="color:#666">=</span>kubernetes

sudo systemctl <span style="color:#a2f">enable</span> --now kubelet
</code></pre></div><!--
  **Note:**

  - Setting SELinux in permissive mode by running `setenforce 0` and `sed ...` effectively disables it.
    This is required to allow containers to access the host filesystem, which is needed by pod networks for example.
    You have to do this until SELinux support is improved in the kubelet.

  - You can leave SELinux enabled if you know how to configure it but it may require settings that are not supported by kubeadm.
  - If the `baseurl` fails because your Red Hat-based distribution cannot interpret `basearch`, replace `\$basearch` with your computer's architecture.
    Type `uname -m` to see that value.
    For example, the `baseurl` URL for `x86_64` could be: `https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64`.
-->
<p><strong>请注意：</strong></p>
<ul>
<li>
<p>通过运行命令 <code>setenforce 0</code> 和 <code>sed ...</code> 将 SELinux 设置为 permissive 模式
可以有效地将其禁用。
这是允许容器访问主机文件系统所必需的，而这些操作时为了例如 Pod 网络工作正常。</p>
<p>你必须这么做，直到 kubelet 做出对 SELinux 的支持进行升级为止。</p>
</li>
<li>
<p>如果你知道如何配置 SELinux 则可以将其保持启用状态，但可能需要设定 kubeadm 不支持的部分配置</p>
</li>
<li>
<p>如果由于该 Red Hat 的发行版无法解析 <code>basearch</code> 导致获取 <code>baseurl</code> 失败，请将 <code>\$basearch</code> 替换为你计算机的架构。
输入 <code>uname -m</code> 以查看该值。
例如，<code>x86_64</code> 的 <code>baseurl</code> URL 可以是：<code>https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</code>。</p>
</li>
</ul>
</div>
  <div id="k8s-install-2" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-2">

<p><!--
Install CNI plugins (required for most pod network):
-->
<p>安装 CNI 插件（大多数 Pod 网络都需要）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#b8860b">CNI_VERSION</span><span style="color:#666">=</span><span style="color:#b44">&#34;v0.8.2&#34;</span>
<span style="color:#b8860b">ARCH</span><span style="color:#666">=</span><span style="color:#b44">&#34;amd64&#34;</span>
sudo mkdir -p /opt/cni/bin
curl -L <span style="color:#b44">&#34;https://github.com/containernetworking/plugins/releases/download/</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CNI_VERSION</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">/cni-plugins-linux-</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">ARCH</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">-</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CNI_VERSION</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">.tgz&#34;</span> | sudo tar -C /opt/cni/bin -xz
</code></pre></div><!--
Define the directory to download command files  
-->
<p>定义要下载命令文件的目录。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The `DOWNLOAD_DIR` variable must be set to a writable directory.
If you are running Flatcar Container Linux, set `DOWNLOAD_DIR=/opt/bin`.
-->
<p><code>DOWNLOAD_DIR</code> 变量必须被设置为一个可写入的目录。
如果你在运行 Flatcar Container Linux，可将 <code>DOWNLOAD_DIR</code> 设置为 <code>/opt/bin</code>。
</div>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#b8860b">DOWNLOAD_DIR</span><span style="color:#666">=</span>/usr/local/bin
sudo mkdir -p <span style="color:#b8860b">$DOWNLOAD_DIR</span>
</code></pre></div><!--
Install crictl (required for kubeadm / Kubelet Container Runtime Interface (CRI))
-->
<p>安装 crictl（kubeadm/kubelet 容器运行时接口（CRI）所需）</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#b8860b">CRICTL_VERSION</span><span style="color:#666">=</span><span style="color:#b44">&#34;v1.22.0&#34;</span>
<span style="color:#b8860b">ARCH</span><span style="color:#666">=</span><span style="color:#b44">&#34;amd64&#34;</span>
curl -L <span style="color:#b44">&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CRICTL_VERSION</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">/crictl-</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CRICTL_VERSION</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">-linux-</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">ARCH</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">.tar.gz&#34;</span> | sudo tar -C <span style="color:#b8860b">$DOWNLOAD_DIR</span> -xz
</code></pre></div><!--
Install `kubeadm`, `kubelet`, `kubectl` and add a `kubelet` systemd service:
-->
<p>安装 <code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code> 并添加 <code>kubelet</code> 系统服务：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#b8860b">RELEASE</span><span style="color:#666">=</span><span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>curl -sSL https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>
<span style="color:#b8860b">ARCH</span><span style="color:#666">=</span><span style="color:#b44">&#34;amd64&#34;</span>
<span style="color:#a2f">cd</span> <span style="color:#b8860b">$DOWNLOAD_DIR</span>
sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">RELEASE</span><span style="color:#b68;font-weight:bold">}</span>/bin/linux/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">ARCH</span><span style="color:#b68;font-weight:bold">}</span>/<span style="color:#666">{</span>kubeadm,kubelet,kubectl<span style="color:#666">}</span>
sudo chmod +x <span style="color:#666">{</span>kubeadm,kubelet,kubectl<span style="color:#666">}</span>

<span style="color:#b8860b">RELEASE_VERSION</span><span style="color:#666">=</span><span style="color:#b44">&#34;v0.4.0&#34;</span>
curl -sSL <span style="color:#b44">&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">RELEASE_VERSION</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> | sed <span style="color:#b44">&#34;s:/usr/bin:</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">DOWNLOAD_DIR</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service
sudo mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL <span style="color:#b44">&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">RELEASE_VERSION</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> | sed <span style="color:#b44">&#34;s:/usr/bin:</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">DOWNLOAD_DIR</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre></div><!--
Enable and start `kubelet`:
-->
<p>激活并启动 <code>kubelet</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl <span style="color:#a2f">enable</span> --now kubelet
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The Flatcar Container Linux distribution mounts the `/usr` directory as a read-only filesystem.
Before bootstrapping your cluster, you need to take additional steps to configure a writable directory.
See the [Kubeadm Troubleshooting guide](/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#usr-mounted-read-only/) to learn how to set up a writable directory.
-->
<p>Flatcar Container Linux 发行版会将 <code>/usr/</code> 目录挂载为一个只读文件系统。
在启动引导你的集群之前，你需要执行一些额外的操作来配置一个可写入的目录。
参见 <a href="/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#usr-mounted-read-only/">kubeadm 故障排查指南</a>
以了解如何配置一个可写入的目录。
</div>
</div></div>

<!--
The kubelet is now restarting every few seconds, as it waits in a crashloop for
kubeadm to tell it what to do.
-->
<p>kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。</p>
<!--
## Configure cgroup driver

Both the container runtime and the kubelet have a property called
["cgroup driver"](/docs/setup/production-environment/container-runtimes/), which is important
for the management of cgroups on Linux machines.
-->
<h2 id="configure-cgroup-driver">配置 cgroup 驱动程序 </h2>
<p>容器运行时和 kubelet 都具有名字为
<a href="/zh/docs/setup/production-environment/container-runtimes/">&quot;cgroup driver&quot;</a>
的属性，该属性对于在 Linux 机器上管理 CGroups 而言非常重要。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
Matching the container runtime and kubelet cgroup drivers is required or otherwise the kubelet process will fail.

See [Configuring a cgroup driver](/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/) for more details.
-->
<p>你需要确保容器运行时和 kubelet 所使用的是相同的 cgroup 驱动，否则 kubelet
进程会失败。</p>
<p>相关细节可参见<a href="/zh/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/">配置 cgroup 驱动</a>。</p>

</div>


<!--
## Troubleshooting

If you are running into difficulties with kubeadm, please consult our [troubleshooting docs](/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/).
-->
<h2 id="troubleshooting">故障排查  </h2>
<p>如果你在使用 kubeadm 时遇到困难，请参阅我们的
<a href="/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/">故障排查文档</a>。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* [Using kubeadm to Create a Cluster](/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)
-->
<ul>
<li><a href="/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">使用 kubeadm 创建集群</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c3689df4b0c61a998e79d91a865aa244">2.3.1.2 - 对 kubeadm 进行故障排查</h1>
    
	<!--
title: Troubleshooting kubeadm
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
As with any program, you might run into an error installing or running kubeadm.
This page lists some common failure scenarios and have provided steps that can help you understand and fix the problem.

If your problem is not listed below, please follow the following steps:

- If you think your problem is a bug with kubeadm:
  - Go to [github.com/kubernetes/kubeadm](https://github.com/kubernetes/kubeadm/issues) and search for existing issues.
  - If no issue exists, please [open one](https://github.com/kubernetes/kubeadm/issues/new) and follow the issue template.

- If you are unsure about how kubeadm works, you can ask on [Slack](http://slack.k8s.io/) in #kubeadm, or open a question on [StackOverflow](https://stackoverflow.com/questions/tagged/kubernetes). Please include
  relevant tags like `#kubernetes` and `#kubeadm` so folks can help you.
-->
<p>与任何程序一样，你可能会在安装或者运行 kubeadm 时遇到错误。
本文列举了一些常见的故障场景，并提供可帮助你理解和解决这些问题的步骤。</p>
<p>如果你的问题未在下面列出，请执行以下步骤：</p>
<ul>
<li>
<p>如果你认为问题是 kubeadm 的错误：</p>
<ul>
<li>转到 <a href="https://github.com/kubernetes/kubeadm/issues">github.com/kubernetes/kubeadm</a> 并搜索存在的问题。</li>
<li>如果没有问题，请 <a href="https://github.com/kubernetes/kubeadm/issues/new">打开</a> 并遵循问题模板。</li>
</ul>
</li>
<li>
<p>如果你对 kubeadm 的工作方式有疑问，可以在 <a href="https://slack.k8s.io/">Slack</a> 上的 #kubeadm 频道提问，
或者在 <a href="https://stackoverflow.com/questions/tagged/kubernetes">StackOverflow</a> 上提问。
请加入相关标签，例如 <code>#kubernetes</code> 和 <code>#kubeadm</code>，这样其他人可以帮助你。</p>
</li>
</ul>
<!-- body -->
<!--
## `ebtables` or some similar executable not found during installation

If you see the following warnings while running `kubeadm init`

```sh
[preflight] WARNING: ebtables not found in system path
[preflight] WARNING: ethtool not found in system path
```

Then you may be missing `ebtables`, `ethtool` or a similar executable on your node. You can install them with the following commands:

- For Ubuntu/Debian users, run `apt install ebtables ethtool`.
- For CentOS/Fedora users, run `yum install ebtables ethtool`.
-->
<h2 id="在安装过程中没有找到-ebtables-或者其他类似的可执行文件">在安装过程中没有找到 <code>ebtables</code> 或者其他类似的可执行文件</h2>
<p>如果在运行 <code>kubeadm init</code> 命令时，遇到以下的警告</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#666">[</span>preflight<span style="color:#666">]</span> WARNING: ebtables not found in system path
<span style="color:#666">[</span>preflight<span style="color:#666">]</span> WARNING: ethtool not found in system path
</code></pre></div><p>那么或许在你的节点上缺失 <code>ebtables</code>、<code>ethtool</code> 或者类似的可执行文件。
你可以使用以下命令安装它们：</p>
<ul>
<li>对于 Ubuntu/Debian 用户，运行 <code>apt install ebtables ethtool</code> 命令。</li>
<li>对于 CentOS/Fedora 用户，运行 <code>yum install ebtables ethtool</code> 命令。</li>
</ul>
<!--
## kubeadm blocks waiting for control plane during installation

If you notice that `kubeadm init` hangs after printing out the following line:

```sh
[apiclient] Created API client, waiting for the control plane to become ready
```
-->
<h2 id="在安装过程中-kubeadm-一直等待控制平面就绪">在安装过程中，kubeadm 一直等待控制平面就绪</h2>
<p>如果你注意到 <code>kubeadm init</code> 在打印以下行后挂起：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#666">[</span>apiclient<span style="color:#666">]</span> Created API client, waiting <span style="color:#a2f;font-weight:bold">for</span> the control plane to become ready
</code></pre></div><!--
This may be caused by a number of problems. The most common are:

- network connection problems. Check that your machine has full network connectivity before continuing.
- the cgroup driver of the container runtime differs from that of the kubelet. To understand how to
configure it properly see [Configuring a cgroup driver](/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/).
- control plane containers are crashlooping or hanging. You can check this by running `docker ps`
and investigating each container by running `docker logs`. For other container runtime see
[Debugging Kubernetes nodes with crictl](/docs/tasks/debug-application-cluster/crictl/).
-->
<p>这可能是由许多问题引起的。最常见的是：</p>
<ul>
<li>网络连接问题。在继续之前，请检查你的计算机是否具有全部联通的网络连接。</li>
<li>容器运行时的 cgroup 驱动不同于 kubelet 使用的 cgroup 驱动。要了解如何正确配置 cgroup 驱动，
请参阅<a href="/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/">配置 cgroup 驱动</a>。</li>
<li>控制平面上的 Docker 容器持续进入崩溃状态或（因其他原因）挂起。你可以运行 <code>docker ps</code> 命令来检查以及 <code>docker logs</code> 命令来检视每个容器的运行日志。
对于其他容器运行时，请参阅<a href="/zh/docs/tasks/debug-application-cluster/crictl/">使用 crictl 对 Kubernetes 节点进行调试</a>。</li>
</ul>
<!--
## kubeadm blocks when removing managed containers

The following could happen if Docker halts and does not remove any Kubernetes-managed containers:

```shell
sudo kubeadm reset
```

```console
[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Removing kubernetes-managed containers
(block)
```

A possible solution is to restart the Docker service and then re-run `kubeadm reset`:

```shell
sudo systemctl restart docker.service
sudo kubeadm reset
```

Inspecting the logs for docker may also be useful:

```shell
journalctl -ul docker
```
-->
<h2 id="当删除托管容器时-kubeadm-阻塞">当删除托管容器时 kubeadm 阻塞</h2>
<p>如果 Docker 停止并且不删除 Kubernetes 所管理的所有容器，可能发生以下情况：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo kubeadm reset
</code></pre></div><pre><code class="language-none" data-lang="none">[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in &quot;/var/lib/kubelet&quot;
[reset] Removing kubernetes-managed containers
(block)
</code></pre><p>一个可行的解决方案是重新启动 Docker 服务，然后重新运行 <code>kubeadm reset</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl restart docker.service
sudo kubeadm reset
</code></pre></div><p>检查 docker 的日志也可能有用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">journalctl -ul docker
</code></pre></div><!--
## Pods in `RunContainerError`, `CrashLoopBackOff` or `Error` state

Right after `kubeadm init` there should not be any pods in these states.

- If there are pods in one of these states _right after_ `kubeadm init`, please open an
  issue in the kubeadm repo. `coredns` (or `kube-dns`) should be in the `Pending` state
  until you have deployed the network add-on.
- If you see Pods in the `RunContainerError`, `CrashLoopBackOff` or `Error` state
  after deploying the network add-on and nothing happens to `coredns` (or `kube-dns`),
  it's very likely that the Pod Network add-on that you installed is somehow broken.
  You might have to grant it more RBAC privileges or use a newer version. Please file
  an issue in the Pod Network providers' issue tracker and get the issue triaged there.
- If you install a version of Docker older than 1.12.1, remove the `MountFlags=slave` option
  when booting `dockerd` with `systemd` and restart `docker`. You can see the MountFlags in `/usr/lib/systemd/system/docker.service`.
  MountFlags can interfere with volumes mounted by Kubernetes, and put the Pods in `CrashLoopBackOff` state.
  The error happens when Kubernetes does not find `var/run/secrets/kubernetes.io/serviceaccount` files.
-->
<h2 id="pods-处于-runcontainererror-crashloopbackoff-或者-error-状态">Pods 处于 <code>RunContainerError</code>、<code>CrashLoopBackOff</code> 或者 <code>Error</code> 状态</h2>
<p>在 <code>kubeadm init</code> 命令运行后，系统中不应该有 pods 处于这类状态。</p>
<ul>
<li>
<p>在 <code>kubeadm init</code> 命令执行完后，如果有 pods 处于这些状态之一，请在 kubeadm
仓库提起一个 issue。<code>coredns</code> (或者 <code>kube-dns</code>) 应该处于 <code>Pending</code> 状态，
直到你部署了网络插件为止。</p>
</li>
<li>
<p>如果在部署完网络插件之后，有 Pods 处于 <code>RunContainerError</code>、<code>CrashLoopBackOff</code>
或 <code>Error</code> 状态之一，并且<code>coredns</code> （或者 <code>kube-dns</code>）仍处于 <code>Pending</code> 状态，
那很可能是你安装的网络插件由于某种原因无法工作。你或许需要授予它更多的
RBAC 特权或使用较新的版本。请在 Pod Network 提供商的问题跟踪器中提交问题，
然后在此处分类问题。</p>
</li>
<li>
<p>如果你安装的 Docker 版本早于 1.12.1，请在使用 <code>systemd</code> 来启动 <code>dockerd</code> 和重启 <code>docker</code> 时，
删除 <code>MountFlags=slave</code> 选项。
你可以在 <code>/usr/lib/systemd/system/docker.service</code> 中看到 MountFlags。
MountFlags 可能会干扰 Kubernetes 挂载的卷， 并使 Pods 处于 <code>CrashLoopBackOff</code> 状态。
当 Kubernetes 不能找到 <code>var/run/secrets/kubernetes.io/serviceaccount</code> 文件时会发生错误。</p>
</li>
</ul>
<!--
## `coredns` is stuck in the `Pending` state

This is **expected** and part of the design. kubeadm is network provider-agnostic, so the admin
should [install the pod network add-on](/docs/concepts/cluster-administration/addons/)
of choice. You have to install a Pod Network
before CoreDNS may be deployed fully. Hence the `Pending` state before the network is set up.
-->
<h2 id="coredns-停滞在-pending-状态"><code>coredns</code> 停滞在 <code>Pending</code> 状态</h2>
<p>这一行为是 <strong>预期之中</strong> 的，因为系统就是这么设计的。
kubeadm 的网络供应商是中立的，因此管理员应该选择
<a href="/zh/docs/concepts/cluster-administration/addons/">安装 pod 的网络插件</a>。
你必须完成 Pod 的网络配置，然后才能完全部署 CoreDNS。
在网络被配置好之前，DNS 组件会一直处于 <code>Pending</code> 状态。</p>
<!--
## `HostPort` services do not work

The `HostPort` and `HostIP` functionality is available depending on your Pod Network
provider. Please contact the author of the Pod Network add-on to find out whether
`HostPort` and `HostIP` functionality are available.

Calico, Canal, and Flannel CNI providers are verified to support HostPort.

For more information, see the [CNI portmap documentation](https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md).

If your network provider does not support the portmap CNI plugin, you may need to use the [NodePort feature of
services](/docs/concepts/services-networking/service/#type-nodeport) or use `HostNetwork=true`.
-->
<h2 id="hostport-服务无法工作"><code>HostPort</code> 服务无法工作</h2>
<p>此 <code>HostPort</code> 和 <code>HostIP</code> 功能是否可用取决于你的 Pod 网络配置。请联系 Pod 网络插件的作者，
以确认 <code>HostPort</code> 和 <code>HostIP</code> 功能是否可用。</p>
<p>已验证 Calico、Canal 和 Flannel CNI 驱动程序支持 HostPort。</p>
<p>有关更多信息，请参考 <a href="https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md">CNI portmap 文档</a>.</p>
<p>如果你的网络提供商不支持 portmap CNI 插件，你或许需要使用
<a href="/zh/docs/concepts/services-networking/service/#type-nodeport">NodePort 服务的功能</a>
或者使用 <code>HostNetwork=true</code>。</p>
<!--
## Pods are not accessible via their Service IP

- Many network add-ons do not yet enable [hairpin mode](/docs/tasks/debug-application-cluster/debug-service/#a-pod-fails-to-reach-itself-via-the-service-ip)
  which allows pods to access themselves via their Service IP. This is an issue related to
  [CNI](https://github.com/containernetworking/cni/issues/476). Please contact the network
  add-on provider to get the latest status of their support for hairpin mode.

- If you are using VirtualBox (directly or via Vagrant), you will need to
  ensure that `hostname -i` returns a routable IP address. By default the first
  interface is connected to a non-routable host-only network. A work around
  is to modify `/etc/hosts`, see this [Vagrantfile](https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11)
  for an example.
-->
<h2 id="无法通过其服务-ip-访问-pod">无法通过其服务 IP 访问 Pod</h2>
<ul>
<li>
<p>许多网络附加组件尚未启用 <a href="/zh/docs/tasks/debug-application-cluster/debug-service/#a-pod-fails-to-reach-itself-via-the-service-ip">hairpin 模式</a>
该模式允许 Pod 通过其服务 IP 进行访问。这是与 <a href="https://github.com/containernetworking/cni/issues/476">CNI</a> 有关的问题。
请与网络附加组件提供商联系，以获取他们所提供的 hairpin 模式的最新状态。</p>
</li>
<li>
<p>如果你正在使用 VirtualBox (直接使用或者通过 Vagrant 使用)，你需要
确保 <code>hostname -i</code> 返回一个可路由的 IP 地址。默认情况下，第一个接口连接不能路由的仅主机网络。
解决方法是修改 <code>/etc/hosts</code>，请参考示例 <a href="https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11">Vagrantfile</a>。</p>
</li>
</ul>
<!--
## TLS certificate errors

The following error indicates a possible certificate mismatch.

```none
# kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
```

- Verify that the `$HOME/.kube/config` file contains a valid certificate, and
  regenerate a certificate if necessary. The certificates in a kubeconfig file
  are base64 encoded. The `base64 -d` command can be used to decode the certificate
  and `openssl x509 -text -noout` can be used for viewing the certificate information.
- Unset the `KUBECONFIG` environment variable using:

  ```sh
  unset KUBECONFIG
  ```

  Or set it to the default `KUBECONFIG` location:

  ```sh
  export KUBECONFIG=/etc/kubernetes/admin.conf
  ```

- Another workaround is to overwrite the existing `kubeconfig` for the "admin" user:

  ```sh
  mv  $HOME/.kube $HOME/.kube.bak
  mkdir $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
  ```
-->
<h2 id="tls-证书错误">TLS 证书错误</h2>
<p>以下错误指出证书可能不匹配。</p>
<pre><code class="language-none" data-lang="none"># kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;kubernetes&quot;)
</code></pre><ul>
<li>
<p>验证 <code>$HOME/.kube/config</code> 文件是否包含有效证书，并
在必要时重新生成证书。在 kubeconfig 文件中的证书是 base64 编码的。
该 <code>base64 -d</code> 命令可以用来解码证书，<code>openssl x509 -text -noout</code> 命令
可以用于查看证书信息。</p>
</li>
<li>
<p>使用如下方法取消设置 <code>KUBECONFIG</code> 环境变量的值：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">unset</span> KUBECONFIG
</code></pre></div><p>或者将其设置为默认的 <code>KUBECONFIG</code> 位置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">export</span> <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>/etc/kubernetes/admin.conf
</code></pre></div></li>
<li>
<p>另一个方法是覆盖 <code>kubeconfig</code> 的现有用户 &quot;管理员&quot; ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">mv  <span style="color:#b8860b">$HOME</span>/.kube <span style="color:#b8860b">$HOME</span>/.kube.bak
mkdir <span style="color:#b8860b">$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span style="color:#b8860b">$HOME</span>/.kube/config
sudo chown <span style="color:#a2f;font-weight:bold">$(</span>id -u<span style="color:#a2f;font-weight:bold">)</span>:<span style="color:#a2f;font-weight:bold">$(</span>id -g<span style="color:#a2f;font-weight:bold">)</span> <span style="color:#b8860b">$HOME</span>/.kube/config
</code></pre></div></li>
</ul>
<!--
## Default NIC When using flannel as the pod network in Vagrant

The following error might indicate that something was wrong in the pod network:

```sh
Error from server (NotFound): the server could not find the requested resource
```

- If you're using flannel as the pod network inside Vagrant, then you will have to specify the default interface name for flannel.

  Vagrant typically assigns two interfaces to all VMs. The first, for which all hosts are assigned the IP address `10.0.2.15`, is for external traffic that gets NATed.

  This may lead to problems with flannel, which defaults to the first interface on a host. This leads to all hosts thinking they have the same public IP address. To prevent this, pass the `-iface eth1` flag to flannel so that the second interface is chosen.
-->
<!--
## Kubelet client certificate rotation fails {#kubelet-client-cert}

By default, kubeadm configures a kubelet with automatic rotation of client certificates by using the `/var/lib/kubelet/pki/kubelet-client-current.pem` symlink specified in `/etc/kubernetes/kubelet.conf`.
If this rotation process fails you might see errors such as `x509: certificate has expired or is not yet valid`
in kube-apiserver logs. To fix the issue you must follow these steps:
-->
<h2 id="kubelet-client-cert">Kubelet 客户端证书轮换失败  </h2>
<p>默认情况下，kubeadm 使用 <code>/etc/kubernetes/kubelet.conf</code> 中指定的 <code>/var/lib/kubelet/pki/kubelet-client-current.pem</code> 符号链接
来配置 kubelet 自动轮换客户端证书。如果此轮换过程失败，你可能会在 kube-apiserver 日志中看到
诸如 <code>x509: certificate has expired or is not yet valid</code> 之类的错误。要解决此问题，你必须执行以下步骤：</p>
<!--
1. Backup and delete `/etc/kubernetes/kubelet.conf` and `/var/lib/kubelet/pki/kubelet-client*` from the failed node.
1. From a working control plane node in the cluster that has `/etc/kubernetes/pki/ca.key` execute
`kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE > kubelet.conf`.
`$NODE` must be set to the name of the existing failed node in the cluster.
Modify the resulted `kubelet.conf` manually to adjust the cluster name and server endpoint,
or pass `kubeconfig user --config` (it accepts `InitConfiguration`). If your cluster does not have
the `ca.key` you must sign the embedded certificates in the `kubelet.conf` externally.
-->
<ol>
<li>从故障节点备份和删除 <code>/etc/kubernetes/kubelet.conf</code> 和 <code>/var/lib/kubelet/pki/kubelet-client*</code>。</li>
<li>在集群中具有 <code>/etc/kubernetes/pki/ca.key</code> 的、正常工作的控制平面节点上
执行 <code>kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE &gt; kubelet.conf</code>。
<code>$NODE</code> 必须设置为集群中现有故障节点的名称。
手动修改生成的 <code>kubelet.conf</code> 以调整集群名称和服务器端点，
或传递 <code>kubeconfig user --config</code>（此命令接受 <code>InitConfiguration</code>）。
如果你的集群没有 <code>ca.key</code>，你必须在外部对 <code>kubelet.conf</code> 中的嵌入式证书进行签名。</li>
</ol>
<!--
1. Copy this resulted `kubelet.conf` to `/etc/kubernetes/kubelet.conf` on the failed node.
1. Restart the kubelet (`systemctl restart kubelet`) on the failed node and wait for
`/var/lib/kubelet/pki/kubelet-client-current.pem` to be recreated.
-->
<ol start="3">
<li>将得到的 <code>kubelet.conf</code> 文件复制到故障节点上，作为 <code>/etc/kubernetes/kubelet.conf</code>。</li>
<li>在故障节点上重启 kubelet（<code>systemctl restart kubelet</code>），等待 <code>/var/lib/kubelet/pki/kubelet-client-current.pem</code> 重新创建。</li>
</ol>
<!--
1. Manually edit the `kubelet.conf` to point to the rotated kubelet client certificates, by replacing
`client-certificate-data` and `client-key-data` with:
-->
<ol start="5">
<li>
<p>手动编辑 <code>kubelet.conf</code> 指向轮换的 kubelet 客户端证书，方法是将 <code>client-certificate-data</code> 和 <code>client-key-data</code> 替换为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">client-certificate</span>:<span style="color:#bbb"> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">client-key</span>:<span style="color:#bbb"> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
1. Restart the kubelet.
1. Make sure the node becomes `Ready`.
-->
<ol start="6">
<li>重新启动 kubelet。</li>
<li>确保节点状况变为 <code>Ready</code>。</li>
</ol>
<h2 id="在-vagrant-中使用-flannel-作为-pod-网络时的默认-nic">在 Vagrant 中使用 flannel 作为 pod 网络时的默认 NIC</h2>
<p>以下错误可能表明 Pod 网络中出现问题：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">Error from server <span style="color:#666">(</span>NotFound<span style="color:#666">)</span>: the server could not find the requested resource
</code></pre></div><ul>
<li>
<p>如果你正在 Vagrant 中使用 flannel 作为 pod 网络，则必须指定 flannel 的默认接口名称。</p>
<p>Vagrant 通常为所有 VM 分配两个接口。第一个为所有主机分配了 IP 地址 <code>10.0.2.15</code>，用于获得 NATed 的外部流量。</p>
<p>这可能会导致 flannel 出现问题，它默认为主机上的第一个接口。这导致所有主机认为它们具有
相同的公共 IP 地址。为防止这种情况，传递 <code>--iface eth1</code> 标志给 flannel 以便选择第二个接口。</p>
</li>
</ul>
<!--
## Non-public IP used for containers

In some situations `kubectl logs` and `kubectl run` commands may return with the following errors in an otherwise functional cluster:

```sh
Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
```

- This may be due to Kubernetes using an IP that can not communicate with other IPs on the seemingly same subnet, possibly by policy of the machine provider.
- Digital Ocean assigns a public IP to `eth0` as well as a private one to be used internally as anchor for their floating IP feature, yet `kubelet` will pick the latter as the node's `InternalIP` instead of the public one.

  Use `ip addr show` to check for this scenario instead of `ifconfig` because `ifconfig` will not display the offending alias IP address. Alternatively an API endpoint specific to Digital Ocean allows to query for the anchor IP from the droplet:

  ```sh
  curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
  ```

  The workaround is to tell `kubelet` which IP to use using `--node-ip`.
  When using DigitalOcean, it can be the public one (assigned to `eth0`) or
  the private one (assigned to `eth1`) should you want to use the optional
  private network. The `kubeletExtraArgs` section of the kubeadm
  [`NodeRegistrationOptions` structure](/docs/reference/config-api/kubeadm-config.v1beta3/#kubeadm-k8s-io-v1beta3-NodeRegistrationOptions)
  can be used for this.
  
  Then restart `kubelet`:

  ```sh
  systemctl daemon-reload
  systemctl restart kubelet
  ```
-->
<h2 id="容器使用的非公共-ip">容器使用的非公共 IP</h2>
<p>在某些情况下 <code>kubectl logs</code> 和 <code>kubectl run</code> 命令或许会返回以下错误，即便除此之外集群一切功能正常：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
</code></pre></div><ul>
<li>
<p>这或许是由于 Kubernetes 使用的 IP 无法与看似相同的子网上的其他 IP 进行通信的缘故，
可能是由机器提供商的政策所导致的。</p>
</li>
<li>
<p>Digital Ocean 既分配一个共有 IP 给 <code>eth0</code>，也分配一个私有 IP 在内部用作其浮动 IP 功能的锚点，
然而 <code>kubelet</code> 将选择后者作为节点的 <code>InternalIP</code> 而不是公共 IP</p>
<p>使用 <code>ip addr show</code> 命令代替 <code>ifconfig</code> 命令去检查这种情况，因为 <code>ifconfig</code> 命令
不会显示有问题的别名 IP 地址。或者指定的 Digital Ocean 的 API 端口允许从 droplet 中
查询 anchor IP：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
</code></pre></div><p>解决方法是通知 <code>kubelet</code> 使用哪个 <code>--node-ip</code>。当使用 Digital Ocean 时，可以是公网IP（分配给 <code>eth0</code>的），
或者是私网IP（分配给 <code>eth1</code> 的）。私网 IP 是可选的。
<a href="/zh/docs/reference/config-api/kubeadm-config.v1beta3/#kubeadm-k8s-io-v1beta3-NodeRegistrationOptions">kubadm <code>NodeRegistrationOptions</code> 结构</a>
的 <code>KubeletExtraArgs</code> 部分被用来处理这种情况。</p>
<p>然后重启 <code>kubelet</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">systemctl daemon-reload
systemctl restart kubelet
</code></pre></div></li>
</ul>
<!--
## `coredns` pods have `CrashLoopBackOff` or `Error` state

If you have nodes that are running SELinux with an older version of Docker you might experience a scenario
where the `coredns` pods are not starting. To solve that you can try one of the following options:

- Upgrade to a [newer version of Docker](/docs/setup/production-environment/container-runtimes/#docker).

- [Disable SELinux](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux).
- Modify the `coredns` deployment to set `allowPrivilegeEscalation` to `true`:

```bash
kubectl -n kube-system get deployment coredns -o yaml | \
  sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
  kubectl apply -f -
```

Another cause for CoreDNS to have `CrashLoopBackOff` is when a CoreDNS Pod deployed in Kubernetes detects a loop. [A number of workarounds](https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters)
are available to avoid Kubernetes trying to restart the CoreDNS Pod every time CoreDNS detects the loop and exits.
-->
<h2 id="coredns-pods-有-crashloopbackoff-或者-error-状态"><code>coredns</code> pods 有 <code>CrashLoopBackOff</code> 或者 <code>Error</code> 状态</h2>
<p>如果有些节点运行的是旧版本的 Docker，同时启用了 SELinux，你或许会遇到 <code>coredns</code> pods 无法启动的情况。
要解决此问题，你可以尝试以下选项之一：</p>
<ul>
<li>
<p>升级到 <a href="/zh/docs/setup/production-environment/container-runtimes/#docker">Docker 的较新版本</a>。</p>
</li>
<li>
<p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux">禁用 SELinux</a>.</p>
</li>
<li>
<p>修改 <code>coredns</code> 部署以设置 <code>allowPrivilegeEscalation</code> 为 <code>true</code>：</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl -n kube-system get deployment coredns -o yaml | <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  sed <span style="color:#b44">&#39;s/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g&#39;</span> | <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  kubectl apply -f -
</code></pre></div><p>CoreDNS 处于 <code>CrashLoopBackOff</code> 时的另一个原因是当 Kubernetes 中部署的 CoreDNS Pod 检测
到环路时。<a href="https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters">有许多解决方法</a>
可以避免在每次 CoreDNS 监测到循环并退出时，Kubernetes 尝试重启 CoreDNS Pod 的情况。</p>
<!--
Disabling SELinux or setting `allowPrivilegeEscalation` to `true` can compromise
the security of your cluster.
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 禁用 SELinux 或设置 <code>allowPrivilegeEscalation</code> 为 <code>true</code> 可能会损害集群的安全性。
</div>


<!--
## etcd pods restart continually

If you encounter the following error:

```
rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused "process_linux.go:110: decoding init error from pipe caused \"read parent: connection reset by peer\""
```

this issue appears if you run CentOS 7 with Docker 1.13.1.84.
This version of Docker can prevent the kubelet from executing into the etcd container.

To work around the issue, choose one of these options:

- Roll back to an earlier version of Docker, such as 1.13.1-75
```
yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
```

- Install one of the more recent recommended versions, such as 18.06:
```bash
sudo yum-config-manager -add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce-18.06.1.ce-3.el7.x86_64
```
-->
<h2 id="etcd-pods-持续重启">etcd pods 持续重启</h2>
<p>如果你遇到以下错误：</p>
<pre><code>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused &quot;process_linux.go:110: decoding init error from pipe caused \&quot;read parent: connection reset by peer\&quot;&quot;
</code></pre><p>如果你使用 Docker 1.13.1.84 运行 CentOS 7 就会出现这种问题。
此版本的 Docker 会阻止 kubelet 在 etcd 容器中执行。</p>
<p>为解决此问题，请选择以下选项之一：</p>
<ul>
<li>
<p>回滚到早期版本的 Docker，例如 1.13.1-75</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
</code></pre></div></li>
<li>
<p>安装较新的推荐版本之一，例如 18.06:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce-18.06.1.ce-3.el7.x86_64
</code></pre></div></li>
</ul>
<!--
## Not possible to pass a comma separated list of values to arguments inside a `-component-extra-args` flag

`kubeadm init` flags such as `-component-extra-args` allow you to pass custom arguments to a control-plane
component like the kube-apiserver. However, this mechanism is limited due to the underlying type used for parsing
the values (`mapStringString`).

If you decide to pass an argument that supports multiple, comma-separated values such as
`-apiserver-extra-args "enable-admission-plugins=LimitRanger,NamespaceExists"` this flag will fail with
`flag: malformed pair, expect string=string`. This happens because the list of arguments for
`-apiserver-extra-args` expects `key=value` pairs and in this case `NamespacesExists` is considered
as a key that is missing a value.

Alternatively, you can try separating the `key=value` pairs like so:
`-apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"`
but this will result in the key `enable-admission-plugins` only having the value of `NamespaceExists`.

A known workaround is to use the kubeadm [configuration file](/docs/reference/config-api/kubeadm-config.v1beta3/).
-->
<h2 id="无法将以逗号分隔的值列表传递给-component-extra-args-标志内的参数">无法将以逗号分隔的值列表传递给 <code>--component-extra-args</code> 标志内的参数</h2>
<p><code>kubeadm init</code> 标志例如 <code>--component-extra-args</code> 允许你将自定义参数传递给像
kube-apiserver 这样的控制平面组件。然而，由于解析 (<code>mapStringString</code>) 的基础类型值，此机制将受到限制。</p>
<p>如果你决定传递一个支持多个逗号分隔值（例如
<code>--apiserver-extra-args &quot;enable-admission-plugins=LimitRanger,NamespaceExists&quot;</code>）参数，
将出现 <code>flag: malformed pair, expect string=string</code> 错误。
发生这种问题是因为参数列表 <code>--apiserver-extra-args</code> 预期的是 <code>key=value</code> 形式，
而这里的 <code>NamespacesExists</code> 被误认为是缺少取值的键名。</p>
<p>一种解决方法是尝试分离 <code>key=value</code> 对，像这样：
<code>--apiserver-extra-args &quot;enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists&quot;</code>
但这将导致键 <code>enable-admission-plugins</code> 仅有值 <code>NamespaceExists</code>。</p>
<p>已知的解决方法是使用 kubeadm
<a href="/zh/docs/reference/config-api/kubeadm-config.v1beta3/">配置文件</a>。</p>
<!--
## kube-proxy scheduled before node is initialized by cloud-controller-manager

In cloud provider scenarios, kube-proxy can end up being scheduled on new worker nodes before
the cloud-controller-manager has initialized the node addresses. This causes kube-proxy to fail
to pick up the node's IP address properly and has knock-on effects to the proxy function managing
load balancers.

The following error can be seen in kube-proxy Pods:
```
server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
```

A known solution is to patch the kube-proxy DaemonSet to allow scheduling it on control-plane
nodes regardless of their conditions, keeping it off of other nodes until their initial guarding
conditions abate:
```
kubectl -n kube-system patch ds kube-proxy -p='{ "spec": { "template": { "spec": { "tolerations": [ { "key": "CriticalAddonsOnly", "operator": "Exists" }, { "effect": "NoSchedule", "key": "node-role.kubernetes.io/master" } ] } } } }'
```

The tracking issue for this problem is [here](https://github.com/kubernetes/kubeadm/issues/1027).
-->
<h2 id="在节点被云控制管理器初始化之前-kube-proxy-就被调度了">在节点被云控制管理器初始化之前，kube-proxy 就被调度了</h2>
<p>在云环境场景中，可能出现在云控制管理器完成节点地址初始化之前，kube-proxy 就被调度到新节点了。
这会导致 kube-proxy 无法正确获取节点的 IP 地址，并对管理负载平衡器的代理功能产生连锁反应。</p>
<p>在 kube-proxy Pod 中可以看到以下错误：</p>
<pre><code>server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
</code></pre><p>一种已知的解决方案是修补 kube-proxy DaemonSet，以允许在控制平面节点上调度它，
而不管它们的条件如何，将其与其他节点保持隔离，直到它们的初始保护条件消除：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl -n kube-system patch ds kube-proxy -p<span style="color:#666">=</span><span style="color:#b44">&#39;{ &#34;spec&#34;: { &#34;template&#34;: { &#34;spec&#34;: { &#34;tolerations&#34;: [ { &#34;key&#34;: &#34;CriticalAddonsOnly&#34;, &#34;operator&#34;: &#34;Exists&#34; }, { &#34;effect&#34;: &#34;NoSchedule&#34;, &#34;key&#34;: &#34;node-role.kubernetes.io/master&#34; } ] } } } }&#39;</span>
</code></pre></div><p>此问题的跟踪<a href="https://github.com/kubernetes/kubeadm/issues/1027">在这里</a>。</p>
<!--
## `/usr` is mounted read-only on nodes {#usr-mounted-read-only}

On Linux distributions such as Fedora CoreOS or Flatcar Container Linux, the directory `/usr` is mounted as a read-only filesystem.
For [flex-volume support](https://github.com/kubernetes/community/blob/ab55d85/contributors/devel/sig-storage/flexvolume.md),
Kubernetes components like the kubelet and kube-controller-manager use the default path of
`/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, yet the flex-volume directory _must be writeable_
for the feature to work.
(**Note**: FlexVolume was deprecated in the Kubernetes v1.23 release)
-->
<h2 id="usr-mounted-read-only">节点上的 <code>/usr</code> 被以只读方式挂载</h2>
<p>在类似 Fedora CoreOS 或者 Flatcar Container Linux 这类 Linux 发行版本中，
目录 <code>/usr</code> 是以只读文件系统的形式挂载的。
在支持 <a href="https://github.com/kubernetes/community/blob/ab55d85/contributors/devel/sig-storage/flexvolume.md">FlexVolume</a>时，
类似 kubelet 和 kube-controller-manager 这类 Kubernetes 组件使用默认路径
<code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code>，
而 FlexVolume 的目录 <em>必须是可写入的</em>，该功能特性才能正常工作。
（<strong>注意</strong>：FlexVolume 在 Kubernetes v1.23 版本中已被弃用）</p>
<!--
To workaround this issue you can configure the flex-volume directory using the kubeadm
[configuration file](/docs/reference/config-api/kubeadm-config.v1beta3/).

On the primary control-plane Node (created using `kubeadm init`) pass the following
file using `--config`:
-->
<p>为了解决这个问题，你可以使用 kubeadm 的<a href="/docs/reference/config-api/kubeadm-config.v1beta3/">配置文件</a> 来配置 FlexVolume 的目录。</p>
<p>在（使用 <code>kubeadm init</code> 创建的）主控制节点上，使用 <code>-config</code>
参数传入如下文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>InitConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">nodeRegistration</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kubeletExtraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volume-plugin-dir</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">controllerManager</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">extraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">flex-volume-plugin-dir</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
On joining Nodes:
-->
<p>在加入到集群中的节点上，使用下面的文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>JoinConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">nodeRegistration</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kubeletExtraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volume-plugin-dir</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
Alternatively, you can modify `/etc/fstab` to make the `/usr` mount writeable, but please
be advised that this is modifying a design principle of the Linux distribution.
-->
<p>或者，你要可以更改 <code>/etc/fstab</code> 使得 <code>/usr</code> 目录能够以可写入的方式挂载，不过
请注意这样做本质上是在更改 Linux 发行版的某种设计原则。</p>
<!--
## `kubeadm upgrade plan` prints out `context deadline exceeded` error message

This error message is shown when upgrading a Kubernetes cluster with `kubeadm` in the case of running an external etcd. This is not a critical bug and happens because older versions of kubeadm perform a version check on the external etcd cluster. You can proceed with `kubeadm upgrade apply ...`.

This issue is fixed as of version 1.19.
-->
<h2 id="kubeadm-upgrade-plan-输出错误信息-context-deadline-exceeded"><code>kubeadm upgrade plan</code> 输出错误信息 <code>context deadline exceeded</code></h2>
<p>在使用 <code>kubeadm</code> 来升级某运行外部 etcd 的 Kubernetes 集群时可能显示这一错误信息。
这并不是一个非常严重的一个缺陷，之所以出现此错误信息，原因是老的 kubeadm
版本会对外部 etcd 集群执行版本检查。你可以继续执行 <code>kubeadm upgrade apply ...</code>。</p>
<p>这一问题已经在 1.19 版本中得到修复。</p>
<!--
## `kubeadm reset` unmounts `/var/lib/kubelet`

If `/var/lib/kubelet` is being mounted, performing a `kubeadm reset` will effectively unmount it.

To workaround the issue, re-mount the `/var/lib/kubelet` directory after performing the `kubeadm reset` operation.

This is a regression introduced in kubeadm 1.15. The issue is fixed in 1.20.
-->
<h2 id="kubeadm-reset-会卸载-var-lib-kubelet"><code>kubeadm reset</code> 会卸载 <code>/var/lib/kubelet</code></h2>
<p>如果已经挂载了 <code>/var/lib/kubelet</code> 目录，执行 <code>kubeadm reset</code> 操作的时候
会将其卸载。</p>
<p>要解决这一问题，可以在执行了 <code>kubeadm reset</code> 操作之后重新挂载
<code>/var/lib/kubelet</code> 目录。</p>
<p>这是一个在 1.15 中引入的故障，已经在 1.20 版本中修复。</p>
<!--
## Cannot use the metrics-server securely in a kubeadm cluster

In a kubeadm cluster, the [metrics-server](https://github.com/kubernetes-sigs/metrics-server)
can be used insecurely by passing the `--kubelet-insecure-tls` to it. This is not recommended for production clusters.
-->
<h2 id="无法在-kubeadm-集群中安全地使用-metrics-server">无法在 kubeadm 集群中安全地使用 metrics-server</h2>
<p>在 kubeadm 集群中可以通过为 <a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a>
设置 <code>--kubelet-insecure-tls</code> 来以不安全的形式使用该服务。
建议不要在生产环境集群中这样使用。</p>
<!--
If you want to use TLS between the metrics-server and the kubelet there is a problem,
since kubeadm deploys a self-signed serving certificate for the kubelet. This can cause the following errors
on the side of the metrics-server:
-->
<p>如果你需要在 metrics-server 和 kubelet 之间使用 TLS，会有一个问题，
kubeadm 为 kubelet 部署的是自签名的服务证书。这可能会导致 metrics-server
端报告下面的错误信息：</p>
<pre><code>x509: certificate signed by unknown authority
x509: certificate is valid for IP-foo not IP-bar
</code></pre><!--
See [Enabling signed kubelet serving certificates](/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs)
to understand how to configure the kubelets in a kubeadm cluster to have properly signed serving certificates.

Also see [How to run the metrics-server securely](https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md#how-to-run-metrics-server-securely).
-->
<p>参见<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs">为 kubelet 启用签名的服务证书</a>
以进一步了解如何在 kubeadm 集群中配置 kubelet 使用正确签名了的服务证书。</p>
<p>另请参阅<a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md#how-to-run-metrics-server-securely">How to run the metrics-server securely</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-134ed1f6142a98e6ac681a1ba4920e53">2.3.1.3 - 使用 kubeadm 创建集群</h1>
    
	<!--
reviewers:
- sig-cluster-lifecycle
title: Creating a cluster with kubeadm
content_type: task
weight: 30
-->
<!-- overview -->
<!--
<img src="https://raw.githubusercontent.com/kubernetes/kubeadm/master/logos/stacked/color/kubeadm-stacked-color.png" align="right" width="150px">Using `kubeadm`, you can create a minimum viable Kubernetes cluster that conforms to best practices. In fact, you can use `kubeadm` to set up a cluster that will pass the [Kubernetes Conformance tests](https://kubernetes.io/blog/2017/10/software-conformance-certification).
`kubeadm` also supports other cluster
lifecycle functions, such as [bootstrap tokens](/docs/reference/access-authn-authz/bootstrap-tokens/) and cluster upgrades.
-->
<p><img src="/images/kubeadm-stacked-color.png" align="right" width="150px">使用 <code>kubeadm</code>，你能创建一个符合最佳实践的最小化 Kubernetes 集群。事实上，你可以使用 <code>kubeadm</code> 配置一个通过 <a href="https://kubernetes.io/blog/2017/10/software-conformance-certification">Kubernetes 一致性测试</a> 的集群。
<code>kubeadm</code> 还支持其他集群生命周期功能，
例如 <a href="/zh/docs/reference/access-authn-authz/bootstrap-tokens/">启动引导令牌</a> 和集群升级。</p>
<!--
The `kubeadm` tool is good if you need:

- A simple way for you to try out Kubernetes, possibly for the first time.
- A way for existing users to automate setting up a cluster and test their application.
- A building block in other ecosystem and/or installer tools with a larger
  scope.
-->
<p>kubeadm 工具很棒，如果你需要：</p>
<ul>
<li>一个尝试 Kubernetes 的简单方法。</li>
<li>一个现有用户可以自动设置集群并测试其应用程序的途径。</li>
<li>其他具有更大范围的生态系统和/或安装工具中的构建模块。</li>
</ul>
<!--
You can install and use `kubeadm` on various machines: your laptop, a set
of cloud servers, a Raspberry Pi, and more. Whether you're deploying into the
cloud or on-premises, you can integrate `kubeadm` into provisioning systems such
as Ansible or Terraform.
-->
<p>你可以在各种机器上安装和使用 <code>kubeadm</code>：笔记本电脑，
一组云服务器，Raspberry Pi 等。无论是部署到云还是本地，
你都可以将 <code>kubeadm</code> 集成到预配置系统中，例如 Ansible 或 Terraform。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
To follow this guide, you need:

- One or more machines running a deb/rpm-compatible Linux OS; for example: Ubuntu or CentOS.
- 2 GiB or more of RAM per machine--any less leaves little room for your
   apps.
- At least 2 CPUs on the machine that you use as a control-plane node.
- Full network connectivity among all machines in the cluster. You can use either a
  public or a private network.
-->
<p>要遵循本指南，你需要：</p>
<ul>
<li>一台或多台运行兼容 deb/rpm 的 Linux 操作系统的计算机；例如：Ubuntu 或 CentOS。</li>
<li>每台机器 2 GB 以上的内存，内存不足时应用会受限制。</li>
<li>用作控制平面节点的计算机上至少有2个 CPU。</li>
<li>集群中所有计算机之间具有完全的网络连接。你可以使用公共网络或专用网络。</li>
</ul>
<!--
You also need to use a version of `kubeadm` that can deploy the version
of Kubernetes that you want to use in your new cluster.
-->
<p>你还需要使用可以在新集群中部署特定 Kubernetes 版本对应的 <code>kubeadm</code>。</p>
<!--
[Kubernetes' version and version skew support policy](/docs/setup/release/version-skew-policy/#supported-versions) applies to `kubeadm` as well as to Kubernetes overall.
Check that policy to learn about what versions of Kubernetes and `kubeadm`
are supported. This page is written for Kubernetes v1.23.
-->
<p><a href="/zh/docs/setup/release/version-skew-policy/#supported-versions">Kubernetes 版本及版本倾斜支持策略</a> 适用于 <code>kubeadm</code> 以及整个 Kubernetes。
查阅该策略以了解支持哪些版本的 Kubernetes 和 <code>kubeadm</code>。
该页面是为 Kubernetes v1.23 编写的。</p>
<!--
The `kubeadm` tool's overall feature state is General Availability (GA). Some sub-features are
still under active development. The implementation of creating the cluster may change
slightly as the tool evolves, but the overall implementation should be pretty stable.
-->
<p><code>kubeadm</code> 工具的整体功能状态为一般可用性（GA）。一些子功能仍在积极开发中。
随着工具的发展，创建集群的实现可能会略有变化，但总体实现应相当稳定。</p>
<!--
Any commands under `kubeadm alpha` are, by definition, supported on an alpha level.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 根据定义，在 <code>kubeadm alpha</code> 下的所有命令均在 alpha 级别上受支持。
</div>
<!-- steps -->
<!--
## Objectives
-->
<h2 id="目标">目标</h2>
<!--
* Install a single control-plane Kubernetes cluster
* Install a Pod network on the cluster so that your Pods can
  talk to each other
-->
<ul>
<li>安装单个控制平面的 Kubernetes 集群</li>
<li>在集群上安装 Pod 网络，以便你的 Pod 可以相互连通</li>
</ul>
<!--
## Instructions
-->
<h2 id="操作指南">操作指南</h2>
<!--
### Installing kubeadm on your hosts
-->
<h3 id="在你的主机上安装-kubeadm">在你的主机上安装 kubeadm</h3>
<!--
See ["Installing kubeadm"](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/).
-->
<p>查看 <a href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">&quot;安装 kubeadm&quot;</a>。</p>
<!--
If you have already installed kubeadm, run `apt-get update &&
apt-get upgrade` or `yum update` to get the latest version of kubeadm.

When you upgrade, the kubelet restarts every few seconds as it waits in a crashloop for
kubeadm to tell it what to do. This crashloop is expected and normal.
After you initialize your control-plane, the kubelet runs normally.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>如果你已经安装了kubeadm，执行 <code>apt-get update &amp;&amp; apt-get upgrade</code> 或 <code>yum update</code> 以获取 kubeadm 的最新版本。</p>
<p>升级时，kubelet 每隔几秒钟重新启动一次，
在 crashloop 状态中等待 kubeadm 发布指令。crashloop 状态是正常现象。
初始化控制平面后，kubelet 将正常运行。</p>

</div>
<!--
### Preparing the required container images
-->
<h3 id="准备所需的容器镜像">准备所需的容器镜像</h3>
<!--
This step is optional and only applies in case you wish `kubeadm init` and `kubeadm join`
to not download the default container images which are hosted at `k8s.gcr.io`.

Kubeadm has commands that can help you pre-pull the required images
when creating a cluster without an internet connection on its nodes.
See [Running kubeadm without an internet connection](/docs/reference/setup-tools/kubeadm/kubeadm-init#without-internet-connection) for more details.

Kubeadm allows you to use a custom image repository for the required images.
See [Using custom images](/docs/reference/setup-tools/kubeadm/kubeadm-init#custom-images)
for more details.
-->
<p>这个步骤是可选的，只适用于你希望 <code>kubeadm init</code> 和 <code>kubeadm join</code> 不去下载存放在 <code>k8s.gcr.io</code> 上的默认的容器镜像的情况。</p>
<p>当你在离线的节点上创建一个集群的时候，Kubeadm 有一些命令可以帮助你预拉取所需的镜像。
阅读<a href="/zh/docs/reference/setup-tools/kubeadm/kubeadm-init#custom-images">离线运行 kubeadm</a>
获取更多的详情。</p>
<p>Kubeadm 允许你给所需要的镜像指定一个自定义的镜像仓库。
阅读<a href="/zh/docs/reference/setup-tools/kubeadm/kubeadm-init#custom-images">使用自定义镜像</a>
获取更多的详情。</p>
<!--
### Initializing your control-plane node
-->
<h3 id="初始化控制平面节点">初始化控制平面节点</h3>
<!--
The control-plane node is the machine where the control plane components run, including
<a class='glossary-tooltip' title='etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/' target='_blank' aria-label='etcd'>etcd</a> (the cluster database) and the
<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API Server'>API Server</a>
(which the <a class='glossary-tooltip' title='kubectl 是用来和 Kubernetes 集群进行通信的命令行工具。' data-toggle='tooltip' data-placement='top' href='/docs/user-guide/kubectl-overview/' target='_blank' aria-label='kubectl'>kubectl</a> command line tool
communicates with).
-->
<p>控制平面节点是运行控制平面组件的机器，
包括 <a class='glossary-tooltip' title='etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/' target='_blank' aria-label='etcd'>etcd</a> （集群数据库）
和 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API Server'>API Server</a>
（命令行工具 <a class='glossary-tooltip' title='kubectl 是用来和 Kubernetes 集群进行通信的命令行工具。' data-toggle='tooltip' data-placement='top' href='/docs/user-guide/kubectl-overview/' target='_blank' aria-label='kubectl'>kubectl</a> 与之通信）。</p>
<!--
1. (Recommended) If you have plans to upgrade this single control-plane `kubeadm` cluster
to high availability you should specify the `--control-plane-endpoint` to set the shared endpoint
for all control-plane nodes. Such an endpoint can be either a DNS name or an IP address of a load-balancer.
1. Choose a Pod network add-on, and verify whether it requires any arguments to
be passed to `kubeadm init`. Depending on which
third-party provider you choose, you might need to set the `--pod-network-cidr` to
a provider-specific value. See [Installing a Pod network add-on](#pod-network).
1. (Optional) Since version 1.14, `kubeadm` tries to detect the container runtime on Linux
by using a list of well known domain socket paths. To use different container runtime or
if there are more than one installed on the provisioned node, specify the `--cri-socket`
argument to `kubeadm init`. See [Installing runtime](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime).
1. (Optional) Unless otherwise specified, `kubeadm` uses the network interface associated
with the default gateway to set the advertise address for this particular control-plane node's API server.
To use a different network interface, specify the `--apiserver-advertise-address=<ip-address>` argument
to `kubeadm init`. To deploy an IPv6 Kubernetes cluster using IPv6 addressing, you
must specify an IPv6 address, for example `--apiserver-advertise-address=fd00::101`
-->
<ol>
<li>（推荐）如果计划将单个控制平面 kubeadm 集群升级成高可用，
你应该指定 <code>--control-plane-endpoint</code> 为所有控制平面节点设置共享端点。
端点可以是负载均衡器的 DNS 名称或 IP 地址。</li>
<li>选择一个 Pod 网络插件，并验证是否需要为 <code>kubeadm init</code> 传递参数。
根据你选择的第三方网络插件，你可能需要设置 <code>--pod-network-cidr</code> 的值。
请参阅 <a href="#pod-network">安装Pod网络附加组件</a>。</li>
<li>（可选）从版本1.14开始，<code>kubeadm</code> 尝试使用一系列众所周知的域套接字路径来检测 Linux 上的容器运行时。
要使用不同的容器运行时，
或者如果在预配置的节点上安装了多个容器，请为 <code>kubeadm init</code> 指定 <code>--cri-socket</code> 参数。
请参阅<a href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime">安装运行时</a>。</li>
<li>（可选）除非另有说明，否则 <code>kubeadm</code> 使用与默认网关关联的网络接口来设置此控制平面节点 API server 的广播地址。
要使用其他网络接口，请为 <code>kubeadm init</code> 设置 <code>--apiserver-advertise-address=&lt;ip-address&gt;</code> 参数。
要部署使用 IPv6 地址的 Kubernetes 集群，
必须指定一个 IPv6 地址，例如 <code>--apiserver-advertise-address=fd00::101</code></li>
</ol>
<!--
To initialize the control-plane node run:
-->
<p>要初始化控制平面节点，请运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeadm init &lt;args&gt;
</code></pre></div><!--
### Considerations about apiserver-advertise-address and ControlPlaneEndpoint
-->
<h3 id="关于-apiserver-advertise-address-和-controlplaneendpoint-的注意事项">关于 apiserver-advertise-address 和 ControlPlaneEndpoint 的注意事项</h3>
<!--
While `--apiserver-advertise-address` can be used to set the advertise address for this particular
control-plane node's API server, `--control-plane-endpoint` can be used to set the shared endpoint
for all control-plane nodes.
-->
<p><code>--apiserver-advertise-address</code>  可用于为控制平面节点的 API server 设置广播地址，
<code>--control-plane-endpoint</code> 可用于为所有控制平面节点设置共享端点。</p>
<!--
`--control-plane-endpoint` allows both IP addresses and DNS names that can map to IP addresses.
Please contact your network administrator to evaluate possible solutions with respect to such mapping.
-->
<p><code>--control-plane-endpoint</code> 允许 IP 地址和可以映射到 IP 地址的 DNS 名称。
请与你的网络管理员联系，以评估有关此类映射的可能解决方案。</p>
<!--
Here is an example mapping:
-->
<p>这是一个示例映射：</p>
<pre><code>192.168.0.102 cluster-endpoint
</code></pre><!--
Where `192.168.0.102` is the IP address of this node and `cluster-endpoint` is a custom DNS name that maps to this IP.
This will allow you to pass `--control-plane-endpoint=cluster-endpoint` to `kubeadm init` and pass the same DNS name to
`kubeadm join`. Later you can modify `cluster-endpoint` to point to the address of your load-balancer in an
high availability scenario.
-->
<p>其中 <code>192.168.0.102</code> 是此节点的 IP 地址，<code>cluster-endpoint</code> 是映射到该 IP 的自定义 DNS 名称。
这将允许你将 <code>--control-plane-endpoint=cluster-endpoint</code> 传递给 <code>kubeadm init</code>，并将相同的 DNS 名称传递给 <code>kubeadm join</code>。
稍后你可以修改 <code>cluster-endpoint</code> 以指向高可用性方案中的负载均衡器的地址。</p>
<!--
Turning a single control plane cluster created without `--control-plane-endpoint` into a highly available cluster
is not supported by kubeadm.
-->
<p>kubeadm 不支持将没有 <code>--control-plane-endpoint</code> 参数的单个控制平面集群转换为高可用性集群。</p>
<!--
### More information
-->
<h3 id="更多信息">更多信息</h3>
<!--
For more information about `kubeadm init` arguments, see the [kubeadm reference guide](/docs/reference/setup-tools/kubeadm/).
-->
<p>有关 <code>kubeadm init</code> 参数的更多信息，请参见 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm 参考指南</a>。</p>
<!--
To configure `kubeadm init` with a configuration file see [Using kubeadm init with a configuration file](/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file).
-->
<p>要使用配置文件配置 <code>kubeadm init</code> 命令，请参见<a href="/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file">带配置文件使用 kubeadm init</a>。</p>
<!--
To customize control plane components, including optional IPv6 assignment to liveness probe for control plane components and etcd server, provide extra arguments to each component as documented in [custom arguments](/docs/setup/production-environment/tools/kubeadm/control-plane-flags/).
-->
<p>要自定义控制平面组件，包括可选的对控制平面组件和 etcd 服务器的活动探针提供 IPv6 支持，请参阅<a href="/zh/docs/setup/production-environment/tools/kubeadm/control-plane-flags/">自定义参数</a>。</p>
<!--
To run `kubeadm init` again, you must first [tear down the cluster](#tear-down).
-->
<p>要再次运行 <code>kubeadm init</code>，你必须首先<a href="#tear-down">卸载集群</a>。</p>
<!--
If you join a node with a different architecture to your cluster, make sure that your deployed DaemonSets
have container image support for this architecture.
-->
<p>如果将具有不同架构的节点加入集群，
请确保已部署的 DaemonSet 对这种体系结构具有容器镜像支持。</p>
<!--
`kubeadm init` first runs a series of prechecks to ensure that the machine
is ready to run Kubernetes. These prechecks expose warnings and exit on errors. `kubeadm init`
then downloads and installs the cluster control plane components. This may take several minutes.
After it finishes you should see:
-->
<p><code>kubeadm init</code> 首先运行一系列预检查以确保机器
准备运行 Kubernetes。这些预检查会显示警告并在错误时退出。然后 <code>kubeadm init</code>
下载并安装集群控制平面组件。这可能会需要几分钟。
完成之后你应该看到：</p>
<pre><code class="language-none" data-lang="none">Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><!--
To make kubectl work for your non-root user, run these commands, which are
also part of the `kubeadm init` output:
-->
<p>要使非 root 用户可以运行 kubectl，请运行以下命令，
它们也是 <code>kubeadm init</code> 输出的一部分：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p <span style="color:#b8860b">$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span style="color:#b8860b">$HOME</span>/.kube/config
sudo chown <span style="color:#a2f;font-weight:bold">$(</span>id -u<span style="color:#a2f;font-weight:bold">)</span>:<span style="color:#a2f;font-weight:bold">$(</span>id -g<span style="color:#a2f;font-weight:bold">)</span> <span style="color:#b8860b">$HOME</span>/.kube/config
</code></pre></div><!--
Alternatively, if you are the `root` user, you can run:
-->
<p>或者，如果你是 <code>root</code> 用户，则可以运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">export</span> <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>/etc/kubernetes/admin.conf
</code></pre></div><div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
Kubeadm signs the certificate in the `admin.conf` to have `Subject: O = system:masters, CN = kubernetes-admin`.
`system:masters` is a break-glass, super user group that bypasses the authorization layer (e.g. RBAC).
Do not share the `admin.conf` file with anyone and instead grant users custom permissions by generating
them a kubeconfig file using the `kubeadm kubeconfig user` command.
-->
<p>kubeadm 对 <code>admin.conf</code> 中的证书进行签名时，将其配置为
<code>Subject: O = system:masters, CN = kubernetes-admin</code>。
<code>system:masters</code> 是一个例外的、超级用户组，可以绕过鉴权层（例如 RBAC）。
不要将 <code>admin.conf</code> 文件与任何人共享，应该使用 <code>kubeadm kubeconfig user</code>
命令为其他用户生成 kubeconfig 文件，完成对他们的定制授权。
</div>


<!--
Make a record of the `kubeadm join` command that `kubeadm init` outputs. You
need this command to [join nodes to your cluster](#join-nodes).
-->
<p>记录 <code>kubeadm init</code> 输出的 <code>kubeadm join</code> 命令。
你需要此命令<a href="#join-nodes">将节点加入集群</a>。</p>
<!--
The token is used for mutual authentication between the control-plane node and the joining
nodes. The token included here is secret. Keep it safe, because anyone with this
token can add authenticated nodes to your cluster. These tokens can be listed,
created, and deleted with the `kubeadm token` command. See the
[kubeadm reference guide](/docs/reference/setup-tools/kubeadm/kubeadm-token/).
-->
<p>令牌用于控制平面节点和加入节点之间的相互身份验证。
这里包含的令牌是密钥。确保它的安全，
因为拥有此令牌的任何人都可以将经过身份验证的节点添加到你的集群中。
可以使用 <code>kubeadm token</code> 命令列出，创建和删除这些令牌。
请参阅 <a href="/zh/docs/reference/setup-tools/kubeadm/kubeadm-token/">kubeadm 参考指南</a>。</p>
<!--
### Installing a Pod network add-on {#pod-network}
-->
<h3 id="pod-network">安装 Pod 网络附加组件</h3>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
This section contains important information about networking setup and
deployment order.
Read all of this advice carefully before proceeding.
-->
<p>本节包含有关网络设置和部署顺序的重要信息。
在继续之前，请仔细阅读所有建议。</p>
<!--
**You must deploy a
<a class='glossary-tooltip' title='容器网络接口 (CNI) 插件是遵循 appc/CNI 协议的一类网络插件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni' target='_blank' aria-label='Container Network Interface'>Container Network Interface</a>
(CNI) based Pod network add-on so that your Pods can communicate with each other.
Cluster DNS (CoreDNS) will not start up before a network is installed.**
-->
<p><strong>你必须部署一个基于 Pod 网络插件的
<a class='glossary-tooltip' title='容器网络接口 (CNI) 插件是遵循 appc/CNI 协议的一类网络插件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni' target='_blank' aria-label='容器网络接口'>容器网络接口</a>
(CNI)，以便你的 Pod 可以相互通信。
在安装网络之前，集群 DNS (CoreDNS) 将不会启动。</strong></p>
<!--
- Take care that your Pod network must not overlap with any of the host
  networks: you are likely to see problems if there is any overlap.
  (If you find a collision between your network plugin's preferred Pod
  network and some of your host networks, you should think of a suitable
  CIDR block to use instead, then use that during `kubeadm init` with
  `--pod-network-cidr` and as a replacement in your network plugin's YAML).
-->
<ul>
<li>注意你的 Pod 网络不得与任何主机网络重叠：
如果有重叠，你很可能会遇到问题。
（如果你发现网络插件的首选 Pod 网络与某些主机网络之间存在冲突，
则应考虑使用一个合适的 CIDR 块来代替，
然后在执行 <code>kubeadm init</code> 时使用 <code>--pod-network-cidr</code> 参数并在你的网络插件的 YAML 中替换它）。</li>
</ul>
<!--
- By default, `kubeadm` sets up your cluster to use and enforce use of
  [RBAC](/docs/reference/access-authn-authz/rbac/) (role based access
  control).
  Make sure that your Pod network plugin supports RBAC, and so do any manifests
  that you use to deploy it.
-->
<ul>
<li>默认情况下，<code>kubeadm</code> 将集群设置为使用和强制使用 <a href="/zh/docs/reference/access-authn-authz/rbac/">RBAC</a>（基于角色的访问控制）。
确保你的 Pod 网络插件支持 RBAC，以及用于部署它的 manifests 也是如此。</li>
</ul>
<!--
- If you want to use IPv6--either dual-stack, or single-stack IPv6 only
  networking--for your cluster, make sure that your Pod network plugin
  supports IPv6.
  IPv6 support was added to CNI in [v0.6.0](https://github.com/containernetworking/cni/releases/tag/v0.6.0).
-->
<ul>
<li>如果要为集群使用 IPv6（双协议栈或仅单协议栈 IPv6 网络），
请确保你的 Pod 网络插件支持 IPv6。
IPv6 支持已在 CNI <a href="https://github.com/containernetworking/cni/releases/tag/v0.6.0">v0.6.0</a> 版本中添加。</li>
</ul>

</div>

<!--
Kubeadm should be CNI agnostic and the validation of CNI providers is out of the scope of our current e2e testing.
If you find an issue related to a CNI plugin you should log a ticket in its respective issue
tracker instead of the kubeadm or kubernetes issue trackers.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> kubeadm 应该是与 CNI 无关的，对 CNI 驱动进行验证目前不在我们的端到端测试范畴之内。
如果你发现与 CNI 插件相关的问题，应在其各自的问题跟踪器中记录而不是在 kubeadm
或 kubernetes 问题跟踪器中记录。
</div>
<!--
Several external projects provide Kubernetes Pod networks using CNI, some of which also
support [Network Policy](/docs/concepts/services-networking/network-policies/).
-->
<p>一些外部项目为 Kubernetes 提供使用 CNI 的 Pod 网络，其中一些还支持<a href="/zh/docs/concepts/services-networking/network-policies/">网络策略</a>。</p>
<!--
See a list of add-ons that implement the
[Kubernetes networking model](/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model).
-->
<p>请参阅实现 <a href="/zh/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model">Kubernetes 网络模型</a> 的附加组件列表。</p>
<!--
You can install a Pod network add-on with the following command on the
control-plane node or a node that has the kubeconfig credentials:
-->
<p>你可以使用以下命令在控制平面节点或具有 kubeconfig 凭据的节点上安装 Pod 网络附加组件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f &lt;add-on.yaml&gt;
</code></pre></div><!--
You can install only one Pod network per cluster.
-->
<p>每个集群只能安装一个 Pod 网络。</p>
<!--
Once a Pod network has been installed, you can confirm that it is working by
checking that the CoreDNS Pod is `Running` in the output of `kubectl get pods --all-namespaces`.
And once the CoreDNS Pod is up and running, you can continue by joining your nodes.
-->
<p>安装 Pod 网络后，您可以通过在 <code>kubectl get pods --all-namespaces</code> 输出中检查 CoreDNS Pod 是否 <code>Running</code> 来确认其是否正常运行。
一旦 CoreDNS Pod 启用并运行，你就可以继续加入节点。</p>
<!--
If your network is not working or CoreDNS is not in the `Running` state, check out the
[troubleshooting guide](/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/)
for `kubeadm`.
-->
<p>如果您的网络无法正常工作或 CoreDNS 不在“运行中”状态，请查看 <code>kubeadm</code> 的
<a href="/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/">故障排除指南</a>。</p>
<!--
### Control plane node isolation
-->
<h3 id="控制平面节点隔离">控制平面节点隔离</h3>
<!--
By default, your cluster will not schedule Pods on the control-plane node for security
reasons. If you want to be able to schedule Pods on the control-plane node, for example for a
single-machine Kubernetes cluster for development, run:
-->
<p>默认情况下，出于安全原因，你的集群不会在控制平面节点上调度 Pod。
如果你希望能够在控制平面节点上调度 Pod，
例如用于开发的单机 Kubernetes 集群，请运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl taint nodes --all node-role.kubernetes.io/master-
</code></pre></div><!--
With output looking something like:
-->
<p>输出看起来像：</p>
<pre><code>node &quot;test-01&quot; untainted
taint &quot;node-role.kubernetes.io/master:&quot; not found
taint &quot;node-role.kubernetes.io/master:&quot; not found
</code></pre><!--
This will remove the `node-role.kubernetes.io/master` taint from any nodes that
have it, including the control-plane node, meaning that the scheduler will then be able
to schedule Pods everywhere.
-->
<p>这将从任何拥有 <code>node-role.kubernetes.io/master</code> taint 标记的节点中移除该标记，
包括控制平面节点，这意味着调度程序将能够在任何地方调度 Pods。</p>
<!--
### Joining your nodes {#join-nodes}
-->
<h3 id="join-nodes">加入节点</h3>
<!--
The nodes are where your workloads (containers and Pods, etc) run. To add new nodes to your cluster do the following for each machine:
-->
<p>节点是你的工作负载（容器和 Pod 等）运行的地方。要将新节点添加到集群，请对每台计算机执行以下操作：</p>
<!--
* SSH to the machine
* Become root (e.g. `sudo su -`)
* Run the command that was output by `kubeadm init`. For example:
-->
<ul>
<li>SSH 到机器</li>
<li>成为 root （例如 <code>sudo su -</code>）</li>
<li>运行 <code>kubeadm init</code> 输出的命令。例如：</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre></div><!--
If you do not have the token, you can get it by running the following command on the control-plane node:
-->
<p>如果没有令牌，可以通过在控制平面节点上运行以下命令来获取令牌：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeadm token list
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于以下内容：</p>
<pre><code class="language-console" data-lang="console">TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
                                                   signing          token generated by     bootstrappers:
                                                                    'kubeadm init'.        kubeadm:
                                                                                           default-node-token
</code></pre><!--
By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired,
you can create a new token by running the following command on the control-plane node:
-->
<p>默认情况下，令牌会在24小时后过期。如果要在当前令牌过期后将节点加入集群，
则可以通过在控制平面节点上运行以下命令来创建新令牌：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeadm token create
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于以下内容：</p>
<pre><code class="language-console" data-lang="console">5didvk.d09sbcov8ph2amjw
</code></pre><!--
If you don't have the value of `--discovery-token-ca-cert-hash`, you can get it by running the following command chain on the control-plane node:
-->
<p>如果你没有 <code>--discovery-token-ca-cert-hash</code> 的值，则可以通过在控制平面节点上执行以下命令链来获取它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   openssl dgst -sha256 -hex | sed <span style="color:#b44">&#39;s/^.* //&#39;</span>
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于以下内容：</p>
<pre><code class="language-console" data-lang="console">8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</code></pre><!--
To specify an IPv6 tuple for `<control-plane-host>:<control-plane-port>`, IPv6 address must be enclosed in square brackets, for example: `[fd00::101]:2073`.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 要为 <code>&lt;control-plane-host&gt;:&lt;control-plane-port&gt;</code> 指定 IPv6 元组，必须将 IPv6 地址括在方括号中，例如：<code>[fd00::101]:2073</code>
</div>
<!--
The output should look something like:
-->
<p>输出应类似于：</p>
<pre><code>[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on control-plane to see this machine join.
</code></pre><!--
A few seconds later, you should notice this node in the output from `kubectl get
nodes` when run on the control-plane node.
-->
<p>几秒钟后，当你在控制平面节点上执行 <code>kubectl get nodes</code>，你会注意到该节点出现在输出中。</p>
<!--
As the cluster nodes are usually initialized sequentially, the CoreDNS Pods are likely to all run 
on the first control-plane node. To provide higher availability, please rebalance the CoreDNS Pods 
with `kubectl -n kube-system rollout restart deployment coredns` after at least one new node is joined.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 由于集群节点通常是按顺序初始化的，CoreDNS Pods 很可能都运行在第一个控制面节点上。
为了提供更高的可用性，请在加入至少一个新节点后
使用 <code>kubectl -n kube-system rollout restart deployment coredns</code> 命令，重新平衡 CoreDNS Pods。
</div>
<!--
### (Optional) Controlling your cluster from machines other than the control-plane node
-->
<h3 id="可选-从控制平面节点以外的计算机控制集群">（可选）从控制平面节点以外的计算机控制集群</h3>
<!--
In order to get a kubectl on some other computer (e.g. laptop) to talk to your
cluster, you need to copy the administrator kubeconfig file from your control-plane node
to your workstation like this:
-->
<p>为了使 kubectl 在其他计算机（例如笔记本电脑）上与你的集群通信，
你需要将管理员 kubeconfig 文件从控制平面节点复制到工作站，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf get nodes
</code></pre></div><!--
The example above assumes SSH access is enabled for root. If that is not the
case, you can copy the `admin.conf` file to be accessible by some other user
and `scp` using that other user instead.

The `admin.conf` file gives the user _superuser_ privileges over the cluster.
This file should be used sparingly. For normal users, it's recommended to
generate an unique credential to which you grant privileges. You can do
this with the `kubeadm alpha kubeconfig user --client-name <CN>`
command. That command will print out a KubeConfig file to STDOUT which you
should save to a file and distribute to your user. After that, grant
privileges by using `kubectl create (cluster)rolebinding`.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>上面的示例假定为 root 用户启用了SSH访问。如果不是这种情况，
你可以使用 <code>scp</code> 将 admin.conf 文件复制给其他允许访问的用户。</p>
<p>admin.conf 文件为用户提供了对集群的超级用户特权。
该文件应谨慎使用。对于普通用户，建议生成一个你为其授予特权的唯一证书。
你可以使用 <code>kubeadm alpha kubeconfig user --client-name &lt;CN&gt;</code> 命令执行此操作。
该命令会将 KubeConfig 文件打印到 STDOUT，你应该将其保存到文件并分发给用户。
之后，使用 <code>kubectl create (cluster)rolebinding</code> 授予特权。</p>

</div>
<!--
### (Optional) Proxying API Server to localhost
-->
<h3 id="可选-将api服务器代理到本地主机">（可选）将API服务器代理到本地主机</h3>
<!--
If you want to connect to the API Server from outside the cluster you can use
`kubectl proxy`:
-->
<p>如果要从集群外部连接到 API 服务器，则可以使用 <code>kubectl proxy</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf proxy
</code></pre></div><!--
You can now access the API Server locally at `http://localhost:8001/api/v1`
-->
<p>你现在可以在本地访问API服务器 http://localhost:8001/api/v1</p>
<!--
## Clean up {#tear-down}
-->
<h2 id="tear-down">清理</h2>
<!--
If you used disposable servers for your cluster, for testing, you can
switch those off and do no further clean up. You can use
`kubectl config delete-cluster` to delete your local references to the
cluster.
-->
<p>如果你在集群中使用了一次性服务器进行测试，则可以关闭这些服务器，而无需进一步清理。你可以使用 <code>kubectl config delete-cluster</code> 删除对集群的本地引用。</p>
<!--
However, if you want to deprovision your cluster more cleanly, you should
first [drain the node](/docs/reference/generated/kubectl/kubectl-commands#drain)
and make sure that the node is empty, then deconfigure the node.
-->
<p>但是，如果要更干净地取消配置群集，
则应首先<a href="/docs/reference/generated/kubectl/kubectl-commands#drain">清空节点</a>并确保该节点为空，
然后取消配置该节点。</p>
<!--
### Remove the node
-->
<h3 id="删除节点">删除节点</h3>
<!--
Talking to the control-plane node with the appropriate credentials, run:
-->
<p>使用适当的凭证与控制平面节点通信，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl drain &lt;node name&gt; --delete-emptydir-data --force --ignore-daemonsets
</code></pre></div><!--
Before removing the node, reset the state installed by `kubeadm`:
-->
<p>在删除节点之前，请重置 <code>kubeadm</code> 安装的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeadm reset
</code></pre></div><!--
The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:
-->
<p>重置过程不会重置或清除 iptables 规则或 IPVS 表。如果你希望重置 iptables，则必须手动进行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">iptables -F <span style="color:#666">&amp;&amp;</span> iptables -t nat -F <span style="color:#666">&amp;&amp;</span> iptables -t mangle -F <span style="color:#666">&amp;&amp;</span> iptables -X
</code></pre></div><!--
If you want to reset the IPVS tables, you must run the following command:
-->
<p>如果要重置 IPVS 表，则必须运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ipvsadm -C
</code></pre></div><!--
Now remove the node:
-->
<p>现在删除节点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl delete node &lt;node name&gt;
</code></pre></div><!--
If you wish to start over simply run `kubeadm init` or `kubeadm join` with the
appropriate arguments.
-->
<p>如果你想重新开始，只需运行 <code>kubeadm init</code> 或 <code>kubeadm join</code> 并加上适当的参数。</p>
<!--
### Clean up the control plane
-->
<h3 id="清理控制平面">清理控制平面</h3>
<!--
You can use `kubeadm reset` on the control plane host to trigger a best-effort
clean up.
-->
<p>你可以在控制平面主机上使用 <code>kubeadm reset</code> 来触发尽力而为的清理。</p>
<!--
See the [`kubeadm reset`](/docs/reference/setup-tools/kubeadm/kubeadm-reset/)
reference documentation for more information about this subcommand and its
options.
-->
<p>有关此子命令及其选项的更多信息，请参见<a href="/zh/docs/reference/setup-tools/kubeadm/kubeadm-reset/"><code>kubeadm reset</code></a>参考文档。</p>
<!-- discussion -->
<!--
## What's next {#whats-next}
-->
<h2 id="whats-next">下一步</h2>
<!--
* Verify that your cluster is running properly with [Sonobuoy](https://github.com/heptio/sonobuoy)
* <a id="lifecycle" />See [Upgrading kubeadm clusters](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
  for details about upgrading your cluster using `kubeadm`.
* Learn about advanced `kubeadm` usage in the [kubeadm reference documentation](/docs/reference/setup-tools/kubeadm)
* Learn more about Kubernetes [concepts](/docs/concepts/) and [`kubectl`](/docs/reference/kubectl/overview/).
* See the [Cluster Networking](/docs/concepts/cluster-administration/networking/) page for a bigger list
  of Pod network add-ons.
* <a id="other-addons" />See the [list of add-ons](/docs/concepts/cluster-administration/addons/) to
  explore other add-ons, including tools for logging, monitoring, network policy, visualization &amp;
  control of your Kubernetes cluster.
* Configure how your cluster handles logs for cluster events and from
  applications running in Pods.
  See [Logging Architecture](/docs/concepts/cluster-administration/logging/) for
  an overview of what is involved.
-->
<ul>
<li>使用 <a href="https://github.com/heptio/sonobuoy">Sonobuoy</a> 验证集群是否正常运行。</li>
<li><a id="lifecycle"/>有关使用 kubeadm 升级集群的详细信息，请参阅<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">升级 kubeadm 集群</a>。</li>
<li>在 <a href="/zh/docs/reference/setup-tools/kubeadm">kubeadm 参考文档</a>中了解有关高级 <code>kubeadm</code> 用法的信息。</li>
<li>了解有关 Kubernetes <a href="/zh/docs/concepts/">概念</a>和 <a href="/zh/docs/reference/kubectl/overview/"><code>kubectl</code></a> 的更多信息。</li>
<li>有关 Pod 网络附加组件的更多列表，请参见<a href="/zh/docs/concepts/cluster-administration/networking/">集群网络</a>页面。</li>
<li><a id="other-addons" />请参阅<a href="/zh/docs/concepts/cluster-administration/addons/">附加组件列表</a>以探索其他附加组件，
包括用于 Kubernetes 集群的日志记录，监视，网络策略，可视化和控制的工具。</li>
<li>配置集群如何处理集群事件的日志以及
在 Pods 中运行的应用程序。
有关所涉及内容的概述，请参见<a href="/zh/docs/concepts/cluster-administration/logging/">日志架构</a>。</li>
</ul>
<!--
### Feedback {#feedback}
-->
<h3 id="feedback">反馈</h3>
<!--
* For bugs, visit the [kubeadm GitHub issue tracker](https://github.com/kubernetes/kubeadm/issues)
* For support, visit the
  [#kubeadm](https://kubernetes.slack.com/messages/kubeadm/) Slack channel
* General SIG Cluster Lifecycle development Slack channel:
  [#sig-cluster-lifecycle](https://kubernetes.slack.com/messages/sig-cluster-lifecycle/)
* SIG Cluster Lifecycle [SIG information](https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle#readme)
* SIG Cluster Lifecycle mailing list:
  [kubernetes-sig-cluster-lifecycle](https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle)
-->
<ul>
<li>有关 bugs, 访问 <a href="https://github.com/kubernetes/kubeadm/issues">kubeadm GitHub issue tracker</a></li>
<li>有关支持, 访问
<a href="https://kubernetes.slack.com/messages/kubeadm/">#kubeadm</a> Slack 频道</li>
<li>General SIG 集群生命周期开发 Slack 频道:
<a href="https://kubernetes.slack.com/messages/sig-cluster-lifecycle/">#sig-cluster-lifecycle</a></li>
<li>SIG 集群生命周期 <a href="https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle#readme">SIG information</a></li>
<li>SIG 集群生命周期邮件列表:
<a href="https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle">kubernetes-sig-cluster-lifecycle</a></li>
</ul>
<!--
## Version skew policy {#version-skew-policy}
-->
<h2 id="version-skew-policy">版本倾斜政策</h2>
<!--
The `kubeadm` tool of version v1.23 may deploy clusters with a control plane of version v1.23 or v1.22.
`kubeadm` v1.23 can also upgrade an existing kubeadm-created cluster of version v1.22.
-->
<p>版本 v1.23 的kubeadm 工具可以使用版本 v1.23 或 v1.22 的控制平面部署集群。kubeadm v1.23 还可以升级现有的 kubeadm 创建的 v1.22 版本的集群。</p>
<!--
Due to that we can't see into the future, kubeadm CLI v1.23 may or may not be able to deploy v1.24 clusters.
-->
<p>由于我们不能预见未来，kubeadm CLI v1.23 可能会或可能无法部署 v1.24 集群。</p>
<!--
These resources provide more information on supported version skew between kubelets and the control plane, and other Kubernetes components:
-->
<p>这些资源提供了有关 kubelet 与控制平面以及其他 Kubernetes 组件之间受支持的版本倾斜的更多信息：</p>
<!--
* Kubernetes [version and version-skew policy](/docs/setup/release/version-skew-policy/)
* Kubeadm-specific [installation guide](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl)
-->
<ul>
<li>Kubernetes <a href="/zh/docs/setup/release/version-skew-policy/">版本和版本偏斜政策</a></li>
<li>Kubeadm-specific <a href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl">安装指南</a></li>
</ul>
<!--
## Limitations {#limitations}
-->
<h2 id="limitations">局限性</h2>
<!--
### Cluster resilience {#resilience}
-->
<h3 id="resilience">集群弹性</h3>
<!--
The cluster created here has a single control-plane node, with a single etcd database
running on it. This means that if the control-plane node fails, your cluster may lose
data and may need to be recreated from scratch.
-->
<p>此处创建的集群具有单个控制平面节点，运行单个 etcd 数据库。
这意味着如果控制平面节点发生故障，你的集群可能会丢失数据并且可能需要从头开始重新创建。</p>
<!--
Workarounds:
-->
<p>解决方法:</p>
<!--
* Regularly [back up etcd](https://coreos.com/etcd/docs/latest/admin_guide.html). The
  etcd data directory configured by kubeadm is at `/var/lib/etcd` on the control-plane node.
-->
<ul>
<li>定期<a href="https://coreos.com/etcd/docs/latest/admin_guide.html">备份 etcd</a>。
kubeadm 配置的 etcd 数据目录位于控制平面节点上的 <code>/var/lib/etcd</code> 中。</li>
</ul>
<!--
* Use multiple control-plane nodes. You can read
  [Options for Highly Available topology](/docs/setup/production-environment/tools/kubeadm/ha-topology/) to pick a cluster
  topology that provides [high-availability](/docs/setup/production-environment/tools/kubeadm/high-availability/).
-->
<ul>
<li>使用多个控制平面节点。你可以阅读
<a href="/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/">可选的高可用性拓扑</a> 选择集群拓扑提供的
<a href="/zh/docs/setup/production-environment/tools/kubeadm/high-availability/">高可用性</a>.</li>
</ul>
<!--
### Platform compatibility {#multi-platform}
-->
<h3 id="multi-platform">平台兼容性</h3>
<!--
kubeadm deb/rpm packages and binaries are built for amd64, arm (32-bit), arm64, ppc64le, and s390x
following the [multi-platform
proposal](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md).
-->
<p>kubeadm deb/rpm 软件包和二进制文件是为 amd64，arm (32-bit)，arm64，ppc64le 和 s390x 构建的遵循<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md">多平台提案</a>。</p>
<!--
Multiplatform container images for the control plane and addons are also supported since v1.12.
-->
<p>从 v1.12 开始还支持用于控制平面和附加组件的多平台容器镜像。</p>
<!--
Only some of the network providers offer solutions for all platforms. Please consult the list of
network providers above or the documentation from each provider to figure out whether the provider
supports your chosen platform.
-->
<p>只有一些网络提供商为所有平台提供解决方案。请查阅上方的网络提供商清单或每个提供商的文档以确定提供商是否支持你选择的平台。</p>
<!--
## Troubleshooting {#troubleshooting}
-->
<h2 id="troubleshooting">故障排除</h2>
<!--
If you are running into difficulties with kubeadm, please consult our [troubleshooting docs](/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/).
-->
<p>如果你在使用 kubeadm 时遇到困难，请查阅我们的<a href="/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/">故障排除文档</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4c656c5eda3e1c06ad1aedebdc04a211">2.3.1.4 - 使用 kubeadm API 定制组件</h1>
    
	<!--
---
reviewers:
- sig-cluster-lifecycle
title: Customizing components with the kubeadm API
content_type: concept
weight: 40
---
-->
<!-- overview -->
<!--
This page covers how to customize the components that kubeadm deploys. For control plane components
you can use flags in the `ClusterConfiguration` structure or patches per-node. For the kubelet
and kube-proxy you can use `KubeletConfiguration` and `KubeProxyConfiguration`, accordingly.

All of these options are possible via the kubeadm configuration API.
For more details on each field in the configuration you can navigate to our
[API reference pages](/docs/reference/config-api/kubeadm-config.v1beta3/).
-->
<p>本页面介绍了如何自定义 kubeadm 部署的组件。
你可以使用 <code>ClusterConfiguration</code> 结构中定义的参数，或者在每个节点上应用补丁来定制控制平面组件。
你可以使用 <code>KubeletConfiguration</code> 和 <code>KubeProxyConfiguration</code> 结构分别定制 kubelet 和 kube-proxy 组件。</p>
<p>所有这些选项都可以通过 kubeadm 配置 API 实现。
有关配置中的每个字段的详细信息，你可以导航到我们的
<a href="/docs/reference/config-api/kubeadm-config.v1beta3/">API 参考页面</a> 。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Customizing the CoreDNS deployment of kubeadm is currently not supported. You must manually
patch the `kube-system/coredns` <a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a>
and recreate the CoreDNS <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> after that. Alternatively,
you can skip the default CoreDNS deployment and deploy your own variant.
For more details on that see [Using init phases with kubeadm](/docs/reference/setup-tools/kubeadm/kubeadm-init/#init-phases).
-->
<p>kubeadm 目前不支持对 CoreDNS 部署进行定制。
你必须手动更新 <code>kube-system/coredns</code> <a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a>
并在更新后重新创建 CoreDNS <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>。
或者，你可以跳过默认的 CoreDNS 部署并部署你自己的 CoreDNS 变种。
有关更多详细信息，请参阅<a href="/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#init-phases">在 kubeadm 中使用 init phases</a>.
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
To reconfigure a cluster that has already been created see
[Reconfiguring a kubeadm cluster](/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure).
-->
<p>要重新配置已创建的集群，请参阅<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure">重新配置 kubeadm 集群</a>。
</div>
<!-- body -->
<!--
## Customizing the control plane with flags in `ClusterConfiguration`

The kubeadm `ClusterConfiguration` object exposes a way for users to override the default
flags passed to control plane components such as the APIServer, ControllerManager, Scheduler and Etcd.
The components are defined using the following structures:
-->
<h2 id="customizing-the-control-plane-with-flags-in-clusterconfiguration">使用 <code>ClusterConfiguration</code> 中的标志自定义控制平面  </h2>
<p>kubeadm <code>ClusterConfiguration</code> 对象为用户提供了一种方法，
用以覆盖传递给控制平面组件（如 APIServer、ControllerManager、Scheduler 和 Etcd）的默认参数。
各组件配置使用如下字段定义：</p>
<ul>
<li><code>apiServer</code></li>
<li><code>controllerManager</code></li>
<li><code>scheduler</code></li>
<li><code>etcd</code></li>
</ul>
<!--
These structures contain a common `extraArgs` field, that consists of `key: value` pairs.
To override a flag for a control plane component:
-->
<p>这些结构包含一个通用的 <code>extraArgs</code> 字段，该字段由 <code>key: value</code> 组成。
要覆盖控制平面组件的参数：</p>
<!--
1.  Add the appropriate `extraArgs` to your configuration.
2.  Add flags to the `extraArgs` field.
3.  Run `kubeadm init` with `--config <YOUR CONFIG YAML>`.
-->
<ol>
<li>将适当的字段 <code>extraArgs</code> 添加到配置中。</li>
<li>向字段 <code>extraArgs</code> 添加要覆盖的参数值。</li>
<li>用 <code>--config &lt;YOUR CONFIG YAML&gt;</code> 运行 <code>kubeadm init</code>。</li>
</ol>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
You can generate a `ClusterConfiguration` object with default values by running `kubeadm config print init-defaults` 
and saving the output to a file of your choice. 
-->
<p>你可以通过运行 <code>kubeadm config print init-defaults</code> 并将输出保存到你所选的文件中，
以默认值形式生成 <code>ClusterConfiguration</code> 对象。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
The `ClusterConfiguration` object is currently global in kubeadm clusters. This means that any flags that you add,
will apply to all instances of the same component on different nodes. To apply individual configuration per component
on different nodes you can use [patches](#patches).
-->
<p><code>ClusterConfiguration</code> 对象目前在 kubeadm 集群中是全局的。
这意味着你添加的任何标志都将应用于同一组件在不同节点上的所有实例。
要在不同节点上为每个组件应用单独的配置，您可以使用<a href="#patches">补丁</a>。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
Duplicate flags (keys), or passing the same flag `--foo` multiple times, is currently not supported.
To workaround that you must use [patches](#patches).
-->
<p>当前不支持重复的参数（keys）或多次传递相同的参数 <code>--foo</code>。
要解决此问题，你必须使用<a href="#patches">补丁</a>。
</div>
<!--
## APIServer flags
-->
<h3 id="apiserver-flags">APIServer 参数  </h3>
<!--
For details, see the [reference documentation for kube-apiserver](/docs/reference/command-line-tools-reference/kube-apiserver/).
-->
<p>有关详细信息，请参阅 <a href="/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver 参考文档</a>。</p>
<!--
Example usage:
-->
<p>使用示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kubernetesVersion</span>:<span style="color:#bbb"> </span>v1.16.0<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiServer</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">extraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">anonymous-auth</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;false&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">enable-admission-plugins</span>:<span style="color:#bbb"> </span>AlwaysPullImages,DefaultStorageClass<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">audit-log-path</span>:<span style="color:#bbb"> </span>/home/johndoe/audit.log<span style="color:#bbb">
</span></code></pre></div><!--
## ControllerManager flags
-->
<h3 id="controllermanager-flags">ControllerManager 参数  </h3>
<!--
For details, see the [reference documentation for kube-controller-manager](/docs/reference/command-line-tools-reference/kube-controller-manager/).
-->
<p>有关详细信息，请参阅 <a href="/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager 参考文档</a>。</p>
<!--
Example usage:
-->
<p>使用示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kubernetesVersion</span>:<span style="color:#bbb"> </span>v1.16.0<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">controllerManager</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">extraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster-signing-key-file</span>:<span style="color:#bbb"> </span>/home/johndoe/keys/ca.key<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">deployment-controller-sync-period</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;50&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
### Scheduler flags
-->
<h2 id="scheduler-flags">Scheduler 参数  </h2>
<!--
For details, see the [reference documentation for kube-scheduler](/docs/reference/command-line-tools-reference/kube-scheduler/).
-->
<p>有关详细信息，请参阅 <a href="/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler 参考文档</a>。</p>
<!--
Example usage:
-->
<p>使用示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kubernetesVersion</span>:<span style="color:#bbb"> </span>v1.16.0<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">scheduler</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">extraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">config</span>:<span style="color:#bbb"> </span>/etc/kubernetes/scheduler-config.yaml<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">extraVolumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>schedulerconfig<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb"> </span>/home/johndoe/schedconfig.yaml<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/kubernetes/scheduler-config.yaml<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;File&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
### Etcd flags

For details, see the [etcd server documentation](https://etcd.io/docs/).

Example usage:
-->
<h3 id="etcd-flags">Etcd 参数  </h3>
<p>有关详细信息，请参阅 <a href="https://etcd.io/docs/">etcd 服务文档</a>.</p>
<p>使用示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">etcd</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">local</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">extraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">election-timeout</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span></code></pre></div><!--
## Customizing the control plane with patches {#patches}






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>



Kubeadm allows you to pass a directory with patch files to `InitConfiguration` and `JoinConfiguration`
on individual nodes. These patches can be used as the last customization step before the control
plane component manifests are written to disk.

You can pass this file to `kubeadm init` with `--config <YOUR CONFIG YAML>`:
-->
<h2 id="patches">使用补丁定制控制平面  </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<p>Kubeadm 允许将包含补丁文件的目录传递给各个节点上的 <code>InitConfiguration</code> 和 <code>JoinConfiguration</code>。
这些补丁可被用作控制平面组件清单写入磁盘之前的最后一个自定义步骤。</p>
<p>可以使用 <code>--config &lt;你的 YAML 格式控制文件&gt;</code> 将配置文件传递给 <code>kubeadm init</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>InitConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">patches</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">directory</span>:<span style="color:#bbb"> </span>/home/user/somedir<span style="color:#bbb">
</span></code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
For `kubeadm init` you can pass a file containing both a `ClusterConfiguration` and `InitConfiguration`
separated by `---`.
-->
<p>对于 <code>kubeadm init</code>，你可以传递一个包含 <code>ClusterConfiguration</code> 和 <code>InitConfiguration</code> 的文件，以 <code>---</code> 分隔。
</div>
<!--
You can pass this file to `kubeadm join` with `--config <YOUR CONFIG YAML>`:
-->
<p>你可以使用 <code>--config &lt;你的 YAML 格式配置文件&gt;</code> 将配置文件传递给 <code>kubeadm join</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>JoinConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">patches</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">directory</span>:<span style="color:#bbb"> </span>/home/user/somedir<span style="color:#bbb">
</span></code></pre></div><!--
The directory must contain files named `target[suffix][+patchtype].extension`.
For example, `kube-apiserver0+merge.yaml` or just `etcd.json`.
-->
<p>补丁目录必须包含名为 <code>target[suffix][+patchtype].extension</code> 的文件。
例如，<code>kube-apiserver0+merge.yaml</code>  或只是 <code>etcd.json</code>。</p>
<!--
- `target` can be one of `kube-apiserver`, `kube-controller-manager`, `kube-scheduler` and `etcd`.
- `patchtype` can be one of `strategic`, `merge` or `json` and these must match the patching formats
[supported by kubectl](/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch).
The default `patchtype` is `strategic`.
- `extension` must be either `json` or `yaml`.
- `suffix` is an optional string that can be used to determine which patches are applied first
alpha-numerically.
-->
<ul>
<li><code>target</code> 可以是 <code>kube-apiserver</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>etcd</code> 之一。</li>
<li><code>patchtype</code> 可以是 <code>strategy</code>、<code>merge</code> 或 <code>json</code> 之一，并且这些必须匹配
<a href="/zh/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch">kubectl 支持</a> 的补丁格式。
默认补丁类型是 <code>strategic</code> 的。</li>
<li><code>extension</code> 必须是 <code>json</code> 或 <code>yaml</code>。</li>
<li><code>suffix</code> 是一个可选字符串，可用于确定首先按字母数字应用哪些补丁。</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you are using `kubeadm upgrade` to upgrade your kubeadm nodes you must again provide the same
patches, so that the customization is preserved after upgrade. To do that you can use the `--patches`
flag, which must point to the same directory. `kubeadm upgrade` currently does not support a configuration
API structure that can be used for the same purpose.
-->
<p>如果你使用 <code>kubeadm upgrade</code> 升级 kubeadm 节点，你必须再次提供相同的补丁，以便在升级后保留自定义配置。
为此，你可以使用 <code>--patches</code> 参数，该参数必须指向同一目录。 <code>kubeadm upgrade</code> 目前不支持用于相同目的的 API 结构配置。
</div>
<!--
## Customizing the kubelet

To customize the kubelet you can add a `KubeletConfiguration` next to the `ClusterConfiguration` or
`InitConfiguration` separated by `---` within the same configuration file. This file can then be passed to `kubeadm init`.
-->
<h2 id="customizing-the-kubelet">自定义 kubelet  </h2>
<p>要自定义 kubelet，你可以在同一配置文件中的 <code>ClusterConfiguration</code> 或 <code>InitConfiguration</code>
之外添加一个 <code>KubeletConfiguration</code>，用 <code>---</code> 分隔。
然后可以将此文件传递给 <code>kubeadm init</code>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
kubeadm applies the same `KubeletConfiguration` to all nodes in the cluster. To apply node
specific settings you can use kubelet flags as overrides by passing them in the `nodeRegistration.kubeletExtraArgs`
field supported by both `InitConfiguration` and `JoinConfiguration`. Some kubelet flags are deprecated,
so check their status in the [kubelet reference documentation](/docs/reference/command-line-tools-reference/kubelet)
before using them.
-->
<p>kubeadm 将相同的 <code>KubeletConfiguration</code> 配置应用于集群中的所有节点。
要应用节点特定设置，你可以使用 <code>kubelet</code> 参数进行覆盖，方法是将它们传递到 <code>InitConfiguration</code> 和 <code>JoinConfiguration</code>
支持的 <code>nodeRegistration.kubeletExtraArgs</code> 字段中。一些 kubelet 参数已被弃用，
因此在使用这些参数之前，请在 <a href="/zh/docs/reference/command-line-tools-reference/kubelet">kubelet 参考文档</a> 中检查它们的状态。
</div>
<!--
For more details see [Configuring each kubelet in your cluster using kubeadm](/docs/setup/production-environment/tools/kubeadm/kubelet-integration)
-->
<p>更多详情，请参阅<a href="/zh/docs/setup/production-environment/tools/kubeadm/kubelet-integration">使用 kubeadm 配置集群中的每个 kubelet</a></p>
<!--
## Customizing kube-proxy

To customize kube-proxy you can pass a `KubeProxyConfiguration` next your `ClusterConfiguration` or
`InitConfiguration` to `kubeadm init` separated by `---`.

For more details you can navigate to our [API reference pages](/docs/reference/config-api/kubeadm-config.v1beta3/).
-->
<h2 id="customizing-kube-proxy">自定义 kube-proxy  </h2>
<p>要自定义 kube-proxy，你可以在 <code>ClusterConfiguration</code> 或 <code>InitConfiguration</code> 之外添加一个
由 <code>---</code> 分隔的 <code>KubeProxyConfiguration</code>， 传递给 <code>kubeadm init</code>。</p>
<p>可以导航到 <a href="/docs/reference/config-api/kubeadm-config.v1beta3/">API 参考页面</a> 查看更多详情，</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
kubeadm deploys kube-proxy as a <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>, which means
that the `KubeProxyConfiguration` would apply to all instances of kube-proxy in the cluster.
-->
<p>kubeadm 将 kube-proxy 部署为 <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>，
这意味着 <code>KubeProxyConfiguration</code> 将应用于集群中的所有 kube-proxy 实例。
</div>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-015edbc7cc688d31b1d1edce7c186135">2.3.1.5 - 高可用拓扑选项</h1>
    
	<!--
---
reviewers:
- sig-cluster-lifecycle
title: Options for Highly Available Topology
content_type: concept
weight: 50
---
-->
<!-- overview -->
<!--
This page explains the two options for configuring the topology of your highly available (HA) Kubernetes clusters.
-->
<p>本页面介绍了配置高可用（HA） Kubernetes 集群拓扑的两个选项。</p>
<!--
You can set up an HA cluster:
-->
<p>您可以设置 HA 集群：</p>
<!--
- With stacked control plane nodes, where etcd nodes are colocated with control plane nodes
- With external etcd nodes, where etcd runs on separate nodes from the control plane
-->
<ul>
<li>使用堆叠（stacked）控制平面节点，其中 etcd 节点与控制平面节点共存</li>
<li>使用外部 etcd 节点，其中 etcd 在与控制平面不同的节点上运行</li>
</ul>
<!--
You should carefully consider the advantages and disadvantages of each topology before setting up an HA cluster.
-->
<p>在设置 HA 集群之前，您应该仔细考虑每种拓扑的优缺点。</p>
<!--
kubeadm bootstraps the etcd cluster statically. Read the etcd [Clustering Guide](https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md#static)
for more details.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> kubeadm 静态引导 etcd 集群。 阅读 etcd <a href="https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md#static">集群指南</a>以获得更多详细信息。
</div>
<!-- body -->
<!--
## Stacked etcd topology
-->
<h2 id="堆叠-stacked-etcd-拓扑">堆叠（Stacked） etcd 拓扑</h2>
<!--
A stacked HA cluster is a [topology](https://en.wikipedia.org/wiki/Network_topology) where the distributeddata storage cluster provided by etcd is stacked on top of the cluster formed by the nodes managed by kubeadm that run control plane components.
-->
<p>堆叠（Stacked） HA 集群是一种这样的<a href="https://en.wikipedia.org/wiki/Network_topology">拓扑</a>，其中 etcd 分布式数据存储集群堆叠在 kubeadm 管理的控制平面节点上，作为控制平面的一个组件运行。</p>
<!--
Each control plane node runs an instance of the `kube-apiserver`, `kube-scheduler`, and `kube-controller-manager`.
-->
<p>每个控制平面节点运行 <code>kube-apiserver</code>，<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 实例。</p>
<!--
The `kube-apiserver` is exposed to worker nodes using a load balancer.
-->
<p><code>kube-apiserver</code> 使用负载均衡器暴露给工作节点。</p>
<!--
Each control plane node creates a local etcd member and this etcd member communicates only with
the `kube-apiserver` of this node. The same applies to the local `kube-controller-manager`
and `kube-scheduler` instances.
-->
<p>每个控制平面节点创建一个本地 etcd 成员（member），这个 etcd 成员只与该节点的 <code>kube-apiserver</code> 通信。这同样适用于本地 <code>kube-controller-manager</code> 和 <code>kube-scheduler</code> 实例。</p>
<!--
This topology couples the control planes and etcd members on the same nodes. It is simpler to set up than a cluster with external etcd nodes, and simpler to manage for replication.
-->
<p>这种拓扑将控制平面和 etcd 成员耦合在同一节点上。相对使用外部 etcd 集群，设置起来更简单，而且更易于副本管理。</p>
<!--
However, a stacked cluster runs the risk of failed coupling. If one node goes down, both an etcd member and a controlplane instance are lost, and redundancy is compromised. You can mitigate this risk by adding more control plane nodes.
-->
<p>然而，堆叠集群存在耦合失败的风险。如果一个节点发生故障，则 etcd 成员和控制平面实例都将丢失，并且冗余会受到影响。您可以通过添加更多控制平面节点来降低此风险。</p>
<!--
You should therefore run a minimum of three stacked control plane nodes for an HA cluster.
-->
<p>因此，您应该为 HA 集群运行至少三个堆叠的控制平面节点。</p>
<!--
This is the default topology in kubeadm. A local etcd member is created automatically
on control plane nodes when using `kubeadm init` and `kubeadm join --control-plane`.
-->
<p>这是 kubeadm 中的默认拓扑。当使用 <code>kubeadm init</code> 和 <code>kubeadm join --control-plane</code> 时，在控制平面节点上会自动创建本地 etcd 成员。</p>
<!--
![Stacked etcd topology](/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg)
-->
<p><img src="/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg" alt="堆叠的 etcd 拓扑"></p>
<!--
## External etcd topology
-->
<h2 id="外部-etcd-拓扑">外部 etcd 拓扑</h2>
<!--
An HA cluster with external etcd is a [topology](https://en.wikipedia.org/wiki/Network_topology) where the distributed data storage cluster provided by etcd is external to the cluster formed by the nodes that run control plane components.
-->
<p>具有外部 etcd 的 HA 集群是一种这样的<a href="https://en.wikipedia.org/wiki/Network_topology">拓扑</a>，其中 etcd 分布式数据存储集群在独立于控制平面节点的其他节点上运行。</p>
<!--
Like the stacked etcd topology, each control plane node in an external etcd topology runs an instance of the `kube-apiserver`, `kube-scheduler`, and `kube-controller-manager`. And the `kube-apiserver` is exposed to worker nodes using a load balancer. However, etcd members run on separate hosts, and each etcd host communicates with the `kube-apiserver` of each control plane node.
-->
<p>就像堆叠的 etcd 拓扑一样，外部 etcd 拓扑中的每个控制平面节点都运行 <code>kube-apiserver</code>，<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 实例。同样， <code>kube-apiserver</code> 使用负载均衡器暴露给工作节点。但是，etcd 成员在不同的主机上运行，​​每个 etcd 主机与每个控制平面节点的 <code>kube-apiserver</code> 通信。</p>
<!--
This topology decouples the control plane and etcd member. It therefore provides an HA setup wherelosing a control plane instance or an etcd member has less impact and does not affectthe cluster redundancy as much as the stacked HA topology.
-->
<p>这种拓扑结构解耦了控制平面和 etcd 成员。因此，它提供了一种 HA 设置，其中失去控制平面实例或者 etcd 成员的影响较小，并且不会像堆叠的 HA 拓扑那样影响集群冗余。</p>
<!--
However, this topology requires twice the number of hosts as the stacked HA topology.
-->
<p>但是，此拓扑需要两倍于堆叠 HA 拓扑的主机数量。</p>
<!--
A minimum of three hosts for control plane nodes and three hosts for etcd nodes are required for an HA cluster with this topology.
-->
<p>具有此拓扑的 HA 集群至少需要三个用于控制平面节点的主机和三个用于 etcd 节点的主机。</p>
<!--
![External etcd topology](/images/kubeadm/kubeadm-ha-topology-external-etcd.svg)
-->
<p><img src="/images/kubeadm/kubeadm-ha-topology-external-etcd.svg" alt="外部 etcd 拓扑"></p>
<h2 id="what-s-next">What's next</h2>
<!--
- [Set up a highly available cluster with kubeadm](/docs/setup/production-environment/tools/kubeadm/high-availability/)
-->
<ul>
<li><a href="/zh/docs/setup/production-environment/tools/kubeadm/high-availability/">使用 kubeadm 设置高可用集群</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3941d5c3409342219bf7e03128b8ecb6">2.3.1.6 - 利用 kubeadm 创建高可用集群</h1>
    
	<!--
reviewers:
- sig-cluster-lifecycle
title: Creating Highly Available Clusters with kubeadm
content_type: task
weight: 60
-->
<!-- overview -->
<!--
This page explains two different approaches to setting up a highly available Kubernetes
cluster using kubeadm:

- With stacked control plane nodes. This approach requires less infrastructure. The etcd members
  and control plane nodes are co-located.
- With an external etcd cluster. This approach requires more infrastructure. The
  control plane nodes and etcd members are separated.

-->
<p>本文讲述了使用 kubeadm 设置一个高可用的 Kubernetes 集群的两种不同方式：</p>
<ul>
<li>使用具有堆叠的控制平面节点。这种方法所需基础设施较少。etcd 成员和控制平面节点位于同一位置。</li>
<li>使用外部集群。这种方法所需基础设施较多。控制平面的节点和 etcd 成员是分开的。</li>
</ul>
<!--
Before proceeding, you should carefully consider which approach best meets the needs of your applications
and environment. [Options for Highly Available topology](/docs/setup/production-environment/tools/kubeadm/ha-topology/) outlines the advantages and disadvantages of each.

If you encounter issues with setting up the HA cluster, please report these
in the kubeadm [issue tracker](https://github.com/kubernetes/kubeadm/issues/new).

See also the [upgrade documentation](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15).
-->
<p>在下一步之前，你应该仔细考虑哪种方法更好的满足你的应用程序和环境的需求。
<a href="/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/">高可用拓扑选项</a> 讲述了每种方法的优缺点。</p>
<p>如果你在安装 HA 集群时遇到问题，请在 kubeadm <a href="https://github.com/kubernetes/kubeadm/issues/new">问题跟踪</a>里向我们提供反馈。</p>
<p>你也可以阅读<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">升级文档</a></p>
<!--
This page does not address running your cluster on a cloud provider. In a cloud
environment, neither approach documented here works with Service objects of type
LoadBalancer, or with dynamic PersistentVolumes.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 这篇文档没有讲述在云提供商上运行集群的问题。在云环境中，此处记录的方法不适用于类型为 LoadBalancer 的服务对象，或者具有动态的 PersistentVolumes。
</div>

<h2 id="before-you-begin">Before you begin</h2>
<!--
The prerequisites depend on which topology you have selected for your cluster's
control plane:
-->
<p>根据集群控制平面所选择的拓扑结构不同，准备工作也有所差异：</p>
<ul class="nav nav-tabs" id="prerequisite-tabs" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#prerequisite-tabs-0" role="tab" aria-controls="prerequisite-tabs-0" aria-selected="true">堆叠（Stacked） etcd 拓扑</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#prerequisite-tabs-1" role="tab" aria-controls="prerequisite-tabs-1">外部 etcd 拓扑</a></li></ul>
<div class="tab-content" id="prerequisite-tabs"><div id="prerequisite-tabs-0" class="tab-pane show active" role="tabpanel" aria-labelledby="prerequisite-tabs-0">

<p><!--
    note to reviewers: these prerequisites should match the start of the
    external etc tab
-->
<!--
You need:

- Three or more machines that meet [kubeadm's minimum requirements](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin) for
  the control-plane nodes. Having an odd number of control plane nodes can help
  with leader selection in the case of machine or zone failure.
  - including a <a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>, already set up and working
- Three or more machines that meet [kubeadm's minimum
  requirements](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin) for the workers
  - including a container runtime, already set up and working
- Full network connectivity between all machines in the cluster (public or
  private network)
- Superuser privileges on all machines using `sudo`
  - You can use a different tool; this guide uses `sudo` in the examples.
- SSH access from one device to all nodes in the system
- `kubeadm` and `kubelet` already installed on all machines.

_See [Stacked etcd topology](/docs/setup/production-environment/tools/kubeadm/ha-topology/#stacked-etcd-topology) for context._
-->
<p>需要准备：</p>
<ul>
<li>配置满足 <a href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B">kubeadm 的最低要求</a>
的三台机器作为控制面节点。奇数台控制平面节点有利于机器故障或者网络分区时进行重新选主。
<ul>
<li>机器已经安装好<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>，并正常运行</li>
</ul>
</li>
<li>配置满足 <a href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B">kubeadm 的最低要求</a>
的三台机器作为工作节点
<ul>
<li>机器已经安装好<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>，并正常运行</li>
</ul>
</li>
<li>在集群中，确保所有计算机之间存在全网络连接（公网或私网）</li>
<li>在所有机器上具有 sudo 权限
<ul>
<li>可以使用其他工具；本教程以 <code>sudo</code> 举例</li>
</ul>
</li>
<li>从某台设备通过 SSH 访问系统中所有节点的能力</li>
<li>所有机器上已经安装 <code>kubeadm</code> 和 <code>kubelet</code></li>
</ul>
<p><em>拓扑详情请参考<a href="/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/#%E5%A0%86%E5%8F%A0-stacked-etcd-%E6%8B%93%E6%89%91">堆叠（Stacked）etcd 拓扑</a>。</em></p>
</div>
  <div id="prerequisite-tabs-1" class="tab-pane" role="tabpanel" aria-labelledby="prerequisite-tabs-1">

<p><!--
    note to reviewers: these prerequisites should match the start of the
    stacked etc tab
-->
<!--
You need:

- Three or more machines that meet [kubeadm's minimum requirements](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin) for
  the control-plane nodes. Having an odd number of control plane nodes can help
  with leader selection in the case of machine or zone failure.
  - including a <a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>, already set up and working
- Three or more machines that meet [kubeadm's minimum
  requirements](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin) for the workers
  - including a container runtime, already set up and working
- Full network connectivity between all machines in the cluster (public or
  private network)
- Superuser privileges on all machines using `sudo`
  - You can use a different tool; this guide uses `sudo` in the examples.
- SSH access from one device to all nodes in the system
- `kubeadm` and `kubelet` already installed on all machines.
-->
<p>需要准备：</p>
<ul>
<li>配置满足 <a href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B">kubeadm 的最低要求</a>
的三台机器作为控制面节点。奇数台控制平面节点有利于机器故障或者网络分区时进行重新选主。
<ul>
<li>机器已经安装好<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>，并正常运行</li>
</ul>
</li>
<li>配置满足 <a href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B">kubeadm 的最低要求</a>
的三台机器作为工作节点
<ul>
<li>机器已经安装好<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>，并正常运行</li>
</ul>
</li>
<li>在集群中，确保所有计算机之间存在全网络连接（公网或私网）</li>
<li>在所有机器上具有 sudo 权限
<ul>
<li>可以使用其他工具；本教程以 <code>sudo</code> 举例</li>
</ul>
</li>
<li>从某台设备通过 SSH 访问系统中所有节点的能力</li>
<li>所有机器上已经安装 <code>kubeadm</code> 和 <code>kubelet</code></li>
</ul>
<!-- end of shared prerequisites -->
<!--
And you also need:
- Three or more additional machines, that will become etcd cluster members.
  Having an odd number of members in the etcd cluster is a requirement for achieving
  optimal voting quorum.
  - These machines again need to have `kubeadm` and `kubelet` installed.
  - These machines also require a container runtime, that is already set up and working.
-->
<p>还需要准备：</p>
<ul>
<li>给 etcd 集群使用的另外三台及以上机器。为了分布式一致性算法达到更好的投票效果，集群必须由奇数个节点组成。
<ul>
<li>机器上已经安装 <code>kubeadm</code> 和 <code>kubelet</code>。</li>
<li>机器上同样需要安装好容器运行时，并能正常运行。</li>
</ul>
</li>
</ul>
<!--
_See [External etcd topology](/docs/setup/production-environment/tools/kubeadm/ha-topology/#external-etcd-topology) for context._
-->
<p><em>拓扑详情请参考<a href="/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/#%E5%A4%96%E9%83%A8-etcd-%E6%8B%93%E6%89%91">外部 etcd 拓扑</a>。</em></p>
</div></div>

<!-- ### Container images -->
<h3 id="容器镜像">容器镜像</h3>
<!--
Each host should have access read and fetch images from the Kubernetes container image registry, `k8s.gcr.io`.
If you want to deploy a highly-available cluster where the hosts do not have access to pull images, this is possible. You must ensure by some other means that the correct container images are already available on the relevant hosts.
-->
<p>每台主机需要能够从 Kubernetes 容器镜像仓库（ <code>k8s.gcr.io</code> ）读取和拉取镜像。
想要在无法拉取 Kubernetes 仓库镜像的机器上部署高可用集群也是可行的。通过其他的手段保证主机上已经有对应的容器镜像即可。</p>
<!-- ### Command line interface {#kubectl} -->
<h3 id="kubectl">命令行 </h3>
<!--
To manage Kubernetes once your cluster is set up, you should
[install kubectl](/docs/tasks/tools/#kubectl) on your PC. It is also useful
to install the `kubectl` tool on each control plane node, as this can be
helpful for troubleshooting.
-->
<p>一旦集群创建成功，需要在 PC 上<a href="/zh/docs/tasks/tools/#kubectl">安装 kubectl</a> 用于管理 Kubernetes。为了方便故障排查，也可以在每个控制平面节点上安装 <code>kubectl</code>。</p>
<!-- steps -->
<!--
## First steps for both methods

### Create load balancer for kube-apiserver
-->
<h2 id="这两种方法的第一步">这两种方法的第一步</h2>
<h3 id="为-kube-apiserver-创建负载均衡器">为 kube-apiserver 创建负载均衡器</h3>
<!--
There are many configurations for load balancers. The following example is only one
option. Your cluster requirements may need a different configuration.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 使用负载均衡器需要许多配置。你的集群搭建可能需要不同的配置。
下面的例子只是其中的一方面配置。
</div>
<!--
1. Create a kube-apiserver load balancer with a name that resolves to DNS.

   - In a cloud environment you should place your control plane nodes behind a TCP
     forwarding load balancer. This load balancer distributes traffic to all
     healthy control plane nodes in its target list. The health check for
     an apiserver is a TCP check on the port the kube-apiserver listens on
     (default value `:6443`).

   - It is not recommended to use an IP address directly in a cloud environment.

   - The load balancer must be able to communicate with all control plane nodes
     on the apiserver port. It must also allow incoming traffic on its
     listening port.

   - Make sure the address of the load balancer always matches
     the address of kubeadm's `ControlPlaneEndpoint`.

   - Read the [Options for Software Load Balancing](https://git.k8s.io/kubeadm/docs/ha-considerations.md#options-for-software-load-balancing)
     guide for more details.
-->
<ol>
<li>
<p>创建一个名为 kube-apiserver 的负载均衡器解析 DNS。</p>
<ul>
<li>
<p>在云环境中，应该将控制平面节点放置在 TCP 转发负载平衡后面。
该负载均衡器将流量分配给目标列表中所有运行状况良好的控制平面节点。
API 服务器的健康检查是在 kube-apiserver 的监听端口（默认值 <code>:6443</code>）
上进行的一个 TCP 检查。</p>
</li>
<li>
<p>不建议在云环境中直接使用 IP 地址。</p>
</li>
<li>
<p>负载均衡器必须能够在 API 服务器端口上与所有控制平面节点通信。
它还必须允许其监听端口的入站流量。</p>
</li>
<li>
<p>确保负载均衡器的地址始终匹配 kubeadm 的 <code>ControlPlaneEndpoint</code> 地址。</p>
</li>
<li>
<p>阅读<a href="https://git.k8s.io/kubeadm/docs/ha-considerations.md#options-for-software-load-balancing">软件负载平衡选项指南</a>
以获取更多详细信息。</p>
</li>
</ul>
</li>
</ol>
<!--
1. Add the first control plane nodes to the load balancer and test the
   connection:

   ```sh
   nc -v LOAD_BALANCER_IP PORT
   ```

   - A connection refused error is expected because the apiserver is not yet
     running. A timeout, however, means the load balancer cannot communicate
     with the control plane node. If a timeout occurs, reconfigure the load
     balancer to communicate with the control plane node.

1. Add the remaining control plane nodes to the load balancer target group.
-->
<ol start="2">
<li>
<p>添加第一个控制平面节点到负载均衡器并测试连接：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">nc -v LOAD_BALANCER_IP PORT
</code></pre></div><p>由于 apiserver 尚未运行，预期会出现一个连接拒绝错误。
然而超时意味着负载均衡器不能和控制平面节点通信。
如果发生超时，请重新配置负载均衡器与控制平面节点进行通信。</p>
</li>
<li>
<p>将其余控制平面节点添加到负载均衡器目标组。</p>
</li>
</ol>
<!--
## Stacked control plane and etcd nodes

### Steps for the first control plane node
-->
<h2 id="使用堆控制平面和-etcd-节点">使用堆控制平面和 etcd 节点</h2>
<h3 id="控制平面节点的第一步">控制平面节点的第一步</h3>
<!--
1. Initialize the control plane:

   ```sh
   sudo kubeadm init --control-plane-endpoint "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" --upload-certs
   ```
   - You can use the `--kubernetes-version` flag to set the Kubernetes version to use.
     It is recommended that the versions of kubeadm, kubelet, kubectl and Kubernetes match.
   - The `--control-plane-endpoint` flag should be set to the address or DNS and port of the load balancer.

   - The `--upload-certs` flag is used to upload the certificates that should be shared
     across all the control-plane instances to the cluster. If instead, you prefer to copy certs across
     control-plane nodes manually or using automation tools, please remove this flag and refer to [Manual
     certificate distribution](#manual-certs) section bellow.
-->
<ol>
<li>
<p>初始化控制平面：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo kubeadm init --control-plane-endpoint <span style="color:#b44">&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span> --upload-certs
</code></pre></div><ul>
<li>你可以使用 <code>--kubernetes-version</code> 标志来设置要使用的 Kubernetes 版本。
建议将 kubeadm、kebelet、kubectl 和 Kubernetes 的版本匹配。</li>
<li>这个 <code>--control-plane-endpoint</code> 标志应该被设置成负载均衡器的地址或 DNS 和端口。</li>
<li>这个 <code>--upload-certs</code> 标志用来将在所有控制平面实例之间的共享证书上传到集群。
如果正好相反，你更喜欢手动地通过控制平面节点或者使用自动化工具复制证书，
请删除此标志并参考如下部分<a href="#manual-certs">证书分配手册</a>。</li>
</ul>
<!--
The `kubeadm init` flags `--config` and `--certificate-key` cannot be mixed, therefore if you want
to use the [kubeadm configuration](/docs/reference/config-api/kubeadm-config.v1beta3/) you must add the `certificateKey` field in the appropriate config locations (under `InitConfiguration` and `JoinConfiguration: controlPlane`).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 标志 <code>kubeadm init</code>、<code>--config</code> 和 <code>--certificate-key</code> 不能混合使用，
因此如果你要使用
<a href="/docs/reference/config-api/kubeadm-config.v1beta3/">kubeadm 配置</a>，你必须在相应的配置结构
（位于 <code>InitConfiguration</code> 和 <code>JoinConfiguration: controlPlane</code>）添加 <code>certificateKey</code> 字段。
</div>
<!--
Some CNI network plugins like Calico require a CIDR such as `192.168.0.0/16` and
some like Weave do not. See the [CNI network documentation](/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network).
To add a pod CIDR pass the flag `--pod-network-cidr`, or if you are using a kubeadm configuration file
set the `podSubnet` field under the `networking` object of `ClusterConfiguration`.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 一些 CNI 网络插件如 Calico 需要 CIDR 例如 <code>192.168.0.0/16</code> 和一些像 Weave 没有。参考
<a href="/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network">CNI 网络文档</a>。
通过传递 <code>--pod-network-cidr</code> 标志添加 pod CIDR，或者你可以使用 kubeadm
配置文件，在 <code>ClusterConfiguration</code> 的 <code>networking</code> 对象下设置 <code>podSubnet</code> 字段。
</div>
<!--
- The output looks similar to:
-->
<ul>
<li>
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">...
You can now join any number of control-plane node by running the following <span style="color:#a2f">command</span> on each as a root:
kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:
  kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</code></pre></div></li>
</ul>
<!--
- Copy this output to a text file. You will need it later to join control plane and worker nodes to the cluster.
- When `--upload-certs` is used with `kubeadm init`, the certificates of the primary control plane
are encrypted and uploaded in the `kubeadm-certs` Secret.
- To re-upload the certificates and generate a new decryption key, use the following command on a control plane
node that is already joined to the cluster:
-->
<ul>
<li>
<p>将此输出复制到文本文件。 稍后你将需要它来将控制平面节点和工作节点加入集群。</p>
</li>
<li>
<p>当使用 <code>--upload-certs</code> 调用 <code>kubeadm init</code> 时，主控制平面的证书被加密并上传到 <code>kubeadm-certs</code> Secret 中。</p>
</li>
<li>
<p>要重新上传证书并生成新的解密密钥，请在已加入集群节点的控制平面上使用以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo kubeadm init phase upload-certs --upload-certs
</code></pre></div></li>
</ul>
<!--
- You can also specify a custom `--certificate-key` during `init` that can later be used by `join`.
  To generate such a key you can use the following command:
-->
<ul>
<li>
<p>你还可以在 <code>init</code> 期间指定自定义的 <code>--certificate-key</code>，以后可以由 <code>join</code> 使用。
要生成这样的密钥，可以使用以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm certs certificate-key
</code></pre></div></li>
</ul>
<!--
The `kubeadm-certs` Secret and decryption key expire after two hours.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <code>kubeadm-certs</code> Secret 和解密密钥会在两个小时后失效。
</div>
<!--
As stated in the command output, the certificate key gives access to cluster sensitive data, keep it secret!
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 正如命令输出中所述，证书密钥可访问群集敏感数据。请妥善保管！
</div>

</li>
</ol>
<!--
1. Apply the CNI plugin of your choice:
   [Follow these instructions](/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network) to install the CNI provider. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file (if applicable).
-->
<ol start="2">
<li>应用你所选择的 CNI 插件：
<a href="/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network">请遵循以下指示</a>
安装 CNI 驱动。如果适用，请确保配置与 kubeadm 配置文件中指定的 Pod
CIDR 相对应。
<!--
You must pick a network plugin that suits your use case and deploy it before you move on to next step.
If you don't do this, you will not be able to launch your cluster properly.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在进行下一步之前，必须选择并部署合适的网络插件。
否则集群不会正常运行。
</div>
</li>
</ol>
<!--
1. Type the following and watch the pods of the control plane components get started:
-->
<ol start="3">
<li>
<p>输入以下内容，并查看控制平面组件的 Pods 启动：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -n kube-system -w
</code></pre></div></li>
</ol>
<!--
### Steps for the rest of the control plane nodes
-->
<h3 id="其余控制平面节点的步骤">其余控制平面节点的步骤</h3>
<!--
Since kubeadm version 1.15 you can join multiple control-plane nodes in parallel.
Prior to this version, you must join new control plane nodes sequentially, only after
the first node has finished initializing.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 从 kubeadm 1.15 版本开始，你可以并行加入多个控制平面节点。
在此版本之前，你必须在第一个节点初始化后才能依序的增加新的控制平面节点。
</div>
<!--
For each additional control plane node you should:

1. Execute the join command that was previously given to you by the `kubeadm init` output on the first node.
   It should look something like this:

   ```sh
   sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
   ```

   - The `--control-plane` flag tells `kubeadm join` to create a new control plane.
   - The `--certificate-key ...` will cause the control plane certificates to be downloaded
   from the `kubeadm-certs` Secret in the cluster and be decrypted using the given key.

-->
<p>对于每个其他控制平面节点，你应该：</p>
<ol>
<li>
<p>执行先前由第一个节点上的 <code>kubeadm init</code> 输出提供给你的 join 命令。
它看起来应该像这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</code></pre></div><ul>
<li>这个 <code>--control-plane</code> 标志通知 <code>kubeadm join</code> 创建一个新的控制平面。</li>
<li><code>--certificate-key ...</code> 将导致从集群中的 <code>kubeadm-certs</code> Secret
下载控制平面证书并使用给定的密钥进行解密。</li>
</ul>
</li>
</ol>
<!--
## External etcd nodes

Setting up a cluster with external etcd nodes is similar to the procedure used for stacked etcd
with the exception that you should setup etcd first, and you should pass the etcd information
in the kubeadm config file.
-->
<h2 id="外部-etcd-节点">外部 etcd 节点</h2>
<p>使用外部 etcd 节点设置集群类似于用于堆叠 etcd 的过程，
不同之处在于你应该首先设置 etcd，并在 kubeadm 配置文件中传递 etcd 信息。</p>
<!--
### Set up the etcd cluster

1. Follow [these instructions](/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/) to set up the etcd cluster.

1. Setup SSH as described [here](#manual-certs).

1. Copy the following files from any etcd node in the cluster to the first control plane node:

   ```sh
   export CONTROL_PLANE="ubuntu@10.0.0.7"
   scp /etc/kubernetes/pki/etcd/ca.crt "${CONTROL_PLANE}":
   scp /etc/kubernetes/pki/apiserver-etcd-client.crt "${CONTROL_PLANE}":
   scp /etc/kubernetes/pki/apiserver-etcd-client.key "${CONTROL_PLANE}":
   ```

   - Replace the value of `CONTROL_PLANE` with the `user@host` of the first control-plane machine.
-->
<h3 id="设置-ectd-集群">设置 ectd 集群</h3>
<ol>
<li>
<p>按照<a href="/zh/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">这些指示</a>
去设置 etcd 集群。</p>
</li>
<li>
<p>根据<a href="#manual-certs">这里</a> 的描述配置 SSH。</p>
</li>
<li>
<p>将以下文件从集群中的任何 etcd 节点复制到第一个控制平面节点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">export</span> <span style="color:#b8860b">CONTROL_PLANE</span><span style="color:#666">=</span><span style="color:#b44">&#34;ubuntu@10.0.0.7&#34;</span>
scp /etc/kubernetes/pki/etcd/ca.crt <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CONTROL_PLANE</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>:
scp /etc/kubernetes/pki/apiserver-etcd-client.crt <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CONTROL_PLANE</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>:
scp /etc/kubernetes/pki/apiserver-etcd-client.key <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CONTROL_PLANE</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>:
</code></pre></div><ul>
<li>用第一台控制平面机的 <code>user@host</code> 替换 <code>CONTROL_PLANE</code> 的值。</li>
</ul>
</li>
</ol>
<!--
### Set up the first control plane node

1. Create a file called `kubeadm-config.yaml` with the following contents:

   ```yaml
   ---
   apiVersion: kubeadm.k8s.io/v1beta3
   kind: ClusterConfiguration
   kubernetesVersion: stable
   controlPlaneEndpoint: "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" # change this (see below)
   etcd:
     external:
       endpoints:
         - https://ETCD_0_IP:2379 # change ETCD_0_IP appropriately
         - https://ETCD_1_IP:2379 # change ETCD_1_IP appropriately
         - https://ETCD_2_IP:2379 # change ETCD_2_IP appropriately
       caFile: /etc/kubernetes/pki/etcd/ca.crt
       certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
       keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
   ```
-->
<h3 id="设置第一个控制平面节点">设置第一个控制平面节点</h3>
<ol>
<li>
<p>用以下内容创建一个名为 <code>kubeadm-config.yaml</code> 的文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kubernetesVersion</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">controlPlaneEndpoint</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># change this (see below)</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">etcd</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">external</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">endpoints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- https://ETCD_0_IP:2379<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># change ETCD_0_IP appropriately</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- https://ETCD_1_IP:2379<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># change ETCD_1_IP appropriately</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- https://ETCD_2_IP:2379<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># change ETCD_2_IP appropriately</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">caFile</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki/etcd/ca.crt<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">certFile</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki/apiserver-etcd-client.crt<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">keyFile</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki/apiserver-etcd-client.key<span style="color:#bbb">
</span></code></pre></div><!--
The difference between stacked etcd and external etcd here is that the external etcd setup requires
a configuration file with the etcd endpoints under the `external` object for `etcd`.
In the case of the stacked etcd topology this is managed automatically.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 这里的堆叠（stacked）etcd 和外部 etcd 之前的区别在于设置外部 etcd
需要一个 <code>etcd</code> 的 <code>external</code> 对象下带有 etcd 端点的配置文件。
如果是内部 etcd，是自动管理的。
</div>
<!--
-  Replace the following variables in the config template with the appropriate values for your cluster:
-->
<ul>
<li>
<p>在你的集群中，将配置模板中的以下变量替换为适当值：</p>
<ul>
<li><code>LOAD_BALANCER_DNS</code></li>
<li><code>LOAD_BALANCER_PORT</code></li>
<li><code>ETCD_0_IP</code></li>
<li><code>ETCD_1_IP</code></li>
<li><code>ETCD_2_IP</code></li>
</ul>
</li>
</ul>
</li>
</ol>
<!--
The following steps are similar to the stacked etcd setup:
-->
<p>以下的步骤与设置内置 etcd 的集群是相似的：</p>
<!--
1. Run `sudo kubeadm init --config kubeadm-config.yaml --upload-certs` on this node.

1. Write the output join commands that are returned to a text file for later use.

1. Apply the CNI plugin of your choice. 
-->
<ol>
<li>
<p>在节点上运行 <code>sudo kubeadm init --config kubeadm-config.yaml --upload-certs</code> 命令。</p>
</li>
<li>
<p>记下输出的 join 命令，这些命令将在以后使用。</p>
</li>
<li>
<p>应用你选择的 CNI 插件。</p>
<!--
You must pick a network plugin that suits your use case and deploy it before you move on to next step.
If you don't do this, you will not be able to launch your cluster properly.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在进行下一步之前，必须选择并部署合适的网络插件。
否则集群不会正常运行。
</div>
</li>
</ol>
<!--
### Steps for the rest of the control plane nodes

The steps are the same as for the stacked etcd setup:

- Make sure the first control plane node is fully initialized.
- Join each control plane node with the join command you saved to a text file. It's recommended
to join the control plane nodes one at a time.
- Don't forget that the decryption key from `--certificate-key` expires after two hours, by default.
-->
<h3 id="其他控制平面节点的步骤">其他控制平面节点的步骤</h3>
<p>步骤与设置内置 etcd 相同：</p>
<ul>
<li>确保第一个控制平面节点已完全初始化。</li>
<li>使用保存到文本文件的 join 命令将每个控制平面节点连接在一起。
建议一次加入一个控制平面节点。</li>
<li>不要忘记默认情况下，<code>--certificate-key</code> 中的解密秘钥会在两个小时后过期。</li>
</ul>
<!--
## Common tasks after bootstrapping control plane

### Install workers
-->
<h2 id="列举控制平面之后的常见任务">列举控制平面之后的常见任务</h2>
<h3 id="安装工作节点">安装工作节点</h3>
<!--
Worker nodes can be joined to the cluster with the command you stored previously
as the output from the `kubeadm init` command:
-->
<p>你可以使用之前存储的 <code>kubeadm init</code> 命令的输出将工作节点加入集群中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</code></pre></div><!--
## Manual certificate distribution {#manual-certs}

If you choose to not use `kubeadm init` with the `--upload-certs` flag this means that
you are going to have to manually copy the certificates from the primary control plane node to the
joining control plane nodes.

There are many ways to do this. In the following example we are using `ssh` and `scp`:

SSH is required if you want to control all nodes from a single machine.
-->
<h2 id="manual-certs">手动证书分发</h2>
<p>如果你选择不将 <code>kubeadm init</code> 与 <code>--upload-certs</code> 命令一起使用，
则意味着你将必须手动将证书从主控制平面节点复制到
将要加入的控制平面节点上。</p>
<p>有许多方法可以实现这种操作。在下面的例子中我们使用 <code>ssh</code> 和 <code>scp</code>：</p>
<p>如果要在单独的一台计算机控制所有节点，则需要 SSH。</p>
<!--
1. Enable ssh-agent on your main device that has access to all other nodes in
   the system:
-->
<ol>
<li>
<p>在你的主设备上启用 ssh-agent，要求该设备能访问系统中的所有其他节点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">eval</span> <span style="color:#a2f;font-weight:bold">$(</span>ssh-agent<span style="color:#a2f;font-weight:bold">)</span>
</code></pre></div></li>
</ol>
<!--
1. Add your SSH identity to the session:
-->
<ol start="2">
<li>
<p>将 SSH 身份添加到会话中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ssh-add ~/.ssh/path_to_private_key
</code></pre></div></li>
</ol>
<!--
1. SSH between nodes to check that the connection is working correctly.
-->
<ol start="3">
<li>
<p>检查节点间的 SSH 以确保连接是正常运行的</p>
<!--
- When you SSH to any node, make sure to add the `-A` flag:
-->
<ul>
<li>
<p>SSH 到任何节点时，请确保添加 <code>-A</code> 标志：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ssh -A 10.0.0.7
</code></pre></div></li>
</ul>
<!--
- When using sudo on any node, make sure to preserve the environment so SSH
  forwarding works:
-->
<ul>
<li>
<p>当在任何节点上使用 sudo 时，请确保保持环境变量设置，以便 SSH
转发能够正常工作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo -E -s
</code></pre></div></li>
</ul>
</li>
</ol>
<!--
1. After configuring SSH on all the nodes you should run the following script on the first control plane node after
   running `kubeadm init`. This script will copy the certificates from the first control plane node to the other
   control plane nodes:
-->
<ol start="4">
<li>
<p>在所有节点上配置 SSH 之后，你应该在运行过 <code>kubeadm init</code> 命令的第一个
控制平面节点上运行以下脚本。
该脚本会将证书从第一个控制平面节点复制到另一个控制平面节点：</p>
<!--
In the following example, replace `CONTROL_PLANE_IPS` with the IP addresses of the
other control plane nodes.
-->
<p>在以下示例中，用其他控制平面节点的 IP 地址替换 <code>CONTROL_PLANE_IPS</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#b8860b">USER</span><span style="color:#666">=</span>ubuntu <span style="color:#080;font-style:italic"># 可定制</span>
<span style="color:#b8860b">CONTROL_PLANE_IPS</span><span style="color:#666">=</span><span style="color:#b44">&#34;10.0.0.7 10.0.0.8&#34;</span>
<span style="color:#a2f;font-weight:bold">for</span> host in <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CONTROL_PLANE_IPS</span><span style="color:#b68;font-weight:bold">}</span>; <span style="color:#a2f;font-weight:bold">do</span>
    scp /etc/kubernetes/pki/ca.crt <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>@<span style="color:#b8860b">$host</span>:
    scp /etc/kubernetes/pki/ca.key <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>@<span style="color:#b8860b">$host</span>:
    scp /etc/kubernetes/pki/sa.key <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>@<span style="color:#b8860b">$host</span>:
    scp /etc/kubernetes/pki/sa.pub <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>@<span style="color:#b8860b">$host</span>:
    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>@<span style="color:#b8860b">$host</span>:
    scp /etc/kubernetes/pki/front-proxy-ca.key <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>@<span style="color:#b8860b">$host</span>:
    scp /etc/kubernetes/pki/etcd/ca.crt <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>@<span style="color:#b8860b">$host</span>:etcd-ca.crt
    scp /etc/kubernetes/pki/etcd/ca.key <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>@<span style="color:#b8860b">$host</span>:etcd-ca.key
<span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
Copy only the certificates in the above list. kubeadm will take care of generating the rest of the certificates
with the required SANs for the joining control-plane instances. If you copy all the certificates by mistake,
the creation of additional nodes could fail due to a lack of required SANs.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 只需要复制上面列表中的证书。kubeadm 将负责生成其余证书以及加入控制平面实例所需的 SAN。
如果你错误地复制了所有证书，由于缺少所需的 SAN，创建其他节点可能会失败。
</div>

</li>
</ol>
<!--
1. Then on each joining control plane node you have to run the following script before running `kubeadm join`.
   This script will move the previously copied certificates from the home directory to `/etc/kubernetes/pki`:
-->
<ol start="5">
<li>
<p>然后，在每个即将加入集群的控制平面节点上，你必须先运行以下脚本，然后
再运行 <code>kubeadm join</code>。
该脚本会将先前复制的证书从主目录移动到 <code>/etc/kubernetes/pki</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">USER</span><span style="color:#666">=</span>ubuntu <span style="color:#080;font-style:italic"># 可定制</span>
mkdir -p /etc/kubernetes/pki/etcd
mv /home/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span>/ca.crt /etc/kubernetes/pki/
mv /home/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span>/ca.key /etc/kubernetes/pki/
mv /home/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span>/sa.pub /etc/kubernetes/pki/
mv /home/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span>/sa.key /etc/kubernetes/pki/
mv /home/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
mv /home/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span>/front-proxy-ca.key /etc/kubernetes/pki/
mv /home/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /home/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
</code></pre></div></li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8160424c22d24f7d2d63c521e107dbf8">2.3.1.7 - 使用 kubeadm 创建一个高可用 etcd 集群</h1>
    
	<!--
reviewers:
- sig-cluster-lifecycle
title: Set up a High Availability etcd cluster with kubeadm
content_type: task
weight: 70
-->
<!-- overview -->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
While kubeadm is being used as the management tool for external etcd nodes
in this guide, please note that kubeadm does not plan to support certificate rotation
or upgrades for such nodes. The long term plan is to empower the tool
[etcdadm](https://github.com/kubernetes-sigs/etcdadm) to manage these
aspects.
-->
<p>在本指南中，当 kubeadm 用作为外部 etcd 节点管理工具，请注意 kubeadm 不计划支持此类节点的证书更换或升级。对于长期规划是使用 <a href="https://github.com/kubernetes-sigs/etcdadm">etcdadm</a> 增强工具来管理这方面。
</div>
<!--
Kubeadm defaults to running a single member etcd cluster in a static pod managed
by the kubelet on the control plane node. This is not a high availability setup
as the etcd cluster contains only one member and cannot sustain any members
becoming unavailable. This task walks through the process of creating a high
availability etcd cluster of three members that can be used as an external etcd
when using kubeadm to set up a kubernetes cluster.
-->
<p>默认情况下，kubeadm 运行单成员的 etcd 集群，该集群由控制面节点上的 kubelet 以静态 Pod 的方式进行管理。由于 etcd 集群只包含一个成员且不能在任一成员不可用时保持运行，所以这不是一种高可用设置。本任务，将告诉你如何在使用 kubeadm 创建一个 kubernetes 集群时创建一个外部 etcd：有三个成员的高可用 etcd 集群。</p>
<h2 id="before-you-begin">Before you begin</h2>
<!--
* Three hosts that can talk to each other over ports 2379 and 2380. This document assumes these default ports. However, they are configurable through the kubeadm config file.
-->
<ul>
<li>三个可以通过 2379 和 2380 端口相互通信的主机。本文档使用这些作为默认端口。不过，它们可以通过 kubeadm 的配置文件进行自定义。</li>
</ul>
<!--
* Each host must [have docker, kubelet, and kubeadm installed][toolbox].
-->
<ul>
<li>每个主机必须 <a href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">安装有 docker、kubelet 和 kubeadm</a>。</li>
</ul>
<!--
* Some infrastructure to copy files between hosts. For example `ssh` and `scp` can satisfy this requirement.
-->
<ul>
<li>一些可以用来在主机间复制文件的基础设施。例如 <code>ssh</code> 和 <code>scp</code> 就可以满足需求。</li>
</ul>
<!--
[toolbox]: /docs/setup/production-environment/tools/kubeadm/install-kubeadm/
-->
<!-- steps -->
<!--
## Setting up the cluster
-->
<h2 id="建立集群">建立集群</h2>
<!--
The general approach is to generate all certs on one node and only distribute the *necessary* files to the other nodes.
-->
<p>一般来说，是在一个节点上生成所有证书并且只分发这些<em>必要</em>的文件到其它节点上。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
kubeadm contains all the necessary crytographic machinery to generate the certificates described below; no other cryptographic tooling is required for this example.
-->
<p>kubeadm 包含生成下述证书所需的所有必要的密码学工具；在这个例子中，不需要其他加密工具。
</div>
<!--
1. Configure the kubelet to be a service manager for etcd.
 
   <div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> You must do this on every host where etcd should be running.
</div>
   Since etcd was created first, you must override the service priority by creating a new unit file
   that has higher precedence than the kubeadm-provided kubelet unit file.
-->
<ol>
<li>
<p>将 kubelet 配置为 etcd 的服务管理器。</p>
<p><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你必须在要运行 etcd 的所有主机上执行此操作。
</div>
由于 etcd 是首先创建的，因此你必须通过创建具有更高优先级的新文件来覆盖
kubeadm 提供的 kubelet 单元文件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">cat <span style="color:#b44">&lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
</span><span style="color:#b44">[Service]
</span><span style="color:#b44">ExecStart=
</span><span style="color:#b44"># 将下面的 &#34;systemd&#34; 替换为你的容器运行时所使用的 cgroup 驱动。
</span><span style="color:#b44"># kubelet 的默认值为 &#34;cgroupfs&#34;。
</span><span style="color:#b44">ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd
</span><span style="color:#b44">Restart=always
</span><span style="color:#b44">EOF</span>

systemctl daemon-reload
systemctl restart kubelet
</code></pre></div><!--
Check the kubelet status to ensure it is running.
-->
<p>检查 kubelet 的状态以确保其处于运行状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">systemctl status kubelet
</code></pre></div></li>
</ol>
<!--
1. Create configuration files for kubeadm.

   Generate one kubeadm configuration file for each host that will have an etcd
   member running on it using the following script.
-->
<ol start="2">
<li>
<p>为 kubeadm 创建配置文件。</p>
<p>使用以下脚本为每个将要运行 etcd 成员的主机生成一个 kubeadm 配置文件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="color:#080;font-style:italic"># 使用 IP 或可解析的主机名替换 HOST0、HOST1 和 HOST2</span>
<span style="color:#a2f">export</span> <span style="color:#b8860b">HOST0</span><span style="color:#666">=</span>10.0.0.6
<span style="color:#a2f">export</span> <span style="color:#b8860b">HOST1</span><span style="color:#666">=</span>10.0.0.7
<span style="color:#a2f">export</span> <span style="color:#b8860b">HOST2</span><span style="color:#666">=</span>10.0.0.8

<span style="color:#080;font-style:italic"># 创建临时目录来存储将被分发到其它主机上的文件</span>
mkdir -p /tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST0</span><span style="color:#b68;font-weight:bold">}</span>/ /tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST1</span><span style="color:#b68;font-weight:bold">}</span>/ /tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST2</span><span style="color:#b68;font-weight:bold">}</span>/

<span style="color:#b8860b">ETCDHOSTS</span><span style="color:#666">=(</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST0</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST1</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST2</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#666">)</span>
<span style="color:#b8860b">NAMES</span><span style="color:#666">=(</span><span style="color:#b44">&#34;infra0&#34;</span> <span style="color:#b44">&#34;infra1&#34;</span> <span style="color:#b44">&#34;infra2&#34;</span><span style="color:#666">)</span>

<span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">${</span>!ETCDHOSTS[@]<span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span>; <span style="color:#a2f;font-weight:bold">do</span>
<span style="color:#b8860b">HOST</span><span style="color:#666">=</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">ETCDHOSTS</span>[<span style="color:#b8860b">$i</span>]<span style="color:#b68;font-weight:bold">}</span>
<span style="color:#b8860b">NAME</span><span style="color:#666">=</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NAMES</span>[<span style="color:#b8860b">$i</span>]<span style="color:#b68;font-weight:bold">}</span>
cat <span style="color:#b44">&lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml
</span><span style="color:#b44">apiVersion: &#34;kubeadm.k8s.io/v1beta3&#34;
</span><span style="color:#b44">kind: ClusterConfiguration
</span><span style="color:#b44">etcd:
</span><span style="color:#b44">    local:
</span><span style="color:#b44">        serverCertSANs:
</span><span style="color:#b44">        - &#34;${HOST}&#34;
</span><span style="color:#b44">        peerCertSANs:
</span><span style="color:#b44">        - &#34;${HOST}&#34;
</span><span style="color:#b44">        extraArgs:
</span><span style="color:#b44">            initial-cluster: infra0=https://${ETCDHOSTS[0]}:2380,infra1=https://${ETCDHOSTS[1]}:2380,infra2=https://${ETCDHOSTS[2]}:2380
</span><span style="color:#b44">            initial-cluster-state: new
</span><span style="color:#b44">            name: ${NAME}
</span><span style="color:#b44">            listen-peer-urls: https://${HOST}:2380
</span><span style="color:#b44">            listen-client-urls: https://${HOST}:2379
</span><span style="color:#b44">            advertise-client-urls: https://${HOST}:2379
</span><span style="color:#b44">            initial-advertise-peer-urls: https://${HOST}:2380
</span><span style="color:#b44">EOF</span>
<span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div></li>
</ol>
<!--
1. Generate the certificate authority

   If you already have a CA then the only action that is copying the CA's `crt` and
   `key` file to `/etc/kubernetes/pki/etcd/ca.crt` and
   `/etc/kubernetes/pki/etcd/ca.key`. After those files have been copied,
   proceed to the next step, "Create certificates for each member".
-->
<ol start="3">
<li>
<p>生成证书颁发机构</p>
<p>如果你已经拥有 CA，那么唯一的操作是复制 CA 的 <code>crt</code> 和 <code>key</code> 文件到
<code>etc/kubernetes/pki/etcd/ca.crt</code> 和 <code>/etc/kubernetes/pki/etcd/ca.key</code>。
复制完这些文件后继续下一步，“为每个成员创建证书”。</p>
<!--
If you do not already have a CA then run this command on `$HOST0` (where you generated the configuration files for kubeadm).
-->
<p>如果你还没有 CA，则在 <code>$HOST0</code>（你为 kubeadm 生成配置文件的位置）上运行此命令。</p>
<pre><code>kubeadm init phase certs etcd-ca
</code></pre><!--
This creates two files
-->
<p>这一操作创建如下两个文件</p>
<ul>
<li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li>
<li><code>/etc/kubernetes/pki/etcd/ca.key</code></li>
</ul>
</li>
</ol>
<!--
1. Create certificates for each member
-->
<ol start="4">
<li>
<p>为每个成员创建证书</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm init phase certs etcd-server --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST2</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST2</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST2</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST2</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST2</span><span style="color:#b68;font-weight:bold">}</span>/
<span style="color:#080;font-style:italic"># 清理不可重复使用的证书</span>
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST1</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST1</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST1</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST1</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST1</span><span style="color:#b68;font-weight:bold">}</span>/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST0</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST0</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST0</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST0</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
<span style="color:#080;font-style:italic"># 不需要移动 certs 因为它们是给 HOST0 使用的</span>

<span style="color:#080;font-style:italic"># 清理不应从此主机复制的证书</span>
find /tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST2</span><span style="color:#b68;font-weight:bold">}</span> -name ca.key -type f -delete
find /tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST1</span><span style="color:#b68;font-weight:bold">}</span> -name ca.key -type f -delete
</code></pre></div></li>
</ol>
<!--
1. Copy certificates and kubeadm configs
   The certificates have been generated and now they must be moved to their
   respective hosts.
-->
<ol start="5">
<li>
<p>复制证书和 kubeadm 配置</p>
<p>证书已生成，现在必须将它们移动到对应的主机。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">USER</span><span style="color:#666">=</span>ubuntu
<span style="color:#b8860b">HOST</span><span style="color:#666">=</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST1</span><span style="color:#b68;font-weight:bold">}</span>
scp -r /tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST</span><span style="color:#b68;font-weight:bold">}</span>/* <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span>@<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST</span><span style="color:#b68;font-weight:bold">}</span>:
ssh <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">USER</span><span style="color:#b68;font-weight:bold">}</span>@<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST</span><span style="color:#b68;font-weight:bold">}</span>
USER@HOST $ sudo -Es
root@HOST $ chown -R root:root pki
root@HOST $ mv pki /etc/kubernetes/
</code></pre></div></li>
</ol>
<!--
1. Ensure all expected files exist

   The complete list of required files on `$HOST0` is:
-->
<ol start="6">
<li>
<p>确保已经所有预期的文件都存在</p>
<p><code>$HOST0</code> 所需文件的完整列表如下：</p>
<pre><code class="language-none" data-lang="none">/tmp/${HOST0}
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><!--
On `$HOST1`:
-->
<p>在 <code>$HOST1</code> 上：</p>
<pre><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><!--
On `$HOST2`
-->
<p>在 <code>$HOST2</code> 上：</p>
<pre><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre></li>
</ol>
<!--
1. Create the static pod manifests

   Now that the certificates and configs are in place it's time to create the
   manifests. On each host run the `kubeadm` command to generate a static manifest
   for etcd.
-->
<ol start="7">
<li>
<p>创建静态 Pod 清单</p>
<p>既然证书和配置已经就绪，是时候去创建清单了。
在每台主机上运行 <code>kubeadm</code> 命令来生成 etcd 使用的静态清单。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">root@HOST0 $ kubeadm init phase etcd <span style="color:#a2f">local</span> --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST0</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
root@HOST1 $ kubeadm init phase etcd <span style="color:#a2f">local</span> --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST1</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
root@HOST2 $ kubeadm init phase etcd <span style="color:#a2f">local</span> --config<span style="color:#666">=</span>/tmp/<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST2</span><span style="color:#b68;font-weight:bold">}</span>/kubeadmcfg.yaml
</code></pre></div></li>
</ol>
<!--
1. Optional: Check the cluster health
-->
<ol start="8">
<li>
<p>可选：检查群集运行状况</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker run --rm -it <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--net host <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>-v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">ETCD_TAG</span><span style="color:#b68;font-weight:bold">}</span> etcdctl <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--cert /etc/kubernetes/pki/etcd/peer.crt <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--key /etc/kubernetes/pki/etcd/peer.key <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--cacert /etc/kubernetes/pki/etcd/ca.crt <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--endpoints https://<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">HOST0</span><span style="color:#b68;font-weight:bold">}</span>:2379 endpoint health --cluster
...
https://<span style="color:#666">[</span>HOST0 IP<span style="color:#666">]</span>:2379 is healthy: successfully committed proposal: <span style="color:#b8860b">took</span> <span style="color:#666">=</span> 16.283339ms
https://<span style="color:#666">[</span>HOST1 IP<span style="color:#666">]</span>:2379 is healthy: successfully committed proposal: <span style="color:#b8860b">took</span> <span style="color:#666">=</span> 19.44402ms
https://<span style="color:#666">[</span>HOST2 IP<span style="color:#666">]</span>:2379 is healthy: successfully committed proposal: <span style="color:#b8860b">took</span> <span style="color:#666">=</span> 35.926451ms
</code></pre></div><!--
Set ${ETCD_TAG} to the version tag of your etcd image. For example 3.4.3-0. To see the etcd image and tag that             kubeadm uses execute kubeadm config images list --kubernetes-version ${K8S_VERSION}, where ${K8S_VERSION} is for           example v1.17.0
Set ${HOST0}to the IP address of the host you are testing.
-->
<ul>
<li>将 <code>${ETCD_TAG}</code> 设置为你的 etcd 镜像的版本标签，例如 <code>3.4.3-0</code>。
要查看 kubeadm 使用的 etcd 镜像和标签，请执行
<code>kubeadm config images list --kubernetes-version ${K8S_VERSION}</code>，
例如，其中的 <code>${K8S_VERSION}</code> 可以是 <code>v1.17.0</code>。</li>
<li>将 <code>${HOST0}</code> 设置为要测试的主机的 IP 地址。</li>
</ul>
</li>
</ol>
<h2 id="what-s-next">What's next</h2>
<!--
Once your have a working 3 member etcd cluster, you can continue setting up a
highly available control plane using the [external etcd method with
kubeadm](/docs/setup/independent/high-availability/).
-->
<p>一旦拥有了一个正常工作的 3 成员的 etcd 集群，你就可以基于
<a href="/zh/docs/setup/production-environment/tools/kubeadm/high-availability/">使用 kubeadm 外部 etcd 的方法</a>，
继续部署一个高可用的控制平面。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-07709e71de6b4ac2573041c31213dbeb">2.3.1.8 - 使用 kubeadm 配置集群中的每个 kubelet</h1>
    
	<!--
reviewers:
- sig-cluster-lifecycle
title: Configuring each kubelet in your cluster using kubeadm
content_type: concept
weight: 80
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.11 [stable]</code>
</div>


<!--
The lifecycle of the kubeadm CLI tool is decoupled from the
[kubelet](/docs/reference/command-line-tools-reference/kubelet), which is a daemon that runs
on each node within the Kubernetes cluster. The kubeadm CLI tool is executed by the user when Kubernetes is
initialized or upgraded, where as the kubelet is always running in the background.

Since the kubelet is a daemon, it needs to be maintained by some kind of an init
system or service manager. When the kubelet is installed using DEBs or RPMs,
systemd is configured to manage the kubelet. You can use a different service
manager instead, but you need to configure it manually.

Some kubelet configuration details need to be the same across all kubelets involved in the cluster, while
other configuration aspects need to be set on a per-kubelet basis to accommodate the different
characteristics of a given machine (such as OS, storage, and networking). You can manage the configuration
of your kubelets manually, but kubeadm now provides a `KubeletConfiguration` API type for
[managing your kubelet configurations centrally](#configure-kubelets-using-kubeadm).
-->
<p>kubeadm CLI 工具的生命周期与 <a href="/zh/docs/reference/command-line-tools-reference/kubelet">kubelet</a>
解耦；kubelet 是一个守护程序，在 Kubernetes 集群中的每个节点上运行。
当 Kubernetes 初始化或升级时，kubeadm CLI 工具由用户执行，而 kubelet 始终在后台运行。</p>
<p>由于kubelet是守护程序，因此需要通过某种初始化系统或服务管理器进行维护。
当使用 DEB 或 RPM 安装 kubelet 时，配置系统去管理 kubelet。
你可以改用其他服务管理器，但需要手动地配置。</p>
<p>集群中涉及的所有 kubelet 的一些配置细节都必须相同，
而其他配置方面则需要基于每个 kubelet 进行设置，以适应给定机器的不同特性（例如操作系统、存储和网络）。
你可以手动地管理 kubelet 的配置，但是 kubeadm 现在提供一种 <code>KubeletConfiguration</code> API 类型
用于<a href="#configure-kubelets-using-kubeadm">集中管理 kubelet 的配置</a>。</p>
<!-- body -->
<!--
## Kubelet configuration patterns

The following sections describe patterns to kubelet configuration that are simplified by
using kubeadm, rather than managing the kubelet configuration for each Node manually.
-->
<h2 id="kubelet-配置模式">Kubelet 配置模式</h2>
<p>以下各节讲述了通过使用 kubeadm 简化 kubelet 配置模式，而不是在每个节点上手动地管理 kubelet 配置。</p>
<!--
### Propagating cluster-level configuration to each kubelet

You can provide the kubelet with default values to be used by `kubeadm init` and `kubeadm join`
commands. Interesting examples include using a different CRI runtime or setting the default subnet
used by services.

If you want your services to use the subnet `10.96.0.0/12` as the default for services, you can pass
the `--service-cidr` parameter to kubeadm:

```bash
kubeadm init --service-cidr 10.96.0.0/12
```

Virtual IPs for services are now allocated from this subnet. You also need to set the DNS address used
by the kubelet, using the `--cluster-dns` flag. This setting needs to be the same for every kubelet
on every manager and Node in the cluster. The kubelet provides a versioned, structured API object
that can configure most parameters in the kubelet and push out this configuration to each running
kubelet in the cluster. This object is called
[`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/).
The `KubeletConfiguration` allows the user to specify flags such as the cluster DNS IP addresses expressed as
a list of values to a camelCased key, illustrated by the following example:

```yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
clusterDNS:
- 10.96.0.10
```

For more details on the `KubeletConfiguration` have a look at [this section](#configure-kubelets-using-kubeadm).
-->
<h3 id="将集群级配置传播到每个-kubelet-中">将集群级配置传播到每个 kubelet 中</h3>
<p>你可以通过使用 <code>kubeadm init</code> 和 <code>kubeadm join</code> 命令为 kubelet 提供默认值。
有趣的示例包括使用其他 CRI 运行时或通过服务器设置不同的默认子网。</p>
<p>如果你想使用子网 <code>10.96.0.0/12</code> 作为services的默认网段，你可以给 kubeadm 传递 <code>--service-cidr</code> 参数：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeadm init --service-cidr 10.96.0.0/12
</code></pre></div><p>现在，可以从该子网分配服务的虚拟 IP。
你还需要通过 kubelet 使用 <code>--cluster-dns</code> 标志设置 DNS 地址。
在集群中的每个管理器和节点上的 kubelet 的设置需要相同。
kubelet 提供了一个版本化的结构化 API 对象，该对象可以配置 kubelet 中的大多数参数，并将此配置推送到集群中正在运行的每个 kubelet 上。
此对象被称为 <a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>。
<code>KubeletConfiguration</code> 允许用户指定标志，例如用骆峰值代表集群的 DNS IP 地址，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">clusterDNS</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#666">10.96.0.10</span><span style="color:#bbb">
</span></code></pre></div><p>有关 <code>KubeletConfiguration</code> 的更多详细信息，亲参阅<a href="#configure-kubelets-using-kubeadm">本节</a>。</p>
<!--
### Providing instance-specific configuration details

Some hosts require specific kubelet configurations due to differences in hardware, operating system,
networking, or other host-specific parameters. The following list provides a few examples.

- The path to the DNS resolution file, as specified by the `--resolv-conf` kubelet
  configuration flag, may differ among operating systems, or depending on whether you are using
  `systemd-resolved`. If this path is wrong, DNS resolution will fail on the Node whose kubelet
  is configured incorrectly.

- The Node API object `.metadata.name` is set to the machine's hostname by default,
  unless you are using a cloud provider. You can use the `--hostname-override` flag to override the
  default behavior if you need to specify a Node name different from the machine's hostname.

- Currently, the kubelet cannot automatically detect the cgroup driver used by the CRI runtime,
  but the value of `--cgroup-driver` must match the cgroup driver used by the CRI runtime to ensure
  the health of the kubelet.

- Depending on the CRI runtime your cluster uses, you may need to specify different flags to the kubelet.
  For instance, when using Docker, you need to specify flags such as `--network-plugin=cni`, but if you
  are using an external runtime, you need to specify `--container-runtime=remote` and specify the CRI
  endpoint using the `--container-runtime-endpoint=<path>`.

You can specify these flags by configuring an individual kubelet's configuration in your service manager,
such as systemd.
-->
<h3 id="提供指定实例的详细配置信息">提供指定实例的详细配置信息</h3>
<p>由于硬件、操作系统、网络或者其他主机特定参数的差异。某些主机需要特定的 kubelet 配置。
以下列表提供了一些示例。</p>
<ul>
<li>
<p>由 kubelet 配置标志 <code>--resolv-conf</code> 指定的 DNS 解析文件的路径在操作系统之间可能有所不同，
它取决于你是否使用 <code>systemd-resolved</code>。
如果此路径错误，则在其 kubelet 配置错误的节点上 DNS 解析也将失败。</p>
</li>
<li>
<p>除非你使用云驱动，否则默认情况下 Node API 对象的 <code>.metadata.name</code> 会被设置为计算机的主机名。
如果你需要指定一个与机器的主机名不同的节点名称，你可以使用 <code>--hostname-override</code> 标志覆盖默认值。</p>
</li>
<li>
<p>当前，kubelet 无法自动检测 CRI 运行时使用的 cgroup 驱动程序，
但是值 <code>--cgroup-driver</code> 必须与 CRI 运行时使用的 cgroup 驱动程序匹配，以确保 kubelet 的健康运行状况。</p>
</li>
<li>
<p>取决于你的集群所使用的 CRI 运行时，你可能需要为 kubelet 指定不同的标志。
例如，当使用 Docker 时，你需要指定如 <code>--network-plugin=cni</code> 这类标志；但是如果你使用的是外部运行时，
则需要指定 <code>--container-runtime=remote</code> 并使用 <code>--container-runtime-endpoint=&lt;path&gt;</code> 指定 CRI 端点。</p>
</li>
</ul>
<p>你可以在服务管理器（例如 systemd）中设定某个 kubelet 的配置来指定这些参数。</p>
<!--
## Configure kubelets using kubeadm

It is possible to configure the kubelet that kubeadm will start if a custom `KubeletConfiguration`
API object is passed with a configuration file like so `kubeadm ... --config some-config-file.yaml`.

By calling `kubeadm config print init-defaults --component-configs KubeletConfiguration` you can
see all the default values for this structure.

Also have a look at the
[reference for the KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/)
for more information on the individual fields.
-->
<h2 id="使用-kubeadm-配置-kubelet">使用 kubeadm 配置 kubelet</h2>
<p>如果自定义的 <code>KubeletConfiguration</code> API 对象使用像  <code>kubeadm ... --config some-config-file.yaml</code> 这样的配置文件进行传递，则可以配置 kubeadm 启动的 kubelet。</p>
<p>通过调用 <code>kubeadm config print init-defaults --component-configs KubeletConfiguration</code>，
你可以看到此结构中的所有默认值。</p>
<p>也可以阅读 <a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration 参考</a>
来获取有关各个字段的更多信息。</p>
<!--
### Workflow when using `kubeadm init`

When you call `kubeadm init`, the kubelet configuration is marshalled to disk
at `/var/lib/kubelet/config.yaml`, and also uploaded to a ConfigMap in the cluster. The ConfigMap
is named `kubelet-config-1.X`, where `X` is the minor version of the Kubernetes version you are
initializing. A kubelet configuration file is also written to `/etc/kubernetes/kubelet.conf` with the
baseline cluster-wide configuration for all kubelets in the cluster. This configuration file
points to the client certificates that allow the kubelet to communicate with the API server. This
addresses the need to
[propagate cluster-level configuration to each kubelet](#propagating-cluster-level-configuration-to-each-kubelet).

To address the second pattern of
[providing instance-specific configuration details](#providing-instance-specific-configuration-details),
kubeadm writes an environment file to `/var/lib/kubelet/kubeadm-flags.env`, which contains a list of
flags to pass to the kubelet when it starts. The flags are presented in the file like this:

```bash
KUBELET_KUBEADM_ARGS="--flag1=value1 --flag2=value2 ..."
```

In addition to the flags used when starting the kubelet, the file also contains dynamic
parameters such as the cgroup driver and whether to use a different CRI runtime socket
(`--cri-socket`).

After marshalling these two files to disk, kubeadm attempts to run the following two
commands, if you are using systemd:

```bash
systemctl daemon-reload && systemctl restart kubelet
```

If the reload and restart are successful, the normal `kubeadm init` workflow continues.
-->
<h3 id="当使用-kubeadm-init-时的工作流程">当使用 <code>kubeadm init</code>时的工作流程</h3>
<p>当调用 <code>kubeadm init</code> 时，kubelet 配置被编组到磁盘上的 <code>/var/lib/kubelet/config.yaml</code> 中，
并且上传到集群中的 ConfigMap。
ConfigMap 名为 <code>kubelet-config-1.X</code>，其中 <code>X</code> 是你正在初始化的 kubernetes 版本的次版本。
在集群中所有 kubelet 的基准集群范围内配置，将 kubelet 配置文件写入 <code>/etc/kubernetes/kubelet.conf</code> 中。
此配置文件指向允许 kubelet 与 API 服务器通信的客户端证书。
这解决了<a href="#propagating-cluster-level-configuration-to-each-kubelet">将集群级配置传播到每个 kubelet</a> 的需求。</p>
<p>该文档 <a href="#providing-instance-specific-configuration-details">提供特定实例的配置详细信息</a> 是第二种解决模式，
kubeadm 将环境文件写入 <code>/var/lib/kubelet/kubeadm-flags.env</code>，其中包含了一个标志列表，
当 kubelet 启动时，该标志列表会传递给 kubelet 标志在文件中的显示方式如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#b8860b">KUBELET_KUBEADM_ARGS</span><span style="color:#666">=</span><span style="color:#b44">&#34;--flag1=value1 --flag2=value2 ...&#34;</span>
</code></pre></div><p>除了启动 kubelet 时使用该标志外，该文件还包含动态参数，例如 cgroup 驱动程序以及是否使用其他 CRI 运行时 socket（<code>--cri-socket</code>）。</p>
<p>将这两个文件编组到磁盘后，如果使用 systemd，则 kubeadm 尝试运行以下两个命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl daemon-reload <span style="color:#666">&amp;&amp;</span> systemctl restart kubelet
</code></pre></div><p>如果重新加载和重新启动成功，则正常的 <code>kubeadm init</code> 工作流程将继续。</p>
<!--
### Workflow when using `kubeadm join`

When you run `kubeadm join`, kubeadm uses the Bootstrap Token credential to perform
a TLS bootstrap, which fetches the credential needed to download the
`kubelet-config-1.X` ConfigMap and writes it to `/var/lib/kubelet/config.yaml`. The dynamic
environment file is generated in exactly the same way as `kubeadm init`.
-->
<h3 id="当使用-kubeadm-join-时的工作流程">当使用 <code>kubeadm join</code>时的工作流程</h3>
<p>当运行 <code>kubeadm join</code> 时，kubeadm 使用 Bootstrap Token 证书执行 TLS 引导，该引导会获取一份证书，
该证书需要下载 <code>kubelet-config-1.X</code> ConfigMap 并把它写入 <code>/var/lib/kubelet/config.yaml</code> 中。
动态环境文件的生成方式恰好与 <code>kubeadm init</code> 完全相同。</p>
<!--
Next, `kubeadm` runs the following two commands to load the new configuration into the kubelet:
-->
<p>接下来，<code>kubeadm</code> 运行以下两个命令将新配置加载到 kubelet 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl daemon-reload <span style="color:#666">&amp;&amp;</span> systemctl restart kubelet
</code></pre></div><!--
After the kubelet loads the new configuration, kubeadm writes the
`/etc/kubernetes/bootstrap-kubelet.conf` KubeConfig file, which contains a CA certificate and Bootstrap
Token. These are used by the kubelet to perform the TLS Bootstrap and obtain a unique
credential, which is stored in `/etc/kubernetes/kubelet.conf`.
-->
<p>在 kubelet 加载新配置后，kubeadm 将写入 <code>/etc/kubernetes/bootstrap-kubelet.conf</code> KubeConfig 文件中，
该文件包含 CA 证书和引导程序令牌。
kubelet 使用这些证书执行 TLS 引导程序并获取唯一的凭据，该凭据被存储在 <code>/etc/kubernetes/kubelet.conf</code> 中。</p>
<!--
When the `/etc/kubernetes/kubelet.conf` file is written, the kubelet has finished performing the TLS Bootstrap.
Kubeadm deletes the `/etc/kubernetes/bootstrap-kubelet.conf` file after completing the TLS Bootstrap.
-->
<p>当 <code>/etc/kubernetes/kubelet.conf</code> 文件被写入后，kubelet 就完成了 TLS 引导过程。
Kubeadm 在完成 TLS 引导过程后将删除 <code>/etc/kubernetes/bootstrap-kubelet.conf</code> 文件。</p>
<!--
##  The kubelet drop-in file for systemd

`kubeadm` ships with configuration for how systemd should run the kubelet.
Note that the kubeadm CLI command never touches this drop-in file.

This configuration file installed by the `kubeadm`
[DEB](https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf) or
[RPM package](https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubeadm/10-kubeadm.conf) is written to
`/etc/systemd/system/kubelet.service.d/10-kubeadm.conf` and is used by systemd.
It augments the basic
[`kubelet.service` for RPM](https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubelet/kubelet.service) or
[`kubelet.service` for DEB](https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service):

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> The contents below are just an example. If you don't want to use a package manager
follow the guide outlined in the <a href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#k8s-install-2">Without a package manager</a>)
section.
</div>

```none
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generate at runtime, populating
the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort.
# Preferably, the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead.
# KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
```

This file specifies the default locations for all of the files managed by kubeadm for the kubelet.

- The KubeConfig file to use for the TLS Bootstrap is `/etc/kubernetes/bootstrap-kubelet.conf`,
  but it is only used if `/etc/kubernetes/kubelet.conf` does not exist.
- The KubeConfig file with the unique kubelet identity is `/etc/kubernetes/kubelet.conf`.
- The file containing the kubelet's ComponentConfig is `/var/lib/kubelet/config.yaml`.
- The dynamic environment file that contains `KUBELET_KUBEADM_ARGS` is sourced from `/var/lib/kubelet/kubeadm-flags.env`.
- The file that can contain user-specified flag overrides with `KUBELET_EXTRA_ARGS` is sourced from
  `/etc/default/kubelet` (for DEBs), or `/etc/sysconfig/kubelet` (for RPMs). `KUBELET_EXTRA_ARGS`
  is last in the flag chain and has the highest priority in the event of conflicting settings.
-->
<h2 id="the-kubelet-drop-in-file-for-systemd">kubelet 的 systemd 文件</h2>
<p><code>kubeadm</code> 中附带了有关系统如何运行 kubelet 的 systemd 配置文件。
请注意 kubeadm CLI 命令不会修改此文件。</p>
<p>通过 <code>kubeadm</code> <a href="https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf">DEB</a>
或者 <a href="https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubeadm/10-kubeadm.conf">RPM 包</a>
安装的配置文件被写入 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 并由系统使用。
它对原来的 <a href="https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubelet/kubelet.service">RPM 版本 <code>kubelet.service</code></a>
或者 <a href="https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service">DEB 版本 <code>kubelet.service</code></a>
作了增强：</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 下面的内容只是一个例子。 如果您不想使用包管理器，
请遵循<a href="/zh/docs/setup/productionenvironment/tools/kubeadm/install-kubeadm/#k8s-install-2">没有包管理器</a>)
部分中叙述的指南。
</div>
<pre><code class="language-none" data-lang="none">[Service]
Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;
Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot;
# 这是 &quot;kubeadm init&quot; 和 &quot;kubeadm join&quot; 运行时生成的文件，动态地填充 KUBELET_KUBEADM_ARGS 变量
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# 这是一个文件，用户在不得已下可以将其用作替代 kubelet args。
# 用户最好使用 .NodeRegistration.KubeletExtraArgs 对象在配置文件中替代。
# KUBELET_EXTRA_ARGS 应该从此文件中获取。
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
</code></pre><p>该文件为 kubelet 指定由 kubeadm 管理的所有文件的默认位置。</p>
<ul>
<li>用于 TLS 引导程序的 KubeConfig 文件为 <code>/etc/kubernetes/bootstrap-kubelet.conf</code>，
但仅当 <code>/etc/kubernetes/kubelet.conf</code> 不存在时才能使用。</li>
<li>具有唯一 kubelet 标识的 KubeConfig 文件为 <code>/etc/kubernetes/kubelet.conf</code>。</li>
<li>包含 kubelet 的组件配置的文件为 <code>/var/lib/kubelet/config.yaml</code>。</li>
<li>包含的动态环境的文件 <code>KUBELET_KUBEADM_ARGS</code> 是来源于 <code>/var/lib/kubelet/kubeadm-flags.env</code>。</li>
<li>包含用户指定标志替代的文件 <code>KUBELET_EXTRA_ARGS</code> 是来源于
<code>/etc/default/kubelet</code>（对于 DEB），或者 <code>/etc/sysconfig/kubelet</code>（对于 RPM）。
<code>KUBELET_EXTRA_ARGS</code> 在标志链中排在最后，并且在设置冲突时具有最高优先级。</li>
</ul>
<!--
## Kubernetes binaries and package contents

The DEB and RPM packages shipped with the Kubernetes releases are:

| Package name   | Description |
|----------------|-------------|
| `kubeadm`      | Installs the `/usr/bin/kubeadm` CLI tool and the [kubelet drop-in file](#the-kubelet-drop-in-file-for-systemd) for the kubelet. |
| `kubelet`      | Installs the `/usr/bin/kubelet` binary. |
| `kubectl`      | Installs the `/usr/bin/kubectl` binary. |
| `cri-tools`    | Installs the `/usr/bin/crictl` binary from the [cri-tools git repository](https://github.com/kubernetes-sigs/cri-tools). |
| `kubernetes-cni` | Installs the `/opt/cni/bin` binaries from the [plugins git repository](https://github.com/containernetworking/plugins). |
-->
<h2 id="kubernetes-可执行文件和软件包内容">Kubernetes 可执行文件和软件包内容</h2>
<p>Kubernetes 版本对应的 DEB 和 RPM 软件包是：</p>
<table>
<thead>
<tr>
<th>Package name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>kubeadm</code></td>
<td>给 kubelet 安装 <code>/usr/bin/kubeadm</code> CLI 工具和 <a href="#the-kubelet-drop-in-file-for-systemd">kubelet 的 systemd 文件</a>。</td>
</tr>
<tr>
<td><code>kubelet</code></td>
<td>安装 <code>/usr/bin/kubelet</code> 可执行文件。</td>
</tr>
<tr>
<td><code>kubectl</code></td>
<td>安装 <code>/usr/bin/kubectl</code> 可执行文件。</td>
</tr>
<tr>
<td><code>cri-tools</code></td>
<td>从 <a href="https://github.com/kubernetes-sigs/cri-tools">cri-tools git 仓库</a>中安装 <code>/usr/bin/crictl</code> 可执行文件。</td>
</tr>
<tr>
<td><code>kubernetes-cni</code></td>
<td>从 <a href="https://github.com/containernetworking/plugins">plugins git 仓库</a>中安装 <code>/opt/cni/bin</code> 可执行文件。</td>
</tr>
</tbody>
</table>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-df2f3f20d404ebe2b03fcda1fcee50e7">2.3.1.9 - 使用 kubeadm 支持双协议栈</h1>
    
	<!--
title: Dual-stack support with kubeadm
content_type: task
weight: 110
min-kubernetes-server-version: 1.21
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code>
</div>


<!--
Your Kubernetes cluster includes [dual-stack](/docs/concepts/services-networking/dual-stack/) networking, which means that cluster networking lets you use either address family. In a cluster, the control plane can assign both an IPv4 address and an IPv6 address to a single <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> or a <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>.
-->
<p>你的集群包含<a href="/zh/docs/concepts/services-networking/dual-stack/">双协议栈</a>组网支持，
这意味着集群网络允许你在两种地址族间任选其一。在集群中，控制面可以为同一个
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 或者 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>
同时赋予 IPv4 和 IPv6 地址。</p>
<!-- body -->
<h2 id="before-you-begin">Before you begin</h2>
<!--
You need to have installed the <a class='glossary-tooltip' title='用来快速安装 Kubernetes 并搭建安全稳定的集群的工具。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/tools/kubeadm/' target='_blank' aria-label='kubeadm'>kubeadm</a> tool, following the steps from [Installing kubeadm](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/).
-->
<p>你需要已经遵从<a href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">安装 kubeadm</a>
中所给的步骤安装了 <a class='glossary-tooltip' title='用来快速安装 Kubernetes 并搭建安全稳定的集群的工具。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/tools/kubeadm/' target='_blank' aria-label='kubeadm'>kubeadm</a> 工具。</p>
<!--
For each server that you want to use as a <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a>, make sure it allows IPv6 forwarding. On Linux, you can set this by running run `sysctl -w net.ipv6.conf.all.forwarding=1` as the root user on each server.
-->
<p>针对你要作为<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>使用的每台服务器，
确保其允许 IPv6 转发。在 Linux 节点上，你可以通过以 root 用户在每台服务器上运行
<code>sysctl -w net.ipv6.conf.all.forwarding=1</code> 来完成设置。</p>
<!--
You need to have an IPv4 and and IPv6 address range to use. Cluster operators typically
use private address ranges for IPv4. For IPv6, a cluster operator typically chooses a global
unicast address block from within `2000::/3`, using a range that is assigned to the operator.
You don't have to route the cluster's IP address ranges to the public internet.

The size of the IP address allocations should be suitable for the number of Pods and
Services that you are planning to run.
-->
<p>你需要一个可以使用的 IPv4 和 IPv6 地址范围。集群操作人员通常为 IPv4 使用
私有地址范围。对于 IPv6，集群操作人员通常会基于分配给该操作人员的地址范围，
从 <code>2000::/3</code> 中选择一个全局的单播地址块。你不需要将集群的 IP 地址范围路由
到公众互联网。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you are upgrading an existing cluster with the `kubeadm upgrade` command,
`kubeadm` does not support making modifications to the pod IP address range
(“cluster CIDR”) nor to the cluster's Service address range (“Service CIDR”).
-->
<p>如果你在使用 <code>kubeadm upgrade</code> 命令升级现有的集群，<code>kubeadm</code> 不允许更改 Pod
的 IP 地址范围（“集群 CIDR”），也不允许更改集群的服务地址范围（“Service CIDR”）。
</div>
<!--
### Create a dual-stack cluster

To create a dual-stack cluster with `kubeadm init` you can pass command line arguments
similar to the following example:
-->
<h3 id="create-a-dual-stack-cluster">创建双协议栈集群  </h3>
<p>要使用 <code>kubeadm init</code> 创建一个双协议栈集群，你可以传递与下面的例子类似的命令行参数：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 这里的地址范围仅作示例使用</span>
kubeadm init --pod-network-cidr<span style="color:#666">=</span>10.244.0.0/16,2001:db8:42:0::/56 --service-cidr<span style="color:#666">=</span>10.96.0.0/16,2001:db8:42:1::/112
</code></pre></div><!--
To make things clearer, here is an example kubeadm 
[configuration file](/docs/reference/config-api/kubeadm-config.v1beta3/) 
`kubeadm-config.yaml` for the primary dual-stack control plane node.
-->
<p>为了更便于理解，参看下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href="/zh/docs/reference/config-api/kubeadm-config.v1beta3/">配置文件</a>，
该文件用于双协议栈控制面的主控制节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">networking</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSubnet</span>:<span style="color:#bbb"> </span><span style="color:#666">10.244.0.0</span>/16,2001:db8:42:0::/56<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceSubnet</span>:<span style="color:#bbb"> </span><span style="color:#666">10.96.0.0</span>/16,2001:db8:42:1::/112<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>InitConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">localAPIEndpoint</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">advertiseAddress</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10.100.0.1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">bindPort</span>:<span style="color:#bbb"> </span><span style="color:#666">6443</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">nodeRegistration</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kubeletExtraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">node-ip</span>:<span style="color:#bbb"> </span><span style="color:#666">10.100.0.2</span>,fd00:1:2:3::2<span style="color:#bbb">
</span></code></pre></div><!--
`advertiseAddress` in InitConfiguration specifies the IP address that the API Server will advertise it is listening on. The value of `advertiseAddress` equals the `--apiserver-advertise-address` flag of `kubeadm init`

Run kubeadm to initiate the dual-stack control plane node:
-->
<p>InitConfiguration 中的 <code>advertiseAddress</code> 给出 API 服务器将公告自身要监听的
IP 地址。<code>advertiseAddress</code> 的取值与 <code>kubeadm init</code> 的标志
<code>--apiserver-advertise-address</code> 的取值相同。</p>
<p>运行 kubeadm 来实例化双协议栈控制面节点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm init --config<span style="color:#666">=</span>kubeadm-config.yaml
</code></pre></div><!--
The kube-controller-manager flags `--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6` are set with default values. See [configure IPv4/IPv6 dual stack](/docs/concepts/services-networking/dual-stack#configure-ipv4-ipv6-dual-stack).
-->
<p>kube-controller-manager 标志 <code>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code>
是使用默认值来设置的。参见<a href="/zh/docs/concepts/services-networking/dual-stack#configure-ipv4-ipv6-dual-stack">配置 IPv4/IPv6 双协议栈</a>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The `--apiserver-advertise-address` flag does not support dual-stack.
-->
<p>标志 <code>--apiserver-advertise-address</code> 不支持双协议栈。
</div>
<!--
### Join a node to dual-stack cluster

Before joining a node, make sure that the node has IPv6 routable network interface and allows IPv6 forwarding.

Here is an example kubeadm [configuration file](/docs/reference/config-api/kubeadm-config.v1beta3/)
`kubeadm-config.yaml` for joining a worker node to the cluster.
-->
<h3 id="join-a-node-to-dual-stack-cluster">向双协议栈集群添加节点  </h3>
<p>在添加节点之前，请确保该节点具有 IPv6 可路由的网络接口并且启用了 IPv6 转发。</p>
<p>下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href="/zh/docs/reference/config-api/kubeadm-config.v1beta3/">配置文件</a>
示例用于向集群中添加工作节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>JoinConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">discovery</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">bootstrapToken</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiServerEndpoint</span>:<span style="color:#bbb"> </span><span style="color:#666">10.100.0.1</span>:<span style="color:#666">6443</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">token</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;clvldh.vjjwg16ucnhp94qr&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">caCertHashes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 请更改上面的认证信息，使之与你的集群中实际使用的令牌和 CA 证书匹配</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">nodeRegistration</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kubeletExtraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">node-ip</span>:<span style="color:#bbb"> </span><span style="color:#666">10.100.0.3</span>,fd00:1:2:3::3<span style="color:#bbb">
</span></code></pre></div><!--
Also, here is an example kubeadm [configuration file](/docs/reference/config-api/kubeadm-config.v1beta3/)
`kubeadm-config.yaml` for joining another control plane node to the cluster.
-->
<p>下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href="/zh/docs/reference/config-api/kubeadm-config.v1beta3/">配置文件</a>
示例用于向集群中添加另一个控制面节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>JoinConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">controlPlane</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">localAPIEndpoint</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">advertiseAddress</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10.100.0.2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">bindPort</span>:<span style="color:#bbb"> </span><span style="color:#666">6443</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">discovery</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">bootstrapToken</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiServerEndpoint</span>:<span style="color:#bbb"> </span><span style="color:#666">10.100.0.1</span>:<span style="color:#666">6443</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">token</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;clvldh.vjjwg16ucnhp94qr&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">caCertHashes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 请更改上面的认证信息，使之与你的集群中实际使用的令牌和 CA 证书匹配</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">nodeRegistration</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kubeletExtraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">node-ip</span>:<span style="color:#bbb"> </span><span style="color:#666">10.100.0.4</span>,fd00:1:2:3::4<span style="color:#bbb">
</span></code></pre></div><!--
`advertiseAddress` in JoinConfiguration.controlPlane specifies the IP address that the API Server will advertise it is listening on. The value of `advertiseAddress` equals the `--apiserver-advertise-address` flag of `kubeadm join`.
-->
<p>JoinConfiguration.controlPlane 中的 <code>advertiseAddress</code> 设定 API 服务器将公告自身要监听的
IP 地址。<code>advertiseAddress</code> 的取值与 <code>kubeadm join</code> 的标志
<code>--apiserver-advertise-address</code> 的取值相同。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm join --config<span style="color:#666">=</span>kubeadm-config.yaml
</code></pre></div><!--
### Create a single-stack cluster
-->
<h3 id="create-a-single-stack-cluster">创建单协议栈集群   </h3>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Dual-stack support doesn't mean that you need to use dual-stack addressing.
You can deploy a single-stack cluster that has the dual-stack networking feature enabled.
-->
<p>双协议栈支持并不意味着你需要使用双协议栈来寻址。
你可以部署一个启用了双协议栈联网特性的单协议栈集群。
</div>
<!--
To make things more clear, here is an example kubeadm
[configuration file](/docs/reference/config-api/kubeadm-config.v1beta3/)
`kubeadm-config.yaml` for the single-stack control plane node.
-->
<p>为了更便于理解，参看下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href="/zh/docs/reference/config-api/kubeadm-config.v1beta3/">配置文件</a>示例，
该文件用于单协议栈控制面节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">networking</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSubnet</span>:<span style="color:#bbb"> </span><span style="color:#666">10.244.0.0</span>/16<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceSubnet</span>:<span style="color:#bbb"> </span><span style="color:#666">10.96.0.0</span>/16<span style="color:#bbb">
</span></code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
* [Validate IPv4/IPv6 dual-stack](/docs/tasks/network/validate-dual-stack) networking
* Read about [Dual-stack](/docs/concepts/services-networking/dual-stack/) cluster networking
* Learn more about the kubeadm [configuration format](/docs/reference/config-api/kubeadm-config.v1beta3/)
-->
<ul>
<li><a href="/zh/docs/tasks/network/validate-dual-stack">验证 IPv4/IPv6 双协议栈</a>联网</li>
<li>阅读<a href="/zh/docs/concepts/services-networking/dual-stack/">双协议栈</a>集群网络</li>
<li>进一步了解 kubeadm <a href="/docs/reference/config-api/kubeadm-config.v1beta3/">配置格式</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-478acca1934b6d89a0bc00fb25bfe5b6">2.3.2 - 使用 Kops 安装 Kubernetes</h1>
    
	<!--
title: Installing Kubernetes with kops
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
This quickstart shows you how to easily install a Kubernetes cluster on AWS.
It uses a tool called [`kops`](https://github.com/kubernetes/kops).
-->
<p>本篇快速入门介绍了如何在 AWS 上轻松安装 Kubernetes 集群。
本篇使用了一个名为 <a href="https://github.com/kubernetes/kops"><code>kops</code></a> 的工具。</p>
<!--
kops is an automated provisioning system:
-->
<p>kops 是一个自动化的制备系统：</p>
<!--
* Fully automated installation
* Uses DNS to identify clusters
* Self-healing: everything runs in Auto-Scaling Groups
* Multiple OS support (Debian, Ubuntu 16.04 supported, CentOS & RHEL, Amazon Linux and CoreOS) - see the [images.md](https://github.com/kubernetes/kops/blob/master/docs/operations/images.md)
* High-Availability support - see the [high_availability.md](https://github.com/kubernetes/kops/blob/master/docs/high_availability.md)
* Can directly provision, or generate terraform manifests - see the [terraform.md](https://github.com/kubernetes/kops/blob/master/docs/terraform.md)
-->
<ul>
<li>全自动安装流程</li>
<li>使用 DNS 识别集群</li>
<li>自我修复：一切都在自动扩缩组中运行</li>
<li>支持多种操作系统（如 Debian、Ubuntu 16.04、CentOS、RHEL、Amazon Linux 和 CoreOS） - 参考 <a href="https://github.com/kubernetes/kops/blob/master/docs/operations/images.md">images.md</a></li>
<li>支持高可用 - 参考 <a href="https://github.com/kubernetes/kops/blob/master/docs/high_availability.md">high_availability.md</a></li>
<li>可以直接提供或者生成 terraform 清单 - 参考 <a href="https://github.com/kubernetes/kops/blob/master/docs/terraform.md">terraform.md</a></li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<!--
* You must have [kubectl](/docs/tasks/tools/) installed.

* You must [install](https://github.com/kubernetes/kops#installing) `kops` on a 64-bit (AMD64 and Intel 64) device architecture.

* You must have an [AWS account](https://docs.aws.amazon.com/polly/latest/dg/setting-up.html), generate [IAM keys](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys) and [configure](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration) them. The IAM user will need [adequate permissions](https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md#setup-iam-user).
-->
<ul>
<li>你必须安装 <a href="/zh/docs/tasks/tools/">kubectl</a>。</li>
<li>你必须安装<a href="https://github.com/kubernetes/kops#installing">安装</a> <code>kops</code>
到 64 位的（AMD64 和 Intel 64）设备架构上。</li>
<li>你必须拥有一个 <a href="https://docs.aws.amazon.com/polly/latest/dg/setting-up.html">AWS 账户</a>，
生成 <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">IAM 秘钥</a>
并<a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration">配置</a>
该秘钥。IAM 用户需要<a href="https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md#setup-iam-user">足够的权限许可</a>。</li>
</ul>
<!-- steps -->
<!--
## Creating a cluster

### (1/5) Install kops

#### Installation

Download kops from the [releases page](https://github.com/kubernetes/kops/releases) (it is also convenient to build from source):
-->
<h2 id="创建集群">创建集群</h2>
<h3 id="1-5-安装-kops">(1/5) 安装 kops</h3>
<h4 id="安装">安装</h4>
<p>从<a href="https://github.com/kubernetes/kops/releases">下载页面</a>下载 kops
（从源代码构建也很方便）：</p>
<ul class="nav nav-tabs" id="kops-installation" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#kops-installation-0" role="tab" aria-controls="kops-installation-0" aria-selected="true">macOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#kops-installation-1" role="tab" aria-controls="kops-installation-1">Linux</a></li></ul>
<div class="tab-content" id="kops-installation"><div id="kops-installation-0" class="tab-pane show active" role="tabpanel" aria-labelledby="kops-installation-0">

<p><!--
Download the latest release with the command:
-->
<p>使用下面的命令下载最新发布版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -LO https://github.com/kubernetes/kops/releases/download/<span style="color:#a2f;font-weight:bold">$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style="color:#b44">&#39;&#34;&#39;</span> -f 4<span style="color:#a2f;font-weight:bold">)</span>/kops-darwin-amd64
</code></pre></div><!--
To download a specific version, replace the following portion of the command with the specific kops version.
-->
<p>要下载特定版本，使用特定的 kops 版本替换下面命令中的部分：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style="color:#b44">&#39;&#34;&#39;</span> -f 4<span style="color:#a2f;font-weight:bold">)</span>
</code></pre></div><!--
For example, to download kops version v1.20.0 type:
-->
<p>例如，要下载 kops v1.20.0，输入：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-darwin-amd64
</code></pre></div><!--
Make the kops binary executable.
-->
<p>令 kops 二进制文件可执行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">chmod +x kops-darwin-amd64
</code></pre></div><!--
Move the kops binary in to your PATH.
-->
<p>将 kops 二进制文件移到你的 PATH 下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo mv kops-darwin-amd64 /usr/local/bin/kops
</code></pre></div><p>你也可以使用 <a href="https://brew.sh/">Homebrew</a> 安装 kops：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">brew update <span style="color:#666">&amp;&amp;</span> brew install kops
</code></pre></div></div>
  <div id="kops-installation-1" class="tab-pane" role="tabpanel" aria-labelledby="kops-installation-1">

<p><!--
Download the latest release with the command:
-->
<p>使用命令下载最新发布版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -LO https://github.com/kubernetes/kops/releases/download/<span style="color:#a2f;font-weight:bold">$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style="color:#b44">&#39;&#34;&#39;</span> -f 4<span style="color:#a2f;font-weight:bold">)</span>/kops-linux-amd64
</code></pre></div><!--
To download a specific version of kops, replace the following portion of the command with the specific kops version.
-->
<p>要下载 kops 的特定版本，用特定的 kops 版本替换下面命令中的部分：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style="color:#b44">&#39;&#34;&#39;</span> -f 4<span style="color:#a2f;font-weight:bold">)</span>
</code></pre></div><!--
For example, to download kops version v1.20.0 type:
-->
<p>例如，要下载 kops v1.20 版本，输入：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64
</code></pre></div><!--
Make the kops binary executable
-->
<p>令 kops 二进制文件可执行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">chmod +x kops-linux-amd64
</code></pre></div><!--
Move the kops binary in to your PATH.
-->
<p>将 kops 二进制文件移到 PATH 下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo mv kops-linux-amd64 /usr/local/bin/kops
</code></pre></div><p>你也可以使用 <a href="https://docs.brew.sh/Homebrew-on-Linux">Homebrew</a>
来安装 kops：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">brew update <span style="color:#666">&amp;&amp;</span> brew install kops
</code></pre></div></div></div>

<!--
### (2/5) Create a route53 domain for your cluster

kops uses DNS for discovery, both inside the cluster and outside, so that you can reach the kubernetes API server
from clients.
-->
<h3 id="2-5-为你的集群创建一个-route53-域名">(2/5) 为你的集群创建一个 route53 域名</h3>
<p>kops 在集群内部和外部都使用 DNS 进行发现操作，这样你可以从客户端访问
kubernetes API 服务器。</p>
<!--
kops has a strong opinion on the cluster name: it should be a valid DNS name.  By doing so you will
no longer get your clusters confused, you can share clusters with your colleagues unambiguously,
and you can reach them without relying on remembering an IP address.
-->
<p>kops 对集群名称有明显的要求：它应该是有效的 DNS 名称。这样一来，你就不会再使集群混乱，
可以与同事明确共享集群，并且无需依赖记住 IP 地址即可访问群集。</p>
<!--
You can, and probably should, use subdomains to divide your clusters.  As our example we will use
`useast1.dev.example.com`.  The API server endpoint will then be `api.useast1.dev.example.com`.
-->
<p>你可以，或许应该使用子域名来划分集群。作为示例，我们将使用域名 <code>useast1.dev.example.com</code>。
这样，API 服务器端点域名将为 <code>api.useast1.dev.example.com</code>。</p>
<!--
A Route53 hosted zone can serve subdomains.  Your hosted zone could be `useast1.dev.example.com`,
but also `dev.example.com` or even `example.com`.  kops works with any of these, so typically
you choose for organization reasons (e.g. you are allowed to create records under `dev.example.com`,
but not under `example.com`).
-->
<p>Route53 托管区域可以服务子域名。你的托管区域可能是 <code>useast1.dev.example.com</code>，还有 <code>dev.example.com</code> 甚至 <code>example.com</code>。
kops 可以与以上任何一种配合使用，因此通常你出于组织原因选择不同的托管区域。
例如，允许你在 <code>dev.example.com</code> 下创建记录，但不能在 <code>example.com</code> 下创建记录。</p>
<!--
Let's assume you're using `dev.example.com` as your hosted zone.  You create that hosted zone using
the [normal process](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html), or
with a command such as `aws route53 create-hosted-zone --name dev.example.com --caller-reference 1`.
-->
<p>假设你使用 <code>dev.example.com</code> 作为托管区域。你可以使用
<a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html">正常流程</a>
或者使用诸如 <code>aws route53 create-hosted-zone --name dev.example.com --caller-reference 1</code>
之类的命令来创建该托管区域。</p>
<!--
You must then set up your NS records in the parent domain, so that records in the domain will resolve.  Here,
you would create NS records in `example.com` for `dev`.  If it is a root domain name you would configure the NS
records at your domain registrar (e.g. `example.com` would need to be configured where you bought `example.com`).
-->
<p>然后，你必须在父域名中设置你的 DNS 记录，以便该域名中的记录可以被解析。
在这里，你将在 <code>example.com</code> 中为 <code>dev</code> 创建 DNS 记录。
如果它是根域名，则可以在域名注册机构配置 DNS 记录。
例如，你需要在购买 <code>example.com</code> 的地方配置 <code>example.com</code>。</p>
<!--
Verify your route53 domain setup (it is the #1 cause of problems!). You can double-check that
your cluster is configured correctly if you have the dig tool by running:
-->
<p>检查你的 route53 域已经被正确设置（这是导致问题的最常见原因！）。
如果你安装了 dig 工具，则可以通过运行以下步骤再次检查集群是否配置正确：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">dig DNS dev.example.com
</code></pre></div><!--
You should see the 4 NS records that Route53 assigned your hosted zone.
-->
<p>你应该看到 Route53 分配了你的托管区域的 4 条 DNS 记录。</p>
<!--
### (3/5) Create an S3 bucket to store your clusters state

kops lets you manage your clusters even after installation.  To do this, it must keep track of the clusters
that you have created, along with their configuration, the keys they are using etc.  This information is stored
in an S3 bucket.  S3 permissions are used to control access to the bucket.
-->
<h3 id="3-5-创建一个-s3-存储桶来存储集群状态">(3/5) 创建一个 S3 存储桶来存储集群状态</h3>
<p>kops 使你即使在安装后也可以管理集群。为此，它必须跟踪已创建的集群及其配置、所使用的密钥等。
此信息存储在 S3 存储桶中。S3 权限用于控制对存储桶的访问。</p>
<!--
Multiple clusters can use the same S3 bucket, and you can share an S3 bucket between your colleagues that
administer the same clusters - this is much easier than passing around kubecfg files.  But anyone with access
to the S3 bucket will have administrative access to all your clusters, so you don't want to share it beyond
the operations team.
-->
<p>多个集群可以使用同一 S3 存储桶，并且你可以在管理同一集群的同事之间共享一个
S3 存储桶 - 这比传递 kubecfg 文件容易得多。
但是有权访问 S3 存储桶的任何人都将拥有对所有集群的管理访问权限，
因此你不想在运营团队之外共享它。</p>
<!--
So typically you have one S3 bucket for each ops team (and often the name will correspond
to the name of the hosted zone above!)
-->
<p>因此，通常每个运维团队都有一个 S3 存储桶（而且名称通常对应于上面托管区域的名称！）</p>
<!--
In our example, we chose `dev.example.com` as our hosted zone, so let's pick `clusters.dev.example.com` as
the S3 bucket name.
-->
<p>在我们的示例中，我们选择 <code>dev.example.com</code> 作为托管区域，因此我们选择
<code>clusters.dev.example.com</code> 作为 S3 存储桶名称。</p>
<!--
* Export `AWS_PROFILE` (if you need to select a profile for the AWS CLI to work)
* Create the S3 bucket using `aws s3 mb s3://clusters.dev.example.com`
* You can `export KOPS_STATE_STORE=s3://clusters.dev.example.com` and then kops will use this location by default.
   We suggest putting this in your bash profile or similar.
-->
<ul>
<li>导出 <code>AWS_PROFILE</code> 文件（如果你需要选择一个配置文件用来使 AWS CLI 正常工作）</li>
<li>使用 <code>aws s3 mb s3://clusters.dev.example.com</code> 创建 S3 存储桶</li>
<li>你可以进行 <code>export KOPS_STATE_STORE=s3://clusters.dev.example.com</code> 操作，
然后 kops 将默认使用此位置。
我们建议将其放入你的 bash profile 文件或类似文件中。</li>
</ul>
<!--
### (4/5) Build your cluster configuration

Run `kops create cluster` to create your cluster configuration:
-->
<h3 id="4-5-建立你的集群配置">(4/5) 建立你的集群配置</h3>
<p>运行 <code>kops create cluster</code> 以创建你的集群配置：</p>
<p><code>kops create cluster --zones=us-east-1c useast1.dev.example.com</code></p>
<!--
kops will create the configuration for your cluster.  Note that it _only_ creates the configuration, it does
not actually create the cloud resources - you'll do that in the next step with a `kops update cluster`.  This
give you an opportunity to review the configuration or change it.
-->
<p>kops 将为你的集群创建配置。请注意，它_仅_创建配置，实际上并没有创建云资源 -
你将在下一步中使用 <code>kops update cluster</code> 进行配置。
这使你有机会查看配置或进行更改。</p>
<!--
It prints commands you can use to explore further:
-->
<p>它打印出可用于进一步探索的命令：</p>
<!--
* List your clusters with: `kops get cluster`
* Edit this cluster with: `kops edit cluster useast1.dev.example.com`
* Edit your node instance group: `kops edit ig --name=useast1.dev.example.com nodes`
* Edit your master instance group: `kops edit ig --name=useast1.dev.example.com master-us-east-1c`
-->
<ul>
<li>使用以下命令列出集群：<code>kops get cluster</code></li>
<li>使用以下命令编辑该集群：<code>kops edit cluster useast1.dev.example.com</code></li>
<li>使用以下命令编辑你的节点实例组：<code>kops edit ig --name = useast1.dev.example.com nodes</code></li>
<li>使用以下命令编辑你的主实例组：<code>kops edit ig --name = useast1.dev.example.com master-us-east-1c</code></li>
</ul>
<!--
If this is your first time using kops, do spend a few minutes to try those out!  An instance group is a
set of instances, which will be registered as kubernetes nodes.  On AWS this is implemented via auto-scaling-groups.
You can have several instance groups, for example if you wanted nodes that are a mix of spot and on-demand instances, or
GPU and non-GPU instances.
-->
<p>如果这是你第一次使用 kops，请花几分钟尝试一下！ 实例组是一组实例，将被注册为 kubernetes 节点。
在 AWS 上，这是通过 auto-scaling-groups 实现的。你可以有多个实例组。
例如，如果你想要的是混合实例和按需实例的节点，或者 GPU 和非 GPU 实例。</p>
<!--
### (5/5) Create the cluster in AWS

Run "kops update cluster" to create your cluster in AWS:
-->
<h3 id="5-5-在-aws-中创建集群">(5/5) 在 AWS 中创建集群</h3>
<p>运行 &quot;kops update cluster&quot; 以在 AWS 中创建集群：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kops update cluster useast1.dev.example.com --yes
</code></pre></div><!--
That takes a few seconds to run, but then your cluster will likely take a few minutes to actually be ready.
`kops update cluster` will be the tool you'll use whenever you change the configuration of your cluster; it
applies the changes you have made to the configuration to your cluster - reconfiguring AWS or kubernetes as needed.
-->
<p>这需要几秒钟的时间才能运行，但实际上集群可能需要几分钟才能准备就绪。
每当更改集群配置时，都会使用 <code>kops update cluster</code> 工具。
它将对配置进行的更改应用于你的集群 - 根据需要重新配置 AWS 或者 kubernetes。</p>
<!--
For example, after you `kops edit ig nodes`, then `kops update cluster --yes` to apply your configuration, and
sometimes you will also have to `kops rolling-update cluster` to roll out the configuration immediately.
-->
<p>例如，在你运行 <code>kops edit ig nodes</code> 之后，然后运行 <code>kops update cluster --yes</code>
应用你的配置，有时你还必须运行 <code>kops rolling-update cluster</code> 立即回滚更新配置。</p>
<!--
Without `--yes`, `kops update cluster` will show you a preview of what it is going to do.  This is handy
for production clusters!
-->
<p>如果没有 <code>--yes</code> 参数，<code>kops update cluster</code> 操作将向你显示其操作的预览效果。这对于生产集群很方便！</p>
<!--
### Explore other add-ons

See the [list of add-ons](/docs/concepts/cluster-administration/addons/) to explore other add-ons, including tools for logging, monitoring, network policy, visualization &amp; control of your Kubernetes cluster.
-->
<h3 id="探索其他附加组件">探索其他附加组件</h3>
<p>请参阅<a href="/zh/docs/concepts/cluster-administration/addons/">附加组件列表</a>探索其他附加组件，
包括用于 Kubernetes 集群的日志记录、监视、网络策略、可视化和控制的工具。</p>
<!--
## Cleanup

* To delete your cluster: `kops delete cluster useast1.dev.example.com --yes`
-->
<h2 id="清理">清理</h2>
<ul>
<li>删除集群：<code>kops delete cluster useast1.dev.example.com --yes</code></li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about Kubernetes [concepts](/docs/concepts/) and [`kubectl`](/docs/reference/kubectl/).
* Learn about `kops` [advanced usage](https://github.com/kubernetes/kops)
* See the `kops` [docs](https://github.com/kubernetes/kops) section for tutorials, best practices and advanced configuration options.
-->
<ul>
<li>了解有关 Kubernetes 的<a href="/zh/docs/concepts/">概念</a> 和
<a href="/zh/docs/reference/kubectl/"><code>kubectl</code></a> 有关的更多信息。</li>
<li>了解 <code>kops</code> <a href="https://github.com/kubernetes/kops">高级用法</a>。</li>
<li>请参阅 <code>kops</code> <a href="https://github.com/kubernetes/kops">文档</a> 获取教程、
最佳做法和高级配置选项。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f8b4964187fe973644e06ee629eff1de">2.3.3 - 使用 Kubespray 安装 Kubernetes</h1>
    
	<!--
title: Installing Kubernetes with Kubespray
content_type: concept
weight: 30
-->
<!-- overview -->
<!--
This quickstart helps to install a Kubernetes cluster hosted on GCE, Azure, OpenStack, AWS, vSphere, Packet (bare metal), Oracle Cloud Infrastructure (Experimental) or Baremetal with [Kubespray](https://github.com/kubernetes-sigs/kubespray).
-->
<p>此快速入门有助于使用 <a href="https://github.com/kubernetes-sigs/kubespray">Kubespray</a>
安装在 GCE、Azure、OpenStack、AWS、vSphere、Packet（裸机）、Oracle Cloud
Infrastructure（实验性）或 Baremetal 上托管的 Kubernetes 集群。</p>
<!--
Kubespray is a composition of [Ansible](https://docs.ansible.com/) playbooks, [inventory](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md), provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks. Kubespray provides:
-->
<p>Kubespray 是一个由 <a href="https://docs.ansible.com/">Ansible</a> playbooks、
<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md">清单（inventory）</a>、
制备工具和通用 OS/Kubernetes 集群配置管理任务的领域知识组成的。
Kubespray 提供：</p>
<!--
* a highly available cluster
* composable attributes
* support for most popular Linux distributions
  * Ubuntu 16.04, 18.04, 20.04
  * CentOS/RHEL/Oracle Linux 7, 8
  * Debian Buster, Jessie, Stretch, Wheezy
  * Fedora 31, 32
  * Fedora CoreOS
  * openSUSE Leap 15
  * Flatcar Container Linux by Kinvolk
* continuous integration tests
-->
<ul>
<li>高可用性集群</li>
<li>可组合属性</li>
<li>支持大多数流行的 Linux 发行版
<ul>
<li>Ubuntu 16.04、18.04、20.04</li>
<li>CentOS / RHEL / Oracle Linux 7、8</li>
<li>Debian Buster、Jessie、Stretch、Wheezy</li>
<li>Fedora 31、32</li>
<li>Fedora CoreOS</li>
<li>openSUSE Leap 15</li>
<li>Kinvolk 的 Flatcar Container Linux</li>
</ul>
</li>
<li>持续集成测试</li>
</ul>
<!--
To choose a tool which best fits your use case, read [this comparison](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md) to
[kubeadm](/docs/reference/setup-tools/kubeadm/) and [kops](/docs/setup/production-environment/tools/kops/).
-->
<p>要选择最适合你的用例的工具，请阅读
<a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm</a> 和
<a href="/zh/docs/setup/production-environment/tools/kops/">kops</a> 之间的
<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md">这份比较</a>。
。</p>
<!-- body -->
<!--
## Creating a cluster

### (1/5) Meet the underlay requirements
-->
<h2 id="创建集群">创建集群</h2>
<h3 id="1-5-满足下层设施要求">（1/5）满足下层设施要求</h3>
<!--
Provision servers with the following [requirements](https://github.com/kubernetes-sigs/kubespray#requirements):
-->
<p>按以下<a href="https://github.com/kubernetes-sigs/kubespray#requirements">要求</a>来配置服务器：</p>
<!--
* **Ansible v2.9 and python-netaddr are installed on the machine that will run Ansible commands**
* **Jinja 2.11 (or newer) is required to run the Ansible Playbooks**
* The target servers must have access to the Internet in order to pull docker images. Otherwise, additional configuration is required ([See Offline Environment](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/offline-environment.md))
* The target servers are configured to allow **IPv4 forwarding**
* **Your ssh key must be copied** to all the servers in your inventory
* **Firewalls are not managed by kubespray**. You'll need to implement appropriate rules as needed. You should disable your firewall in order to avoid any issues during deployment
* If kubespray is ran from a non-root user account, correct privilege escalation method should be configured in the target servers and the `ansible_become` flag or command parameters `--become` or `-b` should be specified
-->
<ul>
<li>在将运行 Ansible 命令的计算机上安装 Ansible v2.9 和 python-netaddr</li>
<li><strong>运行 Ansible Playbook 需要 Jinja 2.11（或更高版本）</strong></li>
<li>目标服务器必须有权访问 Internet 才能拉取 Docker 镜像。否则，
需要其他配置（<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/offline-environment.md">请参见离线环境</a>）</li>
<li>目标服务器配置为允许 IPv4 转发</li>
<li><strong>你的 SSH 密钥必须复制</strong>到部署集群的所有服务器中</li>
<li><strong>防火墙不是由 kubespray 管理的</strong>。你需要根据需求设置适当的规则策略。为了避免部署过程中出现问题，可以禁用防火墙。</li>
<li>如果从非 root 用户帐户运行 kubespray，则应在目标服务器中配置正确的特权升级方法
并指定 <code>ansible_become</code> 标志或命令参数 <code>--become</code> 或 <code>-b</code></li>
</ul>
<!--
Kubespray provides the following utilities to help provision your environment:

* [Terraform](https://www.terraform.io/) scripts for the following cloud providers:
  * [AWS](https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws)
  * [OpenStack](https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/openstack)
  * [Packet](https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/packet)
-->
<p>Kubespray 提供以下实用程序来帮助你设置环境：</p>
<ul>
<li>为以下云驱动提供的 <a href="https://www.terraform.io/">Terraform</a> 脚本：</li>
<li><a href="https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws">AWS</a></li>
<li><a href="http://sitebeskuethree/contrigetbernform/contribeskubernform/contribeskupernform/https/sitebesku/master/">OpenStack</a></li>
<li><a href="https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/packet">Packet</a></li>
</ul>
<!--
### (2/5) Compose an inventory file

After you provision your servers, create an [inventory file for Ansible](https://docs.ansible.com/ansible/latest/network/getting_started/first_inventory.html). You can do this manually or via a dynamic inventory script. For more information, see "[Building your own inventory](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#building-your-own-inventory)".

### (3/5) Plan your cluster deployment

Kubespray provides the ability to customize many aspects of the deployment:
-->
<h3 id="2-5-编写清单文件">（2/5）编写清单文件</h3>
<p>设置服务器后，请创建一个
<a href="https://docs.ansible.com/ansible/latest/network/getting_started/first_inventory.html">Ansible 的清单文件</a>。
你可以手动执行此操作，也可以通过动态清单脚本执行此操作。有关更多信息，请参阅
“<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#building-your-own-inventory">建立你自己的清单</a>”。</p>
<h3 id="3-5-规划集群部署">（3/5）规划集群部署</h3>
<p>Kubespray 能够自定义部署的许多方面：</p>
<!--
* Choice deployment mode: kubeadm or non-kubeadm
* CNI (networking) plugins
* DNS configuration
* Choice of control plane: native/binary or containerized
* Component versions
* Calico route reflectors
* Component runtime options
  * <a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a>
  * <a class='glossary-tooltip' title='强调简单性、健壮性和可移植性的一种容器运行时' data-toggle='tooltip' data-placement='top' href='https://containerd.io/docs/' target='_blank' aria-label='containerd'>containerd</a>
  * <a class='glossary-tooltip' title='专用于 Kubernetes 的轻量级容器运行时软件' data-toggle='tooltip' data-placement='top' href='https://cri-o.io/#what-is-cri-o' target='_blank' aria-label='CRI-O'>CRI-O</a>
* Certificate generation methods
-->
<ul>
<li>选择部署模式： kubeadm 或非 kubeadm</li>
<li>CNI（网络）插件</li>
<li>DNS 配置</li>
<li>控制平面的选择：本机/可执行文件或容器化</li>
<li>组件版本</li>
<li>Calico 路由反射器</li>
<li>组件运行时选项
<ul>
<li><a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a></li>
<li><a class='glossary-tooltip' title='强调简单性、健壮性和可移植性的一种容器运行时' data-toggle='tooltip' data-placement='top' href='https://containerd.io/docs/' target='_blank' aria-label='containerd'>containerd</a></li>
<li><a class='glossary-tooltip' title='专用于 Kubernetes 的轻量级容器运行时软件' data-toggle='tooltip' data-placement='top' href='https://cri-o.io/#what-is-cri-o' target='_blank' aria-label='CRI-O'>CRI-O</a></li>
</ul>
</li>
<li>证书生成方式</li>
</ul>
<!--
Kubespray customizations can be made to a [variable file](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html). If you are just getting started with Kubespray, consider using the Kubespray defaults to deploy your cluster and explore Kubernetes.
-->
<p>可以修改<a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html">变量文件</a>
以进行 Kubespray 定制。
如果你刚刚开始使用 Kubespray，请考虑使用 Kubespray 默认设置来部署你的集群
并探索 Kubernetes 。</p>
<!--
### (4/5) Deploy a Cluster

Next, deploy your cluster:

Cluster deployment using [ansible-playbook](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#starting-custom-deployment).
-->
<h3 id="4-5-部署集群">（4/5）部署集群</h3>
<p>接下来，部署你的集群：</p>
<p>使用 <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#starting-custom-deployment">ansible-playbook</a>
进行集群部署。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ansible-playbook -i your/inventory/inventory.ini cluster.yml -b -v <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --private-key<span style="color:#666">=</span>~/.ssh/private_key
</code></pre></div><!--
Large deployments (100+ nodes) may require [specific adjustments](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/large-deployments.md) for best results.
-->
<p>大型部署（超过 100 个节点）可能需要
<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/large-deployments.md">特定的调整</a>，
以获得最佳效果。</p>
<!--
### (5/5) Verify the deployment

Kubespray provides a way to verify inter-pod connectivity and DNS resolve with [Netchecker](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/netcheck.md). Netchecker ensures the netchecker-agents pods can resolve DNS requests and ping each over within the default namespace. Those pods mimic similar behavior as the rest of the workloads and serve as cluster health indicators.
-->
<h3 id="5-5-验证部署">（5/5）验证部署</h3>
<p>Kubespray 提供了一种使用
<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/netcheck.md">Netchecker</a>
验证 Pod 间连接和 DNS 解析的方法。
Netchecker 确保 netchecker-agents Pods 可以解析 DNS 请求，
并在默认命名空间内对每个请求执行 ping 操作。
这些 Pod 模仿其他工作负载类似的行为，并用作集群运行状况指示器。</p>
<!--
## Cluster operations

Kubespray provides additional playbooks to manage your cluster: _scale_ and _upgrade_.
-->
<h2 id="集群操作">集群操作</h2>
<p>Kubespray 提供了其他 Playbooks 来管理集群： <em>scale</em> 和 <em>upgrade</em>。</p>
<!--
### Scale your cluster

You can add worker nodes from your cluster by running the scale playbook. For more information, see "[Adding nodes](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#adding-nodes)".
You can remove worker nodes from your cluster by running the remove-node playbook. For more information, see "[Remove nodes](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#remove-nodes)".
-->
<h3 id="扩展集群">扩展集群</h3>
<p>你可以通过运行 scale playbook 向集群中添加工作节点。有关更多信息，
请参见 “<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#adding-nodes">添加节点</a>”。
你可以通过运行 remove-node playbook 来从集群中删除工作节点。有关更多信息，
请参见 “<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#remove-nodes">删除节点</a>”。</p>
<!--
### Upgrade your cluster

You can upgrade your cluster by running the upgrade-cluster playbook. For more information, see "[Upgrades](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md)".
-->
<h3 id="升级集群">升级集群</h3>
<p>你可以通过运行 upgrade-cluster Playbook 来升级集群。有关更多信息，请参见
“<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md">升级</a>”。</p>
<!--
## Cleanup

You can reset your nodes and wipe out all components installed with Kubespray via the [reset playbook](https://github.com/kubernetes-sigs/kubespray/blob/master/reset.yml).

<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> When running the reset playbook, be sure not to accidentally target your production cluster!
</div>

-->
<h2 id="清理">清理</h2>
<p>你可以通过 <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/reset.yml">reset</a> Playbook
重置节点并清除所有与 Kubespray 一起安装的组件。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 运行 reset playbook 时，请确保不要意外地将生产集群作为目标！
</div>

<!--
## Feedback

* Slack Channel: [#kubespray](https://kubernetes.slack.com/messages/kubespray/) (You can get your invite [here](https://slack.k8s.io/))
* [GitHub Issues](https://github.com/kubernetes-sigs/kubespray/issues)
-->
<h2 id="反馈">反馈</h2>
<ul>
<li>Slack 频道：<a href="https://kubernetes.slack.com/messages/kubespray/">#kubespray</a>
（你可以在<a href="https://slack.k8s.io/">此处</a>获得邀请）</li>
<li><a href="https://github.com/kubernetes-sigs/kubespray/issues">GitHub 问题</a></li>
</ul>
<!--
## What's next

Check out planned work on Kubespray's [roadmap](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/roadmap.md).
-->
<h2 id="what-s-next">What's next</h2>
<p>查看有关 Kubespray 的
<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/roadmap.md">路线图</a>
的计划工作。</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-acce7e24090fea04715a7a516ba3e69b">2.4 - Windows Kubernetes</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-a307d413f1f7430fced233023087e2a1">2.4.1 - Kubernetes 对 Windows 的支持</h1>
    
	<!--
reviewers:
- jayunit100
- jsturtevant
- marosset
- perithompson
title: Intro to Windows support in Kubernetes
content_type: concept
weight: 65
-->
<!-- overview -->
<!--
Windows applications constitute a large portion of the services and
applications that run in many organizations.
[Windows containers](https://aka.ms/windowscontainers) provide a modern way to
encapsulate processes and package dependencies, making it easier to use DevOps
practices and follow cloud native patterns for Windows applications.
Kubernetes has become the defacto standard container orchestrator, and the
release of Kubernetes 1.14 includes production support for scheduling Windows
containers on Windows nodes in a Kubernetes cluster, enabling a vast ecosystem
of Windows applications to leverage the power of Kubernetes. Organizations
with investments in Windows-based applications and Linux-based applications
don't have to look for separate orchestrators to manage their workloads,
leading to increased operational efficiencies across their deployments,
regardless of operating system.
-->
<p>在很多组织中，其服务和应用的很大比例是 Windows 应用。
<a href="https://aka.ms/windowscontainers">Windows 容器</a>提供了一种对进程和包依赖关系
进行封装的现代方式，这使得用户更容易采用 DevOps 实践，令 Windows 应用同样遵从
云原生模式。
Kubernetes 已经成为事实上的标准容器编排器，Kubernetes 1.14 发行版本中包含了将
Windows 容器调度到 Kubernetes 集群中 Windows 节点上的生产级支持，从而使得巨大
的 Windows 应用生态圈能够充分利用 Kubernetes 的能力。
对于同时投入基于 Windows 应用和 Linux 应用的组织而言，他们不必寻找不同的编排系统
来管理其工作负载，其跨部署的运维效率得以大幅提升，而不必关心所用操作系统。</p>
<!-- body -->
<!--
## Windows containers in Kubernetes

To enable the orchestration of Windows containers in Kubernetes, include
Windows nodes in your existing Linux cluster. Scheduling Windows containers in
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> on Kubernetes is similar to
scheduling Linux-based containers.
-->
<h2 id="windows-containers-in-kubernetes">kubernetes 中的 Windows 容器 </h2>
<p>若要在 Kubernetes 中启用对 Windows 容器的编排，可以在现有的 Linux 集群中
包含 Windows 节点。在 Kubernetes 上调度 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>
中的 Windows 容器与调用基于 Linux 的容器类似。</p>
<!--
In order to run Windows containers, your Kubernetes cluster must include
multiple operating systems, with control plane nodes running Linux and workers
running either Windows or Linux depending on your workload needs. Windows
Server 2019 is the only Windows operating system supported, enabling
[Kubernetes Node](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node)
on Windows (including kubelet,
[container runtime](https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/containerd),
and kube-proxy). For a detailed explanation of Windows distribution channels
see the [Microsoft documentation](https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19).
-->
<p>为了运行 Windows 容器，你的 Kubernetes 集群必须包含多个操作系统，控制面
节点运行 Linux，工作节点则可以根据负载需要运行 Windows 或 Linux。
Windows Server 2019 是唯一被支持的 Windows 操作系统，在 Windows 上启用
<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node">Kubernetes 节点</a>
支持（包括 kubelet, <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/containerd">容器运行时</a>、
以及 kube-proxy）。关于 Windows 发行版渠道的详细讨论，可参见
<a href="https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19">Microsoft 文档</a>。</p>
<!--
The Kubernetes control plane, including the
[master components](/docs/concepts/overview/components/),
continues to run on Linux. 
There are no plans to have a Windows-only Kubernetes cluster.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Kubernetes 控制面，包括<a href="/zh/docs/concepts/overview/components/">主控组件</a>，
继续在 Linux 上运行。
目前没有支持完全是 Windows 节点的 Kubernetes 集群的计划。
</div>
<!--
In this document, when we talk about Windows containers we mean Windows
containers with process isolation. Windows containers with
[Hyper-V isolation](https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container)
is planned for a future release.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在本文中，当我们讨论 Windows 容器时，我们所指的是具有进程隔离能力的 Windows
容器。具有 <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container">Hyper-V 隔离能力</a>
的 Windows 容器计划在将来发行版本中推出。
</div>
<!--
## Supported Functionality and Limitations

### Supported Functionality

#### Windows OS Version Support

Refer to the following table for Windows operating system support in
Kubernetes. A single heterogeneous Kubernetes cluster can have both Windows
and Linux worker nodes. Windows containers have to be scheduled on Windows
nodes and Linux containers on Linux nodes.
-->
<h2 id="supported-functionality-and-limitations">支持的功能与局限性 </h2>
<h3 id="supported-functionality">支持的功能   </h3>
<h4 id="windows-os-version-support">Windows 操作系统版本支持   </h4>
<p>参考下面的表格，了解 Kubernetes 中支持的 Windows 操作系统。
同一个异构的 Kubernetes 集群中可以同时包含 Windows 和 Linux 工作节点。
Windows 容器仅能调度到 Windows 节点，Linux 容器则只能调度到 Linux 节点。</p>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Windows Server LTSC 版本</th>
<th>Windows Server SAC 版本</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Kubernetes v1.20</em></td>
<td>Windows Server 2019</td>
<td>Windows Server ver 1909, Windows Server ver 2004</td>
<td></td>
</tr>
<tr>
<td><em>Kubernetes v1.21</em></td>
<td>Windows Server 2019</td>
<td>Windows Server ver 2004, Windows Server ver 20H2</td>
<td></td>
</tr>
<tr>
<td><em>Kubernetes v1.22</em></td>
<td>Windows Server 2019</td>
<td>Windows Server ver 2004, Windows Server ver 20H2</td>
<td></td>
</tr>
</tbody>
</table>
<!--
Information on the different Windows Server servicing channels including their
support models can be found at
[Windows Server servicing channels](https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19).
-->
<p>关于不同的 Windows Server 版本的服务渠道，包括其支持模式等相关信息可以在
<a href="https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19">Windows Server servicing channels</a>
找到。</p>
<!--
We don't expect all Windows customers to update the operating system for their
apps frequently. Upgrading your applications is what dictates and necessitates
upgrading or introducing new nodes to the cluster. For the customers that
chose to upgrade their operating system for containers running on Kubernetes,
we will offer guidance and step-by-step instructions when we add support for a
new operating system version. This guidance will include recommended upgrade
procedures for upgrading user applications together with cluster nodes.
Windows nodes adhere to Kubernetes
[version-skew policy](/docs/setup/release/version-skew-policy/) (node to control plane
versioning) the same way as Linux nodes do today.
-->
<p>我们并不指望所有 Windows 客户都为其应用频繁地更新操作系统。
对应用的更新是向集群中引入新代码的根本原因。
对于想要更新运行于 Kubernetes 之上的容器中操作系统的客户，我们会在添加对新
操作系统版本的支持时提供指南和分步的操作指令。
该指南会包含与集群节点一起来升级用户应用的建议升级步骤。
Windows 节点遵从 Kubernetes
<a href="/zh/docs/setup/release/version-skew-policy/">版本偏差策略</a>（节点到控制面的
版本控制），与 Linux 节点的现行策略相同。</p>
<!--
The Windows Server Host Operating System is subject to the
[Windows Server ](https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing)
licensing. The Windows Container images are subject to the
[Supplemental License Terms for Windows containers](https://docs.microsoft.com/en-us/virtualization/windowscontainers/images-eula).
-->
<p>Windows Server 主机操作系统会受
<a href="https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing">Windows Server</a>
授权策略控制。Windows 容器镜像则遵从
<a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/images-eula">Windows 容器的补充授权条款</a>
约定。</p>
<!--
Windows containers with process isolation have strict compatibility rules,
[where the host OS version must match the container base image OS version](https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility).
Once we support Windows containers with Hyper-V isolation in Kubernetes, the
limitation and compatibility rules will change.
-->
<p>带进程隔离的 Windows 容器受一些严格的兼容性规则约束，
<a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility">其中宿主 OS 版本必须与容器基准镜像的 OS 版本相同</a>。
一旦我们在 Kubernetes 中支持带 Hyper-V 隔离的 Windows 容器，
这一约束和兼容性规则也会发生改变。</p>
<!--
#### Pause Image

Kubernetes maintains a multi-architecture image that includes support for Windows.
For Kubernetes v1.22 the recommended pause image is `k8s.gcr.io/pause:3.5`.
The [source code](https://github.com/kubernetes/kubernetes/tree/master/build/pause)
is available on GitHub.

Microsoft maintains a multi-architecture image with Linux and Windows amd64 support at `mcr.microsoft.com/oss/kubernetes/pause:3.5`.
This image is built from the same source as the Kubernetes maintained image but all of the Windows binaries are [authenticode signed](https://docs.microsoft.com/en-us/windows-hardware/drivers/install/authenticode) by Microsoft.
The Microsoft maintained image is recommended for production environments when signed binaries are required.
-->
<h4 id="pause-image">Pause 镜像  </h4>
<p>Kubernetes 维护着一个多体系结构镜像，其中包括对 Windows 的支持。
对于 Kubernetes v1.22，推荐的 pause 镜像是 <code>k8s.gcr.io/pause:3.5</code>。
<a href="https://github.com/kubernetes/kubernetes/tree/master/build/pause">源代码</a>可在 GitHub 上找到。</p>
<p>Microsoft 维护了一个支持 Linux 和 Windows amd64 的多体系结构镜像： <code>mcr.microsoft.com/oss/kubernetes/pause:3.5</code>。
此镜像与 Kubernetes 维护的镜像是从同一来源构建，但所有 Windows 二进制文件
均由 Microsoft <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/install/authenticode">签名</a>。
当生产环境需要被签名的二进制文件时，建议使用 Microsoft 维护的镜像。</p>
<!--
#### Compute

From an API and kubectl perspective, Windows containers behave in much the
same way as Linux-based containers. However, there are some notable
differences in key functionality which are outlined in the
[limitation section](#limitations).
-->
<h4 id="compute">计算   </h4>
<p>从 API 和 kubectl 的角度，Windows 容器的表现在很大程度上与基于 Linux 的容器
是相同的。不过也有一些与关键功能相关的差别值得注意，这些差别列举于
<a href="#limitations">局限性</a>小节中。</p>
<!--
Key Kubernetes elements work the same way in Windows as they do in Linux. In
this section, we talk about some of the key workload enablers and how they map
to Windows.
-->
<p>关键性的 Kubernetes 元素在 Windows 下与其在 Linux 下工作方式相同。我们在本节中
讨论一些关键性的负载支撑组件及其在 Windows 中的映射。</p>
<ul>
<li>
<p><a href="/zh/docs/concepts/workloads/pods/">Pods</a></p>
<!--
A Pod is the basic building block of Kubernetes–the smallest and simplest
unit in the Kubernetes object model that you create or deploy. You may not
deploy Windows and Linux containers in the same Pod. All containers in a Pod
are scheduled onto a single Node where each Node represents a specific
platform and architecture. The following Pod capabilities, properties and
events are supported with Windows containers:
-->
<p>Pod 是 Kubernetes 中最基本的构造模块，是 Kubernetes 对象模型中你可以创建或部署的
最小、最简单元。你不可以在同一 Pod 中部署 Windows 和 Linux 容器。
Pod 中的所有容器都会被调度到同一节点（Node），而每个节点代表的是一种特定的平台
和体系结构。Windows 容器支持 Pod 的以下能力、属性和事件：</p>
<!--
* Single or multiple containers per Pod with process isolation and volume sharing
* Pod status fields
* Readiness and Liveness probes
* postStart & preStop container lifecycle events
* ConfigMap, Secrets: as environment variables or volumes
* EmptyDir
* Named pipe host mounts
* Resource limits
-->
<ul>
<li>在带进程隔离和卷共享支持的 Pod 中运行一个或多个容器</li>
<li>Pod 状态字段</li>
<li>就绪态（Readiness）和活跃性（Liveness）探针</li>
<li>postStart 和 preStop 容器生命周期事件</li>
<li>ConfigMap、Secrets：用作环境变量或卷</li>
<li>emptyDir 卷</li>
<li>从宿主系统挂载命名管道</li>
<li>资源限制</li>
</ul>
</li>
<li>
<p><a href="/zh/docs/concepts/workloads/controllers/">控制器（Controllers）</a></p>
<!--
Kubernetes controllers handle the desired state of Pods. The following
workload controllers are supported with Windows containers:
-->
<p>Kubernetes 控制器处理 Pod 的期望状态。Windows 容器支持以下负载控制器：</p>
<ul>
<li>ReplicaSet</li>
<li>ReplicationController</li>
<li>Deployment</li>
<li>StatefulSet</li>
<li>DaemonSet</li>
<li>Job</li>
<li>CronJob</li>
</ul>
</li>
<li>
<p><a href="/zh/docs/concepts/services-networking/service/">服务（Services）</a></p>
<!--
A Kubernetes Service is an abstraction which defines a logical set of Pods
and a policy by which to access them - sometimes called a micro-service. You
can use services for cross-operating system connectivity. In Windows, services
can utilize the following types, properties and capabilities:
-->
<p>Kubernetes Service 是一种抽象对象，用来定义 Pod 的一个逻辑集合及用来访问这些
Pod 的策略。Service 有时也称作微服务（Micro-service）。你可以使用服务来实现
跨操作系统的连接。在 Windows 系统中，服务可以使用下面的类型、属性和能力：</p>
<ul>
<li>Service 环境变量</li>
<li>NodePort</li>
<li>ClusterIP</li>
<li>LoadBalancer</li>
<li>ExternalName</li>
<li>无头（Headless）服务</li>
</ul>
</li>
</ul>
<!--
Pods, Controllers and Services are critical elements to managing Windows
workloads on Kubernetes. However, on their own they are not enough to enable
the proper lifecycle management of Windows workloads in a dynamic cloud native
environment. We added support for the following features:

* Pod and container metrics
* Horizontal Pod Autoscaler support
* kubectl Exec
* Resource Quotas
* Scheduler preemption
-->
<p>Pods、控制器和服务是在 Kubernetes 上管理 Windows 负载的关键元素。
不过，在一个动态的云原生环境中，这些元素本身还不足以用来正确管理
Windows 负载的生命周期。我们为此添加了如下功能特性：</p>
<ul>
<li>Pod 和容器的度量（Metrics）</li>
<li>对水平 Pod 自动扩展的支持</li>
<li>对 kubectl exec 命令的支持</li>
<li>资源配额</li>
<li>调度器抢占</li>
</ul>
<!--
#### Container Runtime

##### Docker EE
-->
<h4 id="container-runtime">容器运行时   </h4>
<h5 id="docker-ee">Docker EE</h5>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code>
</div>


<!--
Docker EE-basic 19.03+ is the recommended container runtime for all Windows
Server versions. This works with the dockershim code included in the kubelet.
-->
<p>Docker EE-basic 19.03+ 是建议所有 Windows Server 版本采用的容器运行时。
该容器运行时能够与 kubelet 中的 dockershim 代码协同工作。</p>
<h5 id="cri-containerd">CRI-ContainerD</h5>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>


<!--
<a class='glossary-tooltip' title='强调简单性、健壮性和可移植性的一种容器运行时' data-toggle='tooltip' data-placement='top' href='https://containerd.io/docs/' target='_blank' aria-label='ContainerD'>ContainerD</a> 1.4.0+ can
also be used as the container runtime for Windows Kubernetes nodes.
-->
<p><a class='glossary-tooltip' title='强调简单性、健壮性和可移植性的一种容器运行时' data-toggle='tooltip' data-placement='top' href='https://containerd.io/docs/' target='_blank' aria-label='ContainerD'>ContainerD</a> 1.4.0+
也可作为 Windows Kubernetes 节点上的容器运行时。</p>
<!--
#### Persistent Storage

Kubernetes [volumes](/docs/concepts/storage/volumes/) enable complex
applications, with data persistence and Pod volume sharing requirements, to be
deployed on Kubernetes. Management of persistent volumes associated with a
specific storage back-end or protocol includes actions such as:
provisioning/de-provisioning/resizing of volumes, attaching/detaching a volume
to/from a Kubernetes node and mounting/dismounting a volume to/from individual
containers in a pod that needs to persist data. The code implementing these
volume management actions for a specific storage back-end or protocol is
shipped in the form of a Kubernetes volume
[plugin](/docs/concepts/storage/volumes/#types-of-volumes). The following
broad classes of Kubernetes volume plugins are supported on Windows:
-->
<h4 id="persistent-storage">持久性存储  </h4>
<p>使用 Kubernetes <a href="/zh/docs/concepts/storage/volumes/">卷</a>，对数据持久性和 Pod 卷
共享有需求的复杂应用也可以部署到 Kubernetes 上。
管理与特定存储后端或协议相关的持久卷时，相关的操作包括：对卷的配备（Provisioning）、
去配（De-provisioning）和调整大小，将卷挂接到 Kubernetes 节点或从节点上解除挂接，
将卷挂载到需要持久数据的 Pod 中的某容器或从容器上卸载。
负责实现为特定存储后端或协议实现卷管理动作的代码以 Kubernetes 卷
<a href="/zh/docs/concepts/storage/volumes/#types-of-volumes">插件</a>的形式发布。
Windows 支持以下大类的 Kubernetes 卷插件：</p>
<!--
##### In-Tree Volume Plugins

Code associated with in-tree volume plugins ship as part of the core
Kubernetes code base. Deployment of in-tree volume plugins do not require
installation of additional scripts or deployment of separate containerized
plugin components. These plugins can handle: provisioning/de-provisioning and
resizing of volumes in the storage backend, attaching/detaching of volumes
to/from a Kubernetes node and mounting/dismounting a volume to/from individual
containers in a pod. The following in-tree plugins support Windows nodes:
-->
<h5 id="in-tree-volume-plugins">树内卷插件  </h5>
<p>与树内卷插件（In-Tree Volume Plugin）相关的代码都作为核心 Kubernetes 代码基
的一部分发布。树内卷插件的部署不需要安装额外的脚本，也不需要额外部署独立的
容器化插件组件。这些插件可以处理：对应存储后端上存储卷的配备、去配和尺寸更改，
将卷挂接到 Kubernetes 或从其上解挂，以及将卷挂载到 Pod 中各个容器上或从其上
卸载。以下树内插件支持 Windows 节点：</p>
<ul>
<li><a href="/zh/docs/concepts/storage/volumes/#awselasticblockstore">awsElasticBlockStore</a></li>
<li><a href="/zh/docs/concepts/storage/volumes/#azuredisk">azureDisk</a></li>
<li><a href="/zh/docs/concepts/storage/volumes/#azurefile">azureFile</a></li>
<li><a href="/zh/docs/concepts/storage/volumes/#gcepersistentdisk">gcePersistentDisk</a></li>
<li><a href="/zh/docs/concepts/storage/volumes/#vspherevolume">vsphereVolume</a></li>
</ul>
<!--
##### FlexVolume Plugins

Code associated with [FlexVolume](/docs/concepts/storage/volumes/#flexVolume)
plugins ship as out-of-tree scripts or binaries that need to be deployed
directly on the host. FlexVolume plugins handle attaching/detaching of volumes
to/from a Kubernetes node and mounting/dismounting a volume to/from individual
containers in a pod. Provisioning/De-provisioning of persistent volumes
associated with FlexVolume plugins may be handled through an external
provisioner that is typically separate from the FlexVolume plugins. The
following FlexVolume
[plugins](https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows),
deployed as powershell scripts on the host, support Windows nodes:
-->
<h5 id="flexvolume-plugins">FlexVolume 插件  </h5>
<p>与 <a href="/zh/docs/concepts/storage/volumes/#flexVolume">FlexVolume</a> 插件相关的代码是作为
树外（Out-of-tree）脚本或可执行文件来发布的，因此需要在宿主系统上直接部署。
FlexVolume 插件处理将卷挂接到 Kubernetes 节点或从其上解挂、将卷挂载到 Pod 中
各个容器上或从其上卸载等操作。对于与 FlexVolume 插件相关联的持久卷的配备和
去配操作，可以通过外部的配置程序来处理。这类配置程序通常与 FlexVolume 插件
相分离。下面的 FlexVolume
<a href="https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows">插件</a>
可以以 PowerShell 脚本的形式部署到宿主系统上，支持 Windows 节点：</p>
<ul>
<li><a href="https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~smb.cmd">SMB</a></li>
<li><a href="https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~iscsi.cmd">iSCSI</a></li>
</ul>
<!--
##### CSI Plugins
-->
<h5 id="csi-plugins">CSI 插件  </h5>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [stable]</code>
</div>


<!--
Code associated with <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> plugins
ship as out-of-tree scripts and binaries that are typically distributed as
container images and deployed using standard Kubernetes constructs like
DaemonSets and StatefulSets. CSI plugins handle a wide range of volume
management actions in Kubernetes: provisioning/de-provisioning/resizing of
volumes, attaching/detaching of volumes to/from a Kubernetes node and
mounting/dismounting a volume to/from individual containers in a pod,
backup/restore of persistent data using snapshots and cloning.
-->
<p>与 <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> 插件相关联的代码作为
树外脚本和可执行文件来发布且通常发布为容器镜像形式，并使用 DaemonSet 和
StatefulSet 这类标准的 Kubernetes 构造体来部署。
CSI 插件处理 Kubernetes 中的很多卷管理操作：对卷的配备、去配和调整大小，
将卷挂接到 Kubernetes 节点或从节点上解除挂接，将卷挂载到需要持久数据的 Pod
中的某容器或从容器上卸载，使用快照和克隆来备份或恢复持久数据。</p>
<!--
CSI plugins communicate with a CSI node plugin which performs the local storage operations.
On Windows nodes CSI node plugins typically call APIs exposed by the community-managed
[csi-proxy](https://github.com/kubernetes-csi/csi-proxy) which handles the local storage operations.

Please refer to the deployment guide of the environment where you wish to deploy a Windows CSI plugin
for further details around installation.
You may also refer to the following [installation steps](https://github.com/kubernetes-csi/csi-proxy#installation).
-->
<p>来支持；csi-proxy 是一个社区管理的、独立的可执行文件，需要预安装在每个
Windows 节点之上。请参考你要部署的 CSI 插件的部署指南以进一步了解其细节。</p>
<p>CSI 插件与执行本地存储操作的 CSI 节点插件通信。
在 Windows 节点上，CSI 节点插件通常调用处理本地存储操作的 <a href="https://github.com/kubernetes-csi/csi-proxy">csi-proxy</a>
公开的 API, csi-proxy 由社区管理。</p>
<p>有关安装的更多详细信息，请参阅你要部署的 Windows CSI 插件的环境部署指南。
你也可以参考以下<a href="https://github.com/kubernetes-csi/csi-proxy#installation">安装步骤</a> 。</p>
<!--
#### Networking

Networking for Windows containers is exposed through
[CNI plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/).
Windows containers function similarly to virtual machines in regards to
networking. Each container has a virtual network adapter (vNIC) which is
connected to a Hyper-V virtual switch (vSwitch). The Host Networking Service
(HNS) and the Host Compute Service (HCS) work together to create containers
and attach container vNICs to networks. HCS is responsible for the management
of containers whereas HNS is responsible for the management of networking
resources such as:
-->
<h4 id="networking">联网    </h4>
<p>Windows 容器的联网是通过
<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">CNI 插件</a>
来暴露出来的。Windows 容器的联网行为与虚拟机的联网行为类似。
每个容器有一块虚拟的网络适配器（vNIC）连接到 Hyper-V 的虚拟交换机（vSwitch）。
宿主的联网服务（Host Networking Service，HNS）和宿主计算服务（Host Compute
Service，HCS）协同工作，创建容器并将容器的虚拟网卡连接到网络上。
HCS 负责管理容器，HNS 则负责管理网络资源，例如：</p>
<!--
* Virtual networks (including creation of vSwitches)
* Endpoints / vNICs
* Namespaces
* Policies (Packet encapsulations, Load-balancing rules, ACLs, NAT'ing rules, etc.)

The following service spec types are supported:
-->
<ul>
<li>虚拟网络（包括创建 vSwitch）</li>
<li>端点（Endpoint）/ vNIC</li>
<li>名字空间（Namespace）</li>
<li>策略（报文封装、负载均衡规则、访问控制列表、网络地址转译规则等等）</li>
</ul>
<p>支持的服务规约类型如下：</p>
<ul>
<li>NodePort</li>
<li>ClusterIP</li>
<li>LoadBalancer</li>
<li>ExternalName</li>
</ul>
<!--
##### Network modes

Windows supports five different networking drivers/modes: L2bridge, L2tunnel,
Overlay, Transparent, and NAT. In a heterogeneous cluster with Windows and
Linux worker nodes, you need to select a networking solution that is
compatible on both Windows and Linux. The following out-of-tree plugins are
supported on Windows, with recommendations on when to use each CNI:
-->
<h5 id="network-modes">网络模式   </h5>
<p>Windows 支持五种不同的网络驱动/模式：二层桥接（L2bridge）、二层隧道（L2tunnel）、
覆盖网络（Overlay）、透明网络（Transparent）和网络地址转译（NAT）。
在一个包含 Windows 和 Linux 工作节点的异构集群中，你需要选择一种对 Windows 和
Linux 兼容的联网方案。下面是 Windows 上支持的一些树外插件及何时使用某种
CNI 插件的建议：</p>
<table>
 <thead>
  <tr>
   <th><!--Network Driver-->网络驱动</th>
   <th><!--Description-->描述</th>
   <th><!--Container Packet Modifications-->容器报文更改</th>
   <th><!--Network Plugins-->网络插件</th>
   <th><!--Network Plugin Characteristics-->网络插件特点</th>
  </tr>
 </thead>
 <tbody>
  <tr>
   <td>L2bridge</td>
   <td><!--Containers are attached to an external vSwitch. Containers are attached
      to the underlay network, although the physical network doesn't need to learn
      the container. MACs because they are rewritten on ingress/egress.-->
      容器挂接到外部 vSwitch 上。容器挂接到下层网络之上，但由于容器的 MAC
      地址在入站和出站时被重写，物理网络不需要这些地址。
   </td>
   <td>
      <!--MAC is rewritten to host MAC, IP may be rewritten to host IP using HNS
      OutboundNAT policy.-->
      MAC 地址被重写为宿主系统的 MAC 地址，IP 地址也可能依据 HNS OutboundNAT
      策略重写为宿主的 IP 地址。     
   </td>
   <td>
    <a href="https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-bridge">win-bridge<a>、
    <a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md">Azure-CNI</a>、
    <!--Flannel host-gateway uses win-bridge-->
    Flannel 宿主网关（host-gateway）使用 win-bridge
   </td>
   <td>
     <!--win-bridge uses L2bridge network mode,
     connects containers to the underlay of hosts, offering best performance.
     Requires user-defined routes (UDR) for inter-node connectivity.-->
     win-bridge 使用二层桥接（L2bridge）网络模式，将容器连接到下层宿主系统上，
    从而提供最佳性能。需要用户定义的路由（User-Defined Routes，UDR）才能
    实现节点间的连接。
   </td>
  </tr>
  <tr>
   <td>L2Tunnel</td>
   <td>
    <!--This is a special case of l2bridge, but only used on Azure. All packets
    are sent to the virtualization host where SDN policy is applied.-->
    这是二层桥接的一种特殊情形，但仅被用于 Azure 上。
    所有报文都被发送到虚拟化环境中的宿主机上并根据 SDN 策略进行处理。
   </td>
   <td>
    <!--MAC rewritten, IP visible on the underlay network-->
    MAC 地址被改写，IP 地址在下层网络上可见。
   </td>
   <td>
    <a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md">Azure-CNI</a>
   </td>
   <td>
     <!-- Azure-CNI allows integration of containers with Azure vNET, and allows them
      to leverage the set of capabilities that
      <a href="https://azure.microsoft.com/en-us/services/virtual-network/">Azure Virtual Network</a>
      provides. For example, securely connect to Azure services or use Azure NSGs.
      See <a href="https://docs.microsoft.com/en-us/azure/aks/concepts-network#azure-cni-advanced-networking">azure-cni</a>
      for some examples.-->
     Azure-CNI 使得容器能够与 Azure vNET 集成，并允许容器利用
     [Azure 虚拟网络](https://azure.microsoft.com/en-us/services/virtual-network/)
     所提供的功能特性集合。例如，可以安全地连接到 Azure 服务上或者使用 Azure NSG。
     你可以参考
     [azure-cni](https://docs.microsoft.com/en-us/azure/aks/concepts-network#azure-cni-advanced-networking)
     所提供的一些示例。
   </td>
  </tr>
  <tr>
   <td><!--Overlay (Overlay networking for Windows in Kubernetes is in <B>Alpha</B> stage)-->覆盖网络（Kubernetes 中为 Windows 提供的覆盖网络支持处于 *alpha* 阶段）</td>
   <td>
    <!--Containers are given a vNIC connected to an external vSwitch. Each overlay
    network gets its own IP subnet, defined by a custom IP prefix.The overlay
    network driver uses VXLAN encapsulation.-->
    每个容器会获得一个连接到外部 vSwitch 的虚拟网卡（vNIC）。
    每个覆盖网络都有自己的、通过定制 IP 前缀来定义的 IP 子网。
    覆盖网络驱动使用 VxLAN 封装。
   </td>
   <td>
    <!--Encapsulated with an outer header.-->
    封装于外层包头内。
   </td>
   <td>
    <a href="https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-overlay">Win-overlay</a>、
    Flannel VXLAN<!--(uses win-overlay)-->（使用 win-overlay）
   </td>
   <td>
    <!--win-overlay should be used when virtual container networks are desired to
    be isolated from underlay of hosts (e.g. for security reasons). Allows for IPs
    to be re-used for different overlay networks (which have different VNID tags)
    if you are restricted on IPs in your datacenter.  This option requires
    <a href="https://support.microsoft.com/help/4489899">KB4489899</a> on Windows Server
    2019.-->
    当（比如出于安全原因）期望虚拟容器网络与下层宿主网络隔离时，
    应该使用 win-overlay。如果你的数据中心可用 IP 地址受限，
    覆盖网络允许你在不同的网络中复用 IP 地址（每个覆盖网络有不同的 VNID 标签）。
    这一选项要求在 Windows Server 2009 上安装
    [KB4489899](https://support.microsoft.com/help/4489899) 补丁。
   </td>
  </tr>
  <tr>
   <td>
    <!--Transparent (special use case for <a href="https://github.com/openvswitch/ovn-kubernetes">ovn-kubernetes</a>)-->
    透明网络（[ovn-kubernetes](https://github.com/openvswitch/ovn-kubernetes) 的特殊用例） 
   </td>
   <td>
    <!--Requires an external vSwitch. Containers are attached to an external
    vSwitch which enables intra-pod communication via logical networks (logical
    switches and routers).-->
    需要一个外部 vSwitch。容器挂接到某外部 vSwitch 上，该 vSwitch
    通过逻辑网络（逻辑交换机和路由器）允许 Pod 间通信。
   </td>
   <td>
    <!--Packet is encapsulated either via
    <a href="https://datatracker.ietf.org/doc/draft-gross-geneve/">GENEVE</a>,
    <a href="https://datatracker.ietf.org/doc/draft-davie-stt/">STT</a> tunneling to reach
    pods which are not on the same host.<br/> Packets are forwarded or dropped
    via the tunnel metadata information supplied by the ovn network controller.
    <br/>
    NAT is done for north-south communication.
    -->
    报文或者通过 [GENEVE](https://datatracker.ietf.org/doc/draft-gross-geneve/) 来封装，
    或者通过 [STT](https://datatracker.ietf.org/doc/draft-davie-stt/) 隧道来封装，
    以便能够到达不在同一宿主系统上的每个 Pod。<br/>
    报文通过 OVN 网络控制器所提供的隧道元数据信息来判定是转发还是丢弃。<br/>
    北-南向通信通过 NAT 网络地址转译来实现。
   </td>
   <td>
    <a href="https://github.com/openvswitch/ovn-kubernetes">ovn-kubernetes</a>
   </td>
   <td>
     <!-- Deploy via <a href="https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib">Ansible</a>.
     Distributed ACLs can be applied via Kubernetes policies. IPAM support.
     Load-balancing can be achieved without kube-proxy. NATing is done without
     using iptables/netsh.-->
     [通过 Ansible 来部署](https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib)。
     所发布的 ACL 可以通过 Kubernetes 策略来应用实施。支持 IPAM 。
     负载均衡能力不依赖 kube-proxy。
     网络地址转译（NAT）也不需要 iptables 或 netsh。
   </td>
  </tr>
  <tr>
   <td><!--NAT (<B>not used in Kubernetes</B>)-->NAT（<B>未在 Kubernetes 中使用</B>）</td>
   <td>
    <!--Containers are given a vNIC connected to an internal vSwitch. DNS/DHCP is
    provided using an internal component called
    <a href="https://blogs.technet.microsoft.com/virtualization/2016/05/25/windows-nat-winnat-capabilities-and-limitations">WinNAT</a>.-->
    容器获得一个连接到某内部 vSwitch 的 vNIC 接口。
    DNS/DHCP 服务通过名为
    [WinNAT](https://blogs.technet.microsoft.com/virtualization/2016/05/25/windows-nat-winnat-capabilities-and-limitations/)
    的内部组件来提供。
   </td>
   <td>
    <!--MAC and IP is rewritten to host MAC/IP.-->
    MAC 地址和 IP 地址都被重写为宿主系统的 MAC 地址和 IP 地址。
   </td>
   <td>
    <a href="https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat">nat</a>
   </td>
   <td>
    <!--Included here for completeness-->
    列在此表中仅出于完整性考虑
   </td>
  </tr>
 </tbody>
</table>
<!--
As outlined above, the [Flannel](https://github.com/coreos/flannel) CNI
[meta plugin](https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel)
is also supported on
[Windows](https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel#windows-support-experimental)
via the [VXLAN network backend](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan)
(**alpha support** ; delegates to win-overlay) and
[host-gateway network backend](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw)
(stable support; delegates to win-bridge). This plugin supports delegating to
one of the reference CNI plugins (win-overlay, win-bridge), to work in
conjunction with Flannel daemon on Windows (Flanneld) for automatic node
subnet lease assignment and HNS network creation. This plugin reads in its own
configuration file (cni.conf), and aggregates it with the environment
variables from the FlannelD generated subnet.env file. It then delegates to
one of the reference CNI plugins for network plumbing, and sends the correct
configuration containing the node-assigned subnet to the IPAM plugin (e.g.
host-local).
-->
<p>如前所述，<a href="https://github.com/coreos/flannel">Flannel</a> CNI
<a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel">meta 插件</a>
在 Windows 上也是
<a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel#windows-support-experimental">被支持</a>
的，方法是通过 <a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan">VXLAN 网络后端</a>
（<strong>alpha 阶段</strong> ：委托给 win-overlay）和
<a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw">主机-网关（host-gateway）网络后端</a>
（稳定版本；委托给 win-bridge 实现）。
此插件支持将操作委托给所引用的 CNI 插件（win-overlay、win-bridge）之一，
从而能够与 Windows 上的 Flannel 守护进程（Flanneld）一同工作，自动为节点
分配子网租期，创建 HNS 网络。
该插件读入其自身的配置文件（cni.conf），并将其与 FlannelD 所生成的 subnet.env
文件中的环境变量整合，之后将其操作委托给所引用的 CNI 插件之一以完成网络发现，
并将包含节点所被分配的子网信息的正确配置发送给 IPAM 插件（例如 host-local）。</p>
<!--
For the node, pod, and service objects, the following network flows are
supported for TCP/UDP traffic:

* Pod -> Pod (IP)
* Pod -> Pod (Name)
* Pod -> Service (Cluster IP)
* Pod -> Service (PQDN, but only if there are no ".")
* Pod -> Service (FQDN)
* Pod -> External (IP)
* Pod -> External (DNS)
* Node -> Pod
* Pod -> Node
-->
<p>对于节点、Pod 和服务对象，可针对 TCP/UDP 流量支持以下网络数据流：</p>
<ul>
<li>Pod -&gt; Pod （IP 寻址）</li>
<li>Pod -&gt; Pod （名字寻址）</li>
<li>Pod -&gt; 服务（集群 IP）</li>
<li>Pod -&gt; 服务（部分限定域名，仅适用于名称中不包含“.”的情形）</li>
<li>Pod -&gt; 服务（全限定域名）</li>
<li>Pod -&gt; 集群外部（IP 寻址）</li>
<li>Pod -&gt; 集群外部（DNS 寻址）</li>
<li>节点 -&gt; Pod</li>
<li>Pod -&gt; 节点</li>
</ul>
<!--
##### IP address management (IPAM) {#ipam}

The following IPAM options are supported on Windows:

* [Host-local](https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local)
* HNS IPAM (Inbox platform IPAM, this is a fallback when no IPAM is set)
* [Azure-vnet-ipam](https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md) (for azure-cni only)
-->
<h5 id="ipam">IP 地址管理（IPAM）      </h5>
<p>Windows 上支持以下 IPAM 选项：</p>
<ul>
<li><a href="https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local">host-local</a></li>
<li>HNS IPAM (Inbox 平台 IPAM，未指定 IPAM 时的默认设置）</li>
<li><a href="https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md">Azure-vnet-ipam</a>（仅适用于 azure-cni ）</li>
</ul>
<!--
##### Load balancing and Services

On Windows, you can use the following settings to configure Services and load
balancing behavior:
-->
<h5 id="load-balancing-and-services">负载均衡与服务    </h5>
<p>在 Windows 系统上，你可以使用以下配置来设定服务和负载均衡行为：</p>





<table><caption style="display: none;">Windows 服务设置</caption>
 <thead>
  <tr>
   <th><!--Feature-->功能特性</th>
   <th><!--Description-->描述</th>
   <th><!--Supported Kubernetes version-->所支持的 Kubernetes 版本</th>
   <th><!--Supported Windows OS build-->所支持的 Windows OS 版本</th>
   <th><!--How to enable-->如何启用</th>
  </tr>
 <thead>
 <tbody>
  <tr>
   <td><!--Session affinity-->会话亲和性</td>
   <td>
    <!--Ensures that connections from a particular client are passed to the same
    Pod each time.-->
    确保来自特定客户的连接每次都被交给同一 Pod。
   </td>
   <td>v1.20+</td>
   <td>
    <!--a href="https://blogs.windows.com/windowsexperience/2020/01/28/announcing-windows-server-vnext-insider-preview-build-19551/">Windows Server vNext Insider Preview Build 19551</a> (or higher)-->
    [Windows Server vNext Insider Preview Build 19551](https://blogs.windows.com/windowsexperience/2020/01/28/announcing-windows-server-vnext-insider-preview-build-19551/)
    或更高版本
   </td>
   <td>
    <!--Set <code>service.spec.sessionAffinity</code> to "ClientIP"-->
    将 <code>service.spec.sessionAffinitys</code> 设置为 "ClientIP"
   </td>
  </tr>
  <tr>
   <td><!--Direct Server Return (DSR)-->直接服务器返回（DSR）</td>
   <td>
    <!--Load balancing mode where the IP address fixups and the LBNAT occurs at
    the container vSwitch port directly; service traffic arrives with the source
    IP set as the originating pod IP.
    -->
    这是一种负载均衡模式，IP 地址的修正和负载均衡地址转译（LBNAT）
    直接在容器的 vSwitch 端口上处理；服务流量到达时，其源端 IP 地址
    设置为来源 Pod 的 IP。
   </td>
   <td>v1.20+</td>
   <td>
    Windows Server 2019
   </td>
   <td>
    <!--Set the following flags in kube-proxy:
    <code>--feature-gates="WinDSR=true" --enable-dsr=true</code>-->
    为 kube-proxy 设置标志：`--feature-gates="WinDSR=true" --enable-dsr=true`
   </td>
  </tr>
  <tr>
   <td><!--Preserve-Destination-->保留目标地址</td>
   <td>
    <!--Skips DNAT of service traffic, thereby preserving the virtual IP of the target
    service in packets reaching the backend Pod. Also disables node-node forwarding.-->
    对服务流量略过 DNAT 步骤，这样就可以在到达后端 Pod 的报文中保留目标服务的
    虚拟 IP 地址。还要禁止节点之间的转发。 
   </td>
   <td>v1.20+</td>
   <td><!--Windows Server, version 1903 (or higher)-->Windows Server 1903 或更高版本</td>
   <td>
    <!--Set <code>"preserve-destination": "true"</code> in service annotations
    and enable DSR in kube-proxy.-->
    在服务注解中设置 `"preserve-destination": "true"` 并启用
    kube-proxy 中的 DSR 标志。
   </td>
  </tr>
  <tr>
   <td><!--IPv4/IPv6 dual-stack networking-->IPv4/IPv6 双栈网络</td>
   <td>
    <!--Native IPv4-to-IPv4 in parallel with IPv6-to-IPv6 communications to, from,
    and within a cluster-->
    在集群内外同时支持原生的 IPv4-到-IPv4 和 IPv6-到-IPv6 通信。
   </td>
   <td>v1.19+</td>
   <td><!--Windows Server, version 2004 (or higher)-->Windows Server 2004 或更高版本</td>
   <td>
    <!--See <a href="#ipv4ipv6-dual-stack">IPv4/IPv6 dual-stack</a>-->
    参见 [IPv4/IPv6 双栈网络](#ipv4ipv6-dual-stack)
   </td>
  </tr>
  <tr>
   <td><!--Client IP preservation-->保留客户端 IP</td>
   <td>
    <!--Ensures that source IP of incoming ingress traffic gets preserved. Also
    disables node-node forwarding.-->
    确保入站流量的源 IP 地址被保留。同样要禁止节点之间的转发。
   </td>
   <td>v1.20+</td>
   <td><!--Windows Server, version 2019 (or higher)-->Windows Server 2019 或更高版本</td>
   <td>
    <!--Set <code>service.spec.externalTrafficPolicy</code> to "Local" and enable
    DSR in kube-proxy.-->
    将 <code>service.spec.externalTrafficPolicy</code> 设置为 "Local"，
    并在 kube-proxy 上启用 DSR。
   </td>
  </tr>
 </tbody>
</table>

<!--
#### IPv4/IPv6 dual-stack

You can enable IPv4/IPv6 dual-stack networking for `l2bridge` networks using
the `IPv6DualStack` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/). See
[enable IPv4/IPv6 dual stack](/docs/concepts/services-networking/dual-stack#enable-ipv4ipv6-dual-stack)
for more details.
-->
<h4 id="ipv4ipv6-dual-stack">IPv4/IPv6 双栈支持  </h4>
<p>你可以通过使用 <code>IPv6DualStack</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
来为 <code>l2bridge</code> 网络启用 IPv4/IPv6 双栈联网支持。
进一步的细节可参见
<a href="/zh/docs/concepts/services-networking/dual-stack#enable-ipv4ipv6-dual-stack">启用 IPv4/IPv6 双协议栈</a>。</p>
<!--
On Windows, using IPv6 with Kubernetes require Windows Server, version 2004
(kernel version 10.0.19041.610) or later.

Overlay (VXLAN) networks on Windows do not support dual-stack networking today.
-->
<p>对 Windows 而言，在 Kubernetes 中使用 IPv6 需要
Windows Server 2004 （内核版本 10.0.19041.610）或更高版本。</p>
<p>目前 Windows 上的覆盖网络（VXLAN）还不支持双协议栈联网。</p>
<!--
### Limitations

Windows is only supported as a worker node in the Kubernetes architecture and
component matrix. This means that a Kubernetes cluster must always include
Linux master nodes, zero or more Linux worker nodes, and zero or more Windows
worker nodes.
-->
<h3 id="limitations">局限性   </h3>
<p>在 Kubernetes 架构和节点阵列中仅支持将 Windows 作为工作节点使用。
这意味着 Kubernetes 集群必须总是包含 Linux 主控节点，零个或者多个 Linux
工作节点以及零个或者多个 Windows 工作节点。</p>
<!--
#### Resource Handling

Linux cgroups are used as a pod boundary for resource controls in Linux.
Containers are created within that boundary for network, process and file
system isolation. The cgroups APIs can be used to gather cpu/io/memory stats.
In contrast, Windows uses a Job object per container with a system namespace
filter to contain all processes in a container and provide logical isolation
from the host. There is no way to run a Windows container without the
namespace filtering in place. This means that system privileges cannot be
asserted in the context of the host, and thus privileged containers are not
available on Windows. Containers cannot assume an identity from the host
because the Security Account Manager (SAM) is separate.
-->
<h4 id="resource-handling">资源处理</h4>
<p>Linux 上使用 Linux 控制组（CGroups）作为 Pod 的边界，以实现资源控制。
容器都创建于这一边界之内，从而实现网络、进程和文件系统的隔离。
控制组 CGroups API 可用来收集 CPU、I/O 和内存的统计信息。
与此相比，Windows 为每个容器创建一个带有系统名字空间过滤设置的 Job 对象，
以容纳容器中的所有进程并提供其与宿主系统间的逻辑隔离。
没有现成的名字空间过滤设置是无法运行 Windows 容器的。
这也意味着，系统特权无法在宿主环境中评估，因而 Windows 上也就不存在特权容器。
归咎于独立存在的安全账号管理器（Security Account Manager，SAM），容器也不能
获得宿主系统上的任何身份标识。</p>
<!--
#### Resource Reservations

##### Memory Reservations

Windows does not have an out-of-memory process killer as Linux does. Windows
always treats all user-mode memory allocations as virtual, and pagefiles are
mandatory. The net effect is that Windows won't reach out of memory conditions
the same way Linux does, and processes page to disk instead of being subject
to out of memory (OOM) termination. If memory is over-provisioned and all
physical memory is exhausted, then paging can slow down performance.
-->
<h4 id="resource-reservations">资源预留  </h4>
<h5 id="memory-reservations">内存预留   </h5>
<p>Windows 不像 Linux 一样有一个内存耗尽（Out-of-memory）进程杀手（Process
Killer）机制。Windows 总是将用户态的内存分配视为虚拟请求，页面文件（Pagefile）
是必需的。这一差异的直接结果是 Windows 不会像 Linux 那样出现内存耗尽的状况，
系统会将进程内存页面写入磁盘而不会因内存耗尽而终止进程。
当内存被过量使用且所有物理内存都被用光时，系统的换页行为会导致性能下降。</p>
<!--
Keeping memory usage within reasonable bounds is possible using the kubelet
parameters `--kubelet-reserve` and/or `--system-reserve` to account for memory
usage on the node (outside of containers). This reduces
[NodeAllocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)).
-->
<p>使用 kubelet 参数 <code>--kubelet-reserve</code> 与/或 <code>-system-reserve</code> 可以统计
节点上的内存用量（各容器之外），进而可能将内存用量限制在一个合理的范围，。
这样做会减少节点可分配内存
（<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">NodeAllocatable</a>）。</p>
<!--
As you deploy workloads, use resource limits (must set only limits or limits
must equal requests) on containers. This also subtracts from NodeAllocatable
and prevents the scheduler from adding more pods once a node is full.
-->
<p>在你部署工作负载时，对容器使用资源限制（必须仅设置 limits 或者让 limits 等于
requests 值）。这也会从 NodeAllocatable 中耗掉部分内存量，从而避免在节点
负荷已满时调度器继续向节点添加 Pods。</p>
<!--
A best practice to avoid over-provisioning is to configure the kubelet with a
system reserved memory of at least 2GB to account for Windows, Docker, and
Kubernetes processes.
-->
<p>避免过量分配的最佳实践是为 kubelet 配置至少 2 GB 的系统预留内存，以供
Windows、Docker 和 Kubernetes 进程使用。</p>
<!--
##### CPU Reservations

To account for Windows, Docker and other Kubernetes host processes it is
recommended to reserve a percentage of CPU so they are able to respond to
events. This value needs to be scaled based on the number of CPU cores
available on the Windows node.To determine this percentage a user should
identify the maximum pod density for each of their nodes and monitor the CPU
usage of the system services choosing a value that meets their workload needs.
-->
<h5 id="cpu-reservations">CPU 预留  </h5>
<p>为了统计 Windows、Docker 和其他 Kubernetes 宿主进程的 CPU 用量，建议
预留一定比例的 CPU，以便对事件作出相应。此值需要根据 Windows 节点上
CPU 核的个数来调整，要确定此百分比值，用户需要为其所有节点确定 Pod
密度的上线，并监控系统服务的 CPU 用量，从而选择一个符合其负载需求的值。</p>
<!--
Keeping CPU usage within reasonable bounds is possible using the kubelet
parameters `--kubelet-reserve` and/or `--system-reserve` to account for CPU
usage on the node (outside of containers). This reduces
[NodeAllocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable). 
-->
<p>使用 kubelet 参数 <code>--kubelet-reserve</code> 与/或 <code>-system-reserve</code> 可以统计
节点上的 CPU 用量（各容器之外），进而可能将 CPU 用量限制在一个合理的范围，。
这样做会减少节点可分配 CPU
（<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">NodeAllocatable</a>）。</p>
<!--
##### Feature Restrictions

* TerminationGracePeriod: not implemented
* Single file mapping: to be implemented with CRI-ContainerD
* Termination message: to be implemented with CRI-ContainerD
* Privileged Containers: not currently supported in Windows containers
* HugePages: not currently supported in Windows containers
* The existing node problem detector is Linux-only and requires privileged
  containers. In general, we don't expect this to be used on Windows because
  privileged containers are not supported
* Not all features of shared namespaces are supported (see API section for
  more details)
-->
<h5 id="feature-restrictions">功能特性限制   </h5>
<ul>
<li>终止宽限期（Termination Grace Period）：未实现</li>
<li>单文件映射：将用 CRI-ContainerD 来实现</li>
<li>终止消息（Termination message）：将用 CRI-ContainerD 来实现</li>
<li>特权容器：Windows 容器当前不支持</li>
<li>巨页（Huge Pages）：Windows 容器当前不支持</li>
<li>现有的节点问题探测器（Node Problem Detector）仅适用于 Linux，且要求使用特权容器。
一般而言，我们不设想此探测器能用于 Windows 节点，因为 Windows 不支持特权容器。</li>
<li>并非支持共享名字空间的所有功能特性（参见 API 节以了解详细信息）</li>
</ul>
<!--
#### Difference in behavior of flags when compared to Linux

The behavior of the following kubelet flags is different on Windows nodes as described below:

* `--kubelet-reserve`, `--system-reserve` , and `--eviction-hard` flags update
  Node Allocatable
* Eviction by using `--enforce-node-allocable` is not implemented
* Eviction by using `--eviction-hard` and `--eviction-soft` are not implemented
* MemoryPressure Condition is not implemented
* There are no OOM eviction actions taken by the kubelet
* Kubelet running on the windows node does not have memory restrictions.
  `--kubelet-reserve` and `--system-reserve` do not set limits on kubelet or
  processes running on the host. This means kubelet or a process on the host
  could cause memory resource starvation outside the node-allocatable and
  scheduler
* An additional flag to set the priority of the kubelet process is available
  on the Windows nodes called `--windows-priorityclass`. This flag allows
  kubelet process to get more CPU time slices when compared to other processes
  running on the Windows host. More information on the allowable values and
  their meaning is available at
  [Windows Priority Classes](https://docs.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities#priority-class).
  In order for kubelet to always have enough CPU cycles it is recommended to set
  this flag to `ABOVE_NORMAL_PRIORITY_CLASS` and above
-->
<h4 id="与-linux-相比参数行为的差别">与 Linux 相比参数行为的差别</h4>
<p>以下 kubelet 参数的行为在 Windows 节点上有些不同，描述如下：</p>
<ul>
<li><code>--kubelet-reserve</code>、<code>--system-reserve</code> 和 <code>--eviction-hard</code> 标志
会更新节点可分配资源量</li>
<li>未实现通过使用 <code>--enforce-node-allocable</code> 来完成的 Pod 驱逐</li>
<li>未实现通过使用 <code>--eviction-hard</code> 和 <code>--eviction-soft</code> 来完成的 Pod 驱逐</li>
<li><code>MemoryPressure</code> 状况未实现</li>
<li><code>kubelet</code> 不会采取措施来执行基于 OOM 的驱逐动作</li>
<li>Windows 节点上运行的 kubelet 没有内存约束。
<code>--kubelet-reserve</code> 和 <code>--system-reserve</code> 不会为 kubelet 或宿主系统上运行
的进程设限。这意味着 kubelet 或宿主系统上的进程可能导致内存资源紧张，
而这一情况既不受节点可分配量影响，也不会被调度器感知。</li>
<li>在 Windows 节点上存在一个额外的参数用来设置 kubelet 进程的优先级，称作
<code>--windows-priorityclass</code>。此参数允许 kubelet 进程获得与 Windows 宿主上
其他进程相比更多的 CPU 时间片。
关于可用参数值及其含义的进一步信息可参考
<a href="https://docs.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities#priority-class">Windows Priority Classes</a>。
为了让 kubelet 总能够获得足够的 CPU 周期，建议将此参数设置为
<code>ABOVE_NORMAL_PRIORITY_CLASS</code> 或更高。</li>
</ul>
<!--
#### Storage

Windows has a layered filesystem driver to mount container layers and create a
copy filesystem based on NTFS. All file paths in the container are resolved
only within the context of that container.

* With Docker Volume mounts can only target a directory in the container, and
  not an individual file. This limitation does not exist with CRI-containerD.

* Volume mounts cannot project files or directories back to the host
  filesystem

* Read-only filesystems are not supported because write access is always
  required for the Windows registry and SAM database. However, read-only
  volumes are supported
* Volume user-masks and permissions are not available. Because the SAM is not
  shared between the host & container, there's no mapping between them. All
  permissions are resolved within the context of the container
-->
<h4 id="storage">存储    </h4>
<p>Windows 上包含一个分层的文件系统来挂载容器的分层，并会基于 NTFS 来创建一个
拷贝文件系统。容器中的所有文件路径都仅在该容器的上下文内完成解析。</p>
<ul>
<li>Docker 卷挂载仅可针对容器中的目录进行，不可针对独立的文件。
这一限制不适用于 CRI-containerD。</li>
<li>卷挂载无法将文件或目录投射回宿主文件系统。</li>
<li>不支持只读文件系统，因为 Windows 注册表和 SAM 数据库总是需要写访问权限。
不过，Windows 支持只读的卷。</li>
<li>不支持卷的用户掩码和访问许可，因为宿主与容器之间并不共享 SAM，二者之间不存在
映射关系。所有访问许可都是在容器上下文中解析的。</li>
</ul>
<!--
As a result, the following storage functionality is not supported on Windows nodes

* Volume subpath mounts. Only the entire volume can be mounted in a Windows container.
* Subpath volume mounting for Secrets
* Host mount projection
* DefaultMode (due to UID/GID dependency)
* Read-only root filesystem. Mapped volumes still support readOnly
* Block device mapping
* Memory as the storage medium
* File system features like uui/guid, per-user Linux filesystem permissions
* NFS based storage/volume support
* Expanding the mounted volume (resizefs)
-->
<p>因此，Windows 节点上不支持以下存储功能特性：</p>
<ul>
<li>卷的子路径挂载；只能在 Windows 容器上挂载整个卷。</li>
<li>为 Secret 执行子路径挂载；</li>
<li>宿主挂载投射；</li>
<li>默认访问模式 defaultMode（因为该特性依赖 UID/GID）；</li>
<li>只读的根文件系统；映射的卷仍然支持 <code>readOnly</code>；</li>
<li>块设备映射；</li>
<li>将内存作为存储介质；</li>
<li>类似 UUID/GUID、每用户不同的 Linux 文件系统访问许可等文件系统特性；</li>
<li>基于 NFS 的存储和卷支持；</li>
<li>扩充已挂载卷（resizefs）。</li>
</ul>
<!--
#### Networking {#networking-limitations}

Windows Container Networking differs in some important ways from Linux
networking. The [Microsoft documentation for Windows Container Networking](https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture)
contains additional details and background.
-->
<h4 id="networking-limitations">联网  </h4>
<p>Windows 容器联网与 Linux 联网有着非常重要的差别。
<a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture">Microsoft documentation for Windows Container Networking</a>
中包含额外的细节和背景信息。</p>
<!--
The Windows host networking service and virtual switch implement namespacing
and can create virtual NICs as needed for a pod or container. However, many
configurations such as DNS, routes, and metrics are stored in the Windows
registry database rather than /etc/... files as they are on Linux. The Windows
registry for the container is separate from that of the host, so concepts like
mapping /etc/resolv.conf from the host into a container don't have the same
effect they would on Linux. These must be configured using Windows APIs run in
the context of that container. Therefore CNI implementations need to call the
HNS instead of relying on file mappings to pass network details into the pod
or container.
-->
<p>Windows 宿主联网服务和虚拟交换机实现了名字空间隔离，可以根据需要为 Pod 或容器
创建虚拟的网络接口（NICs）。不过，很多类似 DNS、路由、度量值之类的配置数据都
保存在 Windows 注册表数据库中而不是像 Linux 一样保存在 <code>/etc/...</code> 文件中。
Windows 为容器提供的注册表与宿主系统的注册表是分离的，因此类似于将 /etc/resolv.conf
文件从宿主系统映射到容器中的做法不会产生与 Linux 系统相同的效果。
这些信息必须在容器内部使用 Windows API 来配置。
因此，CNI 实现需要调用 HNS，而不是依赖文件映射来将网络细节传递到 Pod
或容器中。</p>
<!--
The following networking functionality is not supported on Windows nodes

* Host networking mode is not available for Windows pods

* Local NodePort access from the node itself fails (works for other nodes or
  external clients)

* Accessing service VIPs from nodes will be available with a future release of
  Windows Server

* A single service can only support up to 64 backend pods / unique destination IPs

* Overlay networking support in kube-proxy is a beta feature. In addition, it
  requires
  [KB4482887](https://support.microsoft.com/en-us/help/4482887/windows-10-update-kb4482887)
  to be installed on Windows Server 2019

* Local Traffic Policy in non-DSR mode
* Windows containers connected to overlay networks do not support
  communicating over the IPv6 stack. There is outstanding Windows platform
  work required to enable this network driver to consume IPv6 addresses and
  subsequent Kubernetes work in kubelet, kube-proxy, and CNI plugins.
-->
<p>Windows 节点不支持以下联网功能：</p>
<ul>
<li>Windows Pod 不能使用宿主网络模式；</li>
<li>从节点本地访问 NodePort 会失败（但从其他节点或外部客户端可访问）</li>
<li>Windows Server 的未来版本中会支持从节点访问服务的 VIP；</li>
<li>每个服务最多支持 64 个后端 Pod 或独立的目标 IP 地址；</li>
<li>kube-proxy 的覆盖网络支持是 Beta 特性。此外，它要求在 Windows Server 2019 上安装
<a href="https://support.microsoft.com/en-us/help/4482887/windows-10-update-kb4482887">KB4482887</a> 补丁；</li>
<li>非 DSR（保留目标地址）模式下的本地流量策略；</li>
<li>连接到覆盖网络的 Windows 容器不支持使用 IPv6 协议栈通信。
要使得这一网络驱动支持 IPv6 地址需要在 Windows 平台上开展大量的工作，
还需要在 Kubernetes 侧修改 kubelet、kube-proxy 以及 CNI 插件。</li>
</ul>
<!--
* Outbound communication using the ICMP protocol via the win-overlay,
  win-bridge, and Azure-CNI plugin. Specifically, the Windows data plane
  ([VFP](https://www.microsoft.com/en-us/research/project/azure-virtual-filtering-platform/))
  doesn't support ICMP packet transpositions. This means:

  * ICMP packets directed to destinations within the same network (e.g. pod to
    pod communication via ping) work as expected and without any limitations

  * TCP/UDP packets work as expected and without any limitations

  * ICMP packets directed to pass through a remote network (e.g. pod to
    external internet communication via ping) cannot be transposed and thus
    will not be routed back to their source

  * Since TCP/UDP packets can still be transposed, one can substitute `ping
    <destination>` with `curl <destination>` to be able to debug connectivity
    to the outside world.
-->
<ul>
<li>
<p>通过 win-overlay、win-bridge 和 Azure-CNI 插件使用 ICMP 协议向集群外通信。
尤其是，Windows 数据面
（<a href="https://www.microsoft.com/en-us/research/project/azure-virtual-filtering-platform/">VFP</a>）
不支持转换 ICMP 报文。这意味着：</p>
<ul>
<li>指向同一网络内目标地址的 ICMP 报文（例如 Pod 之间的 ping 通信）是可以工作的，
没有局限性；</li>
<li>TCP/UDP 报文可以正常工作，没有局限性；</li>
<li>指向远程网络的 ICMP 报文（例如，从 Pod 中 ping 外部互联网的通信）无法被转换，
因此也无法被路由回到其源点；</li>
<li>由于 TCP/UDP 包仍可被转换，用户可以将 <code>ping &lt;目标&gt;</code> 操作替换为 <code>curl &lt;目标&gt;</code>
以便能够调试与外部世界的网络连接。</li>
</ul>
</li>
</ul>
<!--
These features were added in Kubernetes v1.15:

* `kubectl port-forward`
-->
<p>Kubernetes v1.15 中添加了以下功能特性：</p>
<ul>
<li><code>kubectl port-forward</code></li>
</ul>
<!--
##### CNI Plugins

* Windows reference network plugins win-bridge and win-overlay do not
  currently implement [CNI spec](https://github.com/containernetworking/cni/blob/master/SPEC.md) v0.4.0
  due to missing "CHECK" implementation.
* The Flannel VXLAN CNI has the following limitations on Windows:

  1. Node-pod connectivity isn't possible by design. It's only possible for
     local pods with Flannel v0.12.0 (or higher).
  1. We are restricted to using VNI 4096 and UDP port 4789. The VNI limitation
     is being worked on and will be overcome in a future release (open-source
     flannel changes). See the official
     [Flannel VXLAN](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan)
     backend docs for more details on these parameters.
-->
<h5 id="cni-plugins">CNI 插件   </h5>
<ul>
<li>
<p>Windows 参考网络插件 win-bridge 和 win-overlay 当前未实现
<a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI spec</a> v0.4.0，
原因是缺少检查（CHECK）用的实现。</p>
</li>
<li>
<p>Windows 上的 Flannel VXLAN CNI 有以下局限性：</p>
<ol>
<li>其设计上不支持从节点到 Pod 的连接。
只有在 Flannel v0.12.0 或更高版本后才有可能访问本地 Pods。</li>
<li>我们被限制只能使用 VNI 4096 和 UDP 端口 4789。
VNI 的限制正在被解决，会在将来的版本中消失（开源的 Flannel 更改）。
参见官方的 <a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan">Flannel VXLAN</a>
后端文档以了解关于这些参数的详细信息。</li>
</ol>
</li>
</ul>
<h5 id="dns-limitations">DNS</h5>
<!--
* ClusterFirstWithHostNet is not supported for DNS. Windows treats all names
  with a '.' as a FQDN and skips PQDN resolution

* On Linux, you have a DNS suffix list, which is used when trying to resolve
  PQDNs. On Windows, we only have 1 DNS suffix, which is the DNS suffix
  associated with that pod's namespace (`mydns.svc.cluster.local` for example).
  Windows can resolve FQDNs and services or names resolvable with just that
  suffix. For example, a pod spawned in the default namespace, will have the DNS
  suffix `default.svc.cluster.local`. On a Windows pod, you can resolve both
  `kubernetes.default.svc.cluster.local` and `kubernetes`, but not the
  in-betweens, like `kubernetes.default` or `kubernetes.default.svc`.

* On Windows, there are multiple DNS resolvers that can be used. As these come
  with slightly different behaviors, using the `Resolve-DNSName` utility for
  name query resolutions is recommended.
-->
<ul>
<li>
<p>不支持 DNS 的 ClusterFirstWithHostNet 配置。Windows 将所有包含 “.” 的名字
视为全限定域名（FQDN），因而不会对其执行部分限定域名（PQDN）解析。</p>
</li>
<li>
<p>在 Linux 上，你可以有一个 DNS 后缀列表供解析部分限定域名时使用。
在 Windows 上，我们只有一个 DNS 后缀，即与该 Pod 名字空间相关联的 DNS
后缀（例如 <code>mydns.svc.cluster.local</code>）。
Windows 可以解析全限定域名、或者恰好可用该后缀来解析的服务名称。
例如，在 default 名字空间中生成的 Pod 会获得 DNS 后缀
<code>default.svc.cluster.local</code>。在 Windows Pod 中，你可以解析
<code>kubernetes.default.svc.cluster.local</code> 和 <code>kubernetes</code>，但无法解析二者
之间的形式，如 <code>kubernetes.default</code> 或 <code>kubernetes.default.svc</code>。</p>
</li>
<li>
<p>在 Windows 上，可以使用的 DNS 解析程序有很多。由于这些解析程序彼此之间
会有轻微的行为差别，建议使用 <code>Resolve-DNSName</code> 工具来完成名字查询解析。</p>
</li>
</ul>
<!--
##### IPv6

Kubernetes on Windows does not support single-stack "IPv6-only" networking.
However,dual-stack IPv4/IPv6 networking for pods and nodes with single-family
services is supported.
See [IPv4/IPv6 dual-stack networking](#ipv4ipv6-dual-stack) for more details.
-->
<h5 id="ipv6">IPv6</h5>
<p>Windows 上的 Kubernetes 不支持单协议栈的“只用 IPv6”联网选项。
不过，系统支持在 IPv4/IPv6 双协议栈的 Pod 和节点上运行单协议家族的服务。
更多细节可参阅 <a href="#ipv4ipv6-dual-stack">IPv4/IPv6 双协议栈联网</a>一节。</p>
<!--
##### Session affinity

Setting the maximum session sticky time for Windows services using
`service.spec.sessionAffinityConfig.clientIP.timeoutSeconds` is not supported.
-->
<h5 id="session-affinity">会话亲和性   </h5>
<p>不支持使用 <code>service.spec.sessionAffinityConfig.clientIP.timeoutSeconds</code> 来为
Windows 服务设置最大会话粘滞时间。</p>
<!--
##### Security

Secrets are written in clear text on the node's volume (as compared to
tmpfs/in-memory on linux). This means customers have to do two things

1. Use file ACLs to secure the secrets file location
1. Use volume-level encryption using
   [BitLocker](https://docs.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server)
-->
<h5 id="security">安全性    </h5>
<p>Secret 以明文形式写入节点的卷中（而不是像 Linux 那样写入内存或 tmpfs 中）。
这意味着客户必须做以下两件事：</p>
<ol>
<li>使用文件访问控制列表来保护 Secret 文件所在的位置</li>
<li>使用 <a href="https://docs.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server">BitLocker</a>
来执行卷层面的加密</li>
</ol>
<!--
[RunAsUserName](/docs/tasks/configure-pod-container/configure-runasusername)
can be specified for Windows Pod's or Container's to execute the Container
processes as a node-default user. This is roughly equivalent to
[RunAsUser](/docs/concepts/policy/pod-security-policy/#users-and-groups).

Linux specific pod security context privileges such as SELinux, AppArmor,
Seccomp, Capabilities (POSIX Capabilities), and others are not supported.

In addition, as mentioned already, privileged containers are not supported on
Windows.
-->
<p>用户可以为 Windows Pods 或 Container 设置
<a href="/zh/docs/tasks/configure-pod-container/configure-runasusername"><code>RunAsUserName</code></a>
以便以非节点默认用户来执行容器中的进程。这大致等价于设置
<a href="/zh/docs/concepts/policy/pod-security-policy/#users-and-groups"><code>RunAsUser</code></a>。</p>
<p>不支持特定于 Linux 的 Pod 安全上下文特权，例如 SELinux、AppArmor、Seccomp、
权能字（POSIX 权能字）等等。</p>
<p>此外，如前所述，Windows 不支持特权容器。</p>
<!--
#### API

There are no differences in how most of the Kubernetes APIs work for Windows.
The subtleties around what's different come down to differences in the OS and
container runtime. In certain situations, some properties on workload APIs
such as Pod or Container were designed with an assumption that they are
implemented on Linux, failing to run on Windows.

At a high level, these OS concepts are different:
-->
<h4 id="api">API</h4>
<p>对 Windows 而言，大多数 Kubernetes API 的工作方式没有变化。
一些不易察觉的差别通常体现在 OS 和容器运行时上的不同。
在某些场合，负载 API （如 Pod 或 Container）的某些属性在设计时假定其
在 Linux 上实现，因此会无法在 Windows 上运行。</p>
<p>在较高层面，不同的 OS 概念有：</p>
<!--
* Identity - Linux uses userID (UID) and groupID (GID) which are represented
  as integer types. User and group names are not canonical - they are an alias
  in `/etc/groups` or `/etc/passwd` back to UID+GID. Windows uses a larger
  binary security identifier (SID) which is stored in the Windows Security
  Access Manager (SAM) database. This database is not shared between the host
  and containers, or between containers.

* File permissions - Windows uses an access control list based on SIDs, rather
  than a bitmask of permissions and UID+GID
-->
<ul>
<li>
<p>身份标识 - Linux 使用证书类型来表示用户 ID（UID）和组 ID（GID）。用户和组名
没有特定标准，它们是 <code>/etc/groups</code> 或 <code>/etc/passwd</code> 中的别名表项，会映射回
UID+GID。Windows 使用一个更大的二进制安全标识符（SID），保存在 Windows
安全访问管理器（Security Access Manager，SAM）数据库中。此数据库并不在宿主系统
与容器间，或者任意两个容器之间共享。</p>
</li>
<li>
<p>文件许可 - Windows 使用基于 SID 的访问控制列表，而不是基于 UID+GID 的访问权限位掩码。</p>
</li>
</ul>
<!--
* File paths - convention on Windows is to use `\` instead of `/`. The Go IO
  libraries accept both types of file path separators. However, when you're
  setting a path or command line that's interpreted inside a container, `\` may
  be needed.

* Signals - Windows interactive apps handle termination differently, and can
  implement one or more of these:

  * A UI thread handles well-defined messages including `WM_CLOSE`

  * Console apps handle ctrl-c or ctrl-break using a Control Handler

  * Services register a Service Control Handler function that can accept
    `SERVICE_CONTROL_STOP` control codes
-->
<ul>
<li>
<p>文件路径 - Windows 上的习惯是使用 <code>\</code> 而非 <code>/</code>。Go 语言的 IO
库同时接受这两种文件路径分隔符。不过，当你在指定要在容器内解析的路径或命令行时，
可能需要使用 <code>\</code>。</p>
</li>
<li>
<p>信号（Signal） - Windows 交互式应用以不同方式来处理终止事件，并可实现以下方式之一或组合：</p>
<ul>
<li>
<p>UI 线程处理包含 <code>WM_CLOSE</code> 在内的良定的消息</p>
</li>
<li>
<p>控制台应用使用控制处理程序来处理 Ctrl-C 或 Ctrl-Break</p>
</li>
<li>
<p>服务会注册服务控制处理程序，接受 <code>SERVICE_CONTROL_STOP</code> 控制代码</p>
</li>
</ul>
</li>
</ul>
<!--
Exit Codes follow the same convention where 0 is success, nonzero is failure.
The specific error codes may differ across Windows and Linux. However, exit
codes passed from the Kubernetes components (kubelet, kube-proxy) are
unchanged.
-->
<p>退出代码遵从相同的习惯，0 表示成功，非 0 值表示失败。
特定的错误代码在 Windows 和 Linux 上可能会不同。不过，从 Kubernetes 组件
（kubelet、kube-proxy）所返回的退出代码是没有变化的。</p>
<!--
##### V1.Container

* V1.Container.ResourceRequirements.limits.cpu and
  V1.Container.ResourceRequirements.limits.memory - Windows doesn't use hard
  limits for CPU allocations. Instead, a share system is used. The existing
  fields based on millicores are scaled into relative shares that are followed
  by the Windows scheduler.
  See [kuberuntime/helpers_windows.go](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/helpers_windows.go),
  and [resource controls in Microsoft docs](https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/resource-controls)

  * Huge pages are not implemented in the Windows container runtime, and are
    not available. They require
    [asserting a user privilege](https://docs.microsoft.com/en-us/windows/desktop/Memory/large-page-support)
    that's not configurable for containers.
-->
<ul>
<li>
<p><code>v1.Container.ResourceRequirements.limits.cpu</code> 和
<code>v1.Container.ResourceRequirements.limits.memory</code> - Windows
不对 CPU 分配设置硬性的限制。与之相反，Windows 使用一个份额（share）系统。
基于毫核（millicores）的现有字段值会被缩放为相对的份额值，供 Windows 调度器使用。
参见 <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/helpers_windows.go">kuberuntime/helpers_windows.go</a> 和
<a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/resource-controls">Microsoft 文档中关于资源控制的部分</a>。</p>
<ul>
<li>Windows 容器运行时中没有实现巨页支持，因此相关特性不可用。
巨页支持需要<a href="https://docs.microsoft.com/en-us/windows/desktop/Memory/large-page-support">判定用户的特权</a>
而这一特性无法在容器级别配置。</li>
</ul>
</li>
</ul>
<!--
* V1.Container.ResourceRequirements.requests.cpu and
  V1.Container.ResourceRequirements.requests.memory - Requests are subtracted
  from node available resources, so they can be used to avoid overprovisioning a
  node. However, they cannot be used to guarantee resources in an
  overprovisioned node. They should be applied to all containers as a best
  practice if the operator wants to avoid overprovisioning entirely.

* V1.Container.SecurityContext.allowPrivilegeEscalation - not possible on
  Windows, none of the capabilities are hooked up
-->
<ul>
<li>
<p><code>v1.Container.ResourceRequirements.requests.cpu</code> 和
<code>v1.Container.ResourceRequirements.requests.memory</code> - 请求
值会从节点可分配资源中扣除，从而可用来避免节点上的资源过量分配。
但是，它们无法用来在一个已经过量分配的节点上提供资源保障。
如果操作员希望彻底避免过量分配，作为最佳实践，他们就需要为所有容器设置资源请求值。</p>
</li>
<li>
<p><code>v1.Container.SecurityContext.allowPrivilegeEscalation</code> - 在 Windows
上无法实现，对应的权能无一可在 Windows 上生效。</p>
</li>
</ul>
<!--
* V1.Container.SecurityContext.Capabilities - POSIX capabilities are not
  implemented on Windows

* V1.Container.SecurityContext.privileged - Windows doesn't support privileged
  containers

* V1.Container.SecurityContext.procMount - Windows doesn't have a /proc
  filesystem

* V1.Container.SecurityContext.readOnlyRootFilesystem - not possible on
  Windows, write access is required for registry & system processes to run
  inside the container
-->
<ul>
<li><code>v1.Container.SecurityContext.Capabilities</code> - Windows 上未实现 POSIX 权能机制</li>
<li><code>v1.Container.SecurityContext.privileged</code> - Windows 不支持特权容器</li>
<li><code>v1.Container.SecurityContext.procMount</code> - Windows 不包含 <code>/proc</code> 文件系统</li>
<li><code>v1.Container.SecurityContext.readOnlyRootFilesystem</code> - 在 Windows 上无法实现，
要在容器内使用注册表或运行系统进程就必需写访问权限。</li>
</ul>
<!--
* V1.Container.SecurityContext.runAsGroup - not possible on Windows, no GID
  support

* V1.Container.SecurityContext.runAsNonRoot - Windows does not have a root
  user. The closest equivalent is ContainerAdministrator which is an identity
  that doesn't exist on the node.

* V1.Container.SecurityContext.runAsUser - not possible on Windows, no UID
  support as int.

* V1.Container.SecurityContext.seLinuxOptions - not possible on Windows, no SELinux

* V1.Container.terminationMessagePath - this has some limitations in that
  Windows doesn't support mapping single files. The default value is
  `/dev/termination-log`, which does work because it does not exist on Windows by
  default.
-->
<ul>
<li>
<p><code>v1.Container.SecurityContext.runAsGroup</code> - 在 Windows 上无法实现，没有 GID 支持</p>
</li>
<li>
<p><code>v1.Container.SecurityContext.runAsNonRoot</code> - Windows 上没有 root 用户。
与之最接近的等价用户是 <code>ContainerAdministrator</code>，而该身份标识在节点上并不存在。</p>
</li>
<li>
<p><code>v1.Container.SecurityContext.runAsUser</code> - 在 Windows 上无法实现，
因为没有作为整数支持的 GID。</p>
</li>
<li>
<p><code>v1.Container.SecurityContext.seLinuxOptions</code> - 在 Windows 上无法实现，
因为没有 SELinux</p>
</li>
<li>
<p><code>V1.Container.terminationMessagePath</code> - 因为 Windows 不支持单个文件的映射，这一功能
在 Windows 上也受限。默认值 <code>/dev/termination-log</code> 在 Windows 上也无法使用因为
对应路径在 Windows 上不存在。</p>
</li>
</ul>
<!--
##### V1.Pod

* V1.Pod.hostIPC, v1.pod.hostpid - host namespace sharing is not possible on Windows

* V1.Pod.hostNetwork - There is no Windows OS support to share the host network

* V1.Pod.dnsPolicy - ClusterFirstWithHostNet - is not supported because Host
  Networking is not supported on Windows.

* V1.Pod.podSecurityContext - see V1.PodSecurityContext below

* V1.Pod.shareProcessNamespace - this is a beta feature, and depends on Linux
  namespaces which are not implemented on Windows. Windows cannot share
  process namespaces or the container's root filesystem. Only the network can be
  shared.
-->
<h5 id="v1-pod">V1.Pod</h5>
<ul>
<li><code>v1.Pod.hostIPC</code>、<code>v1.Pod.hostPID</code> - Windows 不支持共享宿主系统的名字空间</li>
<li><code>v1.Pod.hostNetwork</code> - Windows 操作系统不支持共享宿主网络</li>
<li><code>v1.Pod.dnsPolicy</code> - 不支持 <code>ClusterFirstWithHostNet</code>，因为 Windows 不支持宿主网络</li>
<li><code>v1.Pod.podSecurityContext</code> - 参见下面的 <code>v1.PodSecurityContext</code></li>
<li><code>v1.Pod.shareProcessNamespace</code> - 此为 Beta 特性且依赖于 Windows 上未实现
的 Linux 名字空间。
Windows 无法共享进程名字空间或者容器的根文件系统。只能共享网络。</li>
</ul>
<!--
* V1.Pod.terminationGracePeriodSeconds - this is not fully implemented in
  Docker on Windows, see:
  [reference](https://github.com/moby/moby/issues/25982). The behavior today is
  that the `ENTRYPOINT` process is sent `CTRL_SHUTDOWN_EVENT`, then Windows waits 5
  seconds by default, and finally shuts down all processes using the normal
  Windows shutdown behavior. The 5 second default is actually in the Windows
  registry [inside the container](https://github.com/moby/moby/issues/25982#issuecomment-426441183),
  so it can be overridden when the container is built.

* V1.Pod.volumeDevices - this is a beta feature, and is not implemented on
  Windows. Windows cannot attach raw block devices to pods.
-->
<ul>
<li>
<p><code>v1.Pod.terminationGracePeriodSeconds</code> - 这一特性未在 Windows 版本的 Docker 中完全实现。
参见<a href="https://github.com/moby/moby/issues/25982">问题报告</a>。
目前实现的行为是向 <code>ENTRYPOINT</code> 进程发送 <code>CTRL_SHUTDOWN_EVENT</code> 事件，之后 Windows 默认
等待 5 秒钟，并最终使用正常的 Windows 关机行为关闭所有进程。
这里的 5 秒钟默认值实际上保存在
<a href="https://github.com/moby/moby/issues/25982#issuecomment-426441183">容器内</a>
的 Windows 注册表中，因此可以在构造容器时重载。</p>
</li>
<li>
<p><code>v1.Pod.volumeDevices</code> - 此为 Beta 特性且未在 Windows 上实现。Windows 无法挂接
原生的块设备到 Pod 中。</p>
</li>
</ul>
<!--
* V1.Pod.volumes - EmptyDir, Secret, ConfigMap, HostPath - all work and have
  tests in TestGrid

  * V1.emptyDirVolumeSource - the Node default medium is disk on Windows.
    Memory is not supported, as Windows does not have a built-in RAM disk.

* V1.VolumeMount.mountPropagation - mount propagation is not supported on Windows.
-->
<ul>
<li>
<p><code>v1.Pod.volumes</code> - <code>emptyDir</code>、<code>secret</code>、<code>configMap</code> 和 <code>hostPath</code>
都可正常工作且在 TestGrid 中测试。</p>
<ul>
<li><code>v1.emptyDir.volumeSource</code> - Windows 上节点的默认介质是磁盘。
不支持将内存作为介质，因为 Windows 不支持内置的 RAM 磁盘。</li>
</ul>
</li>
<li>
<p><code>v1.VolumeMount.mountPropagation</code> - Windows 上不支持挂载传播。</p>
</li>
</ul>
<!--
##### V1.PodSecurityContext

None of the PodSecurityContext fields work on Windows. They're listed here for
reference.

* V1.PodSecurityContext.SELinuxOptions - SELinux is not available on Windows

* V1.PodSecurityContext.RunAsUser - provides a UID, not available on Windows

* V1.PodSecurityContext.RunAsGroup - provides a GID, not available on Windows

* V1.PodSecurityContext.RunAsNonRoot - Windows does not have a root user. The
  closest equivalent is ContainerAdministrator which is an identity that
  doesn't exist on the node.

* V1.PodSecurityContext.SupplementalGroups - provides GID, not available on Windows

* V1.PodSecurityContext.Sysctls - these are part of the Linux sysctl
  interface. There's no equivalent on Windows.
-->
<h5 id="v1-podsecuritycontext">V1.PodSecurityContext</h5>
<p>PodSecurityContext 的所有选项在 Windows 上都无法工作。这些选项列在下面仅供参考。</p>
<ul>
<li>
<p><code>v1.PodSecurityContext.seLinuxOptions</code> - Windows 上无 SELinux</p>
</li>
<li>
<p><code>v1.PodSecurityContext.runAsUser</code> - 提供 UID；Windows 不支持</p>
</li>
<li>
<p><code>v1.PodSecurityContext.runAsGroup</code> - 提供 GID；Windows 不支持</p>
</li>
<li>
<p><code>v1.PodSecurityContext.runAsNonRoot</code> - Windows 上没有 root 用户
最接近的等价账号是 <code>ContainerAdministrator</code>，而该身份标识在节点上不存在</p>
</li>
<li>
<p><code>v1.PodSecurityContext.supplementalGroups</code> - 提供 GID；Windows 不支持</p>
</li>
<li>
<p><code>v1.PodSecurityContext.sysctls</code> - 这些是 Linux sysctl 接口的一部分；Windows 上
没有等价机制。</p>
</li>
</ul>
<!--
#### Operating System Version Restrictions

Windows has strict compatibility rules, where the host OS version must match
the container base image OS version. Only Windows containers with a container
operating system of Windows Server 2019 are supported. Hyper-V isolation of
containers, enabling some backward compatibility of Windows container image
versions, is planned for a future release.
-->
<h4 id="operating-system-version-restrictions">操作系统版本限制 </h4>
<p>Windows 有着严格的兼容性规则，宿主 OS 的版本必须与容器基准镜像 OS 的版本匹配。
目前仅支持容器操作系统为 Windows Server 2019 的 Windows 容器。
对于容器的 Hyper-V 隔离、允许一定程度上的 Windows 容器镜像版本向后兼容性等等，
都是将来版本计划的一部分。</p>
<!--
## Getting Help and Troubleshooting {#troubleshooting}

Your main source of help for troubleshooting your Kubernetes cluster should
start with this
[section](/docs/tasks/debug-application-cluster/troubleshooting/). Some
additional, Windows-specific troubleshooting help is included in this section.
Logs are an important element of troubleshooting issues in Kubernetes. Make
sure to include them any time you seek troubleshooting assistance from other
contributors. Follow the instructions in the SIG-Windows
[contributing guide on gathering logs](https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs).
-->
<h2 id="troubleshooting">获取帮助和故障排查    </h2>
<p>对你的 Kubernetes 集群进行排查的主要帮助信息来源应该是
<a href="/docs/tasks/debug-application-cluster/troubleshooting/">这份文档</a>。
该文档中包含了一些额外的、特定于 Windows 系统的故障排查帮助信息。
Kubernetes 中日志是故障排查的一个重要元素。确保你在尝试从其他贡献者那里获得
故障排查帮助时提供日志信息。你可以按照 SIG-Windows
<a href="https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs">贡献指南和收集日志</a>
所给的指令来操作。</p>
<!--
* How do I know start.ps1 completed successfully?

  You should see kubelet, kube-proxy, and (if you chose Flannel as your
  networking solution) flanneld host-agent processes running on your node, with
  running logs being displayed in separate PowerShell windows. In addition to
  this, your Windows node should be listed as "Ready" in your Kubernetes
  cluster.
-->
<ul>
<li>
<p>我怎样知道 <code>start.ps1</code> 是否已成功完成？</p>
<p>你应该能看到节点上运行的 kubelet、kube-proxy 和（如果你选择 Flannel
作为联网方案）flanneld 宿主代理进程，它们的运行日志显示在不同的
PowerShell 窗口中。此外，你的 Windows 节点应该在你的 Kubernetes 集群
列举为 &quot;Ready&quot; 节点。</p>
</li>
</ul>
<!--
* Can I configure the Kubernetes node processes to run in the background as services?

  Kubelet and kube-proxy are already configured to run as native Windows
  Services, offering resiliency by re-starting the services automatically in the
  event of failure (for example a process crash). You have two options for
  configuring these node components as services.
-->
<ul>
<li>
<p>我可以将 Kubernetes 节点进程配置为服务运行在后台么？</p>
<p>kubelet 和 kube-proxy 都已经被配置为以本地 Windows 服务运行，
并且在出现失效事件（例如进程意外结束）时通过自动重启服务来提供一定的弹性。
你有两种办法将这些节点组件配置为服务。</p>
<!--
* As native Windows Services

  Kubelet & kube-proxy can be run as native Windows Services using `sc.exe`.

  ```powershell
  # Create the services for kubelet and kube-proxy in two separate commands
  sc.exe create <component_name> binPath= "<path_to_binary> -service <other_args>"

  # Please note that if the arguments contain spaces, they must be escaped.
  sc.exe create kubelet binPath= "C:\kubelet.exe --service --hostname-override 'minion' <other_args>"

  # Start the services
  Start-Service kubelet
  Start-Service kube-proxy

  # Stop the service
  Stop-Service kubelet (-Force)
  Stop-Service kube-proxy (-Force)

  # Query the service status
  Get-Service kubelet
  Get-Service kube-proxy
  ```
-->
<ul>
<li>
<p>以本地 Windows 服务的形式</p>
<p>Kubelet 和 kube-proxy 可以用 <code>sc.exe</code> 以本地 Windows 服务的形式运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#080;font-style:italic"># 用两个单独的命令为 kubelet 和 kube-proxy 创建服务</span>
sc.exe create &lt;组件名称&gt; binPath=<span style="color:#b44">&#34;&lt;可执行文件路径&gt; -service &lt;其它参数&gt;&#34;</span>

<span style="color:#080;font-style:italic"># 请注意如果参数中包含空格，必须使用转义</span>
sc.exe create kubelet binPath= <span style="color:#b44">&#34;C:\kubelet.exe --service --hostname-override &#39;minion&#39; &lt;其它参数&gt;&#34;</span>

<span style="color:#080;font-style:italic"># 启动服务</span>
<span style="color:#a2f">Start-Service</span> kubelet
<span style="color:#a2f">Start-Service</span> <span style="color:#a2f">kube-proxy</span>

<span style="color:#080;font-style:italic"># 停止服务</span>
<span style="color:#a2f">Stop-Service</span> kubelet (-Force)
<span style="color:#a2f">Stop-Service</span> <span style="color:#a2f">kube-proxy</span> (-Force)

<span style="color:#080;font-style:italic"># 查询服务状态</span>
<span style="color:#a2f">Get-Service</span> kubelet
<span style="color:#a2f">Get-Service</span> <span style="color:#a2f">kube-proxy</span>
</code></pre></div></li>
</ul>
<!--
* Using nssm.exe

  You can also always use alternative service managers like
  [nssm.exe](https://nssm.cc/) to run these processes (flanneld, kubelet &
  kube-proxy) in the background for you. You can use this [sample
  script](https://github.com/Microsoft/SDN/tree/master/Kubernetes/flannel/register-svc.ps1),
  leveraging `nssm.exe` to register kubelet, kube-proxy, and `flanneld.exe` to run
  as Windows services in the background.
-->
<ul>
<li>
<p>使用 nssm.exe</p>
<p>你也总是可以使用替代的服务管理器，例如<a href="https://nssm.cc/">nssm.exe</a>，来为你在后台运行
这些进程（<code>flanneld</code>、<code>kubelet</code> 和 <code>kube-proxy</code>）。你可以使用这一
<a href="https://github.com/Microsoft/SDN/tree/master/Kubernetes/flannel/register-svc.ps1">示例脚本</a>，
利用 <code>nssm.exe</code> 将 <code>kubelet</code>、<code>kube-proxy</code> 和 <code>flanneld.exe</code> 注册为要在后台运行的
Windows 服务。</p>
<!--
```powershell
register-svc.ps1 -NetworkMode <Network mode> -ManagementIP <Windows Node IP> -ClusterCIDR <Cluster subnet> -KubeDnsServiceIP <Kube-dns Service IP> -LogDir <Directory to place logs>
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">register-svc</span>.ps1 -NetworkMode &lt;网络模式&gt; -ManagementIP &lt;Windows 节点 IP&gt; -ClusterCIDR &lt;集群子网&gt; -KubeDnsServiceIP &lt;<span style="color:#a2f">kube-dns</span> 服务 IP&gt; -LogDir &lt;日志目录&gt;
</code></pre></div><!--
The parameters are explained below:
-->
<p>这里的参数解释如下：</p>
<!--
- `NetworkMode`: The network mode l2bridge (flannel host-gw, also the
  default value) or overlay (flannel vxlan) chosen as a network solution
- `ManagementIP`: The IP address assigned to the Windows node. You can use
  `ipconfig` to find this.
- `ClusterCIDR`: The cluster subnet range. (Default: 10.244.0.0/16)
- `KubeDnsServiceIP`: The Kubernetes DNS service IP. (Default: 10.96.0.10)
- `LogDir`: The directory where kubelet and kube-proxy logs are redirected
  into their respective output files. (Default value C:\k)
-->
<ul>
<li><code>NetworkMode</code>：网络模式 l2bridge（flannel host-gw，也是默认值）或
overlay（flannel vxlan）选做网络方案</li>
<li><code>ManagementIP</code>：分配给 Windows 节点的 IP 地址。你可以使用 ipconfig 得到此值</li>
<li><code>ClusterCIDR</code>：集群子网范围（默认值为 10.244.0.0/16）</li>
<li><code>KubeDnsServiceIP</code>：Kubernetes DNS 服务 IP（默认值为 10.96.0.10）</li>
<li><code>LogDir</code>：kubelet 和 kube-proxy 的日志会被重定向到这一目录中的对应输出文件，
默认值为 <code>C:\k</code>。</li>
</ul>
<!--
If the above referenced script is not suitable, you can manually configure
`nssm.exe` using the following examples.

Register flanneld.exe:
-->
<p>若以上所引用的脚本不适合，你可以使用下面的例子手动配置 <code>nssm.exe</code>。</p>
<p>注册 flanneld.exe：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">nssm install flanneld C:\flannel\flanneld.exe
nssm <span style="color:#a2f">set </span>flanneld AppParameters --kubeconfig<span style="color:#666">-file</span>=c:\k\config --iface=&lt;ManagementIP&gt; --ip-masq=1 --kube-subnet-mgr=1
nssm <span style="color:#a2f">set </span>flanneld AppEnvironmentExtra NODE_NAME=&lt;hostname&gt;
nssm <span style="color:#a2f">set </span>flanneld AppDirectory C:\flannel
nssm <span style="color:#a2f">start </span>flanneld
</code></pre></div><!--
Register kubelet.exe:
-->
<p>注册 kubelet.exe：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">nssm install kubelet C:\k\kubelet.exe
nssm <span style="color:#a2f">set </span>kubelet AppParameters --hostname-override=&lt;hostname&gt; --v=6 --pod-infra-container-image=k8s.gcr.io/pause<span style="">:</span>3.5 --resolv-conf=<span style="color:#b44">&#34;&#34;</span> --allow-privileged=true --enable-debugging-handlers --cluster-dns=&lt;<span style="color:#a2f">DNS-service</span>-IP&gt; --cluster-domain=cluster.local --kubeconfig=c:\k\config --hairpin-mode=<span style="color:#a2f">promiscuous-bridge</span> --image-pull-progress-deadline=20m --cgroups-per-qos=false  --log-dir=&lt;log directory&gt; --logtostderr=false --enforce-node-allocatable=<span style="color:#b44">&#34;&#34;</span> --network-plugin=cni --cni-bin-dir=c:\k\cni --cni-conf-dir=c:\k\cni\config
nssm <span style="color:#a2f">set </span>kubelet AppDirectory C:\k
nssm <span style="color:#a2f">start </span>kubelet
</code></pre></div><!--
Register kube-proxy.exe (l2bridge / host-gw):
-->
<p>注册 kube-proxy.exe（二层网桥模式和主机网关模式）</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">nssm install <span style="color:#a2f">kube-proxy</span> C:\k\<span style="color:#a2f">kube-proxy</span>.exe
nssm <span style="color:#a2f">set kube-proxy</span> AppDirectory c:\k
nssm <span style="color:#a2f">set kube-proxy</span> AppParameters --v=4 --proxy-mode=kernelspace --hostname-override=&lt;hostname&gt;--kubeconfig=c:\k\config --enable-dsr=false --log-dir=&lt;log directory&gt; --logtostderr=false
nssm.exe <span style="color:#a2f">set kube-proxy</span> AppEnvironmentExtra KUBE_NETWORK=cbr0
nssm <span style="color:#a2f">set kube-proxy</span> DependOnService kubelet
nssm <span style="color:#a2f">start kube-proxy</span>
</code></pre></div><!--
Register kube-proxy.exe (overlay / vxlan):
-->
<p>注册 kube-proxy.exe（覆盖网络模式或 VxLAN 模式）</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">nssm install <span style="color:#a2f">kube-proxy</span> C:\k\<span style="color:#a2f">kube-proxy</span>.exe
nssm <span style="color:#a2f">set kube-proxy</span> AppDirectory c:\k
nssm <span style="color:#a2f">set kube-proxy</span> AppParameters --v=4 --proxy-mode=kernelspace --feature-gates=<span style="color:#b44">&#34;WinOverlay=true&#34;</span> --hostname-override=&lt;hostname&gt; --kubeconfig=c:\k\config --network-name=vxlan0 --source-vip=&lt;<span style="color:#a2f">source-vip</span>&gt; --enable-dsr=false --log-dir=&lt;log directory&gt; --logtostderr=false
nssm <span style="color:#a2f">set kube-proxy</span> DependOnService kubelet
nssm <span style="color:#a2f">start kube-proxy</span>
</code></pre></div><!--
For initial troubleshooting, you can use the following flags in
[nssm.exe](https://nssm.cc/) to redirect stdout and stderr to a output file:

```powershell
nssm set <Service Name> AppStdout C:\k\mysvc.log
nssm set <Service Name> AppStderr C:\k\mysvc.log
```

For additional details, see official [nssm usage](https://nssm.cc/usage) docs.
-->
<p>作为初始的故障排查操作，你可以使用在 <a href="https://nssm.cc/">nssm.exe</a> 中使用下面的标志
以便将标准输出和标准错误输出重定向到一个输出文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">nssm <span style="color:#a2f">set </span>&lt;服务名称&gt; AppStdout C:\k\mysvc.log
nssm <span style="color:#a2f">set </span>&lt;服务名称&gt; AppStderr C:\k\mysvc.log
</code></pre></div><p>要了解更多的细节，可参见官方的 <a href="https://nssm.cc/usage">nssm 用法</a>文档。</p>
</li>
</ul>
</li>
</ul>
<!--
* My Windows Pods do not have network connectivity

  If you are using virtual machines, ensure that MAC spoofing is enabled on
  all the VM network adapter(s).
-->
<ul>
<li>
<p>我的 Windows Pods 无发连接网络</p>
<p>如果你在使用虚拟机，请确保 VM 网络适配器均已开启 MAC 侦听（Spoofing）。</p>
</li>
</ul>
<!--
* My Windows Pods cannot ping external resources

  Windows Pods do not have outbound rules programmed for the ICMP protocol
  today. However, TCP/UDP is supported. When trying to demonstrate connectivity
  to resources outside of the cluster, please substitute `ping <IP>` with
  corresponding `curl <IP>` commands.
-->
<ul>
<li>
<p>我的 Windows Pods 无法 ping 外部资源</p>
<p>Windows Pods 目前没有为 ICMP 协议提供出站规则。不过 TCP/UDP 是支持的。
尝试与集群外资源连接时，可以将 <code>ping &lt;IP&gt;</code> 命令替换为对应的 <code>curl &lt;IP&gt;</code> 命令。</p>
<!--
If you are still facing problems, most likely your network configuration in
[cni.conf](https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf)
deserves some extra attention. You can always edit this static file. The
configuration update will apply to any newly created Kubernetes resources.
-->
<p>如果你还遇到问题，很可能你在
<a href="https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf">cni.conf</a>
中的网络配置值得额外的注意。你总是可以编辑这一静态文件。
配置的更新会应用到所有新创建的 Kubernetes 资源上。</p>
<!--
One of the Kubernetes networking requirements (see
[Kubernetes network model](/docs/concepts/cluster-administration/networking/))
is for cluster communication to occur without NAT internally. To honor this
requirement, there is an
[ExceptionList](https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf#L20)
for all the communication where we do not want outbound NAT to occur. However,
this also means that you need to exclude the external IP you are trying to
query from the ExceptionList. Only then will the traffic originating from your
Windows pods be SNAT'ed correctly to receive a response from the outside
world. In this regard, your ExceptionList in `cni.conf` should look as
follows:
-->
<p>Kubernetes 网络的需求之一（参见
<a href="/zh/docs/concepts/cluster-administration/networking/">Kubernetes 网络模型</a>）
是集群内部无需网络地址转译（NAT）即可实现通信。
为了符合这一要求，对所有我们不希望出站时发生 NAT 的通信都存在一个
<a href="https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf#L20">ExceptionList</a>。
然而这也意味着你需要将你要查询的外部 IP 从 ExceptionList 中移除。
只有这时，从你的 Windows Pod 发起的网络请求才会被正确地通过 SNAT 转换以接收到
来自外部世界的响应。
就此而言，你在 <code>cni.conf</code> 中的 <code>ExceptionList</code> 应该看起来像这样：</p>
<!--
```conf
"ExceptionList": [
     "10.244.0.0/16",  # Cluster subnet
     "10.96.0.0/12",   # Service subnet
     "10.127.130.0/24" # Management (host) subnet
]
```
-->
<pre><code class="language-conf" data-lang="conf">&quot;ExceptionList&quot;: [
    &quot;10.244.0.0/16&quot;,  # 集群子网
    &quot;10.96.0.0/12&quot;,   # 服务子网
    &quot;10.127.130.0/24&quot; # 管理（主机）子网
]
</code></pre></li>
</ul>
<!--
* My Windows node cannot access NodePort service

  Local NodePort access from the node itself fails. This is a known
  limitation. NodePort access works from other nodes or external clients.
-->
<ul>
<li>
<p>我的 Windows 节点无法访问 NodePort 服务</p>
<p>从节点自身发起的本地 NodePort 请求会失败。这是一个已知的局限。
NodePort 服务的访问从其他节点或者外部客户端都可正常进行。</p>
</li>
</ul>
<!--
* vNICs and HNS endpoints of containers are being deleted

  This issue can be caused when the `hostname-override` parameter is not
  passed to
  [kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/).
  To resolve it, users need to pass the hostname to kube-proxy as follows:
-->
<ul>
<li>
<p>容器的 vNICs 和 HNS 端点被删除了</p>
<p>这一问题可能因为 <code>hostname-override</code> 参数未能传递给
<a href="/zh/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a>
而导致。解决这一问题时，用户需要按如下方式将主机名传递给 kube-proxy：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">C:\k\<span style="color:#a2f">kube-proxy</span>.exe --hostname-override=$(hostname)
</code></pre></div></li>
</ul>
<!--
* With flannel my nodes are having issues after rejoining a cluster

  Whenever a previously deleted node is being re-joined to the cluster,
  flannelD tries to assign a new pod subnet to the node. Users should remove the
  old pod subnet configuration files in the following paths:
-->
<ul>
<li>
<p>使用 Flannel 时，我的节点在重新加入集群时遇到问题</p>
<p>无论何时，当一个之前被删除的节点被重新添加到集群时，flannelD 都会将为节点分配
一个新的 Pod 子网。
用户需要将将下面路径中的老的 Pod 子网配置文件删除：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">Remove-Item</span> C:\k\SourceVip.json
<span style="color:#a2f">Remove-Item</span> C:\k\SourceVipRequest.json
</code></pre></div></li>
</ul>
<!--
* After launching `start.ps1`, flanneld is stuck in "Waiting for the Network
  to be created"

  There are numerous reports of this
  [issue](https://github.com/coreos/flannel/issues/1066); most likely it is a
  timing issue for when the management IP of the flannel network is set. A
  workaround is to simply relaunch start.ps1 or relaunch it manually as follows:

  ```powershell
  PS C:> [Environment]::SetEnvironmentVariable("NODE_NAME", "<Windows_Worker_Hostname>")
  PS C:> C:\flannel\flanneld.exe --kubeconfig-file=c:\k\config --iface=<Windows_Worker_Node_IP> --ip-masq=1 --kube-subnet-mgr=1
  ```
-->
<ul>
<li>
<p>在启动了 <code>start.ps1</code> 之后，flanneld 一直停滞在 &quot;Waiting for the Network
to be created&quot; 状态</p>
<p>关于这一<a href="https://github.com/coreos/flannel/issues/1066">问题</a>有很多的报告；
最可能的一种原因是关于何时设置 Flannel 网络的管理 IP 的时间问题。
一种解决办法是重新启动 <code>start.ps1</code> 或者按如下方式手动重启之：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#800">[Environment]</span>::SetEnvironmentVariable(<span style="color:#b44">&#34;NODE_NAME&#34;</span>, <span style="color:#b44">&#34;&lt;Windows 工作节点主机名&gt;&#34;</span>)
C:\flannel\flanneld.exe --kubeconfig<span style="color:#666">-file</span>=c:\k\config --iface=&lt;Windows 工作节点 IP&gt; --ip-masq=1 --kube-subnet-mgr=1
</code></pre></div></li>
</ul>
<!--
* My Windows Pods cannot launch because of missing `/run/flannel/subnet.env`

  This indicates that Flannel didn't launch correctly. You can either try to
  restart flanneld.exe or you can copy the files over manually from
  `/run/flannel/subnet.env` on the Kubernetes master to
  `C:\run\flannel\subnet.env` on the Windows worker node and modify the
  `FLANNEL_SUBNET` row to a different number. For example, if node subnet
  10.244.4.1/24 is desired:
-->
<ul>
<li>
<p>我的 Windows Pods 无法启动，因为缺少 <code>/run/flannel/subnet.env</code> 文件</p>
<p>这表明 Flannel 网络未能正确启动。你可以尝试重启 flanneld.exe 或者将文件手动地
从 Kubernetes 主控节点的 <code>/run/flannel/subnet.env</code> 路径复制到 Windows 工作
节点的 <code>C:\run\flannel\subnet.env</code> 路径，并将 <code>FLANNEL_SUBNET</code> 行改为一个
不同的数值。例如，如果期望节点子网为 <code>10.244.4.1/24</code>：</p>
<pre><code class="language-none" data-lang="none">FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.4.1/24
FLANNEL_MTU=1500
FLANNEL_IPMASQ=true
</code></pre></li>
</ul>
<!--
* My Windows node cannot access my services using the service IP

  This is a known limitation of the current networking stack on Windows.
  Windows Pods are able to access the service IP however.
-->
<ul>
<li>
<p>我的 Windows 节点无法使用服务 IP 访问我的服务</p>
<p>这是 Windows 上当前网络协议栈的一个已知的限制。
Windows Pods 能够访问服务 IP。</p>
</li>
</ul>
<!--
* No network adapter is found when starting kubelet

  The Windows networking stack needs a virtual adapter for Kubernetes
  networking to work. If the following commands return no results (in an admin
  shell), virtual network creation — a necessary prerequisite for Kubelet to
  work — has failed:
-->
<ul>
<li>
<p>启动 kubelet 时找不到网络适配器</p>
<p>Windows 网络堆栈需要一个虚拟的适配器，这样 Kubernetes 网络才能工作。
如果下面的命令（在管理员 Shell 中）没有任何返回结果，证明虚拟网络创建
（kubelet 正常工作的必要前提之一）失败了：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">Get-HnsNetwork</span> | ? Name <span style="color:#666">-ieq</span> <span style="color:#b44">&#34;cbr0&#34;</span>
<span style="color:#a2f">Get-NetAdapter</span> | ? Name <span style="color:#666">-Like</span> <span style="color:#b44">&#34;vEthernet (Ethernet*&#34;</span>
</code></pre></div><!--
Often it is worthwhile to modify the
[InterfaceName](https://github.com/microsoft/SDN/blob/master/Kubernetes/flannel/start.ps1#L7)
parameter of the start.ps1 script, in cases where the host's network adapter
isn't "Ethernet". Otherwise, consult the output of the `start-kubelet.ps1`
script to see if there are errors during virtual network creation.
-->
<p>当宿主系统的网络适配器名称不是 &quot;Ethernet&quot; 时，通常值得更改 <code>start.ps1</code> 脚本中的
<a href="https://github.com/microsoft/SDN/blob/master/Kubernetes/flannel/start.ps1#L7">InterfaceName</a>
参数来重试。否则可以查验 <code>start-kubelet.ps1</code> 的输出，看看是否在虚拟网络创建
过程中报告了其他错误。</p>
</li>
</ul>
<!--
* My Pods are stuck at "Container Creating" or restarting over and over

  Check that your pause image is compatible with your OS version. The
  [instructions](https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/deploying-resources)
  assume that both the OS and the containers are version 1803. If you have a
  later version of Windows, such as an Insider build, you need to adjust the
  images accordingly. Please refer to the Microsoft's [Docker
  repository](https://hub.docker.com/u/microsoft/) for images. Regardless, both
  the pause image Dockerfile and the sample service expect the image to be
  tagged as :latest.
-->
<ul>
<li>
<p>我的 Pods 停滞在 &quot;Container Creating&quot; 状态或者反复重启</p>
<p>检查你的 pause 镜像是与你的 OS 版本兼容的。
<a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/deploying-resources">这里的指令</a>
假定你的 OS 和容器版本都是 1803。如果你安装的是更新版本的 Windows，比如说
某个 Insider 构造版本，你需要相应地调整要使用的镜像。
请参照 Microsoft 的 <a href="https://hub.docker.com/u/microsoft/">Docker 仓库</a>
了解镜像。不管怎样，pause 镜像的 Dockerfile 和示例服务都期望镜像的标签
为 <code>:latest</code>。</p>
</li>
</ul>
<!--
* DNS resolution is not properly working

  Check the DNS limitations for Windows in this [section](#dns-limitations).
-->
<ul>
<li>
<p>DNS 解析无法正常工作</p>
<p>参阅 Windows 上 <a href="#dns-limitations">DNS 相关的局限</a> 节。</p>
</li>
</ul>
<!--
* `kubectl port-forward` fails with "unable to do port forwarding: wincat not found"

  Port forwarding support for Windows requires wincat.exe to be available in the
  [pause infrastructure container](#pause-image).
  Ensure you are using a supported image that is compatable with your Windows OS version.
  If you would like to build your own pause infrastructure container be sure to include
  [wincat](https://github.com/kubernetes/kubernetes/tree/master/build/pause/windows/wincat).
-->
<ul>
<li>
<p><code>kubectl port-forward</code> 失败，错误信息为 &quot;unable to do port forwarding: wincat not found&quot;</p>
<p>此功能是在 Kubernetes v1.15 中实现的，pause 基础设施容器
<code>mcr.microsoft.com/oss/kubernetes/pause:3.4.1</code> 中包含了 wincat.exe。
请确保你使用的是这些版本或者更新版本。
如果你想要自行构造你自己的 pause 基础设施容器，要确保其中包含了
<a href="https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/cmd/wincat">wincat</a></p>
<p>Windows 的端口转发支持需要在 <a href="#pause-image">pause 基础设施容器</a> 中提供 wincat.exe。
确保你使用的是与你的 Windows 操作系统版本兼容的受支持镜像。
如果你想构建自己的 pause 基础架构容器，请确保包含 <a href="https://github.com/kubernetes/kubernetes/tree/master/build/pause/windows/wincat">wincat</a>.。</p>
</li>
</ul>
<!--
* My Kubernetes installation is failing because my Windows Server node is
  behind a proxy

  If you are behind a proxy, the following PowerShell environment variables must be defined:
-->
<ul>
<li>
<p>我的 Kubernetes 安装失败，因为我的 Windows Server 节点在防火墙后面</p>
<p>如果你处于防火墙之后，那么必须定义如下 PowerShell 环境变量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-PowerShell" data-lang="PowerShell"><span style="color:#800">[Environment]</span>::SetEnvironmentVariable(<span style="color:#b44">&#34;HTTP_PROXY&#34;</span>, <span style="color:#b44">&#34;http://proxy.example.com:80/&#34;</span>, <span style="color:#800">[EnvironmentVariableTarget]</span>::Machine)
<span style="color:#800">[Environment]</span>::SetEnvironmentVariable(<span style="color:#b44">&#34;HTTPS_PROXY&#34;</span>, <span style="color:#b44">&#34;http://proxy.example.com:443/&#34;</span>, <span style="color:#800">[EnvironmentVariableTarget]</span>::Machine)
</code></pre></div></li>
</ul>
<!--
* What is a `pause` container?

  In a Kubernetes Pod, an infrastructure or "pause" container is first created
  to host the container endpoint. Containers that belong to the same pod,
  including infrastructure and worker containers, share a common network
  namespace and endpoint (same IP and port space). Pause containers are needed
  to accommodate worker containers crashing or restarting without losing any of
  the networking configuration.

  Refer to the [pause image](#pause-image) section to find the recommended version
  of the pause image.
-->
<ul>
<li>
<p><code>pause</code> 容器是什么？</p>
<p>在一个 Kubernetes Pod 中，一个基础设施容器，或称 &quot;pause&quot; 容器，会被首先创建出来，
用以托管容器端点。属于同一 Pod 的容器，包括基础设施容器和工作容器，会共享相同的
网络名字空间和端点（相同的 IP 和端口空间）。我们需要 pause 容器来工作容器崩溃或
重启的状况，以确保不会丢失任何网络配置。</p>
<p>请参阅 <a href="#pause-image">pause 镜像</a> 部分以查找 pause 镜像的推荐版本。</p>
</li>
</ul>
<!--
### Further investigation

If these steps don't resolve your problem, you can get help running Windows
containers on Windows nodes in Kubernetes through:

* StackOverflow [Windows Server Container](https://stackoverflow.com/questions/tagged/windows-server-container) topic

* Kubernetes Official Forum [discuss.kubernetes.io](https://discuss.kubernetes.io/)

* Kubernetes Slack [#SIG-Windows Channel](https://kubernetes.slack.com/messages/sig-windows)
-->
<h3 id="further-investigation">进一步探究   </h3>
<p>如果以上步骤未能解决你遇到的问题，你可以通过以下方式获得在 Kubernetes
中的 Windows 节点上运行 Windows 容器的帮助：</p>
<ul>
<li>StackOverflow <a href="https://stackoverflow.com/questions/tagged/windows-server-container">Windows Server Container</a> 主题</li>
<li>Kubernetes 官方论坛 <a href="https://discuss.kubernetes.io/">discuss.kubernetes.io</a></li>
<li>Kubernetes Slack <a href="https://kubernetes.slack.com/messages/sig-windows">#SIG-Windows 频道</a></li>
</ul>
<!--
## Reporting Issues and Feature Requests

If you have what looks like a bug, or you would like to make a feature
request, please use the
[GitHub issue tracking system](https://github.com/kubernetes/kubernetes/issues).
You can open issues on
[GitHub](https://github.com/kubernetes/kubernetes/issues/new/choose) and
assign them to SIG-Windows. You should first search the list of issues in case
it was reported previously and comment with your experience on the issue and
add additional logs. SIG-Windows Slack is also a great avenue to get some
initial support and troubleshooting ideas prior to creating a ticket.
-->
<h2 id="reporting-issues-and-feature-requests">报告问题和功能需求 </h2>
<p>如果你遇到看起来像是软件缺陷的问题，或者你想要提起某种功能需求，请使用
<a href="https://github.com/kubernetes/kubernetes/issues">GitHub 问题跟踪系统</a>。
你可以在 <a href="https://github.com/kubernetes/kubernetes/issues/new/choose">GitHub</a>
上发起 Issue 并将其指派给 SIG-Windows。你应该首先搜索 Issue 列表，看看是否
该 Issue 以前曾经被报告过，以评论形式将你在该 Issue 上的体验追加进去，并附上
额外的日志信息。SIG-Windows Slack 频道也是一个获得初步支持的好渠道，可以在
生成新的 Ticket 之前对一些想法进行故障分析。</p>
<!--
If filing a bug, please include detailed information about how to reproduce
the problem, such as:

* Kubernetes version: kubectl version
* Environment details: Cloud provider, OS distro, networking choice and
  configuration, and Docker version
* Detailed steps to reproduce the problem
* [Relevant logs](https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs)
* Tag the issue sig/windows by commenting on the issue with `/sig windows` to
  bring it to a SIG-Windows member's attention
-->
<p>在登记软件缺陷时，请给出如何重现该问题的详细信息，例如：</p>
<ul>
<li>Kubernetes 版本：kubectl 版本</li>
<li>环境细节：云平台、OS 版本、网络选型和配置情况以及 Docker 版本</li>
<li>重现该问题的详细步骤</li>
<li><a href="https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs">相关的日志</a></li>
<li>通过为该 Issue 添加 <code>/sig windows</code> 评论为其添加 <code>sig/windows</code> 标签，
进而引起 SIG-Windows 成员的注意。</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
We have a lot of features in our roadmap. An abbreviated high level list is
included below, but we encourage you to view our
[roadmap project](https://github.com/orgs/kubernetes/projects/8) and help us make
Windows support better by
[contributing](https://github.com/kubernetes/community/blob/master/sig-windows/).
-->
<p>在我们的未来蓝图中包含很多功能特性（要实现）。下面是一个浓缩的简要列表，不过我们
鼓励你查看我们的 <a href="https://github.com/orgs/kubernetes/projects/8">roadmap 项目</a>并
通过<a href="https://github.com/kubernetes/community/blob/master/sig-windows/">贡献</a>的方式
帮助我们把 Windows 支持做得更好。</p>
<!--
### Hyper-V isolation

Hyper-V isolation is requried to enable the following use cases for Windows
containers in Kubernetes:

* Hypervisor-based isolation between pods for additional security

* Backwards compatibility allowing a node to run a newer Windows Server
  version without requiring containers to be rebuilt

* Specific CPU/NUMA settings for a pod

* Memory isolation and reservations
-->
<h3 id="hyper-v-isolation">Hyper-V 隔离 </h3>
<p>要满足 Kubernetes 中 Windows 容器的如下用例，需要利用 Hyper-V 隔离：</p>
<ul>
<li>在 Pod 之间实施基于监管程序（Hypervisor）的隔离，以增强安全性</li>
<li>出于向后兼容需要，允许添加运行新 Windows Server 版本的节点时不必
重新创建容器</li>
<li>为 Pod 设置特定的 CPU/NUMA 配置</li>
<li>实施内存隔离与预留</li>
</ul>
<!--
### Deployment with kubeadm and cluster API

Kubeadm is becoming the de facto standard for users to deploy a Kubernetes
cluster. Windows node support in kubeadm is currently a work-in-progress but a
guide is available
[here](/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/). We are
also making investments in cluster API to ensure Windows nodes are properly
provisioned.
-->
<h3 id="deployment-with-kubeadm-and-cluster-api">使用 kubeadm 和 Cluster API 来部署 </h3>
<p>kubeadm 已经成为用户部署 Kubernetes 集群的事实标准。
kubeadm 对 Windows 节点的支持目前还在开发过程中，不过你可以阅读相关的
<a href="/zh/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/">指南</a>。
我们也在投入资源到 Cluster API，以确保 Windows 节点被正确配置。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3a51e66c5de55f9093a8dc55742006d3">2.4.2 - Kubernetes 中 Windows 容器的调度指南</h1>
    
	<!--
reviewers:
- jayunit100
- jsturtevant
- marosset
- perithompson
title: Guide for scheduling Windows containers in Kubernetes
content_type: concept
weight: 75
-->
<!-- overview -->
<!--
Windows applications constitute a large portion of the services and applications that run in many organizations.
This guide walks you through the steps to configure and deploy a Windows container in Kubernetes.
-->
<p>Windows 应用程序构成了许多组织中运行的服务和应用程序的很大一部分。
本指南将引导您完成在 Kubernetes 中配置和部署 Windows 容器的步骤。</p>
<!-- body -->
<!--
## Objectives

* Configure an example deployment to run Windows containers on the Windows node
* (Optional) Configure an Active Directory Identity for your Pod using Group Managed Service Accounts (GMSA)
-->
<h2 id="目标">目标</h2>
<ul>
<li>配置一个示例 deployment 以在 Windows 节点上运行 Windows 容器</li>
<li>（可选）使用组托管服务帐户（GMSA）为您的 Pod 配置 Active Directory 身份</li>
</ul>
<!--
## Before you begin

* Create a Kubernetes cluster that includes a
control plane and a [worker node running Windows Server](/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/)
* It is important to note that creating and deploying services and workloads on Kubernetes
behaves in much the same way for Linux and Windows containers.
[Kubectl commands](/docs/reference/kubectl/) to interface with the cluster are identical.
The example in the section below is provided to jumpstart your experience with Windows containers.
-->
<h2 id="在你开始之前">在你开始之前</h2>
<ul>
<li>创建一个 Kubernetes 集群，其中包括一个控制平面和
<a href="/zh/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/">运行 Windows 服务器的工作节点</a></li>
<li>重要的是要注意，对于 Linux 和 Windows 容器，在 Kubernetes
上创建和部署服务和工作负载的行为几乎相同。
与集群接口的 <a href="/zh/docs/reference/kubectl/overview/">kubectl 命令</a>相同。
提供以下部分中的示例只是为了快速启动  Windows 容器的使用体验。</li>
</ul>
<!--
## Getting Started: Deploying a Windows container

To deploy a Windows container on Kubernetes, you must first create an example application.
The example YAML file below creates a simple webserver application.
Create a service spec named `win-webserver.yaml` with the contents below:
-->
<h2 id="入门-部署-windows-容器">入门：部署 Windows 容器</h2>
<p>要在 Kubernetes 上部署 Windows 容器，您必须首先创建一个示例应用程序。
下面的示例 YAML  文件创建了一个简单的 Web 服务器应用程序。
创建一个名为  <code>win-webserver.yaml</code>  的服务规约，其内容如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># the port that this service should serve on</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>NodePort<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>win-webserver<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>windowswebserver<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- powershell.exe<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- -command<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#b44">&#34;&lt;#code used from https://gist.github.com/19WAS85/5424431#&gt; ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add(&#39;http://*:80/&#39;) ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host(&#39;Listening at http://*:80/&#39;) ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host &#39;&#39; ;Write-Host(&#39;&gt; {0}&#39; -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header=&#39;&lt;html&gt;&lt;body&gt;&lt;H1&gt;Windows Container Web Server&lt;/H1&gt;&#39; ;$$callerCountsString=&#39;&#39; ;$$callerCounts.Keys | % { $$callerCountsString+=&#39;&lt;p&gt;IP {0} callerCount {1} &#39; -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer=&#39;&lt;/body&gt;&lt;/html&gt;&#39; ;$$content=&#39;{0}{1}{2}&#39; -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host(&#39;&lt; {0}&#39; -f $$responseStatus)  } ; &#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kubernetes.io/os</span>:<span style="color:#bbb"> </span>windows<span style="color:#bbb">
</span></code></pre></div><!--
Port mapping is also supported, but for simplicity in this example
the container port 80 is exposed directly to the service.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 端口映射也是支持的，但为简单起见，在此示例中容器端口 80 直接暴露给服务。
</div>
<!--
1. Check that all nodes are healthy:

    ```bash
    kubectl get nodes
    ```

1. Deploy the service and watch for pod updates:

    ```bash
    kubectl apply -f win-webserver.yaml
    kubectl get pods -o wide -w
    ```

    When the service is deployed correctly both Pods are marked as Ready. To exit the watch command, press Ctrl+C.

1. Check that the deployment succeeded. To verify:

    * Two containers per pod on the Windows node, use `docker ps` 
    * Two pods listed from the Linux control plane node, use `kubectl get pods` 
    * Node-to-pod communication across the network, `curl` port 80 of your pod IPs from the Linux control plane node 
      to check for a web server response
    * Pod-to-pod communication, ping between pods (and across hosts, if you have more than one Windows node)
      using docker exec or kubectl exec
    * Service-to-pod communication, `curl` the virtual service IP (seen under `kubectl get services`)
      from the Linux control plane node and from individual pods
    * Service discovery, `curl` the service name with the Kubernetes [default DNS suffix](/docs/concepts/services-networking/dns-pod-service/#services)
    * Inbound connectivity, `curl` the NodePort from the Linux control plane node or machines outside of the cluster
    * Outbound connectivity, `curl` external IPs from inside the pod using kubectl exec
-->
<ol>
<li>
<p>检查所有节点是否健康：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get nodes
</code></pre></div></li>
<li>
<p>部署服务并观察 pod 更新：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f win-webserver.yaml
kubectl get pods -o wide -w
</code></pre></div><p>正确部署服务后，两个 Pod 都标记为“Ready”。要退出 watch 命令，请按 Ctrl + C。</p>
</li>
<li>
<p>检查部署是否成功。验证：</p>
<ul>
<li>Windows 节点上每个 Pod 有两个容器，使用  <code>docker ps</code></li>
<li>Linux 控制平面节点列出两个 Pod，使用  <code>kubectl get pods</code></li>
<li>跨网络的节点到 Pod 通信，从 Linux 控制平面节点 <code>curl</code> 您的 pod IPs 的端口80，以检查 Web 服务器响应</li>
<li>Pod 到 Pod 的通信，使用 docker exec 或 kubectl exec 在 Pod 之间
（以及跨主机，如果你有多个 Windows 节点）进行 ping 操作</li>
<li>服务到 Pod 的通信，从 Linux 控制平面节点和各个 Pod 中 <code>curl</code> 虚拟服务 IP
（在 <code>kubectl get services</code> 下可见）</li>
<li>服务发现，使用 Kubernetes <code>curl</code> 服务名称
<a href="/zh/docs/concepts/services-networking/dns-pod-service/#services">默认 DNS 后缀</a></li>
<li>入站连接，从 Linux 控制平面节点或集群外部的计算机 <code>curl</code> NodePort</li>
<li>出站连接，使用 kubectl exec 从 Pod 内部 curl 外部 IP</li>
</ul>
</li>
</ol>
<!--
Windows container hosts are not able to access the IP of services scheduled on them due to current platform limitations of the Windows networking stack.
Only Windows pods are able to access service IPs.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 由于当前平台对 Windows 网络堆栈的限制，Windows 容器主机无法访问在其上调度的服务的 IP。只有 Windows pods 才能访问服务 IP。
</div>
<!--
## Observability

### Capturing logs from workloads
-->
<h2 id="observability">可观测性 </h2>
<h3 id="抓取来自工作负载的日志">抓取来自工作负载的日志</h3>
<!--
Logs are an important element of observability; they enable users to gain insights
into the operational aspect of workloads and are a key ingredient to troubleshooting issues.
Because Windows containers and workloads inside Windows containers behave differently from Linux containers,
users had a hard time collecting logs, limiting operational visibility.
Windows workloads for example are usually configured to log to ETW (Event Tracing for Windows)
or push entries to the application event log.
[LogMonitor](https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor), an open source tool by Microsoft,
is the recommended way to monitor configured log sources inside a Windows container.
LogMonitor supports monitoring event logs, ETW providers, and custom application logs,
piping them to STDOUT for consumption by `kubectl logs <pod>`.
-->
<p>日志是可观测性的重要一环；使用日志用户可以获得对负载运行状况的洞察，
因而日志是故障排查的一个重要手法。
因为 Windows 容器中的 Windows 容器和负载与 Linux 容器的行为不同，
用户很难收集日志，因此运行状态的可见性很受限。
例如，Windows 工作负载通常被配置为将日志输出到 Windows 事件跟踪
（Event Tracing for Windows，ETW），或者将日志条目推送到应用的事件日志中。
<a href="https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor">LogMonitor</a>
是 Microsoft 提供的一个开源工具，是监视 Windows 容器中所配置的日志源
的推荐方式。
LogMonitor 支持监视时间日志、ETW 提供者模块以及自定义的应用日志，
并使用管道的方式将其输出到标准输出（stdout），以便 <code>kubectl logs &lt;pod&gt;</code>
这类命令能够读取这些数据。</p>
<!--
Follow the instructions in the LogMonitor GitHub page to copy its binaries and configuration files
to all your containers and add the necessary entrypoints for LogMonitor to push your logs to STDOUT.
-->
<p>请遵照 LogMonitor GitHub 页面上的指令，将其可执行文件和配置文件复制到
你的所有容器中，并为其添加必要的入口点（Entrypoint），以便 LogMonitor
能够将你的日志输出推送到标准输出（stdout）。</p>
<!--
## Using configurable Container usernames

Starting with Kubernetes v1.16, Windows containers can be configured to run their entrypoints and processes
with different usernames than the image defaults.
The way this is achieved is a bit different from the way it is done for Linux containers.
Learn more about it [here](/docs/tasks/configure-pod-container/configure-runasusername/).
-->
<h2 id="使用可配置的容器用户名">使用可配置的容器用户名</h2>
<p>从 Kubernetes v1.16 开始，可以为 Windows  容器配置与其镜像默认值不同的用户名
来运行其入口点和进程。
此能力的实现方式和 Linux 容器有些不同。
在<a href="/zh/docs/tasks/configure-pod-container/configure-runasusername/">此处</a>
可了解更多信息。</p>
<!--
## Managing Workload Identity with Group Managed Service Accounts

Starting with Kubernetes v1.14, Windows container workloads can be configured to use Group Managed Service Accounts (GMSA).
Group Managed Service Accounts are a specific type of Active Directory account that provides automatic password management,
simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.
Containers configured with a GMSA can access external Active Directory Domain resources while carrying the identity configured with the GMSA.
Learn more about configuring and using GMSA for Windows containers [here](/docs/tasks/configure-pod-container/configure-gmsa/).
-->
<h2 id="使用组托管服务帐户管理工作负载身份">使用组托管服务帐户管理工作负载身份</h2>
<p>从 Kubernetes v1.14 开始，可以将 Windows 容器工作负载配置为使用组托管服务帐户（GMSA）。
组托管服务帐户是 Active Directory 帐户的一种特定类型，它提供自动密码管理，
简化的服务主体名称（SPN）管理以及将管理委派给跨多台服务器的其他管理员的功能。
配置了 GMSA 的容器可以访问外部 Active Directory 域资源，同时携带通过 GMSA 配置的身份。
在<a href="/zh/docs/tasks/configure-pod-container/configure-gmsa/">此处</a>了解有关为
Windows 容器配置和使用 GMSA 的更多信息。</p>
<!--
## Taints and Tolerations
-->
<h2 id="污点和容忍度">污点和容忍度</h2>
<!--
Users today need to use some combination of taints and node selectors in order to
keep Linux and Windows workloads on their respective OS-specific nodes.
This likely imposes a burden only on Windows users. The recommended approach is outlined below,
with one of its main goals being that this approach should not break compatibility for existing Linux workloads.
-->
<p>目前，用户需要将 Linux 和 Windows 工作负载运行在各自特定的操作系统的节点上，
因而需要结合使用污点和节点选择算符。 这可能仅给 Windows 用户造成不便。
推荐的方法概述如下，其主要目标之一是该方法不应破坏与现有 Linux 工作负载的兼容性。</p>
<!--
If the `IdentifyPodOS` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is
enabled, you can (and should) set `.spec.os.name` for a Pod to indicate the operating system
that the containers in that Pod are designed for. For Pods that run Linux containers, set
`.spec.os.name` to `linux`. For Pods that run Windows containers, set `.spec.os.name`
to Windows.

The scheduler does not use the value of `.spec.os.name` when assigning Pods to nodes. You should
use normal Kubernetes mechanisms for
[assigning pods to nodes](/docs/concepts/scheduling-eviction/assign-pod-node/)
to ensure that the control plane for your cluster places pods onto nodes that are running the
appropriate operating system.
 no effect on the scheduling of the Windows pods, so taints and tolerations and node selectors are still required
 to ensure that the Windows pods land onto appropriate Windows nodes.
 -->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>如果 <code>IdentifyPodOS</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>是启用的，
你可以（并且应该）为 Pod 设置 <code>.spec.os.name</code> 以表明该 Pod
中的容器所针对的操作系统。 对于运行 Linux 容器的 Pod，设置
<code>.spec.os.name</code> 为 <code>linux</code>。 对于运行 Windows 容器的 Pod，设置 <code>.spec.os.name</code>
为 <code>Windows</code>。</p>
<p>在将 Pod 分配给节点时，调度程序不使用 <code>.spec.os.name</code> 的值。你应该使用正常的 Kubernetes
机制<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">将 Pod 分配给节点</a>，
确保集群的控制平面将 Pod 放置到适合运行的操作系统。
对 Windows Pod 的调度没有影响，因此仍然需要污点、容忍度以及节点选择器，
以确保 Windows Pod 调度至合适的 Windows 节点。</p>

</div>
<!--
### Ensuring OS-specific workloads land on the appropriate container host
-->
<h3 id="确保特定操作系统的工作负载落在适当的容器主机上">确保特定操作系统的工作负载落在适当的容器主机上</h3>
<!--
Users can ensure Windows containers can be scheduled on the appropriate host using Taints and Tolerations.
All Kubernetes nodes today have the following default labels:
-->
<p>用户可以使用污点和容忍度确保 Windows 容器可以调度在适当的主机上。目前所有 Kubernetes 节点都具有以下默认标签：</p>
<ul>
<li>kubernetes.io/os = [windows|linux]</li>
<li>kubernetes.io/arch = [amd64|arm64|...]</li>
</ul>
<!--
If a Pod specification does not specify a nodeSelector like `"kubernetes.io/os": windows`,
it is possible the Pod can be scheduled on any host, Windows or Linux.
This can be problematic since a Windows container can only run on Windows and a Linux container can only run on Linux.
The best practice is to use a nodeSelector.
-->
<p>如果 Pod 规范未指定诸如 <code>&quot;kubernetes.io/os&quot;: windows</code> 之类的 nodeSelector，则该 Pod
可能会被调度到任何主机（Windows 或 Linux）上。
这是有问题的，因为 Windows 容器只能在 Windows 上运行，而 Linux 容器只能在 Linux 上运行。
最佳实践是使用 nodeSelector。</p>
<!--
However, we understand that in many cases users have a pre-existing large number of deployments for Linux containers,
as well as an ecosystem of off-the-shelf configurations, such as community Helm charts, and programmatic Pod generation cases, such as with Operators.
In those situations, you may be hesitant to make the configuration change to add nodeSelectors.
The alternative is to use Taints. Because the kubelet can set Taints during registration,
it could easily be modified to automatically add a taint when running on Windows only.
-->
<p>但是，我们了解到，在许多情况下，用户都有既存的大量的 Linux 容器部署，以及一个现成的配置生态系统，
例如社区 Helm charts，以及程序化 Pod 生成案例，例如 Operators。
在这些情况下，您可能会不愿意更改配置添加 nodeSelector。替代方法是使用污点。
由于 kubelet 可以在注册期间设置污点，因此可以轻松修改它，使其仅在 Windows 上运行时自动添加污点。</p>
<!--
For example:  `--register-with-taints='os=windows:NoSchedule'`
-->
<p>例如：<code>--register-with-taints='os=windows:NoSchedule'</code></p>
<!--
By adding a taint to all Windows nodes, nothing will be scheduled on them (that includes existing Linux Pods).
In order for a Windows Pod to be scheduled on a Windows node,
it would need both the nodeSelector and the appropriate matching toleration to choose Windows.
-->
<p>向所有 Windows 节点添加污点后，Kubernetes 将不会在它们上调度任何负载（包括现有的 Linux Pod）。
为了使某 Windows Pod 调度到 Windows 节点上，该 Pod 需要 nodeSelector 和合适的匹配的容忍度设置来选择 Windows，</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kubernetes.io/os</span>:<span style="color:#bbb"> </span>windows<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">node.kubernetes.io/windows-build</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;10.0.17763&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;os&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Equal&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;windows&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;NoSchedule&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
### Handling multiple Windows versions in the same cluster
-->
<h3 id="处理同一集群中的多个-windows-版本">处理同一集群中的多个 Windows 版本</h3>
<!--
The Windows Server version used by each pod must match that of the node. If you want to use multiple Windows
Server versions in the same cluster, then you should set additional node labels and nodeSelectors.
-->
<p>每个 Pod 使用的 Windows Server 版本必须与该节点的 Windows Server 版本相匹配。
如果要在同一集群中使用多个 Windows Server 版本，则应该设置其他节点标签和
nodeSelector。</p>
<!--
Kubernetes 1.17 automatically adds a new label `node.kubernetes.io/windows-build` to simplify this.
If you're running an older version, then it's recommended to add this label manually to Windows nodes.
-->
<p>Kubernetes 1.17 自动添加了一个新标签 <code>node.kubernetes.io/windows-build</code> 来简化此操作。
如果您运行的是旧版本，则建议手动将此标签添加到 Windows 节点。</p>
<!--
This label reflects the Windows major, minor, and build number that need to match for compatibility.
Here are values used today for each Windows Server version.
-->
<p>此标签反映了需要兼容的 Windows 主要、次要和内部版本号。以下是当前每个
Windows Server 版本使用的值。</p>
<table>
<thead>
<tr>
<th>产品名称</th>
<th>内部编号</th>
</tr>
</thead>
<tbody>
<tr>
<td>Windows Server 2019</td>
<td>10.0.17763</td>
</tr>
<tr>
<td>Windows Server version 1809</td>
<td>10.0.17763</td>
</tr>
<tr>
<td>Windows Server version 1903</td>
<td>10.0.18362</td>
</tr>
</tbody>
</table>
<!--
### Simplifying with RuntimeClass
-->
<h3 id="使用-runtimeclass-简化">使用 RuntimeClass 简化</h3>
<!--
[RuntimeClass] can be used to simplify the process of using taints and tolerations.
A cluster administrator can create a `RuntimeClass` object which is used to encapsulate these taints and tolerations.
-->
<p><a href="/zh/docs/concepts/containers/runtime-class/">RuntimeClass</a> 可用于
简化使用污点和容忍度的过程。
集群管理员可以创建 <code>RuntimeClass</code> 对象，用于封装这些污点和容忍度。</p>
<!--
1. Save this file to `runtimeClasses.yml`. It includes the appropriate `nodeSelector`
   for the Windows OS, architecture, and version.
-->
<ol>
<li>
<p>将此文件保存到 <code>runtimeClasses.yml</code> 文件。
它包括适用于 Windows 操作系统、体系结构和版本的 <code>nodeSelector</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>node.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>RuntimeClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>windows-2019<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">handler</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;docker&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">scheduling</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/os</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;windows&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/arch</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;amd64&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">node.kubernetes.io/windows-build</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;10.0.17763&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>os<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>Equal<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;windows&#34;</span><span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
1. Run `kubectl create -f runtimeClasses.yml` using as a cluster administrator
1. Add `runtimeClassName: windows-2019` as appropriate to Pod specs
-->
<ol start="2">
<li>集群管理员执行 <code>kubectl create -f runtimeClasses.yml</code> 操作</li>
<li>根据需要向 Pod 规约中添加 <code>runtimeClassName: windows-2019</code></li>
</ol>
<!--
For example:
-->
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">runtimeClassName</span>:<span style="color:#bbb"> </span>windows-2019<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>iis<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>800Mi<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>.1<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>300Mi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>iis<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>iis-2019<span style="color:#bbb">
</span></code></pre></div>
</div>



    
	
  

    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-84b6491601d6a2b3da4cd5a105c866ba">3 - 最佳实践</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-970615c97499e3651fd3a98e0387cefc">3.1 - 运行于多可用区环境</h1>
    
	<!--
reviewers:
- jlowdermilk
- justinsb
- quinton-hoole
title: Running in multiple zones
weight: 10
content_type: concept
-->
<!-- overview -->
<!--
This page describes running a cluster across multiple zones.
-->
<p>本页描述如何跨多个区（Zone）中运行集群。</p>
<!-- body -->
<!--
## Background

Kubernetes is designed so that a single Kubernetes cluster can run
across multiple failure zones, typically where these zones fit within
a logical grouping called a _region_. Major cloud providers define a region
as a set of failure zones (also called _availability zones_) that provide
a consistent set of features: within a region, each zone offers the same
APIs and services.

Typical cloud architectures aim to minimize the chance that a failure in
one zone also impairs services in another zone.
-->
<h2 id="背景">背景</h2>
<p>Kubernetes 从设计上允许同一个 Kubernetes 集群跨多个失效区来运行，
通常这些区位于某个称作 <em>区域（region）</em> 逻辑分组中。
主要的云提供商都将区域定义为一组失效区的集合（也称作 <em>可用区（Availability Zones）</em>），
能够提供一组一致的功能特性：每个区域内，各个可用区提供相同的 API 和服务。</p>
<p>典型的云体系结构都会尝试降低某个区中的失效影响到其他区中服务的概率。</p>
<!--
## Control plane behavior

All [control plane components](/docs/concepts/overview/components/#control-plane-components)
support running as a pool of interchangeable resources, replicated per
component.
-->
<h2 id="control-plane-behavior">控制面行为  </h2>
<p>所有的<a href="/zh/docs/concepts/overview/components/#control-plane-components">控制面组件</a>
都支持以一组可相互替换的资源池的形式来运行，每个组件都有多个副本。</p>
<!--
When you deploy a cluster control plane, place replicas of
control plane components across multiple failure zones. If availability is
an important concern, select at least three failure zones and replicate
each individual control plane component (API server, scheduler, etcd,
cluster controller manager) across at least three failure zones.
If you are running a cloud controller manager then you should
also replicate this across all the failure zones you selected.
-->
<p>当你部署集群控制面时，应将控制面组件的副本跨多个失效区来部署。
如果可用性是一个很重要的指标，应该选择至少三个失效区，并将每个
控制面组件（API 服务器、调度器、etcd、控制器管理器）复制多个副本，
跨至少三个失效区来部署。如果你在运行云控制器管理器，则也应该将
该组件跨所选的三个失效区来部署。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Kubernetes does not provide cross-zone resilience for the API server
endpoints. You can use various techniques to improve availability for
the cluster API server, including DNS round-robin, SRV records, or
a third-party load balancing solution with health checking.
-->
<p>Kubernetes 并不会为 API 服务器端点提供跨失效区的弹性。
你可以为集群 API 服务器使用多种技术来提升其可用性，包括使用
DNS 轮转、SRV 记录或者带健康检查的第三方负载均衡解决方案等等。
</div>
<!--
## Node behavior

Kubernetes automatically spreads the Pods for
workload resources (such as <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>
or <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>)
across different nodes in a cluster. This spreading helps
reduce the impact of failures.
-->
<h2 id="node-behavior">节点行为  </h2>
<p>Kubernetes 自动为负载资源（如<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>
或 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>)）
跨集群中不同节点来部署其 Pods。
这种分布逻辑有助于降低失效带来的影响。</p>
<!--
When nodes start up, the kubelet on each node automatically adds
<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='labels'>labels</a> to the Node object
that represents that specific kubelet in the Kubernetes API.
These labels can include
[zone information](/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
-->
<p>节点启动时，每个节点上的 kubelet 会向 Kubernetes API 中代表该 kubelet 的 Node 对象
添加 <a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>。
这些标签可能包含<a href="/zh/docs/reference/labels-annotations-taints/#topologykubernetesiozone">区信息</a>。</p>
<!--
If your cluster spans multiple zones or regions, you can use node labels
in conjunction with
[Pod topology spread constraints](/docs/concepts/workloads/pods/pod-topology-spread-constraints/)
to control how Pods are spread across your cluster among fault domains:
regions, zones, and even specific nodes.
These hints enable the
<a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='scheduler'>scheduler</a> to place
Pods for better expected availability, reducing the risk that a correlated
failure affects your whole workload.
-->
<p>如果你的集群跨了多个可用区或者地理区域，你可以使用节点标签，结合
<a href="/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/">Pod 拓扑分布约束</a>
来控制如何在你的集群中多个失效域之间分布 Pods。这里的失效域可以是
地理区域、可用区甚至是特定节点。
这些提示信息使得<a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='调度器'>调度器</a>
能够更好地分布 Pods，以实现更好的可用性，降低因为某种失效给整个工作负载
带来的风险。</p>
<!--
For example, you can set a constraint to make sure that the
3 replicas of a StatefulSet are all running in different zones to each
other, whenever that is feasible. You can define this declaratively
without explicitly defining which availability zones are in use for
each workload.
-->
<p>例如，你可以设置一种约束，确保某个 StatefulSet 中的三个副本都运行在
不同的可用区中，只要其他条件允许。你可以通过声明的方式来定义这种约束，
而不需要显式指定每个工作负载使用哪些可用区。</p>
<!--
### Distributing nodes across zones

Kubernetes' core does not create nodes for you; you need to do that yourself,
or use a tool such as the [Cluster API](https://cluster-api.sigs.k8s.io/) to
manage nodes on your behalf.

Using tools such as the Cluster API you can define sets of machines to run as
worker nodes for your cluster across multiple failure domains, and rules to
automatically heal the cluster in case of whole-zone service disruption.
-->
<h3 id="distributing-nodes-across-zones">跨多个区分布节点</h3>
<p>Kubernetes 的核心逻辑并不会帮你创建节点，你需要自行完成此操作，或者使用
类似 <a href="https://cluster-api.sigs.k8s.io/">Cluster API</a> 这类工具来替你管理节点。</p>
<!--
Using tools such as the Cluster API you can define sets of machines to run as
worker nodes for your cluster across multiple failure domains, and rules to
automatically heal the cluster in case of whole-zone service disruption.
-->
<p>使用类似 Cluster API 这类工具，你可以跨多个失效域来定义一组用做你的集群
工作节点的机器，以及当整个区的服务出现中断时如何自动治愈集群的策略。</p>
<!--
## Manual zone assignment for Pods

You can apply [node selector constraints](/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
to Pods that you create, as well as to Pod templates in workload resources
such as Deployment, StatefulSet, or Job.
-->
<h2 id="为-pods-手动指定区">为 Pods 手动指定区</h2>
<!--
You can apply [node selector constraints](/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
to Pods that you create, as well as to Pod templates in workload resources
such as Deployment, StatefulSet, or Job.
-->
<p>你可以应用<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector">节点选择算符约束</a>
到你所创建的 Pods 上，或者为 Deployment、StatefulSet 或 Job 这类工作负载资源
中的 Pod 模板设置此类约束。</p>
<!--
## Storage access for zones

When persistent volumes are created, the `PersistentVolumeLabel`
[admission controller](/docs/reference/access-authn-authz/admission-controllers/)
automatically adds zone labels to any PersistentVolumes that are linked to a specific
zone. The <a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='scheduler'>scheduler</a> then ensures,
through its `NoVolumeZoneConflict` predicate, that pods which claim a given PersistentVolume
are only placed into the same zone as that volume.
-->
<h2 id="跨区的存储访问">跨区的存储访问</h2>
<p>当创建持久卷时，<code>PersistentVolumeLabel</code>
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制器</a>
会自动向那些链接到特定区的 PersistentVolume 添加区标签。
<a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='调度器'>调度器</a>通过其
<code>NoVolumeZoneConflict</code> 断言确保申领给定 PersistentVolume 的 Pods 只会
被调度到该卷所在的可用区。</p>
<!--
You can specify a <a class='glossary-tooltip' title='StorageClass 是管理员用来描述可用的不同存储类型的一种方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/storage-classes/' target='_blank' aria-label='StorageClass'>StorageClass</a>
for PersistentVolumeClaims that specifies the failure domains (zones) that the
storage in that class may use.
To learn about configuring a StorageClass that is aware of failure domains or zones,
see [Allowed topologies](/docs/concepts/storage/storage-classes/#allowed-topologies).
-->
<p>你可以为 PersistentVolumeClaim 指定<a class='glossary-tooltip' title='StorageClass 是管理员用来描述可用的不同存储类型的一种方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/storage-classes/' target='_blank' aria-label='StorageClass'>StorageClass</a>
以设置该类中的存储可以使用的失效域（区）。
要了解如何配置能够感知失效域或区的 StorageClass，请参阅
<a href="/zh/docs/concepts/storage/storage-classes/#allowed-topologies">可用的拓扑逻辑</a>。</p>
<!--
## Networking

By itself, Kubernetes does not include zone-aware networking. You can use a
[network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)
to configure cluster networking, and that network solution might have zone-specific
elements. For example, if your cloud provider supports Services with
`type=LoadBalancer`, the load balancer might only send traffic to Pods running in the
same zone as the load balancer element processing a given connection.
Check your cloud provider's documentation for details.
-->
<h2 id="networking">网络 </h2>
<p>Kubernetes 自身不提供与可用区相关的联网配置。
你可以使用<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件</a>
来配置集群的联网，该网络解决方案可能拥有一些与可用区相关的元素。
例如，如果你的云提供商支持 <code>type=LoadBalancer</code> 的 Service，则负载均衡器
可能仅会将请求流量发送到运行在负责处理给定连接的负载均衡器组件所在的区。
请查阅云提供商的文档了解详细信息。</p>
<!--
For custom or on-premises deployments, similar considerations apply.
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> and
<a class='glossary-tooltip' title='Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/ingress/' target='_blank' aria-label='Ingress'>Ingress</a> behavior, including handling
of different failure zones, does vary depending on exactly how your cluster is set up.
-->
<p>对于自定义的或本地集群部署，也可以考虑这些因素
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>
<a class='glossary-tooltip' title='Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/ingress/' target='_blank' aria-label='Ingress'>Ingress</a> 的行为，
包括处理不同失效区的方法，在很大程度上取决于你的集群是如何搭建的。</p>
<!--
## Fault recovery

When you set up your cluster, you might also need to consider whether and how
your setup can restore service if all the failure zones in a region go
off-line at the same time. For example, do you rely on there being at least
one node able to run Pods in a zone?  
Make sure that any cluster-critical repair work does not rely
on there being at least one healthy node in your cluster. For example: if all nodes
are unhealthy, you might need to run a repair Job with a special
<a class='glossary-tooltip' title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='toleration'>toleration</a> so that the repair
can complete enough to bring at least one node into service.

Kubernetes doesn't come with an answer for this challenge; however, it's
something to consider.
-->
<h2 id="fault-recovery">失效恢复   </h2>
<p>在搭建集群时，你可能需要考虑当某区域中的所有失效区都同时掉线时，是否以及如何
恢复服务。例如，你是否要求在某个区中至少有一个节点能够运行 Pod？
请确保任何对集群很关键的修复工作都不要指望集群中至少有一个健康节点。
例如：当所有节点都不健康时，你可能需要运行某个修复性的 Job，
该 Job 要设置特定的<a class='glossary-tooltip' title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='容忍度'>容忍度</a>
以便修复操作能够至少将一个节点恢复为可用状态。</p>
<p>Kubernetes 对这类问题没有现成的解决方案；不过这也是要考虑的因素之一。</p>
<h2 id="what-s-next">What's next</h2>
<!--
To learn how the scheduler places Pods in a cluster, honoring the configured constraints,
visit [Scheduling and Eviction](/docs/concepts/scheduling-eviction/).
-->
<p>要了解调度器如何在集群中放置 Pods 并遵从所配置的约束，可参阅
<a href="/zh/docs/concepts/scheduling-eviction/">调度与驱逐</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c797ee17120176c685455db89ae091a9">3.2 - 大规模集群的注意事项</h1>
    
	<!-- 
reviewers:
- davidopp
- lavalamp
title: Considerations for large clusters
weight: 20
-->
<!--
A cluster is a set of <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='nodes'>nodes</a> (physical
or virtual machines) running Kubernetes agents, managed by the
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>.
Kubernetes v1.23 supports clusters with up to 5000 nodes. More specifically,
Kubernetes is designed to accommodate configurations that meet *all* of the following criteria:
-->
<p>集群是运行 Kubernetes 代理的、
由<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>管理的一组
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>（物理机或虚拟机）。
Kubernetes v1.23 支持的最大节点数为 5000。
更具体地说，Kubernetes旨在适应满足以下<em>所有</em>标准的配置：</p>
<!--
* No more than 110 pods per node
* No more than 5000 nodes
* No more than 150000 total pods
* No more than 300000 total containers
-->
<ul>
<li>每个节点的 Pod 数量不超过 110</li>
<li>节点数不超过 5000</li>
<li>Pod 总数不超过 150000</li>
<li>容器总数不超过 300000</li>
</ul>
<!-- 
You can scale your cluster by adding or removing nodes. The way you do this depends
on how your cluster is deployed.
-->
<p>你可以通过添加或删除节点来扩展集群。集群扩缩的方式取决于集群的部署方式。</p>
<!--  
## Cloud provider resource quotas {#quota-issues}

To avoid running into cloud provider quota issues, when creating a cluster with many nodes,
consider:
* Requesting a quota increase for cloud resources such as:
    * Computer instances
    * CPUs
    * Storage volumes
    * In-use IP addresses
    * Packet filtering rule sets
    * Number of load balancers
    * Network subnets
    * Log streams
* Gating the cluster scaling actions to brings up new nodes in batches, with a pause
  between batches, because some cloud providers rate limit the creation of new instances.
-->
<h2 id="quota-issues">云供应商资源配额</h2>
<p>为避免遇到云供应商配额问题，在创建具有大规模节点的集群时，请考虑以下事项：</p>
<ul>
<li>请求增加云资源的配额，例如：
<ul>
<li>计算实例</li>
<li>CPUs</li>
<li>存储卷</li>
<li>使用中的 IP 地址</li>
<li>数据包过滤规则集</li>
<li>负载均衡数量</li>
<li>网络子网</li>
<li>日志流</li>
</ul>
</li>
<li>由于某些云供应商限制了创建新实例的速度，因此通过分批启动新节点来控制集群扩展操作，并在各批之间有一个暂停。</li>
</ul>
<!--  
## Control plane components

For a large cluster, you need a control plane with sufficient compute and other
resources.

Typically you would run one or two control plane instances per failure zone,
scaling those instances vertically first and then scaling horizontally after reaching
the point of falling returns to (vertical) scale.
-->
<h2 id="控制面组件">控制面组件</h2>
<p>对于大型集群，你需要一个具有足够计算能力和其他资源的控制平面。</p>
<p>通常，你将在每个故障区域运行一个或两个控制平面实例，
先垂直缩放这些实例，然后在到达下降点（垂直）后再水平缩放。</p>
<!-- 
You should run at least one instance per failure zone to provide fault-tolerance. Kubernetes
nodes do not automatically steer traffic towards control-plane endpoints that are in the
same failure zone; however, your cloud provider might have its own mechanisms to do this.

For example, using a managed load balancer, you configure the load balancer to send traffic
that originates from the kubelet and Pods in failure zone _A_, and direct that traffic only
to the control plane hosts that are also in zone _A_. If a single control-plane host or
endpoint failure zone _A_ goes offline, that means that all the control-plane traffic for
nodes in zone _A_ is now being sent between zones. Running multiple control plane hosts in
each zone makes that outcome less likely.
-->
<p>你应该在每个故障区域至少应运行一个实例，以提供容错能力。
Kubernetes 节点不会自动将流量引向相同故障区域中的控制平面端点。
但是，你的云供应商可能有自己的机制来执行此操作。</p>
<p>例如，使用托管的负载均衡器时，你可以配置负载均衡器发送源自故障区域 <em>A</em> 中的 kubelet 和 Pod 的流量，
并将该流量仅定向到也位于区域 <em>A</em> 中的控制平面主机。
如果单个控制平面主机或端点故障区域 <em>A</em> 脱机，则意味着区域 <em>A</em> 中的节点的所有控制平面流量现在都在区域之间发送。
在每个区域中运行多个控制平面主机能降低出现这种结果的可能性。</p>
<!--
### etcd storage
-->
<h3 id="etcd-存储">etcd 存储</h3>
<!--
To improve performance of large clusters, you can store Event objects in a separate
dedicated etcd instance.
-->
<p>为了提高大规模集群的性能，你可以将事件对象存储在单独的专用 etcd 实例中。</p>
<!--
When creating a cluster, you can (using custom tooling):

* start and configure additional etcd instance
* configure the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a> to use it for storing events
-->
<p>在创建集群时，你可以（使用自定义工具）：</p>
<ul>
<li>启动并配置额外的 etcd 实例</li>
<li>配置 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>，将它用于存储事件</li>
</ul>
<!--
See [Operating etcd clusters for Kubernetes](/docs/tasks/administer-cluster/configure-upgrade-etcd/) and
[Set up a High Availability etcd cluster with kubeadm](/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)
for details on configuring and managing etcd for a large cluster.
-->
<p>有关为大型集群配置和管理 etcd 的详细信息，请参阅
<a href="/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/">为 Kubernetes 运行 etcd 集群</a>
和使用 <a href="/zh/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">kubeadm 创建一个高可用 etcd 集群</a>。</p>
<!--
### Addon Resources
-->
<h3 id="addon-resources">插件资源  </h3>
<!--
Kubernetes [resource limits](/docs/concepts/configuration/manage-resources-containers/)
help to minimize the impact of memory leaks and other ways that pods and containers can
impact on other components. These resource limits apply to
<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='addon'>addon</a> resources just as they apply to application workloads.

  For example, you can set CPU and memory limits for a logging component:
-->
<p>Kubernetes <a href="/zh/docs/concepts/configuration/manage-resources-containers/">资源限制</a>
有助于最大程度地减少内存泄漏的影响以及 Pod 和容器可能对其他组件的其他方式的影响。
这些资源限制适用于<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='插件'>插件</a>资源，
就像它们适用于应用程序工作负载一样。</p>
<p>例如，你可以对日志组件设置 CPU 和内存限制</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-cloud-logging<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>fluent/fluentd-kubernetes-daemonset:v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>100m<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span></code></pre></div><!-- 
Addons' default limits are typically based on data collected from experience running
each addon on small or medium Kubernetes clusters. When running on large
clusters, addons often consume more of some resources than their default limits.
If a large cluster is deployed without adjusting these values, the addon(s)
may continuously get killed because they keep hitting the memory limit.
Alternatively, the addon may run but with poor performance due to CPU time
slice restrictions.
-->
<p>插件的默认限制通常基于从中小规模 Kubernetes 集群上运行每个插件的经验收集的数据。
插件在大规模集群上运行时，某些资源消耗常常比其默认限制更多。
如果在不调整这些值的情况下部署了大规模集群，则插件可能会不断被杀死，因为它们不断达到内存限制。
或者，插件可能会运行，但由于 CPU 时间片的限制而导致性能不佳。</p>
<!--  
To avoid running into cluster addon resource issues, when creating a cluster with
many nodes, consider the following:

* Some addons scale vertically - there is one replica of the addon for the cluster
  or serving a whole failure zone. For these addons, increase requests and limits
  as you scale out your cluster.
* Many addons scale horizontally - you add capacity by running more pods - but with
  a very large cluster you may also need to raise CPU or memory limits slightly.
  The VerticalPodAutoscaler can run in _recommender_ mode to provide suggested
  figures for requests and limits.
* Some addons run as one copy per node, controlled by a <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>: for example, a node-level log aggregator. Similar to
  the case with horizontally-scaled addons, you may also need to raise CPU or memory
  limits slightly.
-->
<p>为避免遇到集群插件资源问题，在创建大规模集群时，请考虑以下事项：</p>
<ul>
<li>部分垂直扩展插件 —— 总有一个插件副本服务于整个集群或服务于整个故障区域。
对于这些附加组件，请在扩大集群时加大资源请求和资源限制。</li>
<li>许多水平扩展插件 —— 你可以通过运行更多的 Pod 来增加容量——但是在大规模集群下，
可能还需要稍微提高 CPU 或内存限制。
VerticalPodAutoscaler 可以在 <em>recommender</em> 模式下运行，
以提供有关请求和限制的建议数字。</li>
<li>一些插件在每个节点上运行一个副本，并由 DaemonSet 控制：
例如，节点级日志聚合器。与水平扩展插件的情况类似，
你可能还需要稍微提高 CPU 或内存限制。</li>
</ul>
<!-- 
## What's next

`VerticalPodAutoscaler` is a custom resource that you can deploy into your cluster
to help you manage resource requests and limits for pods.  
Visit [Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme)
to learn more about `VerticalPodAutoscaler` and how you can use it to scale cluster
components, including cluster-critical addons.

The [cluster autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme)
integrates with a number of cloud providers to help you run the right number of
nodes for the level of resource demand in your cluster.
-->
<h2 id="what-s-next">What's next</h2>
<p><code>VerticalPodAutoscaler</code> 是一种自定义资源，你可以将其部署到集群中，帮助你管理资源请求和 Pod 的限制。
访问 <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme">Vertical Pod Autoscaler</a>
以了解有关 <code>VerticalPodAutoscaler</code> 的更多信息，
以及如何使用它来扩展集群组件（包括对集群至关重要的插件）的信息。</p>
<p><a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme">集群自动扩缩器</a>
与许多云供应商集成在一起，帮助你在你的集群中，按照资源需求级别运行正确数量的节点。</p>
<!-- 
The [addon resizer](https://github.com/kubernetes/autoscaler/tree/master/addon-resizer#readme)
helps you in resizing the addons automatically as your cluster's scale changes.
-->
<p><a href="https://github.com/kubernetes/autoscaler/tree/master/addon-resizer#readme">addon resizer</a>
可帮助你在集群规模变化时自动调整插件的大小。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f89867de1d34943f1524f67a241f5cc9">3.3 - 校验节点设置</h1>
    
	<!--
---
reviewers:
- Random-Liu
title: Validate node setup
weight: 30
---
-->
<nav id="TableOfContents">
  <ul>
    <li><a href="#节点一致性测试">节点一致性测试</a></li>
    <li><a href="#节点的前提条件">节点的前提条件</a></li>
    <li><a href="#运行节点一致性测试">运行节点一致性测试</a></li>
    <li><a href="#针对其他硬件体系结构运行节点一致性测试">针对其他硬件体系结构运行节点一致性测试</a></li>
    <li><a href="#运行特定的测试">运行特定的测试</a></li>
    <li><a href="#注意">注意</a></li>
  </ul>
</nav>
<!--
## Node Conformance Test
-->
<h2 id="节点一致性测试">节点一致性测试</h2>
<!--
*Node conformance test* is a containerized test framework that provides a system
verification and functionality test for a node. 
-->
<p><em>节点一致性测试</em> 是一个容器化的测试框架，提供了针对节点的系统验证和功能测试。</p>
<!--
The test validates whether the node meets the minimum requirements for Kubernetes; a node that passes the testis qualified to join a Kubernetes cluster.
-->
<p>该测试主要检测节点是否满足 Kubernetes 的最低要求，通过检测的节点有资格加入 Kubernetes 集群。</p>
<!--
## Node Prerequisite
-->
<h2 id="节点的前提条件">节点的前提条件</h2>
<!--
To run node conformance test, a node must satisfy the same prerequisites as astandard Kubernetes node. At a minimum, the node should have the following daemons installed:
-->
<p>要运行节点一致性测试，节点必须满足与标准 Kubernetes 节点相同的前提条件。节点至少应安装以下守护程序：</p>
<!--
* Container Runtime (Docker)
* Kubelet
-->
<ul>
<li>容器运行时 (Docker)</li>
<li>Kubelet</li>
</ul>
<!--
## Running Node Conformance Test
-->
<h2 id="运行节点一致性测试">运行节点一致性测试</h2>
<!--
To run the node conformance test, perform the following steps:
-->
<p>要运行节点一致性测试，请执行以下步骤：</p>
<!--
1. Work out the value of the `--kubeconfig` option for the kubelet; for example:
   `--kubeconfig=/var/lib/kubelet/config.yaml`.
    Because the test framework starts a local control plane to test the kubelet,
    use `http://localhost:8080` as the URL of the API server.
    There are some other kubelet command line parameters you may want to use:
  * `--pod-cidr`: If you are using `kubenet`, you should specify an arbitrary CIDR
    to Kubelet, for example `--pod-cidr=10.180.0.0/24`.
  * `--cloud-provider`: If you are using `--cloud-provider=gce`, you should
    remove the flag to run the test.
-->
<ol>
<li>得出 kubelet 的 <code>--kubeconfig</code> 的值；例如：<code>--kubeconfig=/var/lib/kubelet/config.yaml</code>.
由于测试框架启动了本地控制平面来测试 kubelet， 因此使用 <code>http://localhost:8080</code>
作为API 服务器的 URL。
一些其他的 kubelet 命令行参数可能会被用到：
<ul>
<li><code>--pod-cidr</code>： 如果使用 <code>kubenet</code>， 需要为 Kubelet 任意指定一个 CIDR，
例如 <code>--pod-cidr=10.180.0.0/24</code>。</li>
<li><code>--cloud-provider</code>： 如果使用 <code>--cloud-provider=gce</code>，需要移除这个参数
来运行测试。</li>
</ul>
</li>
</ol>
<!--
2. Run the node conformance test with command:
-->
<ol start="2">
<li>
<p>使用以下命令运行节点一致性测试：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># $CONFIG_DIR 是您 Kubelet 的 pod manifest 路径。</span>
<span style="color:#080;font-style:italic"># $LOG_DIR 是测试的输出路径。</span>
sudo docker run -it --rm --privileged --net<span style="color:#666">=</span>host <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -v /:/rootfs -v <span style="color:#b8860b">$CONFIG_DIR</span>:<span style="color:#b8860b">$CONFIG_DIR</span> -v <span style="color:#b8860b">$LOG_DIR</span>:/var/result <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  k8s.gcr.io/node-test:0.2
</code></pre></div></li>
</ol>
<!--
## Running Node Conformance Test for Other Architectures
-->
<h2 id="针对其他硬件体系结构运行节点一致性测试">针对其他硬件体系结构运行节点一致性测试</h2>
<!--
Kubernetes also provides node conformance test docker images for other architectures:
-->
<p>Kubernetes 也为其他硬件体系结构的系统提供了节点一致性测试的 Docker 镜像：</p>
<!--
| Arch  |      Image      |      |
| ----- | :-------------: | ---- |
| amd64 | node-test-amd64 |      |
| arm   |  node-test-arm  |      |
| arm64 | node-test-arm64 |      |
-->
<table>
<thead>
<tr>
<th>架构</th>
<th style="text-align:center">镜像</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>amd64</td>
<td style="text-align:center">node-test-amd64</td>
<td></td>
</tr>
<tr>
<td>arm</td>
<td style="text-align:center">node-test-arm</td>
<td></td>
</tr>
<tr>
<td>arm64</td>
<td style="text-align:center">node-test-arm64</td>
<td></td>
</tr>
</tbody>
</table>
<!--
## Running Selected Test
-->
<h2 id="运行特定的测试">运行特定的测试</h2>
<!--
To run specific tests, overwrite the environment variable `FOCUS` with theregular expression of tests you want to run.
-->
<p>要运行特定测试，请使用您希望运行的测试的特定表达式覆盖环境变量 <code>FOCUS</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo docker run -it --rm --privileged --net<span style="color:#666">=</span>host <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   -v /:/rootfs:ro -v <span style="color:#b8860b">$CONFIG_DIR</span>:<span style="color:#b8860b">$CONFIG_DIR</span> -v <span style="color:#b8860b">$LOG_DIR</span>:/var/result <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   -e <span style="color:#b8860b">FOCUS</span><span style="color:#666">=</span>MirrorPod <span style="color:#b62;font-weight:bold">\ </span><span style="color:#080;font-style:italic"># Only run MirrorPod test</span>
k8s.gcr.io/node-test:0.2
</code></pre></div><!--
To skip specific tests, overwrite the environment variable `SKIP` with theregular expression of tests you want to skip.
-->
<p>要跳过特定的测试，请使用您希望跳过的测试的常规表达式覆盖环境变量 <code>SKIP</code>。</p>
<!--
```shell
sudo docker run -it --rm --privileged --net=host \
  -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \
  -e SKIP=MirrorPod \ # Run all conformance tests but skip MirrorPod test
k8s.gcr.io/node-test:0.2
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo docker run -it --rm --privileged --net<span style="color:#666">=</span>host <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -v /:/rootfs:ro -v <span style="color:#b8860b">$CONFIG_DIR</span>:<span style="color:#b8860b">$CONFIG_DIR</span> -v <span style="color:#b8860b">$LOG_DIR</span>:/var/result <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -e <span style="color:#b8860b">SKIP</span><span style="color:#666">=</span>MirrorPod <span style="color:#b62;font-weight:bold">\ </span><span style="color:#080;font-style:italic"># 运行除 MirrorPod 测试外的所有一致性测试内容</span>
k8s.gcr.io/node-test:0.2
</code></pre></div><!--
Node conformance test is a containerized version of [node e2e test](https://github.com/kubernetes/community/blob/main/contributors/devel/e2e-node-tests.md).
-->
<p>节点一致性测试是<a href="https://github.com/kubernetes/community/blob/main/contributors/devel/e2e-node-tests.md">节点端到端测试</a>的容器化版本。</p>
<!--
By default, it runs all conformance tests.
-->
<p>默认情况下，它会运行所有一致性测试。</p>
<!--
Theoretically, you can run any node e2e test if you configure the container andmount required volumes properly. But **it is strongly recommended to only run conformance test**, because it requires much more complex configuration to run non-conformance test.
-->
<p>理论上，只要合理地配置容器和挂载所需的卷，就可以运行任何的节点端到端测试用例。 但是这里<strong>强烈建议只运行一致性测试</strong>，因为运行非一致性测试需要很多复杂的配置。</p>
<!--
## Caveats
-->
<h2 id="注意">注意</h2>
<!--
* The test leaves some docker images on the node, including the node conformance
  test image and images of containers used in the functionality
  test.
* The test leaves dead containers on the node. These containers are created
  during the functionality test.
-->
<ul>
<li>测试会在节点上遗留一些 Docker 镜像， 包括节点一致性测试本身的镜像和功能测试相关的镜像。</li>
<li>测试会在节点上遗留一些死的容器。这些容器是在功能测试的过程中创建的。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0394f813094b7a35058dffe5b8bacd20">3.4 - PKI 证书和要求</h1>
    
	<!--
title: PKI certificates and requirements
reviewers:
- sig-cluster-lifecycle
content_type: concept
weight: 40
-->
<!-- overview -->
<!--
Kubernetes requires PKI certificates for authentication over TLS.
If you install Kubernetes with [kubeadm](/docs/reference/setup-tools/kubeadm/), the certificates that your cluster requires are automatically generated.
You can also generate your own certificates - for example, to keep your private keys more secure by not storing them on the API server.
This page explains the certificates that your cluster requires.
-->
<p>Kubernetes 需要 PKI 证书才能进行基于 TLS 的身份验证。如果你是使用
<a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm</a> 安装的 Kubernetes，
则会自动生成集群所需的证书。你还可以生成自己的证书。
例如，不将私钥存储在 API 服务器上，可以让私钥更加安全。此页面说明了集群必需的证书。</p>
<!-- body -->
<!--
## How certificates are used by your cluster

Kubernetes requires PKI for the following operations:
-->
<h2 id="集群是如何使用证书的">集群是如何使用证书的</h2>
<p>Kubernetes 需要 PKI 才能执行以下操作：</p>
<!--
* Client certificates for the kubelet to authenticate to the API server
* Server certificate for the API server endpoint
* Client certificates for administrators of the cluster to authenticate to the API server
* Client certificates for the API server to talk to the kubelets
* Client certificate for the API server to talk to etcd
* Client certificate/kubeconfig for the controller manager to talk to the API server
* Client certificate/kubeconfig for the scheduler to talk to the API server.
* Client and server certificates for the [front-proxy](/docs/tasks/extend-kubernetes/configure-aggregation-layer/)
-->
<ul>
<li>Kubelet 的客户端证书，用于 API 服务器身份验证</li>
<li>API 服务器端点的证书</li>
<li>集群管理员的客户端证书，用于 API 服务器身份认证</li>
<li>API 服务器的客户端证书，用于和 Kubelet 的会话</li>
<li>API 服务器的客户端证书，用于和 etcd 的会话</li>
<li>控制器管理器的客户端证书/kubeconfig，用于和 API 服务器的会话</li>
<li>调度器的客户端证书/kubeconfig，用于和 API 服务器的会话</li>
<li><a href="/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/">前端代理</a> 的客户端及服务端证书</li>
</ul>
<!--
`front-proxy` certificates are required only if you run kube-proxy to support [an extension API server](/docs/tasks/access-kubernetes-api/setup-extension-api-server/).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 只有当你运行 kube-proxy 并要支持
<a href="/zh/docs/tasks/extend-kubernetes/setup-extension-api-server/">扩展 API 服务器</a>
时，才需要 <code>front-proxy</code> 证书
</div>
<!--
etcd also implements mutual TLS to authenticate clients and peers.
-->
<p>etcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证。</p>
<!--
## Where certificates are stored

If you install Kubernetes with kubeadm, certificates are stored in `/etc/kubernetes/pki`. All paths in this documentation are relative to that directory.
-->
<h2 id="证书存放的位置">证书存放的位置</h2>
<p>如果你是通过 kubeadm 安装的 Kubernetes，所有证书都存放在 <code>/etc/kubernetes/pki</code> 目录下。本文所有相关的路径都是基于该路径的相对路径。</p>
<!--
## Configure certificates manually

If you don't want kubeadm to generate the required certificates, you can create them in either of the following ways.
-->
<h2 id="手动配置证书">手动配置证书</h2>
<p>如果你不想通过 kubeadm 生成这些必需的证书，你可以通过下面两种方式之一来手动创建他们。</p>
<!--
### Single root CA

You can create a single root CA, controlled by an administrator. This root CA can then create multiple intermediate CAs, and delegate all further creation to Kubernetes itself.
-->
<h3 id="单根-ca">单根 CA</h3>
<p>你可以创建一个单根 CA，由管理员控制器它。该根 CA 可以创建多个中间 CA，并将所有进一步的创建委托给 Kubernetes。</p>
<!--
Required CAs:

| 路径                    | 默认 CN                    | 描述                             |
|------------------------|---------------------------|----------------------------------|
| ca.crt,key             | kubernetes-ca             | Kubernetes general CA            |
| etcd/ca.crt,key        | etcd-ca                   | For all etcd-related functions   |
| front-proxy-ca.crt,key | kubernetes-front-proxy-ca | For the [front-end proxy](/docs/tasks/extend-kubernetes/configure-aggregation-layer/) |

On top of the above CAs, it is also necessary to get a public/private key pair for service account management, `sa.key` and `sa.pub`.
-->
<p>需要这些 CA：</p>
<table>
<thead>
<tr>
<th>路径</th>
<th>默认 CN</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>ca.crt,key</td>
<td>kubernetes-ca</td>
<td>Kubernetes 通用 CA</td>
</tr>
<tr>
<td>etcd/ca.crt,key</td>
<td>etcd-ca</td>
<td>与 etcd 相关的所有功能</td>
</tr>
<tr>
<td>front-proxy-ca.crt,key</td>
<td>kubernetes-front-proxy-ca</td>
<td>用于 <a href="/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/">前端代理</a></td>
</tr>
</tbody>
</table>
<p>上面的 CA 之外，还需要获取用于服务账户管理的密钥对，也就是 <code>sa.key</code> 和 <code>sa.pub</code>。</p>
<!--
### All certificates

If you don't wish to copy the CA private keys to your cluster, you can generate all certificates yourself.

Required certificates:
-->
<h3 id="所有的证书">所有的证书</h3>
<p>如果你不想将 CA 的私钥拷贝至你的集群中，你也可以自己生成全部的证书。</p>
<p>需要这些证书：</p>
<table>
<thead>
<tr>
<th>默认 CN</th>
<th>父级 CA</th>
<th>O (位于 Subject 中)</th>
<th>类型</th>
<th>主机 (SAN)</th>
</tr>
</thead>
<tbody>
<tr>
<td>kube-etcd</td>
<td>etcd-ca</td>
<td></td>
<td>server, client</td>
<td><code>localhost</code>, <code>127.0.0.1</code></td>
</tr>
<tr>
<td>kube-etcd-peer</td>
<td>etcd-ca</td>
<td></td>
<td>server, client</td>
<td><code>&lt;hostname&gt;</code>, <code>&lt;Host_IP&gt;</code>, <code>localhost</code>, <code>127.0.0.1</code></td>
</tr>
<tr>
<td>kube-etcd-healthcheck-client</td>
<td>etcd-ca</td>
<td></td>
<td>client</td>
<td></td>
</tr>
<tr>
<td>kube-apiserver-etcd-client</td>
<td>etcd-ca</td>
<td>system:masters</td>
<td>client</td>
<td></td>
</tr>
<tr>
<td>kube-apiserver</td>
<td>kubernetes-ca</td>
<td></td>
<td>server</td>
<td><code>&lt;hostname&gt;</code>, <code>&lt;Host_IP&gt;</code>, <code>&lt;advertise_IP&gt;</code>, <code>[1]</code></td>
</tr>
<tr>
<td>kube-apiserver-kubelet-client</td>
<td>kubernetes-ca</td>
<td>system:masters</td>
<td>client</td>
<td></td>
</tr>
<tr>
<td>front-proxy-client</td>
<td>kubernetes-front-proxy-ca</td>
<td></td>
<td>client</td>
<td></td>
</tr>
</tbody>
</table>
<!--
[1]: any other IP or DNS name you contact your cluster on (as used by [kubeadm](/docs/reference/setup-tools/kubeadm/) the load balancer stable IP and/or DNS name, `kubernetes`, `kubernetes.default`, `kubernetes.default.svc`,
`kubernetes.default.svc.cluster`, `kubernetes.default.svc.cluster.local`)

where `kind` maps to one or more of the [x509 key usage](https://godoc.org/k8s.io/api/certificates/v1beta1#KeyUsage) types:
-->
<p>[1]: 用来连接到集群的不同 IP 或 DNS 名
（就像 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm</a> 为负载均衡所使用的固定
IP 或 DNS 名，<code>kubernetes</code>、<code>kubernetes.default</code>、<code>kubernetes.default.svc</code>、
<code>kubernetes.default.svc.cluster</code>、<code>kubernetes.default.svc.cluster.local</code>）。</p>
<p>其中，<code>kind</code> 对应一种或多种类型的 <a href="https://godoc.org/k8s.io/api/certificates/v1beta1#KeyUsage">x509 密钥用途</a>：</p>
<!--
| kind   | Key usage                                                                       |
|--------|---------------------------------------------------------------------------------|
| server | digital signature, key encipherment, server auth                                |
| client | digital signature, key encipherment, client auth                                |
-->
<table>
<thead>
<tr>
<th>kind</th>
<th>密钥用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>server</td>
<td>数字签名、密钥加密、服务端认证</td>
</tr>
<tr>
<td>client</td>
<td>数字签名、密钥加密、客户端认证</td>
</tr>
</tbody>
</table>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Hosts/SAN listed above are the recommended ones for getting a working cluster; if required by a specific setup, it is possible to add additional SANs on all the server certificates.
-->
<p>上面列出的 Hosts/SAN 是推荐的配置方式；如果需要特殊安装，则可以在所有服务器证书上添加其他 SAN。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
For kubeadm users only:

* The scenario where you are copying to your cluster CA certificates without private keys is referred as external CA in the kubeadm documentation.
* If you are comparing the above list with a kubeadm generated PKI, please be aware that `kube-etcd`, `kube-etcd-peer` and `kube-etcd-healthcheck-client` certificates
  are not generated in case of external etcd.
-->
<p>对于 kubeadm 用户：</p>
<ul>
<li>不使用私钥，将证书复制到集群 CA 的方案，在 kubeadm 文档中将这种方案称为外部 CA。</li>
<li>如果将以上列表与 kubeadm 生成的 PKI 进行比较，你会注意到，如果使用外部 etcd，则不会生成 <code>kube-etcd</code>、<code>kube-etcd-peer</code> 和 <code>kube-etcd-healthcheck-client</code> 证书。</li>
</ul>

</div>
<!--
### Certificate paths

Certificates should be placed in a recommended path (as used by [kubeadm](/docs/reference/setup-tools/kubeadm/)). Paths should be specified using the given argument regardless of location.
-->
<h3 id="证书路径">证书路径</h3>
<p>证书应放置在建议的路径中（以便 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm</a>使用）。无论使用什么位置，都应使用给定的参数指定路径。</p>
<table>
<thead>
<tr>
<th>默认 CN</th>
<th>建议的密钥路径</th>
<th>建议的证书路径</th>
<th>命令</th>
<th>密钥参数</th>
<th>证书参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>etcd-ca</td>
<td>etcd/ca.key</td>
<td>etcd/ca.crt</td>
<td>kube-apiserver</td>
<td></td>
<td>--etcd-cafile</td>
</tr>
<tr>
<td>kube-apiserver-etcd-client</td>
<td>apiserver-etcd-client.key</td>
<td>apiserver-etcd-client.crt</td>
<td>kube-apiserver</td>
<td>--etcd-keyfile</td>
<td>--etcd-certfile</td>
</tr>
<tr>
<td>kubernetes-ca</td>
<td>ca.key</td>
<td>ca.crt</td>
<td>kube-apiserver</td>
<td></td>
<td>--client-ca-file</td>
</tr>
<tr>
<td>kubernetes-ca</td>
<td>ca.key</td>
<td>ca.crt</td>
<td>kube-controller-manager</td>
<td>--cluster-signing-key-file</td>
<td>--client-ca-file, --root-ca-file, --cluster-signing-cert-file</td>
</tr>
<tr>
<td>kube-apiserver</td>
<td>apiserver.key</td>
<td>apiserver.crt</td>
<td>kube-apiserver</td>
<td>--tls-private-key-file</td>
<td>--tls-cert-file</td>
</tr>
<tr>
<td>kube-apiserver-kubelet-client</td>
<td>apiserver-kubelet-client.key</td>
<td>apiserver-kubelet-client.crt</td>
<td>kube-apiserver</td>
<td>--kubelet-client-key</td>
<td>--kubelet-client-certificate</td>
</tr>
<tr>
<td>front-proxy-ca</td>
<td>front-proxy-ca.key</td>
<td>front-proxy-ca.crt</td>
<td>kube-apiserver</td>
<td></td>
<td>--requestheader-client-ca-file</td>
</tr>
<tr>
<td>front-proxy-ca</td>
<td>front-proxy-ca.key</td>
<td>front-proxy-ca.crt</td>
<td>kube-controller-manager</td>
<td></td>
<td>--requestheader-client-ca-file</td>
</tr>
<tr>
<td>front-proxy-client</td>
<td>front-proxy-client.key</td>
<td>front-proxy-client.crt</td>
<td>kube-apiserver</td>
<td>--proxy-client-key-file</td>
<td>--proxy-client-cert-file</td>
</tr>
<tr>
<td>etcd-ca</td>
<td>etcd/ca.key</td>
<td>etcd/ca.crt</td>
<td>etcd</td>
<td></td>
<td>--trusted-ca-file, --peer-trusted-ca-file</td>
</tr>
<tr>
<td>kube-etcd</td>
<td>etcd/server.key</td>
<td>etcd/server.crt</td>
<td>etcd</td>
<td>--key-file</td>
<td>--cert-file</td>
</tr>
<tr>
<td>kube-etcd-peer</td>
<td>etcd/peer.key</td>
<td>etcd/peer.crt</td>
<td>etcd</td>
<td>--peer-key-file</td>
<td>--peer-cert-file</td>
</tr>
<tr>
<td>etcd-ca</td>
<td></td>
<td>etcd/ca.crt</td>
<td>etcdctl</td>
<td></td>
<td>--cacert</td>
</tr>
<tr>
<td>kube-etcd-healthcheck-client</td>
<td>etcd/healthcheck-client.key</td>
<td>etcd/healthcheck-client.crt</td>
<td>etcdctl</td>
<td>--key</td>
<td>--cert</td>
</tr>
</tbody>
</table>
<!--
Same considerations apply for the service account key pair:
-->
<p>注意事项同样适用于服务帐户密钥对：</p>
<table>
<thead>
<tr>
<th>私钥路径</th>
<th>公钥路径</th>
<th>命令</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>sa.key</td>
<td></td>
<td>kube-controller-manager</td>
<td>--service-account-private-key-file</td>
</tr>
<tr>
<td></td>
<td>sa.pub</td>
<td>kube-apiserver</td>
<td>--service-account-key-file</td>
</tr>
</tbody>
</table>
<!--
## Configure certificates for user accounts

You must manually configure these administrator account and service accounts:
-->
<h2 id="为用户帐户配置证书">为用户帐户配置证书</h2>
<p>你必须手动配置以下管理员帐户和服务帐户：</p>
<table>
<thead>
<tr>
<th>文件名</th>
<th>凭据名称</th>
<th>默认 CN</th>
<th>O (位于 Subject 中)</th>
</tr>
</thead>
<tbody>
<tr>
<td>admin.conf</td>
<td>default-admin</td>
<td>kubernetes-admin</td>
<td>system:masters</td>
</tr>
<tr>
<td>kubelet.conf</td>
<td>default-auth</td>
<td>system:node:<code>&lt;nodeName&gt;</code> （参阅注释）</td>
<td>system:nodes</td>
</tr>
<tr>
<td>controller-manager.conf</td>
<td>default-controller-manager</td>
<td>system:kube-controller-manager</td>
<td></td>
</tr>
<tr>
<td>scheduler.conf</td>
<td>default-scheduler</td>
<td>system:kube-scheduler</td>
<td></td>
</tr>
</tbody>
</table>
<!--
The value of `<nodeName>` for `kubelet.conf` **must** match precisely the value of the node name provided by the kubelet as it registers with the apiserver. For further details, read the [Node Authorization](/docs/reference/access-authn-authz/node/).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <code>kubelet.conf</code> 中 <code>&lt;nodeName&gt;</code> 的值 <strong>必须</strong> 与 kubelet 向 apiserver 注册时提供的节点名称的值完全匹配。
有关更多详细信息，请阅读<a href="/zh/docs/reference/access-authn-authz/node/">节点授权</a>。
</div>
<!--
1. For each config, generate an x509 cert/key pair with the given CN and O.

1. Run `kubectl` as follows for each config:
-->
<ol>
<li>
<p>对于每个配置，请都使用给定的 CN 和 O 生成 x509 证书/密钥偶对。</p>
</li>
<li>
<p>为每个配置运行下面的 <code>kubectl</code> 命令：</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>&lt;filename&gt; kubectl config set-cluster default-cluster --server<span style="color:#666">=</span>https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs
<span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>&lt;filename&gt; kubectl config set-credentials &lt;credential-name&gt; --client-key &lt;path-to-key&gt;.pem --client-certificate &lt;path-to-cert&gt;.pem --embed-certs
<span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>&lt;filename&gt; kubectl config set-context default-system --cluster default-cluster --user &lt;credential-name&gt;
<span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>&lt;filename&gt; kubectl config use-context default-system
</code></pre></div><!--
These files are used as follows:

| filename                | command                 | comment                                                               |
|-------------------------|-------------------------|-----------------------------------------------------------------------|
| admin.conf              | kubectl                 | Configures administrator user for the cluster                                      |
| kubelet.conf            | kubelet                 | One required for each node in the cluster.                            |
| controller-manager.conf | kube-controller-manager | Must be added to manifest in `manifests/kube-controller-manager.yaml` |
| scheduler.conf          | kube-scheduler          | Must be added to manifest in `manifests/kube-scheduler.yaml`          |
-->
<p>这些文件用途如下：</p>
<table>
<thead>
<tr>
<th>文件名</th>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>admin.conf</td>
<td>kubectl</td>
<td>配置集群的管理员</td>
</tr>
<tr>
<td>kubelet.conf</td>
<td>kubelet</td>
<td>集群中的每个节点都需要一份</td>
</tr>
<tr>
<td>controller-manager.conf</td>
<td>kube-controller-manager</td>
<td>必需添加到 <code>manifests/kube-controller-manager.yaml</code> 清单中</td>
</tr>
<tr>
<td>scheduler.conf</td>
<td>kube-scheduler</td>
<td>必需添加到 <code>manifests/kube-scheduler.yaml</code> 清单中</td>
</tr>
</tbody>
</table>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-92a61cf5b0575aa3500f7665b68127d1">3.5 - 强制实施 Pod 安全性标准</h1>
    
	<!--
reviewers:
- tallclair
- liggitt
title: Enforcing Pod Security Standards
weight: 40
-->
<!-- overview -->
<!--
This page provides an overview of best practices when it comes to enforcing
[Pod Security Standards](/docs/concepts/security/pod-security-standards).
-->
<p>本页提供实施 <a href="/zh/docs/concepts/security/pod-security-standards">Pod 安全标准（Pod Security Standards）</a>
时的一些最佳实践。</p>
<!-- body -->
<!--
## Using the built-in Pod Security Admission Controller
-->
<h2 id="使用内置的-pod-安全性准入控制器">使用内置的 Pod 安全性准入控制器</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!--
The [Pod Security Admission Controller](/docs/reference/access-authn-authz/admission-controllers/#podsecurity)
intends to replace the deprecated PodSecurityPolicies. 
-->
<p><a href="/zh/docs/reference/access-authn-authz/admission-controllers/#podsecurity">Pod 安全性准入控制器</a>
尝试替换已被废弃的 PodSecurityPolicies。</p>
<!--
### Configure all cluster namespaces
-->
<h3 id="configure-all-cluster-namespaces">配置所有集群名字空间   </h3>
<!--
Namespaces that lack any configuration at all should be considered significant gaps in your cluster
security model. We recommend taking the time to analyze the types of workloads occurring in each
namespace, and by referencing the Pod Security Standards, decide on an appropriate level for
each of them. Unlabeled namespaces should only indicate that they've yet to be evaluated.
-->
<p>完全未经配置的名字空间应该被视为集群安全模型中的重大缺陷。
我们建议花一些时间来分析在每个名字空间中执行的负载的类型，
并通过引用 Pod 安全性标准来确定每个负载的合适级别。
未设置标签的名字空间应该视为尚未被评估。</p>
<!--
In the scenario that all workloads in all namespaces have the same security requirements,
we provide an [example](/docs/concepts/security/pod-security-admission/#applying-to-all-namespaces)
that illustrates how the PodSecurity labels can be applied in bulk.
-->
<p>针对所有名字空间中的所有负载都具有相同的安全性需求的场景，
我们提供了一个<a href="/zh/docs/concepts/security/pod-security-admission/#applying-to-all-namespaces">示例</a>
用来展示如何批量应用 Pod 安全性标签。</p>
<!--
### Embrace the principle of least privilege

In an ideal world, every pod in every namespace would meet the requirements of the `restricted`
policy. However, this is not possible nor practical, as some workloads will require elevated
privileges for legitimate reasons.
-->
<h3 id="拥抱最小特权原则">拥抱最小特权原则</h3>
<p>在一个理想环境中，每个名字空间中的每个 Pod 都会满足 <code>restricted</code> 策略的需求。
不过，这既不可能也不现实，某些负载会因为合理的原因而需要特权上的提升。</p>
<!--
- Namespaces allowing `privileged` workloads should establish and enforce appropriate access controls.
- For workloads running in those permissive namespaces, maintain documentation about their unique
  security requirements. If at all possible, consider how those requirements could be further
  constrained.
-->
<ul>
<li>允许 <code>privileged</code> 负载的名字空间需要建立并实施适当的访问控制机制。</li>
<li>对于运行在特权宽松的名字空间中的负载，需要维护其独特安全性需求的文档。
如果可能的话，要考虑如何进一步约束这些需求。</li>
</ul>
<!--
### Adopt a multi-mode strategy

The `audit` and `warn` modes of the Pod Security Standards admission controller make it easy to
collect important security insights about your pods without breaking existing workloads.
-->
<h3 id="采用多种模式的策略">采用多种模式的策略</h3>
<p>Pod 安全性标准准入控制器的 <code>audit</code> 和 <code>warn</code> 模式（mode）
能够在不影响现有负载的前提下，让该控制器更方便地收集关于 Pod 的重要的安全信息。</p>
<!--
It is good practice to enable these modes for all namespaces, setting them to the _desired_ level
and version you would eventually like to `enforce`. The warnings and audit annotations generated in
this phase can guide you toward that state. If you expect workload authors to make changes to fit
within the desired level, enable the `warn` mode. If you expect to use audit logs to monitor/drive
changes to fit within the desired level, enable the `audit` mode.
-->
<p>针对所有名字空间启用这些模式是一种好的实践，将它们设置为你最终打算 <code>enforce</code> 的
<em>期望的</em> 级别和版本。这一阶段中所生成的警告和审计注解信息可以帮助你到达这一状态。
如果你期望负载的作者能够作出变更以便适应期望的级别，可以启用 <code>warn</code> 模式。
如果你希望使用审计日志了监控和驱动变更，以便负载能够适应期望的级别，可以启用 <code>audit</code> 模式。</p>
<!--
When you have the `enforce` mode set to your desired value, these modes can still be useful in a
few different ways:

- By setting `warn` to the same level as `enforce`, clients will receive warnings when attempting
  to create Pods (or resources that have Pod templates) that do not pass validation. This will help
  them update those resources to become compliant.
- In Namespaces that pin `enforce` to a specific non-latest version, setting the `audit` and `warn`
  modes to the same level as `enforce`, but to the `latest` version, gives visibility into settings
  that were allowed by previous versions but are not allowed per current best practices.
-->
<p>当你将 <code>enforce</code> 模式设置为期望的取值时，这些模式在不同的场合下仍然是有用的：</p>
<ul>
<li>通过将 <code>warn</code> 设置为 <code>enforce</code> 相同的级别，客户可以在尝试创建无法通过合法检查的 Pod
（或者包含 Pod 模板的资源）时收到警告信息。这些信息会帮助于更新资源使其合规。</li>
<li>在将 <code>enforce</code> 锁定到特定的非最新版本的名字空间中，将 <code>audit</code> 和 <code>warn</code>
模式设置为 <code>enforce</code> 一样的级别而非 <code>latest</code> 版本，
这样可以方便看到之前版本所允许但当前最佳实践中被禁止的设置。</li>
</ul>
<!--
## Third-party alternatives
-->
<h2 id="third-party-alternatives">第三方替代方案    </h2>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<!--
Other alternatives for enforcing security profiles are being developed in the Kubernetes
ecosystem:
-->
<p>Kubernetes 生态系统中也有一些其他强制实施安全设置的替代方案处于开发状态中：</p>
<ul>
<li><a href="https://github.com/kubewarden">Kubewarden</a>.</li>
<li><a href="https://kyverno.io/policies/">Kyverno</a>.</li>
<li><a href="https://github.com/open-policy-agent/gatekeeper">OPA Gatekeeper</a>.</li>
</ul>
<!--
The decision to go with a _built-in_ solution (e.g. PodSecurity admission controller) versus a
third-party tool is entirely dependent on your own situation. When evaluating any solution,
trust of your supply chain is crucial. Ultimately, using _any_ of the aforementioned approaches
will be better than doing nothing.
-->
<p>采用 <em>内置的</em> 方案（例如 PodSecurity 准入控制器）还是第三方工具，
这一决策完全取决于你自己的情况。在评估任何解决方案时，对供应链的信任都是至关重要的。
最终，使用前述方案中的 <em>任何</em> 一种都好过放任自流。</p>

</div>



    
	
  

    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      
      <a class="text-white" href="/zh/docs/home/">主页</a>
      
      <a class="text-white" href="/zh/blog/">博客</a>
      
      <a class="text-white" href="/zh/training/">培训</a>
      
      <a class="text-white" href="/zh/partners/">合作伙伴</a>
      
      <a class="text-white" href="/zh/community/">社区</a>
      
      <a class="text-white" href="/zh/case-studies/">案例分析</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" href="https://discuss.kubernetes.io">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/kubernetesio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://youtube.com/kubernetescommunity">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/kubernetes/kubernetes">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://slack.k8s.io">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Contribute" aria-label="Contribute">
    <a class="text-white" target="_blank" href="https://git.k8s.io/community/contributors/guide">
      <i class="fas fa-edit"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" href="https://stackoverflow.com/questions/tagged/kubernetes">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2024 The Kubernetes Authors | Documentation Distributed under <a href="https://git.k8s.io/website/LICENSE" class="light-text">CC BY 4.0</a></small>
        <br/>
        <small class="text-white">Copyright &copy; 2024 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage" class="light-text">Trademark Usage page</a></small>
        <br/>
        <small class="text-white">ICP license: 京ICP备17074266号-3</small>
        
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>










<script src="/js/main.js"></script>






  </body>
</html>
