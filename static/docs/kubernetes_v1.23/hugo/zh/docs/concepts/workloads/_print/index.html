<!doctype html>
<html lang="zh" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>


<link rel="alternate" hreflang="en" href="http://localhost:1313/docs/concepts/workloads/">
<link rel="alternate" hreflang="ko" href="http://localhost:1313/ko/docs/concepts/workloads/">
<link rel="alternate" hreflang="ja" href="http://localhost:1313/ja/docs/concepts/workloads/">
<link rel="alternate" hreflang="fr" href="http://localhost:1313/fr/docs/concepts/workloads/">
<link rel="alternate" hreflang="de" href="http://localhost:1313/de/docs/concepts/workloads/">
<link rel="alternate" hreflang="es" href="http://localhost:1313/es/docs/concepts/workloads/">
<link rel="alternate" hreflang="id" href="http://localhost:1313/id/docs/concepts/workloads/">
<link rel="alternate" hreflang="uk" href="http://localhost:1313/uk/docs/concepts/workloads/">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.87.0" />
<link rel="canonical" type="text/html" href="http://localhost:1313/zh/docs/concepts/workloads/">
<link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="apple-touch-icon" href="/images/kubernetes-192x192.png">
<title>工作负载 | Kubernetes</title><meta property="og:title" content="工作负载" />
<meta property="og:description" content="理解 Pods，Kubernetes 中可部署的最小计算对象，以及辅助它运行它们的高层抽象对象。" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/zh/docs/concepts/workloads/" /><meta property="og:site_name" content="Kubernetes" />

<meta itemprop="name" content="工作负载">
<meta itemprop="description" content="理解 Pods，Kubernetes 中可部署的最小计算对象，以及辅助它运行它们的高层抽象对象。"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="工作负载"/>
<meta name="twitter:description" content="理解 Pods，Kubernetes 中可部署的最小计算对象，以及辅助它运行它们的高层抽象对象。"/>






<link href="/scss/main.css" rel="stylesheet">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "http://localhost:1313/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href="/css/feature-states.css">



<meta name="description" content="理解 Pods，Kubernetes 中可部署的最小计算对象，以及辅助它运行它们的高层抽象对象。">
<meta property="og:description" content="理解 Pods，Kubernetes 中可部署的最小计算对象，以及辅助它运行它们的高层抽象对象。">
<meta name="twitter:description" content="理解 Pods，Kubernetes 中可部署的最小计算对象，以及辅助它运行它们的高层抽象对象。">
<meta property="og:url" content="http://localhost:1313/zh/docs/concepts/workloads/">
<meta property="og:title" content="工作负载">
<meta name="twitter:title" content="工作负载">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">

<script src="/js/script.js"></script>


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/zh/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/zh/docs/" >文档</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/blog/" >Kubernetes 博客</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/training/" >培训</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/partners/" >合作伙伴</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/community/" >社区</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/case-studies/" >案例分析</a>
			</li>
			
			
			
			<li class="nav-item dropdown">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/releases">Release Information</a>
	
	<a class="dropdown-item" href="https://kubernetes.io/zh/docs/concepts/workloads/">v1.23</a>
	
	<a class="dropdown-item" href="https://v1-22.docs.kubernetes.io/zh/docs/concepts/workloads/">v1.22</a>
	
	<a class="dropdown-item" href="https://v1-21.docs.kubernetes.io/zh/docs/concepts/workloads/">v1.21</a>
	
	<a class="dropdown-item" href="https://v1-20.docs.kubernetes.io/zh/docs/concepts/workloads/">v1.20</a>
	
	<a class="dropdown-item" href="https://v1-19.docs.kubernetes.io/zh/docs/concepts/workloads/">v1.19</a>
	
</div>
			</li>
			
			
			<li class="nav-item dropdown">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/concepts/workloads/">English</a>
	
	<a class="dropdown-item" href="/ko/docs/concepts/workloads/">한국어 Korean</a>
	
	<a class="dropdown-item" href="/ja/docs/concepts/workloads/">日本語 Japanese</a>
	
	<a class="dropdown-item" href="/fr/docs/concepts/workloads/">Français</a>
	
	<a class="dropdown-item" href="/de/docs/concepts/workloads/">Deutsch</a>
	
	<a class="dropdown-item" href="/es/docs/concepts/workloads/">Español</a>
	
	<a class="dropdown-item" href="/id/docs/concepts/workloads/">Bahasa Indonesia</a>
	
	<a class="dropdown-item" href="/uk/docs/concepts/workloads/">Українська</a>
	
</div>

			</li>
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/zh/docs/concepts/workloads/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">工作负载</h1>
<div class="lead">理解 Pods，Kubernetes 中可部署的最小计算对象，以及辅助它运行它们的高层抽象对象。</div>




    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-4d68b0ccf9c683e6368ffdcc40c838d4">Pods</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>1.1: <a href="#pg-c3c2b9cf30915ec9d46c147201da3332">Pod 的生命周期</a></li>


    
  
    
    
	
<li>1.2: <a href="#pg-1ccbd4eeded6ab138d98b59175bd557e">Init 容器</a></li>


    
  
    
    
	
<li>1.3: <a href="#pg-c8d62295ca703fdcef1aaf89fb4c916a">Pod 拓扑分布约束</a></li>


    
  
    
    
	
<li>1.4: <a href="#pg-4aaf43c715cd764bc8ed4436f3537e68">干扰（Disruptions）</a></li>


    
  
    
    
	
<li>1.5: <a href="#pg-53a1005011e1bda2ce81819aad7c8b32">临时容器</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2: <a href="#pg-89637410cacae45a36ab1cc278c482eb">工作负载资源</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1: <a href="#pg-a2dc0393e0c4079e1c504b6429844e86">Deployments</a></li>


    
  
    
    
	
<li>2.2: <a href="#pg-d459b930218774655fa7fd1620625539">ReplicaSet</a></li>


    
  
    
    
	
<li>2.3: <a href="#pg-6d72299952c37ca8cc61b416e5bdbcd4">StatefulSets</a></li>


    
  
    
    
	
<li>2.4: <a href="#pg-41600eb8b6631c88848156f381e9d588">DaemonSet</a></li>


    
  
    
    
	
<li>2.5: <a href="#pg-cc7cc3c4907039d9f863162e20bfbbef">Jobs</a></li>


    
  
    
    
	
<li>2.6: <a href="#pg-4de50a37ebb6f2340484192126cb7a04">已完成 Job 的自动清理</a></li>


    
  
    
    
	
<li>2.7: <a href="#pg-2e4cec01c525b45eccd6010e21cc76d9">CronJob</a></li>


    
  
    
    
	
<li>2.8: <a href="#pg-27f1331d515d95f76aa1156088b4ad91">ReplicationController</a></li>


    
  

    </ul>
    
  

    </ul>


<div class="content">
      <!--
title: "Workloads"
weight: 50
description: >
  Understand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.
no_list: true
-->
工作负载是在 Kubernetes 上运行的应用程序。
<!--
Whether your workload is a single component or several that work together, on Kubernetes you run
it inside a set of [Pods](/docs/concepts/workloads/pods).
In Kubernetes, a Pod represents a set of running
<a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='containers'>containers</a> on your cluster.

A Pod has a defined lifecycle. For example, once a Pod is running in your cluster then
a critical failure on the <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a> where that
Pod is running means that all the Pods on that node fail. Kubernetes treats that level
of failure as final: you would need to create a new Pod even if the node later recovers.
-->
<p>无论你的负载是单一组件还是由多个一同工作的组件构成，在 Kubernetes 中你
可以在一组 <a href="/zh/docs/concepts/workloads/pods">Pods</a> 中运行它。
在 Kubernetes 中，Pod 代表的是集群上处于运行状态的一组
<a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='容器'>容器</a>。</p>
<!--
Kubernetes pods have a [defined lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/).
For example, once a pod is running in your cluster then a critical fault on the
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a> where that pod is running means that
all the pods on that node fail. Kubernetes treats that level of failure as final: you
would need to create a new `Pod` to recover, even if the node later becomes healthy.
-->
<p>Kubernetes Pods 有<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">确定的生命周期</a>。
例如，当某 Pod 在你的集群中运行时，Pod 运行所在的
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a> 出现致命错误时，
所有该节点上的 Pods 都会失败。Kubernetes 将这类失败视为最终状态：
即使该节点后来恢复正常运行，你也需要创建新的 Pod 来恢复应用。</p>
<!--
However, to make life considerably easier, you don't need to manage each Pod directly.
Instead, you can use _workload resources_ that manage a set of Pods on your behalf.
These resources configure <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controllers'>controllers</a>
that make sure the right number of the right kind of Pod are running, to match the state
you specified.

Kubernetes provides several built-in workload resources:
-->
<p>不过，为了让用户的日子略微好过一些，你并不需要直接管理每个 Pod。
相反，你可以使用 <em>负载资源</em> 来替你管理一组 Pods。
这些资源配置 <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>
来确保合适类型的、处于运行状态的 Pod 个数是正确的，与你所指定的状态相一致。</p>
<p>Kubernetes 提供若干种内置的工作负载资源：</p>
<!--
* [Deployment](/docs/concepts/workloads/controllers/deployment/) and [ReplicaSet](/docs/concepts/workloads/controllers/replicaset/)
  (replacing the legacy resource
  <a class='glossary-tooltip' title='一种管理多副本应用的（已启用）的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-replication-controller' target='_blank' aria-label='ReplicationController'>ReplicationController</a>).
  `Deployment` is a good fit for managing a stateless application workload on your cluster,
  where any `Pod` in the `Deployment` is interchangeable and can be replaced if needed.
* [`StatefulSet`](/docs/concepts/workloads/controllers/statefulset/) lets you
  run one or more related Pods that do track state somehow. For example, if your workload
  records data persistently, you can run a `StatefulSet` that matches each `Pod` with a
  [`PersistentVolume`](/docs/concepts/storage/persistent-volumes/). Your code, running in the
  `Pods` for that `StatefulSet`, can replicate data to other `Pods` in the same `StatefulSet`
  to improve overall resilience.
-->
<ul>
<li><a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a> 和
<a href="/zh/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>
（替换原来的资源 <a class='glossary-tooltip' title='一种管理多副本应用的（已启用）的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-replication-controller' target='_blank' aria-label='ReplicationController'>ReplicationController</a>）。
<code>Deployment</code> 很适合用来管理你的集群上的无状态应用，<code>Deployment</code> 中的所有
<code>Pod</code> 都是相互等价的，并且在需要的时候被换掉。</li>
<li><a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>
让你能够运行一个或者多个以某种方式跟踪应用状态的 Pods。
例如，如果你的负载会将数据作持久存储，你可以运行一个 <code>StatefulSet</code>，将每个
<code>Pod</code> 与某个 <a href="/zh/docs/concepts/storage/persistent-volumes/"><code>PersistentVolume</code></a>
对应起来。你在 <code>StatefulSet</code> 中各个 <code>Pod</code> 内运行的代码可以将数据复制到同一
<code>StatefulSet</code> 中的其它 <code>Pod</code> 中以提高整体的服务可靠性。</li>
</ul>
<!--
* [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) defines `Pods` that provide
  node-local facilities. These might be fundamental to the operation of your cluster, such
  as a networking helper tool, or be part of an
  <a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='add-on'>add-on</a>.
  Every time you add a node to your cluster that matches the specification in a `DaemonSet`,
  the control plane schedules a `Pod` for that `DaemonSet` onto the new node.
* [`Job`](/docs/concepts/workloads/controllers/job/) and
  [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/)
  define tasks that run to completion and then stop. Jobs represent one-off tasks, whereas
  `CronJobs` recur according to a schedule.
-->
<ul>
<li><a href="/zh/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>
定义提供节点本地支撑设施的 <code>Pods</code>。这些 Pods 可能对于你的集群的运维是
非常重要的，例如作为网络链接的辅助工具或者作为网络
<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='插件'>插件</a>
的一部分等等。每次你向集群中添加一个新节点时，如果该节点与某 <code>DaemonSet</code>
的规约匹配，则控制面会为该 <code>DaemonSet</code> 调度一个 <code>Pod</code> 到该新节点上运行。</li>
<li><a href="/zh/docs/concepts/workloads/controllers/job/">Job</a> 和
<a href="/zh/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a>。
定义一些一直运行到结束并停止的任务。<code>Job</code> 用来表达的是一次性的任务，而
<code>CronJob</code> 会根据其时间规划反复运行。</li>
</ul>
<!--
In the wider Kubernetes ecosystem, you can find third-party workload resources that provide
additional behaviors. Using a
[custom resource definition](/docs/concepts/extend-kubernetes/api-extension/custom-resources/),
you can add in a third-party workload resource if you want a specific behavior that's not part
of Kubernetes' core. For example, if you wanted to run a group of `Pods` for your application but
stop work unless _all_ the Pods are available (perhaps for some high-throughput distributed task),
then you can implement or install an extension that does provide that feature.
-->
<p>在庞大的 Kubernetes 生态系统中，你还可以找到一些提供额外操作的第三方
工作负载资源。通过使用
<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">定制资源定义（CRD）</a>，
你可以添加第三方工作负载资源，以完成原本不是 Kubernetes 核心功能的工作。
例如，如果你希望运行一组 <code>Pods</code>，但要求所有 Pods 都可用时才执行操作
（比如针对某种高吞吐量的分布式任务），你可以实现一个能够满足这一需求
的扩展，并将其安装到集群中运行。</p>
<h2 id="what-s-next">What's next</h2>
<!--
As well as reading about each resource, you can learn about specific tasks that relate to them:

* [Run a stateless application using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/)
* Run a stateful application either as a [single instance](/docs/tasks/run-application/run-single-instance-stateful-application/)
  or as a [replicated set](/docs/tasks/run-application/run-replicated-stateful-application/)
* [Run Automated Tasks with a `CronJob`](/docs/tasks/job/automated-tasks-with-cron-jobs/)
-->
<p>除了阅读了解每类资源外，你还可以了解与这些资源相关的任务：</p>
<ul>
<li><a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">使用 Deployment 运行一个无状态的应用</a></li>
<li>以<a href="/zh/docs/tasks/run-application/run-single-instance-stateful-application/">单实例</a>
或者<a href="/zh/docs/tasks/run-application/run-replicated-stateful-application/">多副本集合</a>
的形式运行有状态的应用；</li>
<li><a href="/zh/docs/tasks/job/automated-tasks-with-cron-jobs/">使用 <code>CronJob</code> 运行自动化的任务</a></li>
</ul>
<!--
To learn about Kubernetes' mechanisms for separating code from configuration,
visit [Configuration](/docs/concepts/configuration/).
-->
<p>要了解 Kubernetes 将代码与配置分离的实现机制，可参阅
<a href="/zh/docs/concepts/configuration/">配置部分</a>。</p>
<!--
There are two supporting concepts that provide backgrounds about how Kubernetes manages pods
for applications:
* [Garbage collection](/docs/concepts/workloads/controllers/garbage-collection/) tidies up objects
  from your cluster after their _owning resource_ has been removed.
* The [_time-to-live after finished_ controller](/docs/concepts/workloads/controllers/ttlafterfinished/)
  removes Jobs once a defined time has passed since they completed.
-->
<p>关于 Kubernetes 如何为应用管理 Pods，还有两个支撑概念能够提供相关背景信息：</p>
<ul>
<li><a href="/zh/docs/concepts/workloads/controllers/garbage-collection/">垃圾收集</a>机制负责在
对象的 <em>属主资源</em> 被删除时在集群中清理这些对象。</li>
<li><a href="/zh/docs/concepts/workloads/controllers/ttlafterfinished/"><em>Time-to-Live</em> 控制器</a>
会在 Job 结束之后的指定时间间隔之后删除它们。</li>
</ul>
<!--
Once your application is running, you might want to make it available on the internet as
a [`Service`](/docs/concepts/services-networking/service/) or, for web application only,
using an [`Ingress`](/docs/concepts/services-networking/ingress).
-->
<p>一旦你的应用处于运行状态，你就可能想要以
<a href="/zh/docs/concepts/services-networking/service/"><code>Service</code></a>
的形式使之可在互联网上访问；或者对于 Web 应用而言，使用
<a href="/zh/docs/concepts/services-networking/ingress"><code>Ingress</code></a> 资源将其暴露到互联网上。</p>

</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4d68b0ccf9c683e6368ffdcc40c838d4">1 - Pods</h1>
    
	<!--
reviewers:
- erictune
title: Pods
content_type: concept
weight: 10
no_list: true
card:
  name: concepts
  weight: 60
-->
<!-- overview -->
<!--
_Pods_ are the smallest deployable units of computing that you can create and manage in Kubernetes.

A _Pod_ (as in a pod of whales or pea pod) is a group of one or more
<a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='containers'>containers</a>
with shared storage and network resources, and a specification
for how to run the containers. A Pod's contents are always co-located and
co-scheduled, and run in a shared context. A Pod models an
application-specific "logical host": it contains one or more application
containers which are relatively tightly coupled. 
In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.
-->
<p><em>Pod</em> 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。</p>
<p><em>Pod</em> （就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个）
<a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='容器'>容器</a>；
这些容器共享存储、网络、以及怎样运行这些容器的声明。
Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。
Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器，
这些容器是相对紧密的耦合在一起的。
在非云环境中，在相同的物理机或虚拟机上运行的应用类似于
在同一逻辑主机上运行的云应用。</p>
<!--
As well as application containers, a Pod can contain
[init containers](/docs/concepts/workloads/pods/init-containers/) that run
during Pod startup. You can also inject
[ephemeral containers](/docs/concepts/workloads/pods/ephemeral-containers/)
for debugging if your cluster offers this.
-->
<p>除了应用容器，Pod 还可以包含在 Pod 启动期间运行的
<a href="/zh/docs/concepts/workloads/pods/init-containers/">Init 容器</a>。
你也可以在集群中支持<a href="/zh/docs/concepts/workloads/pods/ephemeral-containers/">临时性容器</a>
的情况下，为调试的目的注入临时性容器。</p>
<!-- body -->
<h2 id="what-is-a-pod">什么是 Pod？  </h2>
<!--
While Kubernetes supports more
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtimes'>container runtimes</a>
than just Docker, [Docker](https://www.docker.com/) is the most commonly known
runtime, and it helps to describe Pods using some terminology from Docker.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 除了 Docker 之外，Kubernetes 支持
很多其他<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>，
<a href="https://www.docker.com/">Docker</a> 是最有名的运行时，
使用 Docker 的术语来描述 Pod 会很有帮助。
</div>
<!--
The shared context of a Pod is a set of Linux namespaces, cgroups, and
potentially other facets of isolation - the same things that isolate a Docker
container.  Within a Pod's context, the individual applications may have
further sub-isolations applied.

In terms of Docker concepts, a Pod is similar to a group of Docker containers
with shared namespaces and shared filesystem volumes.
-->
<p>Pod 的共享上下文包括一组 Linux 名字空间、控制组（cgroup）和可能一些其他的隔离
方面，即用来隔离 Docker 容器的技术。
在 Pod 的上下文中，每个独立的应用可能会进一步实施隔离。</p>
<p>就 Docker 概念的术语而言，Pod 类似于共享名字空间和文件系统卷的一组 Docker
容器。</p>
<!--
## Using Pods

The following is an example of a Pod which consists of a container running the image `nginx:1.14.2`.



 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/simple-pod.yaml" download="pods/simple-pod.yaml"><code>pods/simple-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-simple-pod-yaml')" title="Copy pods/simple-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-simple-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>



To create the Pod shown above, run the following command:
-->
<h2 id="using-pods">使用 Pod  </h2>
<p>下面是一个 Pod 示例，它由一个运行镜像 <code>nginx:1.14.2</code> 的容器组成。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/simple-pod.yaml" download="pods/simple-pod.yaml"><code>pods/simple-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-simple-pod-yaml')" title="Copy pods/simple-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-simple-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<p>要创建上面显示的 Pod，请运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml
</code></pre></div><!--
Pods are generally not created directly and are created using workload resources.
See [Working with Pods](#working-with-pods) for more information on how Pods are used
with workload resources.

### Workload resources for managing pods
-->
<p>Pod 通常不是直接创建的，而是使用工作负载资源创建的。
有关如何将 Pod 用于工作负载资源的更多信息，请参阅 <a href="#working-with-pods">使用 Pod</a>。</p>
<h3 id="用于管理-pod-的工作负载资源">用于管理 pod 的工作负载资源</h3>
<!--
Usually you don't need to create Pods directly, even singleton Pods. 
Instead, create them using workload resources such as <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a> or <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a>.
If your Pods need to track state, consider the 
<a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a> resource.

Pods in a Kubernetes cluster are used in two main ways:
-->
<p>通常你不需要直接创建 Pod，甚至单实例 Pod。
相反，你会使用诸如
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a> 或
<a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a> 这类工作负载资源
来创建 Pod。如果 Pod 需要跟踪状态，
可以考虑 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>
资源。</p>
<p>Kubernetes 集群中的 Pod 主要有两种用法：</p>
<!--
* **Pods that run a single container**. The "one-container-per-Pod" model is the
  most common Kubernetes use case; in this case, you can think of a Pod as a
  wrapper around a single container; Kubernetes manages Pods rather than managing
  the containers directly.
* **Pods that run multiple containers that need to work together**. A Pod can
  encapsulate an application composed of multiple co-located containers that are
  tightly coupled and need to share resources. These co-located containers
  form a single cohesive unit of service—for example, one container serving data
  stored in a shared volume to the public, while a separate _sidecar_ container
  refreshes or updates those files.  
  The Pod wraps these containers, storage resources, and an ephemeral network
  identity together as a single unit.

  Grouping multiple co-located and co-managed containers in a single Pod is a
  relatively advanced use case. You should use this pattern only in specific
  instances in which your containers are tightly coupled.
-->
<ul>
<li>
<p><strong>运行单个容器的 Pod</strong>。&quot;每个 Pod 一个容器&quot;模型是最常见的 Kubernetes 用例；
在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。</p>
</li>
<li>
<p><strong>运行多个协同工作的容器的 Pod</strong>。
Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。
这些位于同一位置的容器可能形成单个内聚的服务单元 —— 一个容器将文件从共享卷提供给公众，
而另一个单独的“边车”（sidecar）容器则刷新或更新这些文件。
Pod 将这些容器和存储资源打包为一个可管理的实体。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 将多个并置、同管的容器组织到一个 Pod 中是一种相对高级的使用场景。
只有在一些场景中，容器之间紧密关联时你才应该使用这种模式。
</div>
</li>
</ul>
<!--
Each Pod is meant to run a single instance of a given application. If you want to
scale your application horizontally (to provide more overall resources by running
more instances), you should use multiple Pods, one for each instance. In
Kubernetes, this is typically referred to as _replication_.
Replicated Pods are usually created and managed as a group by a workload resource
and its <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a>.

See [Pods and controllers](#pods-and-controllers) for more information on how
Kubernetes uses workload resources, and their controllers, to implement application
scaling and auto-healing.
-->
<p>每个 Pod 都旨在运行给定应用程序的单个实例。如果希望横向扩展应用程序（例如，运行多个实例
以提供更多的资源），则应该使用多个 Pod，每个实例使用一个 Pod。
在 Kubernetes 中，这通常被称为 <em>副本（Replication）</em>。
通常使用一种工作负载资源及其<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>
来创建和管理一组 Pod 副本。</p>
<p>参见 <a href="#pods-and-controllers">Pod 和控制器</a>以了解 Kubernetes
如何使用工作负载资源及其控制器以实现应用的扩缩和自动修复。</p>
<!--
### How Pods manage multiple containers

Pods are designed to support multiple cooperating processes (as containers) that form
a cohesive unit of service. The containers in a Pod are automatically co-located and
co-scheduled on the same physical or virtual machine in the cluster. The containers
can share resources and dependencies, communicate with one another, and coordinate
when and how they are terminated.
-->
<h3 id="pod-怎样管理多个容器">Pod 怎样管理多个容器</h3>
<p>Pod 被设计成支持形成内聚服务单元的多个协作过程（形式为容器）。
Pod 中的容器被自动安排到集群中的同一物理机或虚拟机上，并可以一起进行调度。
容器之间可以共享资源和依赖、彼此通信、协调何时以及何种方式终止自身。</p>
<!--
For example, you might have a container that
acts as a web server for files in a shared volume, and a separate "sidecar" container
that updates those files from a remote source, as in the following diagram:
-->
<p>例如，你可能有一个容器，为共享卷中的文件提供 Web 服务器支持，以及一个单独的
&quot;边车 (sidercar)&quot; 容器负责从远端更新这些文件，如下图所示：</p>

<figure class="diagram-medium">
    <img src="/images/docs/pod.svg"
         alt="Pod creation diagram"/> 
</figure>

<!--
Some Pods have <a class='glossary-tooltip' title='应用容器运行前必须先运行完成的一个或多个初始化容器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-init-container' target='_blank' aria-label='init containers'>init containers</a>
as well as <a class='glossary-tooltip' title='用于运行部分工作负载的容器。与初始化容器比较而言。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-app-container' target='_blank' aria-label='app containers'>app containers</a>.
Init containers run and complete before the app containers are started.

Pods natively provide two kinds of shared resources for their constituent containers:
[networking](#pod-networking) and [storage](#pod-storage).
-->
<p>有些 Pod 具有 <a class='glossary-tooltip' title='应用容器运行前必须先运行完成的一个或多个初始化容器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-init-container' target='_blank' aria-label='Init 容器'>Init 容器</a> 和
<a class='glossary-tooltip' title='用于运行部分工作负载的容器。与初始化容器比较而言。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-app-container' target='_blank' aria-label='应用容器'>应用容器</a>。
Init 容器会在启动应用容器之前运行并完成。</p>
<p>Pod 天生地为其成员容器提供了两种共享资源：<a href="#pod-networking">网络</a>和
<a href="#pod-storage">存储</a>。</p>
<!--
## Working with Pods

You'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This
is because Pods are designed as relatively ephemeral, disposable entities. When
a Pod gets created (directly by you, or indirectly by a
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a>), the new Pod is
scheduled to run on a <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点（Node）'>节点（Node）</a> in your cluster.
The Pod remains on that node until the Pod finishes execution, the Pod object is deleted,
the Pod is *evicted* for lack of resources, or the node fails.
-->
<h2 id="working-with-pods">使用 Pod  </h2>
<p>你很少在 Kubernetes 中直接创建一个个的 Pod，甚至是单实例（Singleton）的 Pod。
这是因为 Pod 被设计成了相对临时性的、用后即抛的一次性实体。
当 Pod 由你或者间接地由 <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>
创建时，它被调度在集群中的<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>上运行。
Pod 会保持在该节点上运行，直到 Pod 结束执行、Pod 对象被删除、Pod 因资源不足而被
<em>驱逐</em> 或者节点失效为止。</p>
<!--
Restarting a container in a Pod should not be confused with restarting a Pod. A Pod
is not a process, but an environment for running container(s). A Pod persists until
it is deleted.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 重启 Pod 中的容器不应与重启 Pod 混淆。
Pod 不是进程，而是容器运行的环境。
在被删除之前，Pod 会一直存在。
</div>
<!--
When you create the manifest for a Pod object, make sure the name specified is a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>当你为 Pod 对象创建清单时，要确保所指定的 Pod 名称是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
### Pods and controllers

You can use workload resources to create and manage multiple Pods for you. A controller
for the resource handles replication and rollout and automatic healing in case of
Pod failure. For example, if a Node fails, a controller notices that Pods on that
Node have stopped working and creates a replacement Pod. The scheduler places the
replacement Pod onto a healthy Node.

Here are some examples of workload resources that manage one or more Pods:
-->
<h3 id="pods-and-controllers">Pod 和控制器   </h3>
<p>你可以使用工作负载资源来创建和管理多个 Pod。
资源的控制器能够处理副本的管理、上线，并在 Pod 失效时提供自愈能力。
例如，如果一个节点失败，控制器注意到该节点上的 Pod 已经停止工作，
就可以创建替换性的 Pod。调度器会将替身 Pod 调度到一个健康的节点执行。</p>
<p>下面是一些管理一个或者多个 Pod 的工作负载资源的示例：</p>
<ul>
<li><a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a></li>
<li><a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a></li>
<li><a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a></li>
</ul>
<!--
### Pod templates

Controllers for <a class='glossary-tooltip' title='工作负载是在 Kubernetes 上运行的应用程序。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/' target='_blank' aria-label='workload'>workload</a> resources create Pods
from a _pod template_ and manage those Pods on your behalf.

PodTemplates are specifications for creating Pods, and are included in workload resources such as
[Deployments](/docs/concepts/workloads/controllers/deployment/),
[Jobs](/docs/concepts/workloads/controllers/job/), and
[DaemonSets](/docs/concepts/workloads/controllers/daemonset/).
-->
<h3 id="pod-templates">Pod 模版   </h3>
<p><a class='glossary-tooltip' title='工作负载是在 Kubernetes 上运行的应用程序。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/' target='_blank' aria-label='负载'>负载</a>资源的控制器通常使用
<em>Pod 模板（Pod Template）</em> 来替你创建 Pod 并管理它们。</p>
<p>Pod 模板是包含在工作负载对象中的规范，用来创建 Pod。这类负载资源包括
<a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a>、
<a href="/zh/docs/concepts/workloads/controllers/job/">Job</a> 和
<a href="/zh/docs/concepts/workloads/controllers/daemonset/">DaemonSets</a> 等。</p>
<!--
Each controller for a workload resource uses the `PodTemplate` inside the workload
object to make actual Pods. The `PodTemplate` is part of the desired state of whatever
workload resource you used to run your app.

The sample below is a manifest for a simple Job with a `template` that starts one
container. The container in that Pod prints a message then pauses.
-->
<p>工作负载的控制器会使用负载对象中的 <code>PodTemplate</code> 来生成实际的 Pod。
<code>PodTemplate</code> 是你用来运行应用时指定的负载资源的目标状态的一部分。</p>
<p>下面的示例是一个简单的 Job 的清单，其中的 <code>template</code> 指示启动一个容器。
该 Pod 中的容器会打印一条消息之后暂停。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 这里是 Pod 模版</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#39;sh&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;-c&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;echo &#34;Hello, Kubernetes!&#34; &amp;&amp; sleep 3600&#39;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 以上为 Pod 模版</span><span style="color:#bbb">
</span></code></pre></div><!--
Modifying the pod template or switching to a new pod template has no effect on the
Pods that already exist. Pods do not receive template updates directly. Instead,
a new Pod is created to match the revised pod template.

For example, the deployment controller ensures that the running Pods match the current
pod template for each Deployment object. If the template is updated, the Deployment has
to remove the existing Pods and create new Pods based on the updated template. Each workload
resource implements its own rules for handling changes to the Pod template.
-->
<p>修改 Pod 模版或者切换到新的 Pod 模版都不会对已经存在的 Pod 起作用。
Pod 不会直接收到模版的更新。相反，
新的 Pod 会被创建出来，与更改后的 Pod 模版匹配。</p>
<p>例如，Deployment 控制器针对每个 Deployment 对象确保运行中的 Pod 与当前的 Pod
模版匹配。如果模版被更新，则 Deployment 必须删除现有的 Pod，基于更新后的模版
创建新的 Pod。每个工作负载资源都实现了自己的规则，用来处理对 Pod 模版的更新。</p>
<!--
On Nodes, the <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> does not
directly observe or manage any of the details around pod templates and updates; those
details are abstracted away. That abstraction and separation of concerns simplifies
system semantics, and makes it feasible to extend the cluster's behavior without
changing existing code.
-->
<p>在节点上，<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 并不直接监测
或管理与 Pod 模版相关的细节或模版的更新，这些细节都被抽象出来。
这种抽象和关注点分离简化了整个系统的语义，并且使得用户可以在不改变现有代码的
前提下就能扩展集群的行为。</p>
<!--
## Pod update and replacement

As mentioned in the previous section, when the Pod template for a workload
resource is changed, the controller creates new Pods based on the updated
template instead of updating or patching the existing Pods.
-->
<h2 id="pod-update-and-replacement">Pod 更新与替换  </h2>
<p>正如前面章节所述，当某工作负载的 Pod 模板被改变时，控制器会基于更新的模板
创建新的 Pod 对象而不是对现有 Pod 执行更新或者修补操作。</p>
<!--
Kubernetes doesn't prevent you from managing Pods directly. It is possible to
update some fields of a running Pod, in place. However, Pod update operations
like 
[`patch`](/docs/reference/generated/kubernetes-api/v1.23/#patch-pod-v1-core), and
[`replace`](/docs/reference/generated/kubernetes-api/v1.23/#replace-pod-v1-core)
have some limitations:
-->
<p>Kubernetes 并不禁止你直接管理 Pod。对运行中的 Pod 的某些字段执行就地更新操作
还是可能的。不过，类似
<a href="/docs/reference/generated/kubernetes-api/v1.23/#patch-pod-v1-core"><code>patch</code></a> 和
<a href="/docs/reference/generated/kubernetes-api/v1.23/#replace-pod-v1-core"><code>replace</code></a>
这类更新操作有一些限制：</p>
<!--
- Most of the metadata about a Pod is immutable. For example, you cannot
  change the `namespace`, `name`, `uid`, or `creationTimestamp` fields;
  the `generation` field is unique. It only accepts updates that increment the
  field's current value.
- If the `metadata.deletionTimestamp` is set, no new entry can be added to the
  `metadata.finalizers` list.
- Pod updates may not change fields other than `spec.containers[*].image`,
  `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or
  `spec.tolerations`. For `spec.tolerations`, you can only add new entries.
- When updating the `spec.activeDeadlineSeconds` field, two types of updates
  are allowed:

  1. setting the unassigned field to a positive number; 
  1. updating the field from a positive number to a smaller, non-negative
     number.
-->
<ul>
<li>
<p>Pod 的绝大多数元数据都是不可变的。例如，你不可以改变其 <code>namespace</code>、<code>name</code>、
<code>uid</code> 或者 <code>creationTimestamp</code> 字段；<code>generation</code> 字段是比较特别的，如果更新
该字段，只能增加字段取值而不能减少。</p>
</li>
<li>
<p>如果 <code>metadata.deletionTimestamp</code> 已经被设置，则不可以向 <code>metadata.finalizers</code>
列表中添加新的条目。</p>
</li>
<li>
<p>Pod 更新不可以改变除 <code>spec.containers[*].image</code>、<code>spec.initContainers[*].image</code>、
<code>spec.activeDeadlineSeconds</code> 或 <code>spec.tolerations</code> 之外的字段。
对于 <code>spec.tolerations</code>，你只被允许添加新的条目到其中。</p>
</li>
<li>
<p>在更新<code>spec.activeDeadlineSeconds</code> 字段时，以下两种更新操作是被允许的：</p>
<ol>
<li>如果该字段尚未设置，可以将其设置为一个正数；</li>
<li>如果该字段已经设置为一个正数，可以将其设置为一个更小的、非负的整数。</li>
</ol>
</li>
</ul>
<!--
## Resource sharing and communication

Pods enable data sharing and communication among their constituent
containters.
-->
<h3 id="resource-sharing-and-communication">资源共享和通信</h3>
<p>Pod 使它的成员容器间能够进行数据共享和通信。</p>
<!--
### Storage in Pods {#pod-storage}

A Pod can specify a set of shared storage
<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='volumes'>volumes</a>. All containers
in the Pod can access the shared volumes, allowing those containers to
share data. Volumes also allow persistent data in a Pod to survive
in case one of the containers within needs to be restarted. See
[Storage](/docs/concepts/storage/) for more information on how
Kubernetes implements shared storage and makes it available to Pods.
-->
<h3 id="pod-storage">Pod 中的存储</h3>
<p>一个 Pod 可以设置一组共享的存储<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>。
Pod 中的所有容器都可以访问该共享卷，从而允许这些容器共享数据。
卷还允许 Pod 中的持久数据保留下来，即使其中的容器需要重新启动。
有关 Kubernetes 如何在 Pod 中实现共享存储并将其提供给 Pod 的更多信息，
请参考<a href="/zh/docs/concepts/storage/">卷</a>。</p>
<!--
### Pod networking

Each Pod is assigned a unique IP address for each address family. Every
container in a Pod shares the network namespace, including the IP address and
network ports. Inside a Pod (and **only** then), the containers that belong to the Pod
can communicate with one another using `localhost`. When containers in a Pod communicate
with entities *outside the Pod*,
they must coordinate how they use the shared network resources (such as ports).
-->
<h3 id="pod-networking">Pod 联网   </h3>
<p>每个 Pod 都在每个地址族中获得一个唯一的 IP 地址。
Pod 中的每个容器共享网络名字空间，包括 IP 地址和网络端口。
<em>Pod 内</em> 的容器可以使用 <code>localhost</code> 互相通信。
当 Pod 中的容器与 <em>Pod 之外</em> 的实体通信时，它们必须协调如何使用共享的网络资源
（例如端口）。</p>
<!--
Within a Pod, containers share an IP address and port space, and
can find each other via `localhost`. The containers in a Pod can also communicate
with each other using standard inter-process communications like SystemV semaphores
or POSIX shared memory.  Containers in different Pods have distinct IP addresses
and can not communicate by IPC without
and can not communicate by OS-level IPC without special configuration.
Containers that want to interact with a container running in a different Pod can
use IP networking to communicate.
-->
<p>在同一个 Pod 内，所有容器共享一个 IP 地址和端口空间，并且可以通过 <code>localhost</code> 发现对方。
他们也能通过如 SystemV 信号量或 POSIX 共享内存这类标准的进程间通信方式互相通信。
不同 Pod 中的容器的 IP 地址互不相同，没有特殊配置，无法通过 OS 级 IPC 进行通信就不能使用 IPC 进行通信。
如果某容器希望与运行于其他 Pod 中的容器通信，可以通过 IP 联网的方式实现。</p>
<!--
Containers within the Pod see the system hostname as being the same as the configured
`name` for the Pod. There's more about this in the [networking](/docs/concepts/cluster-administration/networking/)
section.
-->
<p>Pod 中的容器所看到的系统主机名与为 Pod 配置的 <code>name</code> 属性值相同。
<a href="/zh/docs/concepts/cluster-administration/networking/">网络</a>部分提供了更多有关此内容的信息。</p>
<!--
## Privileged mode for containers

In Linux, any container in a Pod can enable privileged mode using the `privileged` (Linux) flag on the [security context](/docs/tasks/configure-pod-container/security-context/) of the container spec. This is useful for containers that want to use operating system administrative capabilities such as manipulating the network stack or accessing hardware devices.

If your cluster has the `WindowsHostProcessContainers` feature enabled, you can create a [Windows HostProcess pod](/docs/tasks/configure-pod-container/create-hostprocess-pod) by setting the `windowsOptions.hostProcess` flag on the security context of the pod spec. All containers in these pods must run as Windows HostProcess containers. HostProcess pods run directly on the host and can also be used to perform administrative tasks as is done with Linux privileged containers.
-->
<h2 id="privileged-mode-for-containers">容器的特权模式    </h2>
<p>在 Linux 中，Pod 中的任何容器都可以使用容器规约中的
<a href="/zh/docs/tasks/configure-pod-container/security-context/">安全性上下文</a>中的
<code>privileged</code>（Linux）参数启用特权模式。
这对于想要使用操作系统管理权能（Capabilities，如操纵网络堆栈和访问设备）
的容器很有用。</p>
<p>如果你的集群启用了 <code>WindowsHostProcessContainers</code> 特性，你可以使用 Pod 规约中安全上下文的
<code>windowsOptions.hostProcess</code> 参数来创建
<a href="/zh/docs/tasks/configure-pod-container/create-hostprocess-pod/">Windows HostProcess Pod</a>。
这些 Pod 中的所有容器都必须以 Windows HostProcess 容器方式运行。
HostProcess Pod 可以直接运行在主机上，它也能像 Linux 特权容器一样，用于执行管理任务。</p>
<!--
Your <a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a> must support the concept of a privileged container for this setting to be relevant.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你的<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>必须支持
特权容器的概念才能使用这一配置。
</div>
<!--
## Static Pods

_Static Pods_ are managed directly by the kubelet daemon on a specific node,
without the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a>
observing them.
Whereas most Pods are managed by the control plane (for example, a
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>), for static
Pods, the kubelet directly supervises each static Pod (and restarts it if it fails).
-->
<h2 id="static-pods">静态 Pod   </h2>
<p><em>静态 Pod（Static Pod）</em> 直接由特定节点上的 <code>kubelet</code> 守护进程管理，
不需要<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>看到它们。
尽管大多数 Pod 都是通过控制面（例如，<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>）
来管理的，对于静态 Pod 而言，<code>kubelet</code> 直接监控每个 Pod，并在其失效时重启之。</p>
<!--
Static Pods are always bound to one <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='Kubelet'>Kubelet</a> on a specific node.
The main use for static Pods is to run a self-hosted control plane: in other words,
using the kubelet to supervise the individual [control plane components](/docs/concepts/overview/components/#control-plane-components).

The kubelet automatically tries to create a <a class='glossary-tooltip' title='API 服务器中的一个对象，用于跟踪 kubelet 上的静态 pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-mirror-pod' target='_blank' aria-label='mirror Pod'>mirror Pod</a>
on the Kubernetes API server for each static Pod.
This means that the Pods running on a node are visible on the API server,
but cannot be controlled from there.
-->
<p>静态 Pod 通常绑定到某个节点上的 <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>。
其主要用途是运行自托管的控制面。
在自托管场景中，使用 <code>kubelet</code> 来管理各个独立的
<a href="/zh/docs/concepts/overview/components/#control-plane-components">控制面组件</a>。</p>
<p><code>kubelet</code> 自动尝试为每个静态 Pod 在 Kubernetes API 服务器上创建一个
<a class='glossary-tooltip' title='API 服务器中的一个对象，用于跟踪 kubelet 上的静态 pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-mirror-pod' target='_blank' aria-label='镜像 Pod'>镜像 Pod</a>。
这意味着在节点上运行的 Pod 在 API 服务器上是可见的，但不可以通过 API
服务器来控制。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The `spec` of a static Pod cannot refer to other API objects
(e.g., <a class='glossary-tooltip' title='为在 Pod 中运行的进程提供标识。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-service-account/' target='_blank' aria-label='ServiceAccount'>ServiceAccount</a>,
<a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a>,
<a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>, etc).
-->
<p>静态 Pod 的 <code>spec</code> 不能引用其他的 API 对象（例如：<a class='glossary-tooltip' title='为在 Pod 中运行的进程提供标识。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-service-account/' target='_blank' aria-label='ServiceAccount'>ServiceAccount</a>、<a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a>、<a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>等）。
</div>
<!--
## Container probes

A _probe_ is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions:

- `ExecAction` (performed with the help of the container runtime)
- `TCPSocketAction` (checked directly by the kubelet)
- `HTTPGetAction` (checked directly by the kubelet)

You can read more about [probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes) 
in the Pod Lifecycle documentation.
-->
<h2 id="container-probes">容器探针  </h2>
<p><em>Probe</em> 是由 kubelet 对容器执行的定期诊断。要执行诊断，kubelet 可以执行三种动作：</p>
<ul>
<li><code>ExecAction</code>（借助容器运行时执行）</li>
<li><code>TCPSocketAction</code>（由 kubelet 直接检测）</li>
<li><code>HTTPGetAction</code>（由 kubelet 直接检测）</li>
</ul>
<p>你可以参阅 Pod 的生命周期文档中的<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">探针</a>部分。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about the [lifecycle of a Pod](/docs/concepts/workloads/pods/pod-lifecycle/).
* Learn about [RuntimeClass](/docs/concepts/containers/runtime-class/) and how you can use it to
  configure different Pods with different container runtime configurations.
* Read about [Pod topology spread constraints](/docs/concepts/workloads/pods/pod-topology-spread-constraints/).
* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how you can use it to manage application availability during disruptions.
* Pod is a top-level resource in the Kubernetes REST API.
  The 





<a href=""></a>
  object definition describes the object in detail.
* [The Distributed System Toolkit: Patterns for Composite Containers](/blog/2015/06/the-distributed-system-toolkit-patterns/) explains common layouts for Pods with more than one container.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">Pod 生命周期</a></li>
<li>了解 <a href="/zh/docs/concepts/containers/runtime-class/">RuntimeClass</a>，以及如何使用它
来配置不同的 Pod 使用不同的容器运行时配置</li>
<li>了解 <a href="/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/">Pod 拓扑分布约束</a></li>
<li>了解 <a href="/zh/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>，以及你
如何可以利用它在出现干扰因素时管理应用的可用性。</li>
<li>Pod 在 Kubernetes REST API 中是一个顶层资源。






<a href=""></a>
对象的定义中包含了更多的细节信息。</li>
<li>博客 <a href="/blog/2015/06/the-distributed-system-toolkit-patterns/">分布式系统工具箱：复合容器模式</a>
中解释了在同一 Pod 中包含多个容器时的几种常见布局。</li>
</ul>
<!--
To understand the context for why Kubernetes wraps a common Pod API in other resources (such as <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSets'>StatefulSets</a> or <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployments'>Deployments</a>), you can read about the prior art, including:
-->
<p>要了解为什么 Kubernetes 会在其他资源
（如 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>
或 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>）
封装通用的 Pod API，相关的背景信息可以在前人的研究中找到。具体包括：</p>
<ul>
<li><a href="https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema">Aurora</a></li>
<li><a href="https://research.google.com/pubs/pub43438.html">Borg</a></li>
<li><a href="https://mesosphere.github.io/marathon/docs/rest-api.html">Marathon</a></li>
<li><a href="https://research.google/pubs/pub41684/">Omega</a></li>
<li><a href="https://engineering.fb.com/data-center-engineering/tupperware/">Tupperware</a>.</li>
</ul>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c3c2b9cf30915ec9d46c147201da3332">1.1 - Pod 的生命周期</h1>
    
	<!--
title: Pod Lifecycle
content_type: concept
weight: 30
-->
<!-- overview -->
<!--
This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting
in the `Pending` [phase](#pod-phase), moving through `Running` if at least one
of its primary containers starts OK, and then through either the `Succeeded` or
`Failed` phases depending on whether any container in the Pod terminated in failure.

Whilst a Pod is running, the kubelet is able to restart containers to handle some
kind of faults. Within a Pod, Kubernetes tracks different container
[states](#container-states) and determines what action to take to make the Pod
healthy again.
-->
<p>本页面讲述 Pod 的生命周期。
Pod 遵循一个预定义的生命周期，起始于 <code>Pending</code> <a href="#pod-phase">阶段</a>，如果至少
其中有一个主要容器正常启动，则进入 <code>Running</code>，之后取决于 Pod 中是否有容器以
失败状态结束而进入 <code>Succeeded</code> 或者 <code>Failed</code> 阶段。</p>
<p>在 Pod 运行期间，<code>kubelet</code> 能够重启容器以处理一些失效场景。
在 Pod 内部，Kubernetes 跟踪不同容器的<a href="#container-states">状态</a>
并确定使 Pod 重新变得健康所需要采取的动作。</p>
<!--
In the Kubernetes API, Pods have both a specification and an actual status. The
status for a Pod object consists of a set of [Pod conditions](#pod-conditions).
You can also inject [custom readiness information](#pod-readiness-gate) into the
condition data for a Pod, if that is useful to your application.

Pods are only [scheduled](/docs/concepts/scheduling-eviction/) once in their lifetime.
Once a Pod is scheduled (assigned) to a Node, the Pod runs on that Node until it stops
or is [terminated](#pod-termination).
-->
<p>在 Kubernetes API 中，Pod 包含规约部分和实际状态部分。
Pod 对象的状态包含了一组 <a href="#pod-conditions">Pod 状况（Conditions）</a>。
如果应用需要的话，你也可以向其中注入<a href="#pod-readiness-gate">自定义的就绪性信息</a>。</p>
<p>Pod 在其生命周期中只会被<a href="/zh/docs/concepts/scheduling-eviction/">调度</a>一次。
一旦 Pod 被调度（分派）到某个节点，Pod 会一直在该节点运行，直到 Pod 停止或者
被<a href="#pod-termination">终止</a>。</p>
<!-- body -->
<!--
## Pod lifetime

Like individual application containers, Pods are considered to be relatively
ephemeral (rather than durable) entities. Pods are created, assigned a unique
ID ([UID](/docs/concepts/overview/working-with-objects/names/#uids)), and scheduled
to nodes where they remain until termination (according to restart policy) or
deletion.  
If a <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点（Node）'>节点（Node）</a> dies, the Pods scheduled to that node
are [scheduled for deletion](#pod-garbage-collection) after a timeout period.
-->
<h2 id="pod-lifetime">Pod 生命期  </h2>
<p>和一个个独立的应用容器一样，Pod 也被认为是相对临时性（而不是长期存在）的实体。
Pod 会被创建、赋予一个唯一的
ID（<a href="/zh/docs/concepts/overview/working-with-objects/names/#uids">UID</a>），
并被调度到节点，并在终止（根据重启策略）或删除之前一直运行在该节点。</p>
<p>如果一个<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>死掉了，调度到该节点
的 Pod 也被计划在给定超时期限结束后<a href="#pod-garbage-collection">删除</a>。</p>
<!--
Pods do not, by themselves, self-heal. If a Pod is scheduled to a
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a> that then fails, the Pod is deleted; likewise, a Pod won't
survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a
higher-level abstraction, called a
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a>, that handles the work of
managing the relatively disposable Pod instances.
-->
<p>Pod 自身不具有自愈能力。如果 Pod 被调度到某<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>
而该节点之后失效，Pod 会被删除；类似地，Pod 无法在因节点资源
耗尽或者节点维护而被驱逐期间继续存活。Kubernetes 使用一种高级抽象
来管理这些相对而言可随时丢弃的 Pod 实例，称作
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>。</p>
<!--
A given Pod (as defined by a UID) is never "rescheduled" to a different node; instead,
that Pod can be replaced by a new, near-identical Pod, with even the same name if
desired, but with a different UID.

When something is said to have the same lifetime as a Pod, such as a
<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='volume'>volume</a>,
that means that the thing exists as long as that specific Pod (with that exact UID)
exists. If that Pod is deleted for any reason, and even if an identical replacement
is created, the related thing (a volume, in this example) is also destroyed and
created anew.
-->
<p>任何给定的 Pod （由 UID 定义）从不会被“重新调度（rescheduled）”到不同的节点；
相反，这一 Pod 可以被一个新的、几乎完全相同的 Pod 替换掉。
如果需要，新 Pod 的名字可以不变，但是其 UID 会不同。</p>
<p>如果某物声称其生命期与某 Pod 相同，例如存储<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>，
这就意味着该对象在此 Pod （UID 亦相同）存在期间也一直存在。
如果 Pod 因为任何原因被删除，甚至某完全相同的替代 Pod 被创建时，
这个相关的对象（例如这里的卷）也会被删除并重建。</p>

<figure class="diagram-medium">
    <img src="/images/docs/pod.svg"/> <figcaption>
            <h4>Pod 结构图例</h4>
        </figcaption>
</figure>

<p><em>一个包含多个容器的 Pod 中包含一个用来拉取文件的程序和一个 Web 服务器，
均使用持久卷作为容器间共享的存储。</em></p>
<!--
## Pod phase

A Pod's `status` field is a
[PodStatus](/docs/reference/generated/kubernetes-api/v1.23/#podstatus-v1-core)
object, which has a `phase` field.

The phase of a Pod is a simple, high-level summary of where the Pod is in its
lifecycle. The phase is not intended to be a comprehensive rollup of observations
of container or Pod state, nor is it intended to be a comprehensive state machine.

The number and meanings of Pod phase values are tightly guarded.
Other than what is documented here, nothing should be assumed about Pods that
have a given `phase` value.

Here are the possible values for `phase`:
-->
<h2 id="pod-phase">Pod 阶段    </h2>
<p>Pod 的 <code>status</code> 字段是一个
<a href="/docs/reference/generated/kubernetes-api/v1.23/#podstatus-v1-core">PodStatus</a>
对象，其中包含一个 <code>phase</code> 字段。</p>
<p>Pod 的阶段（Phase）是 Pod 在其生命周期中所处位置的简单宏观概述。
该阶段并不是对容器或 Pod 状态的综合汇总，也不是为了成为完整的状态机。</p>
<p>Pod 阶段的数量和含义是严格定义的。
除了本文档中列举的内容外，不应该再假定 Pod 有其他的 <code>phase</code> 值。</p>
<p>下面是 <code>phase</code> 可能的值：</p>
<!--
Value | Description
`Pending` | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to bescheduled as well as the time spent downloading container images over the network.
`Running` | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.
`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.
`Failed` | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system.
`Unknown` | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.
-->
<table>
<thead>
<tr>
<th style="text-align:left">取值</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>Pending</code>（悬决）</td>
<td style="text-align:left">Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间。</td>
</tr>
<tr>
<td style="text-align:left"><code>Running</code>（运行中）</td>
<td style="text-align:left">Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。</td>
</tr>
<tr>
<td style="text-align:left"><code>Succeeded</code>（成功）</td>
<td style="text-align:left">Pod 中的所有容器都已成功终止，并且不会再重启。</td>
</tr>
<tr>
<td style="text-align:left"><code>Failed</code>（失败）</td>
<td style="text-align:left">Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。</td>
</tr>
<tr>
<td style="text-align:left"><code>Unknown</code>（未知）</td>
<td style="text-align:left">因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。</td>
</tr>
</tbody>
</table>
<!--
If a node dies or is disconnected from the rest of the cluster, Kubernetes
applies a policy for setting the `phase` of all Pods on the lost node to Failed.
-->
<p>如果某节点死掉或者与集群中其他节点失联，Kubernetes
会实施一种策略，将失去的节点上运行的所有 Pod 的 <code>phase</code> 设置为 <code>Failed</code>。</p>
<!--
## Container states

As well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of
each container inside a Pod. You can use
[container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/) to
trigger events to run at certain points in a container's lifecycle.

Once the <a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='scheduler'>scheduler</a>
assigns a Pod to a Node, the kubelet starts creating containers for that Pod
using a <a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>.
There are three possible container states: `Waiting`, `Running`, and `Terminated`.
-->
<h2 id="container-states">容器状态 </h2>
<p>Kubernetes 会跟踪 Pod 中每个容器的状态，就像它跟踪 Pod 总体上的<a href="#pod-phase">阶段</a>一样。
你可以使用<a href="/zh/docs/concepts/containers/container-lifecycle-hooks/">容器生命周期回调</a>
来在容器生命周期中的特定时间点触发事件。</p>
<p>一旦<a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='调度器'>调度器</a>将 Pod
分派给某个节点，<code>kubelet</code> 就通过
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>
开始为 Pod 创建容器。
容器的状态有三种：<code>Waiting</code>（等待）、<code>Running</code>（运行中）和
<code>Terminated</code>（已终止）。</p>
<!--
To check the state of a Pod's containers, you can use
`kubectl describe pod <name-of-pod>`. The output shows the state for each container
within that Pod.

Each state has a specific meaning:
-->
<p>要检查 Pod 中容器的状态，你可以使用 <code>kubectl describe pod &lt;pod 名称&gt;</code>。
其输出中包含 Pod 中每个容器的状态。</p>
<p>每种状态都有特定的含义：</p>
<!--
### `Waiting` {#container-state-waiting}

If a container is not in either the `Running` or `Terminated` state, it is `Waiting`.
A container in the `Waiting` state is still running the operations it requires in
order to complete start up: for example, pulling the container image from a container
image registry, or applying <a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>
data.
When you use `kubectl` to query a Pod with a container that is `Waiting`, you also see
a Reason field to summarize why the container is in that state.
-->
<h3 id="container-state-waiting"><code>Waiting</code> （等待） </h3>
<p>如果容器并不处在 <code>Running</code> 或 <code>Terminated</code> 状态之一，它就处在 <code>Waiting</code> 状态。
处于 <code>Waiting</code> 状态的容器仍在运行它完成启动所需要的操作：例如，从某个容器镜像
仓库拉取容器镜像，或者向容器应用 <a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>
数据等等。
当你使用 <code>kubectl</code> 来查询包含 <code>Waiting</code> 状态的容器的 Pod 时，你也会看到一个
Reason 字段，其中给出了容器处于等待状态的原因。</p>
<!--
### `Running` {#container-state-running}

The `Running` status indicates that a container is executing without issues. If there
was a `postStart` hook configured, it has already executed and finished. When you use
`kubectl` to query a Pod with a container that is `Running`, you also see information
about when the container entered the `Running` state.
-->
<h3 id="container-state-running"><code>Running</code>（运行中）    </h3>
<p><code>Running</code> 状态表明容器正在执行状态并且没有问题发生。
如果配置了 <code>postStart</code> 回调，那么该回调已经执行且已完成。
如果你使用 <code>kubectl</code> 来查询包含 <code>Running</code> 状态的容器的 Pod 时，你也会看到
关于容器进入 <code>Running</code> 状态的信息。</p>
<!--
### `Terminated` {#container-state-terminated}

A container in the `Terminated` state began execution and then either ran to
completion or failed for some reason. When you use `kubectl` to query a Pod with
a container that is `Terminated`, you see a reason, an exit code, and the start and
finish time for that container's period of execution.

If a container has a `preStop` hook configured, this hook runs before the container enters
the `Terminated` state.
-->
<h3 id="container-state-terminated"><code>Terminated</code>（已终止）  </h3>
<p>处于 <code>Terminated</code> 状态的容器已经开始执行并且或者正常结束或者因为某些原因失败。
如果你使用 <code>kubectl</code> 来查询包含 <code>Terminated</code> 状态的容器的 Pod 时，你会看到
容器进入此状态的原因、退出代码以及容器执行期间的起止时间。</p>
<p>如果容器配置了 <code>preStop</code> 回调，则该回调会在容器进入 <code>Terminated</code>
状态之前执行。</p>
<!--
## Container restart policy {#restart-policy}

The `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,
and Never. The default value is Always.

The `restartPolicy` applies to all containers in the Pod. `restartPolicy` only
refers to restarts of the containers by the kubelet on the same node. After containers
in a Pod exit, the kubelet restarts them with an exponential back-off delay (10s, 20s,
40s, …), that is capped at five minutes. Once a container has executed for 10 minutes
without any problems, the kubelet resets the restart backoff timer for
that container.
-->
<h2 id="restart-policy">容器重启策略</h2>
<p>Pod 的 <code>spec</code> 中包含一个 <code>restartPolicy</code> 字段，其可能取值包括
Always、OnFailure 和 Never。默认值是 Always。</p>
<p><code>restartPolicy</code> 适用于 Pod 中的所有容器。<code>restartPolicy</code> 仅针对同一节点上
<code>kubelet</code> 的容器重启动作。当 Pod 中的容器退出时，<code>kubelet</code> 会按指数回退
方式计算重启的延迟（10s、20s、40s、...），其最长延迟为 5 分钟。
一旦某容器执行了 10 分钟并且没有出现问题，<code>kubelet</code> 对该容器的重启回退计时器执行
重置操作。</p>
<!--
## Pod conditions

A Pod has a PodStatus, which has an array of
[PodConditions](/docs/reference/generated/kubernetes-api/v1.23/#podcondition-v1-core)
through which the Pod has or has not passed:
-->
<h2 id="pod-conditions">Pod 状况 </h2>
<p>Pod 有一个 PodStatus 对象，其中包含一个
<a href="/docs/reference/generated/kubernetes-api/v1.23/#podcondition-v1-core">PodConditions</a>
数组。Pod 可能通过也可能未通过其中的一些状况测试。</p>
<!--
* `PodScheduled`: the Pod has been scheduled to a node.
* `ContainersReady`: all containers in the Pod are ready.
* `Initialized`: all [init containers](/docs/concepts/workloads/pods/init-containers/)
  have completed successfully.
* `Ready`: the Pod is able to serve requests and should be added to the load
  balancing pools of all matching Services.
-->
<ul>
<li><code>PodScheduled</code>：Pod 已经被调度到某节点；</li>
<li><code>ContainersReady</code>：Pod 中所有容器都已就绪；</li>
<li><code>Initialized</code>：所有的 <a href="/zh/docs/concepts/workloads/pods/init-containers/">Init 容器</a>
都已成功完成；</li>
<li><code>Ready</code>：Pod 可以为请求提供服务，并且应该被添加到对应服务的负载均衡池中。</li>
</ul>
<!--
Field name           | Description
`type`               | Name of this Pod condition.
`status`             | Indicates whether that condition is applicable, with possible values "`True`", "`False`", or "`Unknown`".
`lastProbeTime`      | Timestamp of when the Pod condition was last probed.
`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.
`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.
`message`            | Human-readable message indicating details about the last status transition.
-->
<table>
<thead>
<tr>
<th style="text-align:left">字段名称</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>type</code></td>
<td style="text-align:left">Pod 状况的名称</td>
</tr>
<tr>
<td style="text-align:left"><code>status</code></td>
<td style="text-align:left">表明该状况是否适用，可能的取值有 &quot;<code>True</code>&quot;, &quot;<code>False</code>&quot; 或 &quot;<code>Unknown</code>&quot;</td>
</tr>
<tr>
<td style="text-align:left"><code>lastProbeTime</code></td>
<td style="text-align:left">上次探测 Pod 状况时的时间戳</td>
</tr>
<tr>
<td style="text-align:left"><code>lastTransitionTime</code></td>
<td style="text-align:left">Pod 上次从一种状态转换到另一种状态时的时间戳</td>
</tr>
<tr>
<td style="text-align:left"><code>reason</code></td>
<td style="text-align:left">机器可读的、驼峰编码（UpperCamelCase）的文字，表述上次状况变化的原因</td>
</tr>
<tr>
<td style="text-align:left"><code>message</code></td>
<td style="text-align:left">人类可读的消息，给出上次状态转换的详细信息</td>
</tr>
</tbody>
</table>
<!--
### Pod readiness {#pod-readiness-gate}

Your application can inject extra feedback or signals into PodStatus:
_Pod readiness_. To use this, set `readinessGates` in the Pod's `spec` to
specify a list of additional conditions that the kubelet evaluates for Pod readiness.
-->
<h3 id="pod-readiness-gate">Pod 就绪态       </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code>
</div>


<p>你的应用可以向 PodStatus 中注入额外的反馈或者信号：<em>Pod Readiness（Pod 就绪态）</em>。
要使用这一特性，可以设置 Pod 规约中的 <code>readinessGates</code> 列表，为 kubelet
提供一组额外的状况供其评估 Pod 就绪态时使用。</p>
<!--
Readiness gates are determined by the current state of `status.condition`
fields for the Pod. If Kubernetes cannot find such a condition in the
`status.conditions` field of a Pod, the status of the condition
is defaulted to "`False`".

Here is an example:
-->
<p>就绪态门控基于 Pod 的 <code>status.conditions</code> 字段的当前值来做决定。
如果 Kubernetes 无法在 <code>status.conditions</code> 字段中找到某状况，则该状况的
状态值默认为 &quot;<code>False</code>&quot;。</p>
<p>这里是一个例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">readinessGates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">conditionType</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;www.example.com/feature-1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conditions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Ready                             <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 内置的 Pod 状况</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;False&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">lastProbeTime</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">lastTransitionTime</span>:<span style="color:#bbb"> </span>2018-01-01T00:00:00Z<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;www.example.com/feature-1&#34;</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 额外的 Pod 状况</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;False&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">lastProbeTime</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">lastTransitionTime</span>:<span style="color:#bbb"> </span>2018-01-01T00:00:00Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containerStatuses</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerID</span>:<span style="color:#bbb"> </span>docker://abcd...<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ready</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
The Pod conditions you add must have names that meet the Kubernetes [label key format](/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).
-->
<p>你所添加的 Pod 状况名称必须满足 Kubernetes
<a href="/zh/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">标签键名格式</a>。</p>
<!--
### Status for Pod readiness {#pod-readiness-status}

The `kubectl patch` command does not support patching object status.
To set these `status.conditions` for the pod, applications and
<a class='glossary-tooltip' title='一种用于管理自定义资源的专用控制器' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/operator/' target='_blank' aria-label='operators'>operators</a> should use
the `PATCH` action.
You can use a [Kubernetes client library](/docs/reference/using-api/client-libraries/) to
write code that sets custom Pod conditions for Pod readiness.
-->
<h3 id="pod-readiness-status">Pod 就绪态的状态</h3>
<p>命令 <code>kubectl patch</code> 不支持修改对象的状态。
如果需要设置 Pod 的 <code>status.conditions</code>，应用或者
<a class='glossary-tooltip' title='一种用于管理自定义资源的专用控制器' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/operator/' target='_blank' aria-label='Operators'>Operators</a>
需要使用 <code>PATCH</code> 操作。
你可以使用 <a href="/zh/docs/reference/using-api/client-libraries/">Kubernetes 客户端库</a>
之一来编写代码，针对 Pod 就绪态设置定制的 Pod 状况。</p>
<!--
For a Pod that uses custom conditions, that Pod is evaluated to be ready **only**
when both the following statements apply:

* All containers in the Pod are ready.
* All conditions specified in `readinessGates` are `True`.

When a Pod's containers are Ready but at least one custom condition is missing or
`False`, the kubelet sets the Pod's [condition](#pod-conditions) to `ContainersReady`.
-->
<p>对于使用定制状况的 Pod 而言，只有当下面的陈述都适用时，该 Pod 才会被评估为就绪：</p>
<ul>
<li>Pod 中所有容器都已就绪；</li>
<li><code>readinessGates</code> 中的所有状况都为 <code>True</code> 值。</li>
</ul>
<p>当 Pod 的容器都已就绪，但至少一个定制状况没有取值或者取值为 <code>False</code>，
<code>kubelet</code> 将 Pod 的<a href="#pod-conditions">状况</a>设置为 <code>ContainersReady</code>。</p>
<!--
## Container probes

A _probe_ is a diagnostic
performed periodically by the
[kubelet](/docs/reference/command-line-tools-reference/kubelet/)
on a container. To perform a diagnostic,
the kubelet either executes code within the container, or makes
a network request.
-->
<h2 id="container-probes">容器探针   </h2>
<p>probe 是由 <a href="/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> 对容器执行的定期诊断。
要执行诊断，kubelet 既可以在容器内执行代码，也可以发出一个网络请求。</p>
<!--
### Check mechanisms {#probe-check-methods}

There are four different ways to check a container using a probe.
Each probe must define exactly one of these four mechanisms:

`exec`
: Executes a specified command inside the container. The diagnostic
  is considered successful if the command exits with a status code of 0.

`grpc`
: Performs a remote procedure call using [gRPC](https://grpc.io/).
  The target should implement
  [gRPC health checks](https://grpc.io/grpc/core/md_doc_health-checking.html).
  The diagnostic is considered successful if the `status`
  of the response is `SERVING`.
  gRPC probes are an alpha feature and are only available if you
  enable the `GRPCContainerProbe`
  [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).

`httpGet`
: Performs an HTTP `GET` request against the Pod's IP
  address on a specified port and path. The diagnostic is
  considered successful if the response has a status code
  greater than or equal to 200 and less than 400.

`tcpSocket`
: Performs a TCP check against the Pod's IP address on
  a specified port. The diagnostic is considered successful if
  the port is open. If the remote system (the container) closes
  the connection immediately after it opens, this counts as healthy.

-->
<h3 id="probe-check-methods">检查机制   </h3>
<p>使用探针来检查容器有四种不同的方法。
每个探针都必须准确定义为这四种机制中的一种：</p>
<dl>
<dt><code>exec</code></dt>
<dd>在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。</dd>
<dt><code>grpc</code></dt>
<dd>使用 <a href="https://grpc.io/">gRPC</a> 执行一个远程过程调用。
目标应该实现
<a href="https://grpc.io/grpc/core/md_doc_health-checking.html">gRPC健康检查</a>。
如果响应的状态是 &quot;SERVING&quot;，则认为诊断成功。
gRPC 探针是一个 alpha 特性，只有在你启用了
&quot;GRPCContainerProbe&quot; <a href="/zh/docs/reference/command-line-tools-reference/feature-gate/">特性门控</a>时才能使用。</dd>
<dt><code>httpGet</code></dt>
<dd>对容器的 IP 地址上指定端口和路径执行 HTTP <code>GET</code> 请求。如果响应的状态码大于等于 200
且小于 400，则诊断被认为是成功的。</dd>
<dt><code>tcpSocket</code></dt>
<dd>对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。
如果远程系统（容器）在打开连接后立即将其关闭，这算作是健康的。</dd>
</dl>
<!--
### Probe outcome
Each probe has one of three results:

`Success`
: The container passed the diagnostic.

`Failure`
: The container failed the diagnostic.

`Unknown`
: The diagnostic failed (no action should be taken, and the kubelet
  will make further checks).

-->
<h3 id="probe-outcome">探测结果   </h3>
<p>每次探测都将获得以下三种结果之一：</p>
<dl>
<dt><code>Success</code>（成功）</dt>
<dd>容器通过了诊断。</dd>
<dt><code>Failure</code>（失败）</dt>
<dd>容器未通过诊断。</dd>
<dt><code>Unknown</code>（未知）</dt>
<dd>诊断失败，因此不会采取任何行动。</dd>
</dl>
<!--
### Types of probe
The kubelet can optionally perform and react to three kinds of probes on running
containers:
-->
<h3 id="types-of-probe">探测类型   </h3>
<p>针对运行中的容器，<code>kubelet</code> 可以选择是否执行以下三种探针，以及如何针对探测结果作出反应：</p>
<!--
`livenessProbe`
: Indicates whether the container is running. If
  the liveness probe fails, the kubelet kills the container, and the container
  is subjected to its [restart policy](#restart-policy). If a container does not
  provide a liveness probe, the default state is `Success`.

`readinessProbe`
: Indicates whether the container is ready to respond to requests.
  If the readiness probe fails, the endpoints controller removes the Pod's IP
  address from the endpoints of all Services that match the Pod. The default
  state of readiness before the initial delay is `Failure`. If a container does
  not provide a readiness probe, the default state is `Success`.

`startupProbe`
: Indicates whether the application within the container is started.
  All other probes are disabled if a startup probe is provided, until it succeeds.
  If the startup probe fails, the kubelet kills the container, and the container
  is subjected to its [restart policy](#restart-policy). If a container does not
  provide a startup probe, the default state is `Success`.
-->
<dl>
<dt><code>livenessProbe</code></dt>
<dd>指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器，
并且容器将根据其<a href="#restart-policy">重启策略</a>决定未来。如果容器不提供存活探针，
则默认状态为 <code>Success</code>。</dd>
<dt><code>readinessProbe</code></dt>
<dd>指示容器是否准备好为请求提供服务。如果就绪态探测失败，
端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。
初始延迟之前的就绪态的状态值默认为 <code>Failure</code>。
如果容器不提供就绪态探针，则默认状态为 <code>Success</code>。</dd>
<dt><code>startupProbe</code></dt>
<dd>指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被
禁用，直到此探针成功为止。如果启动探测失败，<code>kubelet</code> 将杀死容器，而容器依其
<a href="#restart-policy">重启策略</a>进行重启。
如果容器没有提供启动探测，则默认状态为 <code>Success</code>。</dd>
</dl>
<!--
For more information about how to set up a liveness, readiness, or startup probe,
see [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).
-->
<p>如欲了解如何设置存活态、就绪态和启动探针的进一步细节，可以参阅
<a href="/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">配置存活态、就绪态和启动探针</a>。</p>
<!--
#### When should you use a liveness probe?
-->
<h4 id="when-should-you-use-a-liveness-probe">何时该使用存活态探针?   </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.0 [stable]</code>
</div>


<!--
If the process in your container is able to crash on its own whenever it
encounters an issue or becomes unhealthy, you do not necessarily need a liveness
probe; the kubelet will automatically perform the correct action in accordance
with the Pod's `restartPolicy`.

If you'd like your container to be killed and restarted if a probe fails, then
specify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.
-->
<p>如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活态探针;
<code>kubelet</code> 将根据 Pod 的<code>restartPolicy</code> 自动执行修复操作。</p>
<p>如果你希望容器在探测失败时被杀死并重新启动，那么请指定一个存活态探针，
并指定<code>restartPolicy</code> 为 &quot;<code>Always</code>&quot; 或 &quot;<code>OnFailure</code>&quot;。</p>
<!--
#### When should you use a readiness probe?
-->
<h4 id="when-should-you-use-a-readiness-probe">何时该使用就绪态探针?     </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.0 [stable]</code>
</div>


<!--
If you'd like to start sending traffic to a Pod only when a probe succeeds,
specify a readiness probe. In this case, the readiness probe might be the same
as the liveness probe, but the existence of the readiness probe in the spec means
that the Pod will start without receiving any traffic and only start receiving
traffic after the probe starts succeeding.
-->
<p>如果要仅在探测成功时才开始向 Pod 发送请求流量，请指定就绪态探针。
在这种情况下，就绪态探针可能与存活态探针相同，但是规约中的就绪态探针的存在意味着
Pod 将在启动阶段不接收任何数据，并且只有在探针探测成功后才开始接收数据。</p>
<!--
If you want your container to be able to take itself down for maintenance, you
can specify a readiness probe that checks an endpoint specific to readiness that
is different from the liveness probe.
-->
<p>如果你希望容器能够自行进入维护状态，也可以指定一个就绪态探针，检查某个特定于
就绪态的因此不同于存活态探测的端点。</p>
<!--
If your app has a strict dependency on back-end services, you can implement both
a liveness and a readiness probe. The liveness probe passes when the app itself
is healthy, but the readiness probe additionally checks that each required
back-end service is available. This helps you avoid directing traffic to Pods
that can only respond with error messages.

If your container needs to work on loading large data, configuration files, or
migrations during startup, you can use a
[startup probe](#when-should-you-use-a-startup-probe). However, if you want to
detect the difference between an app that has failed and an app that is still
processing its startup data, you might prefer a readiness probe.
-->
<p>如果你的应用程序对后端服务有严格的依赖性，你可以同时实现存活态和就绪态探针。
当应用程序本身是健康的，存活态探针检测通过后，就绪态探针会额外检查每个所需的后端服务是否可用。
这可以帮助你避免将流量导向只能返回错误信息的 Pod。</p>
<p>如果你的容器需要在启动期间加载大型数据、配置文件或执行迁移，你可以使用
<a href="#when-should-you-use-a-startup-probe">启动探针</a>。
然而，如果你想区分已经失败的应用和仍在处理其启动数据的应用，你可能更倾向于使用就绪探针。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you want to be able to drain requests when the Pod is deleted, you do not
necessarily need a readiness probe; on deletion, the Pod automatically puts itself
into an unready state regardless of whether the readiness probe exists.
The Pod remains in the unready state while it waits for the containers in the Pod
to stop.
-->
<p>请注意，如果你只是想在 Pod 被删除时能够排空请求，则不一定需要使用就绪态探针；
在删除 Pod 时，Pod 会自动将自身置于未就绪状态，无论就绪态探针是否存在。
等待 Pod 中的容器停止期间，Pod 会一直处于未就绪状态。
</div>
<!--
#### When should you use a startup probe?
-->
<h4 id="when-should-you-use-a-startup-probe">何时该使用启动探针？  </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<!--
Startup probes are useful for Pods that have containers that take a long time to
come into service. Rather than set a long liveness interval, you can configure
a separate configuration for probing the container as it starts up, allowing
a time longer than the liveness interval would allow.
-->
<p>对于所包含的容器需要较长时间才能启动就绪的 Pod 而言，启动探针是有用的。
你不再需要配置一个较长的存活态探测时间间隔，只需要设置另一个独立的配置选定，
对启动期间的容器执行探测，从而允许使用远远超出存活态时间间隔所允许的时长。</p>
<!--
If your container usually starts in more than
`initialDelaySeconds + failureThreshold × periodSeconds`, you should specify a
startup probe that checks the same endpoint as the liveness probe. The default for
`periodSeconds` is 10s. You should then set its `failureThreshold` high enough to
allow the container to start, without changing the default values of the liveness
probe. This helps to protect against deadlocks.
-->
<p>如果你的容器启动时间通常超出  <code>initialDelaySeconds + failureThreshold × periodSeconds</code>
总值，你应该设置一个启动探测，对存活态探针所使用的同一端点执行检查。
<code>periodSeconds</code> 的默认值是 10 秒。你应该将其 <code>failureThreshold</code> 设置得足够高，
以便容器有充足的时间完成启动，并且避免更改存活态探针所使用的默认值。
这一设置有助于减少死锁状况的发生。</p>
<!--
## Termination of Pods {#pod-termination}

Because Pods represent processes running on nodes in the cluster, it is important to
allow those processes to gracefully terminate when they are no longer needed (rather
than being abruptly stopped with a `KILL` signal and having no chance to clean up).
-->
<h2 id="pod-termination">Pod 的终止   </h2>
<p>由于 Pod 所代表的是在集群中节点上运行的进程，当不再需要这些进程时允许其体面地
终止是很重要的。一般不应武断地使用 <code>KILL</code> 信号终止它们，导致这些进程没有机会
完成清理操作。</p>
<!--
The design aim is for you to be able to request deletion and know when processes
terminate, but also be able to ensure that deletes eventually complete.
When you request deletion of a Pod, the cluster records and tracks the intended grace period
before the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in
place, the {< glossary_tooltip text="kubelet" term_id="kubelet" >}} attempts graceful
shutdown.
-->
<p>设计的目标是令你能够请求删除进程，并且知道进程何时被终止，同时也能够确保删除
操作终将完成。当你请求删除某个 Pod 时，集群会记录并跟踪 Pod 的体面终止周期，
而不是直接强制地杀死 Pod。在存在强制关闭设施的前提下，
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 会尝试体面地终止
Pod。</p>
<!--
Typically, the container runtime sends a TERM signal to the main process in each
container. Many container runtimes respect the `STOPSIGNAL` value defined in the container
image and send this instead of TERM.
Once the grace period has expired, the KILL signal is sent to any remainig
processes, and the Pod is then deleted from the
<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API Server'>API Server</a>. If the kubelet or the
container runtime's management service is restarted while waiting for processes to terminate, the
cluster retries from the start including the full original grace period.
-->
<p>通常情况下，容器运行时会发送一个 TERM 信号到每个容器中的主进程。
很多容器运行时都能够注意到容器镜像中 <code>STOPSIGNAL</code> 的值，并发送该信号而不是 TERM。
一旦超出了体面终止限期，容器运行时会向所有剩余进程发送 KILL 信号，之后
Pod 就会被从 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>
上移除。如果 <code>kubelet</code> 或者容器运行时的管理服务在等待进程终止期间被重启，
集群会从头开始重试，赋予 Pod 完整的体面终止限期。</p>
<!--
An example flow:

1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period
   (30 seconds).
1. The Pod in the API server is updated with the time beyond which the Pod is considered "dead"
   along with the grace period.
   If you use `kubectl describe` to check on the Pod you're deleting, that Pod shows up as
   "Terminating".
   On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked
   as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod
   shutdown process.
-->
<p>下面是一个例子：</p>
<ol>
<li>
<p>你使用 <code>kubectl</code> 工具手动删除某个特定的 Pod，而该 Pod 的体面终止限期是默认值（30 秒）。</p>
</li>
<li>
<p>API 服务器中的 Pod 对象被更新，记录涵盖体面终止限期在内 Pod
的最终死期，超出所计算时间点则认为 Pod 已死（dead）。
如果你使用 <code>kubectl describe</code> 来查验你正在删除的 Pod，该 Pod 会显示为
&quot;Terminating&quot; （正在终止）。
在 Pod 运行所在的节点上：<code>kubelet</code> 一旦看到 Pod
被标记为正在终止（已经设置了体面终止限期），<code>kubelet</code> 即开始本地的 Pod 关闭过程。</p>
<!--
1. If one of the Pod's containers has defined a `preStop`
   [hook](/docs/concepts/containers/container-lifecycle-hooks), the kubelet
   runs that hook inside of the container. If the `preStop` hook is still running after the
   grace period expires, the kubelet requests a small, one-off grace period extension of 2
   seconds.
   If the `preStop` hook needs longer to complete than the default grace period allows,
   you must modify `terminationGracePeriodSeconds` to suit this.
1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each
   container.
   The containers in the Pod receive the TERM signal at different times and in an arbitrary
   order. If the order of shutdowns matters, consider using a `preStop` hook to synchronize.
-->
<ol>
<li>
<p>如果 Pod 中的容器之一定义了 <code>preStop</code>
<a href="/zh/docs/concepts/containers/container-lifecycle-hooks">回调</a>，
<code>kubelet</code> 开始在容器内运行该回调逻辑。如果超出体面终止限期时，<code>preStop</code> 回调逻辑
仍在运行，<code>kubelet</code> 会请求给予该 Pod 的宽限期一次性增加 2 秒钟。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果 <code>preStop</code> 回调所需要的时间长于默认的体面终止限期，你必须修改
<code>terminationGracePeriodSeconds</code> 属性值来使其正常工作。
</div>
</li>
<li>
<p><code>kubelet</code> 接下来触发容器运行时发送 TERM 信号给每个容器中的进程 1。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Pod 中的容器会在不同时刻收到 TERM 信号，接收顺序也是不确定的。
如果关闭的顺序很重要，可以考虑使用 <code>preStop</code> 回调逻辑来协调。
</div>
</li>
</ol>
</li>
</ol>
<!--
1. At the same time as the kubelet is starting graceful shutdown, the control plane removes that
   shutting-down Pod from Endpoints (and, if enabled, EndpointSlice) objects where these represent
   a <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> with a configured
   <a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='selector'>selector</a>.
   <a class='glossary-tooltip' title='ReplicaSet 是下一代副本控制器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/replicaset/' target='_blank' aria-label='ReplicaSets'>ReplicaSets</a> and other workload resources
   no longer treat the shutting-down Pod as a valid, in-service replica. Pods that shut down slowly
   cannot continue to serve traffic as load balancers (like the service proxy) remove the Pod from
   the list of endpoints as soon as the termination grace period _begins_.
-->
<ol start="3">
<li>与此同时，<code>kubelet</code> 启动体面关闭逻辑，控制面会将 Pod 从对应的端点列表（以及端点切片列表，
如果启用了的话）中移除，过滤条件是 Pod 被对应的
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务'>服务</a>以某
<a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='选择算符'>选择算符</a>选定。
<a class='glossary-tooltip' title='ReplicaSet 是下一代副本控制器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/replicaset/' target='_blank' aria-label='ReplicaSets'>ReplicaSets</a>和其他工作负载资源
不再将关闭进程中的 Pod 视为合法的、能够提供服务的副本。关闭动作很慢的 Pod
也无法继续处理请求数据，因为负载均衡器（例如服务代理）已经在终止宽限期开始的时候
将其从端点列表中移除。</li>
</ol>
<!--
1. When the grace period expires, the kubelet triggers forcible shutdown. The container runtime sends
   `SIGKILL` to any processes still running in any container in the Pod.
   The kubelet also cleans up a hidden `pause` container if that container runtime uses one.
1. The kubelet triggers forcible removal of Pod object from the API server, by setting grace period
   to 0 (immediate deletion).
1. The API server deletes the Pod's API object, which is then no longer visible from any client.
-->
<ol start="4">
<li>
<p>超出终止宽限期限时，<code>kubelet</code> 会触发强制关闭过程。容器运行时会向 Pod 中所有容器内
仍在运行的进程发送 <code>SIGKILL</code> 信号。
<code>kubelet</code> 也会清理隐藏的 <code>pause</code> 容器，如果容器运行时使用了这种容器的话。</p>
</li>
<li>
<p><code>kubelet</code> 触发强制从 API 服务器上删除 Pod 对象的逻辑，并将体面终止限期设置为 0
（这意味着马上删除）。</p>
</li>
<li>
<p>API 服务器删除 Pod 的 API 对象，从任何客户端都无法再看到该对象。</p>
</li>
</ol>
<!--
### Forced Pod termination {#pod-termination-forced}

Forced deletions can be potentially disruptive for some workloads and their Pods.

By default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports
the `-grace-period=<seconds>` option which allows you to override the default and specify your
own value.
-->
<h3 id="pod-termination-forced">强制终止 Pod    </h3>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 对于某些工作负载及其 Pod 而言，强制删除很可能会带来某种破坏。
</div>

<p>默认情况下，所有的删除操作都会附有 30 秒钟的宽限期限。
<code>kubectl delete</code> 命令支持 <code>--grace-period=&lt;seconds&gt;</code> 选项，允许你重载默认值，
设定自己希望的期限值。</p>
<!--
Setting the grace period to `0` forcibly and immediately deletes the Pod from the API
server. If the pod was still running on a node, that forcible deletion triggers the kubelet to
begin immediate cleanup.
-->
<p>将宽限期限强制设置为 <code>0</code> 意味着立即从 API 服务器删除 Pod。
如果 Pod 仍然运行于某节点上，强制删除操作会触发 <code>kubelet</code> 立即执行清理操作。</p>
<!--
You must specify an additional flag `--force` along with `--grace-period=0` in order to perform force deletions.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你必须在设置 <code>--grace-period=0</code> 的同时额外设置 <code>--force</code>
参数才能发起强制删除请求。
</div>
<!--
When a force deletion is performed, the API server does not wait for confirmation
from the kubelet that the Pod has been terminated on the node it was running on. It
removes the Pod in the API immediately so a new Pod can be created with the same
name. On the node, Pods that are set to terminate immediately will still be given
a small grace period before being force killed.

If you need to force-delete Pods that are part of a StatefulSet, refer to the task
documentation for
[deleting Pods from a StatefulSet](/docs/tasks/run-application/force-delete-stateful-set-pod/).
-->
<p>执行强制删除操作时，API 服务器不再等待来自 <code>kubelet</code> 的、关于 Pod
已经在原来运行的节点上终止执行的确认消息。
API 服务器直接删除 Pod 对象，这样新的与之同名的 Pod 即可以被创建。
在节点侧，被设置为立即终止的 Pod 仍然会在被强行杀死之前获得一点点的宽限时间。</p>
<p>如果你需要强制删除 StatefulSet 的 Pod，请参阅
<a href="/zh/docs/tasks/run-application/force-delete-stateful-set-pod/">从 StatefulSet 中删除 Pod</a>
的任务文档。</p>
<!--
### Garbage collection of failed Pods {#pod-garbage-collection}

For failed Pods, the API objects remain in the cluster's API until a human or
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> process
explicitly removes them.

The control plane cleans up terminated Pods (with a phase of `Succeeded` or
`Failed`), when the number of Pods exceeds the configured threshold
(determined by `terminated-pod-gc-threshold` in the kube-controller-manager).
This avoids a resource leak as Pods are created and terminated over time.
-->
<h3 id="pod-garbage-collection">失效 Pod 的垃圾收集   </h3>
<p>对于已失败的 Pod 而言，对应的 API 对象仍然会保留在集群的 API 服务器上，直到
用户或者<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>进程显式地
将其删除。</p>
<p>控制面组件会在 Pod 个数超出所配置的阈值
（根据 <code>kube-controller-manager</code> 的 <code>terminated-pod-gc-threshold</code> 设置）时
删除已终止的 Pod（阶段值为 <code>Succeeded</code> 或 <code>Failed</code>）。
这一行为会避免随着时间演进不断创建和终止 Pod 而引起的资源泄露问题。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Get hands-on experience
  [attaching handlers to container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).

* Get hands-on experience
  [configuring Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).

* Learn more about [container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).

* For detailed information about Pod and container status in the API, see
  the API reference documentation covering
  [`.status`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus) for Pod.
-->
<ul>
<li>动手实践<a href="/zh/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">为容器生命周期时间关联处理程序</a>。</li>
<li>动手实践<a href="/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">配置存活态、就绪态和启动探针</a>。</li>
<li>进一步了解<a href="/zh/docs/concepts/containers/container-lifecycle-hooks/">容器生命周期回调</a>。</li>
<li>关于 API 中定义的有关 Pod 和容器状态的详细规范信息，
可参阅 API 参考文档中 Pod 的 <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus"><code>.status</code></a> 字段。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1ccbd4eeded6ab138d98b59175bd557e">1.2 - Init 容器</h1>
    
	<!---
reviewers:
- erictune
title: Init Containers
content_type: concept
weight: 40
-->
<!-- overview -->
<!--
This page provides an overview of init containers: specialized containers that run
before app containers in a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>.
Init containers can contain utilities or setup scripts not present in an app image.
-->
<p>本页提供了 Init 容器的概览。Init 容器是一种特殊容器，在 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
内的应用容器启动之前运行。Init 容器可以包括一些应用镜像中不存在的实用工具和安装脚本。</p>
<!--
You can specify init containers in the Pod specification alongside the `containers`
array (which describes app containers).
-->
<p>你可以在 Pod 的规约中与用来描述应用容器的 <code>containers</code> 数组平行的位置指定
Init 容器。</p>
<!-- body -->
<!--
## Understanding init containers

A <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> can have multiple containers
running apps within it, but it can also have one or more init containers, which are run
before the app containers are started.
-->
<h2 id="理解-init-容器">理解 Init 容器</h2>
<p>每个 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 中可以包含多个容器，
应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。</p>
<!--
Init containers are exactly like regular containers, except:

* Init containers always run to completion.
* Each init container must complete successfully before the next one starts.
-->
<p>Init 容器与普通的容器非常像，除了如下两点：</p>
<ul>
<li>它们总是运行到完成。</li>
<li>每个都必须在下一个启动之前成功完成。</li>
</ul>
<!--
If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.
However, if the Pod has a `restartPolicy` of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.
-->
<p>如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。
然而，如果 Pod 对应的 <code>restartPolicy</code> 值为 &quot;Never&quot;，并且 Pod 的 Init 容器失败，
则 Kubernetes 会将整个 Pod 状态设置为失败。</p>
<!--
To specify an init container for a Pod, add the `initContainers` field into
the [Pod specification](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec),
as an array of `container` items (similar to the app `containers` field and its contents).
See [Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container) in the
API reference for more details.

The status of the init containers is returned in `.status.initContainerStatuses`
field as an array of the container statuses (similar to the `.status.containerStatuses`
field).
-->
<p>为 Pod 设置 Init 容器需要在 <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec">Pod 规约</a>
中添加 <code>initContainers</code> 字段，
该字段以 <a href="/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core">Container</a>
类型对象数组的形式组织，和应用的 <code>containers</code> 数组同级相邻。
参阅 API 参考的<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">容器</a>章节了解详情。</p>
<p>Init 容器的状态在 <code>status.initContainerStatuses</code> 字段中以容器状态数组的格式返回
（类似 <code>status.containerStatuses</code> 字段）。</p>
<!--
### Differences from regular containers

Init containers support all the fields and features of app containers,
including resource limits, volumes, and security settings. However, the
resource requests and limits for an init container are handled differently,
as documented in [Resources](#resources).

Also, init containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or
`startupProbe` because they must run to completion before the Pod can be ready.

If you specify multiple init containers for a Pod, Kubelet runs each init
container sequentially. Each init container must succeed before the next can run.
When all of the init containers have run to completion, Kubelet initializes
the application containers for the Pod and runs them as usual.
-->
<h3 id="与普通容器的不同之处">与普通容器的不同之处</h3>
<p>Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。
然而，Init 容器对资源请求和限制的处理稍有不同，在下面<a href="#resources">资源</a>节有说明。</p>
<p>同时 Init 容器不支持 <code>lifecycle</code>、<code>livenessProbe</code>、<code>readinessProbe</code> 和 <code>startupProbe</code>，
因为它们必须在 Pod 就绪之前运行完成。</p>
<p>如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。
每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时，
Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。</p>
<!--
## Using init containers

Because init containers have separate images from app containers, they
have some advantages for start-up related code:

* Init containers can contain utilities or custom code for setup that are not present in an app
  image. For example, there is no need to make an image `FROM` another image just to use a tool like
  `sed`, `awk`, `python`, or `dig` during setup.
* The application image builder and deployer roles can work independently without
  the need to jointly build a single app image.
* Init containers can run with a different view of the filesystem than app containers in the
  same Pod. Consequently, they can be given access to
  <a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secrets'>Secrets</a> that app containers cannot access.
* Because init containers run to completion before any app containers start, init containers offer
  a mechanism to block or delay app container startup until a set of preconditions are met. Once
  preconditions are met, all of the app containers in a Pod can start in parallel.
* Init containers can securely run utilities or custom code that would otherwise make an app
  container image less secure. By keeping unnecessary tools separate you can limit the attack
  surface of your app container image.
-->
<h2 id="使用-init-容器">使用 Init 容器</h2>
<p>因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势：</p>
<ul>
<li>
<p>Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。
例如，没有必要仅为了在安装过程中使用类似 <code>sed</code>、<code>awk</code>、<code>python</code> 或 <code>dig</code>
这样的工具而去 <code>FROM</code> 一个镜像来生成一个新的镜像。</p>
</li>
<li>
<p>Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。</p>
</li>
<li>
<p>应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。</p>
</li>
<li>
<p>Init 容器能以不同于 Pod 内应用容器的文件系统视图运行。因此，Init 容器可以访问
应用容器不能访问的 <a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a> 的权限。</p>
</li>
<li>
<p>由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器
提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。
一旦前置条件满足，Pod 内的所有的应用容器会并行启动。</p>
</li>
</ul>
<!--
### Examples

Here are some ideas for how to use init containers:

* Wait for a <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> to
  be created, using a shell one-line command like:
  ```shell
  for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1
  ```

 * Register this Pod with a remote server from the downward API with a command like:
  ```shell
  curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'
  ```
* Wait for some time before starting the app container with a command like
  ```shell
  sleep 60
  ```

* Clone a Git repository into a <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='Volume'>Volume</a>

* Place values into a configuration file and run a template tool to dynamically
  generate a configuration file for the main app container. For example,
  place the `POD_IP` value in a configuration and generate the main app
  configuration file using Jinja.
-->
<h3 id="examples">示例 </h3>
<p>下面是一些如何使用 Init 容器的想法：</p>
<ul>
<li>
<p>等待一个 Service 完成创建，通过类似如下 shell 命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">{</span>1..100<span style="color:#666">}</span>; <span style="color:#a2f;font-weight:bold">do</span> sleep 1; <span style="color:#a2f;font-weight:bold">if</span> dig myservice; <span style="color:#a2f;font-weight:bold">then</span> <span style="color:#a2f">exit</span> 0; <span style="color:#a2f;font-weight:bold">fi</span>; <span style="color:#a2f;font-weight:bold">done</span>; <span style="color:#a2f">exit</span> <span style="color:#666">1</span>
</code></pre></div></li>
<li>
<p>注册这个 Pod 到远程服务器，通过在命令中调用 API，类似如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -X POST http://<span style="color:#b8860b">$MANAGEMENT_SERVICE_HOST</span>:<span style="color:#b8860b">$MANAGEMENT_SERVICE_PORT</span>/register <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -d <span style="color:#b44">&#39;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&#39;</span>
</code></pre></div></li>
<li>
<p>在启动应用容器之前等一段时间，使用类似命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sleep <span style="color:#666">60</span>
</code></pre></div></li>
<li>
<p>克隆 Git 仓库到<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>中。</p>
</li>
<li>
<p>将配置值放到配置文件中，运行模板工具为主应用容器动态地生成配置文件。
例如，在配置文件中存放 <code>POD_IP</code> 值，并使用 Jinja 生成主应用配置文件。</p>
</li>
</ul>
<!--
#### Init containers in use

This example defines a simple Pod that has two init containers.
The first waits for `myservice`, and the second waits for `mydb`. Once both
init containers complete, the Pod runs the app container from its `spec` section.
-->
<h3 id="使用-init-容器的情况">使用 Init 容器的情况</h3>
<p>下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 <code>myservice</code> 启动，
第二个等待 <code>mydb</code> 启动。 一旦这两个 Init容器 都启动完成，Pod 将启动 <code>spec</code> 节中的应用容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myapp-pod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>myapp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myapp-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#39;sh&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;-c&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;echo The app is running! &amp;&amp; sleep 3600&#39;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">initContainers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>init-myservice<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#39;sh&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;-c&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>init-mydb<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#39;sh&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;-c&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
You can start this Pod by running:
-->
<p>你通过运行下面的命令启动 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f myapp.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>pod/myapp-pod created
</code></pre><!--
And check on its status with:
-->
<p>使用下面的命令检查其状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get -f myapp.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
</code></pre><!--
or for more details:
-->
<p>或者查看更多详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe -f myapp.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Name:          myapp-pod
Namespace:     default
[...]
Labels:        app=myapp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &quot;busybox&quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &quot;busybox&quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634
</code></pre><!--
To see logs for the init containers in this Pod, run:
-->
<p>如需查看 Pod 内 Init 容器的日志，请执行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs myapp-pod -c init-myservice <span style="color:#080;font-style:italic"># 查看第一个 Init 容器</span>
kubectl logs myapp-pod -c init-mydb      <span style="color:#080;font-style:italic"># 查看第二个 Init 容器</span>
</code></pre></div><!--
At this point, those init containers will be waiting to discover Services named
`mydb` and `myservice`.

Here's a configuration you can use to make those Services appear:
-->
<p>在这一刻，Init 容器将会等待至发现名称为 <code>mydb</code> 和 <code>myservice</code> 的 Service。</p>
<p>如下为创建这些 Service 的配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myservice<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mydb<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9377</span><span style="color:#bbb">
</span></code></pre></div><!--
To create the `mydb` and `myservice` services:
-->
<p>创建 <code>mydb</code> 和 <code>myservice</code> 服务的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f services.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>service &quot;myservice&quot; created
service &quot;mydb&quot; created
</code></pre><!--
You'll then see that those init containers complete, and that the `myapp-pod`
Pod moves into the Running state:
-->
<p>这样你将能看到这些 Init 容器执行完毕，随后 <code>my-app</code> 的 Pod 进入 <code>Running</code> 状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get -f myapp.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
</code></pre><!--
This simple example should provide some inspiration for you to create your own
init containers. [What's next](#what-s-next) contains a link to a more detailed example.
-->
<p>这个简单例子应该能为你创建自己的 Init 容器提供一些启发。
<a href="#what-s-next">接下来</a>节提供了更详细例子的链接。</p>
<!--
## Detailed behavior

During Pod startup, the kubelet delays running init containers until the networking
and storage are ready. Then the kubelet runs the Pod's init containers in the order
they appear in the Pod's spec.

Each init container must exit successfully before
the next container starts. If a container fails to start due to the runtime or
exits with failure, it is retried according to the Pod `restartPolicy`. However,
if the Pod `restartPolicy` is set to Always, the init containers use
`restartPolicy` OnFailure.

A Pod cannot be `Ready` until all init containers have succeeded. The ports on an
init container are not aggregated under a Service. A Pod that is initializing
is in the `Pending` state but should have a condition `Initialized` set to false.

If the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers
must execute again.
-->
<h2 id="detailed-behavior">具体行为</h2>
<p>在 Pod 启动过程中，每个 Init 容器会在网络和数据卷初始化之后按顺序启动。
kubelet 运行依据 Init 容器在 Pod 规约中的出现顺序依次运行之。</p>
<p>每个 Init 容器成功退出后才会启动下一个 Init 容器。
如果某容器因为容器运行时的原因无法启动，或以错误状态退出，kubelet 会根据
Pod 的 <code>restartPolicy</code> 策略进行重试。
然而，如果 Pod 的 <code>restartPolicy</code> 设置为 &quot;Always&quot;，Init 容器失败时会使用
<code>restartPolicy</code> 的 &quot;OnFailure&quot; 策略。</p>
<p>在所有的 Init 容器没有成功之前，Pod 将不会变成 <code>Ready</code> 状态。
Init 容器的端口将不会在 Service 中进行聚集。正在初始化中的 Pod 处于 <code>Pending</code> 状态，
但会将状况 <code>Initializing</code> 设置为 false。</p>
<p>如果 Pod <a href="#pod-restart-reasons">重启</a>，所有 Init 容器必须重新执行。</p>
<!--
Changes to the init container spec are limited to the container image field.
Altering an init container image field is equivalent to restarting the Pod.

Because init containers can be restarted, retried, or re-executed, init container
code should be idempotent. In particular, code that writes to files on `EmptyDirs`
should be prepared for the possibility that an output file already exists.

Init containers have all of the fields of an app container. However, Kubernetes
prohibits `readinessProbe` from being used because init containers cannot
define readiness distinct from completion. This is enforced during validation.

-->
<p>对 Init 容器规约的修改仅限于容器的 <code>image</code> 字段。
更改 Init 容器的 <code>image</code> 字段，等同于重启该 Pod。</p>
<p>因为 Init 容器可能会被重启、重试或者重新执行，所以 Init 容器的代码应该是幂等的。
特别地，基于 <code>emptyDirs</code> 写文件的代码，应该对输出文件可能已经存在做好准备。</p>
<p>Init 容器具有应用容器的所有字段。然而 Kubernetes 禁止使用 <code>readinessProbe</code>，
因为 Init 容器不能定义不同于完成态（Completion）的就绪态（Readiness）。
Kubernetes 会在校验时强制执行此检查。</p>
<!--
Use `activeDeadlineSeconds` on the Pod to prevent init containers from failing forever.
The active deadline includes init containers.
However it is recommended to use `activeDeadlineSeconds` only if teams deploy their application
as a Job, because `activeDeadlineSeconds` has an effect even after initContainer finished.
The Pod which is already running correctly would be killed by `activeDeadlineSeconds` if you set.

The name of each app and init container in a Pod must be unique; a
validation error is thrown for any container sharing a name with another.
-->
<p>在 Pod 上使用 <code>activeDeadlineSeconds</code> 和在容器上使用 <code>livenessProbe</code> 可以避免
Init 容器一直重复失败。
<code>activeDeadlineSeconds</code> 时间包含了 Init 容器启动的时间。
但建议仅在团队将其应用程序部署为 Job 时才使用 <code>activeDeadlineSeconds</code>，
因为 <code>activeDeadlineSeconds</code> 在 Init 容器结束后仍有效果。
如果你设置了 <code>activeDeadlineSeconds</code>，已经在正常运行的 Pod 会被杀死。</p>
<p>在 Pod 中的每个应用容器和 Init 容器的名称必须唯一；
与任何其它容器共享同一个名称，会在校验时抛出错误。</p>
<!--
### Resources

Given the ordering and execution for init containers, the following rules
for resource usage apply:

* The highest of any particular resource request or limit defined on all init
  containers is the *effective init request/limit*. If any resource has no
  resource limit specified this is considered as the highest limit.
* The Pod's *effective request/limit* for a resource is the higher of:
  * the sum of all app containers request/limit for a resource
  * the effective init request/limit for a resource
* Scheduling is done based on effective requests/limits, which means
  init containers can reserve resources for initialization that are not used
  during the life of the Pod.
* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the
  QoS tier for init containers and app containers alike.
-->
<h3 id="resources">资源</h3>
<p>在给定的 Init 容器执行顺序下，资源使用适用于如下规则：</p>
<ul>
<li>所有 Init 容器上定义的任何特定资源的 limit 或 request 的最大值，作为 Pod <em>有效初始 request/limit</em>。
如果任何资源没有指定资源限制，这被视为最高限制。</li>
<li>Pod 对资源的 <em>有效 limit/request</em> 是如下两者的较大者：
<ul>
<li>所有应用容器对某个资源的 limit/request 之和</li>
<li>对某个资源的有效初始 limit/request</li>
</ul>
</li>
<li>基于有效 limit/request 完成调度，这意味着 Init 容器能够为初始化过程预留资源，
这些资源在 Pod 生命周期过程中并没有被使用。</li>
<li>Pod 的 <em>有效 QoS 层</em> ，与 Init 容器和应用容器的一样。</li>
</ul>
<!--
Quota and limits are applied based on the effective Pod request and limit.
Pod level control groups (cgroups) are based on the effective Pod request and limit, the same as the scheduler.
-->
<p>配额和限制适用于有效 Pod 的请求和限制值。
Pod 级别的 cgroups 是基于有效 Pod 的请求和限制值，和调度器相同。</p>
<!--
### Pod restart reasons

A Pod can restart, causing re-execution of init containers, for the following
reasons:

* The Pod infrastructure container is restarted. This is uncommon and would
  have to be done by someone with root access to nodes.
* All containers in a Pod are terminated while `restartPolicy` is set to Always,
  forcing a restart, and the init container completion record has been lost due
  to garbage collection.
-->
<h3 id="pod-restart-reasons">Pod 重启的原因 </h3>
<p>Pod 重启会导致 Init 容器重新执行，主要有如下几个原因：</p>
<ul>
<li>
<p>Pod 的基础设施容器 (译者注：如 <code>pause</code> 容器) 被重启。这种情况不多见，
必须由具备 root 权限访问节点的人员来完成。</p>
</li>
<li>
<p>当 <code>restartPolicy</code> 设置为 &quot;<code>Always</code>&quot;，Pod 中所有容器会终止而强制重启。
由于垃圾收集机制的原因，Init 容器的完成记录将会丢失。</p>
</li>
</ul>
<!--
The Pod will not be restarted when the init container image is changed, or the
init container completion record has been lost due to garbage collection. This
applies for Kubernetes v1.20 and later. If you are using an earlier version of
Kubernetes, consult the documentation for the version you are using.
-->
<p>当 Init 容器的镜像发生改变或者 Init 容器的完成记录因为垃圾收集等原因被丢失时，
Pod 不会被重启。这一行为适用于 Kubernetes v1.20 及更新版本。如果你在使用较早
版本的 Kubernetes，可查阅你所使用的版本对应的文档。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container)
* Learn how to [debug init containers](/docs/tasks/debug-application-cluster/debug-init-containers/)
-->
<ul>
<li>阅读<a href="/zh/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container">创建包含 Init 容器的 Pod</a></li>
<li>学习如何<a href="/zh/docs/tasks/debug-application-cluster/debug-init-containers/">调试 Init 容器</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c8d62295ca703fdcef1aaf89fb4c916a">1.3 - Pod 拓扑分布约束</h1>
    
	<!--
title: Pod Topology Spread Constraints
content_type: concept
weight: 40
-->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code>
</div>


<!--
leave this shortcode in place until the note about EvenPodsSpread is obsolete
-->
<!-- overview -->
<!--
You can use _topology spread constraints_ to control how <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource utilization.
-->
<p>你可以使用 <em>拓扑分布约束（Topology Spread Constraints）</em> 来控制
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> 在集群内故障域
之间的分布，例如区域（Region）、可用区（Zone）、节点和其他用户自定义拓扑域。
这样做有助于实现高可用并提升资源利用率。</p>
<!-- body -->
<!--
## Prerequisites

### Node Labels
-->
<h2 id="prerequisites">先决条件  </h2>
<h3 id="node-labels">节点标签  </h3>
<!--
Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in. For example, a Node might have labels: `node=node1,zone=us-east-1a,region=us-east-1`
-->
<p>拓扑分布约束依赖于节点标签来标识每个节点所在的拓扑域。
例如，某节点可能具有标签：<code>node=node1,zone=us-east-1a,region=us-east-1</code></p>
<!--
Suppose you have a 4-node cluster with the following labels:
-->
<p>假设你拥有具有以下标签的一个 4 节点集群：</p>
<pre><code>NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre><!--
Then the cluster is logically viewed as below:
-->
<p>那么，从逻辑上看集群如下：</p>
<figure>
<div class="mermaid">
    
graph TB
    subgraph "zoneB"
        n3(Node3)
        n4(Node4)
    end
    subgraph "zoneA"
        n1(Node1)
        n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4 k8s;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
Instead of manually applying labels, you can also reuse the [well-known labels](/docs/reference/labels-annotations-taints/) that are created and populated automatically on most clusters.
-->
<p>你可以复用在大多数集群上自动创建和填充的<a href="/zh/docs/reference/labels-annotations-taints/">常用标签</a>，
而不是手动添加标签。</p>
<!--
## Spread Constraints for Pods
-->
<h2 id="spread-constraints-for-pods">Pod 的分布约束   </h2>
<h3 id="api">API</h3>
<!--
The API field `pod.spec.topologySpreadConstraints` is defined as below:
-->
<p><code>pod.spec.topologySpreadConstraints</code> 字段定义如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologySpreadConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span>&lt;integer&gt;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>&lt;string&gt;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>&lt;string&gt;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb"> </span>&lt;object&gt;<span style="color:#bbb">
</span></code></pre></div><!--
You can define one or multiple `topologySpreadConstraint` to instruct the kube-scheduler how to place each incoming Pod in relation to the existing Pods across your cluster. The fields are:
-->
<p>你可以定义一个或多个 <code>topologySpreadConstraint</code> 来指示 kube-scheduler
如何根据与现有的 Pod 的关联关系将每个传入的 Pod 部署到集群中。字段包括：</p>
<!--
- **maxSkew** describes the degree to which Pods may be unevenly distributed.
  It's the maximum permitted difference between the number of matching Pods in
  any two topology domains of a given topology type. It must be greater than
  zero. Its semantics differs according to the value of `whenUnsatisfiable`:
  - when `whenUnsatisfiable` equals to "DoNotSchedule", `maxSkew` is the maximum
    permitted difference between the number of matching pods in the target
    topology and the global minimum.
  - when `whenUnsatisfiable` equals to "ScheduleAnyway", scheduler gives higher
    precedence to topologies that would help reduce the skew.
- **topologyKey** is the key of node labels. If two Nodes are labelled with this key and have identical values for that label, the scheduler treats both Nodes as being in the same topology. The scheduler tries to place a balanced number of Pods into each topology domain.
- **whenUnsatisfiable** indicates how to deal with a Pod if it doesn't satisfy the spread constraint:
    - `DoNotSchedule` (default) tells the scheduler not to schedule it.
    - `ScheduleAnyway` tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.
- **labelSelector** is used to find matching Pods. Pods that match this label selector are counted to determine the number of Pods in their corresponding topology domain. See [Label Selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors) for more details.
-->
<ul>
<li><strong>maxSkew</strong> 描述 Pod 分布不均的程度。这是给定拓扑类型中任意两个拓扑域中
匹配的 pod 之间的最大允许差值。它必须大于零。取决于 <code>whenUnsatisfiable</code> 的
取值，其语义会有不同。
<ul>
<li>当 <code>whenUnsatisfiable</code> 等于 &quot;DoNotSchedule&quot; 时，<code>maxSkew</code> 是目标拓扑域
中匹配的 Pod 数与全局最小值之间可存在的差异。</li>
<li>当 <code>whenUnsatisfiable</code> 等于 &quot;ScheduleAnyway&quot; 时，调度器会更为偏向能够降低
偏差值的拓扑域。</li>
</ul>
</li>
<li><strong>topologyKey</strong> 是节点标签的键。如果两个节点使用此键标记并且具有相同的标签值，
则调度器会将这两个节点视为处于同一拓扑域中。调度器试图在每个拓扑域中放置数量
均衡的 Pod。</li>
<li><strong>whenUnsatisfiable</strong> 指示如果 Pod 不满足分布约束时如何处理：
<ul>
<li><code>DoNotSchedule</code>（默认）告诉调度器不要调度。</li>
<li><code>ScheduleAnyway</code> 告诉调度器仍然继续调度，只是根据如何能将偏差最小化来对
节点进行排序。</li>
</ul>
</li>
<li><strong>labelSelector</strong> 用于查找匹配的 pod。匹配此标签的 Pod 将被统计，以确定相应
拓扑域中 Pod 的数量。
有关详细信息，请参考<a href="/zh/docs/concepts/overview/working-with-objects/labels/#label-selectors">标签选择算符</a>。</li>
</ul>
<!--
When a Pod defines more than one `topologySpreadConstraint`, those constraints are ANDed: The kube-scheduler looks for a node for the incoming Pod that satisfies all the constraints.
-->
<p>当 Pod 定义了不止一个 <code>topologySpreadConstraint</code>，这些约束之间是逻辑与的关系。
kube-scheduler 会为新的 Pod 寻找一个能够满足所有约束的节点。</p>
<!--
You can read more about this field by running `kubectl explain Pod.spec.topologySpreadConstraints`.
-->
<p>你可以执行 <code>kubectl explain Pod.spec.topologySpreadConstraints</code> 命令以
了解关于 topologySpreadConstraints 的更多信息。</p>
<!--
### Example: One TopologySpreadConstraint

Suppose you have a 4-node cluster where 3 Pods labeled `foo:bar` are located in node1, node2 and node3 respectively:
-->
<h3 id="例子-单个-topologyspreadconstraint">例子：单个 TopologySpreadConstraint</h3>
<p>假设你拥有一个 4 节点集群，其中标记为 <code>foo:bar</code> 的 3 个 Pod 分别位于
node1、node2 和 node3 中：</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p3(Pod) --> n3(Node3)
        n4(Node4)
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4,p1,p2,p3 k8s;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
If we want an incoming Pod to be evenly spread with existing Pods across zones, the spec can be given as:
-->
<p>如果希望新来的 Pod 均匀分布在现有的可用区域，则可以按如下设置其规约：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/topology-spread-constraints/one-constraint.yaml" download="pods/topology-spread-constraints/one-constraint.yaml"><code>pods/topology-spread-constraints/one-constraint.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-topology-spread-constraints-one-constraint-yaml')" title="Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-topology-spread-constraints-one-constraint-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologySpreadConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:3.1</code></pre></div>
    </div>
</div>


<!--
`topologyKey: zone` implies the even distribution will only be applied to the nodes which have label pair "zone:&lt;any value&gt;" present. `whenUnsatisfiable: DoNotSchedule` tells the scheduler to let it stay pending if the incoming Pod can’t satisfy the constraint.
-->
<p><code>topologyKey: zone</code> 意味着均匀分布将只应用于存在标签键值对为
&quot;zone:&lt;any value&gt;&quot; 的节点。
<code>whenUnsatisfiable: DoNotSchedule</code> 告诉调度器如果新的 Pod 不满足约束，
则让它保持悬决状态。</p>
<!--
If the scheduler placed this incoming Pod into "zoneA", the Pods distribution would become [3, 1],
hence the actual skew is 2 (3 - 1) - which violates `maxSkew: 1`. In this example, the incoming Pod can only be placed onto "zoneB":
-->
<p>如果调度器将新的 Pod 放入 &quot;zoneA&quot;，Pods 分布将变为 [3, 1]，因此实际的偏差
为 2（3 - 1）。这违反了 <code>maxSkew: 1</code> 的约定。此示例中，新 Pod 只能放置在
&quot;zoneB&quot; 上：</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p3(Pod) --> n3(Node3)
        p4(mypod) --> n4(Node4)
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4,p1,p2,p3 k8s;
    class p4 plain;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<p>或者</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p3(Pod) --> n3(Node3)
        p4(mypod) --> n3
        n4(Node4)
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4,p1,p2,p3 k8s;
    class p4 plain;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
You can tweak the Pod spec to meet various kinds of requirements:
-->
<p>你可以调整 Pod 规约以满足各种要求：</p>
<!--
- Change `maxSkew` to a bigger value like "2" so that the incoming Pod can be placed onto "zoneA" as well.
- Change `topologyKey` to "node" so as to distribute the Pods evenly across nodes instead of zones. In the above example, if `maxSkew` remains "1", the incoming Pod can only be placed onto "node4".
- Change `whenUnsatisfiable: DoNotSchedule` to `whenUnsatisfiable: ScheduleAnyway` to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs are satisfied). However, it’s preferred to be placed onto the topology domain which has fewer matching Pods. (Be aware that this preferability is jointly normalized with other internal scheduling priorities like resource usage ratio, etc.)
-->
<ul>
<li>将 <code>maxSkew</code> 更改为更大的值，比如 &quot;2&quot;，这样新的 Pod 也可以放在 &quot;zoneA&quot; 上。</li>
<li>将 <code>topologyKey</code> 更改为 &quot;node&quot;，以便将 Pod 均匀分布在节点上而不是区域中。
在上面的例子中，如果 <code>maxSkew</code> 保持为 &quot;1&quot;，那么传入的 Pod 只能放在 &quot;node4&quot; 上。</li>
<li>将 <code>whenUnsatisfiable: DoNotSchedule</code> 更改为 <code>whenUnsatisfiable: ScheduleAnyway</code>，
以确保新的 Pod 始终可以被调度（假设满足其他的调度 API）。
但是，最好将其放置在匹配 Pod 数量较少的拓扑域中。
（请注意，这一优先判定会与其他内部调度优先级（如资源使用率等）排序准则一起进行标准化。）</li>
</ul>
<!--
### Example: Multiple TopologySpreadConstraints
-->
<h3 id="例子-多个-topologyspreadconstraints">例子：多个 TopologySpreadConstraints</h3>
<!--
This builds upon the previous example. Suppose you have a 4-node cluster where 3 Pods labeled `foo:bar` are located in node1, node2 and node3 respectively (`P` represents Pod):
-->
<p>下面的例子建立在前面例子的基础上。假设你拥有一个 4 节点集群，其中 3 个标记为 <code>foo:bar</code> 的
Pod 分别位于 node1、node2 和 node3 上：</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p3(Pod) --> n3(Node3)
        n4(Node4)
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4,p1,p2,p3 k8s;
    class p4 plain;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
You can use 2 TopologySpreadConstraints to control the Pods spreading on both zone and node:
-->
<p>可以使用 2 个 TopologySpreadConstraint 来控制 Pod 在 区域和节点两个维度上的分布：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/topology-spread-constraints/two-constraints.yaml" download="pods/topology-spread-constraints/two-constraints.yaml"><code>pods/topology-spread-constraints/two-constraints.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-topology-spread-constraints-two-constraints-yaml')" title="Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-topology-spread-constraints-two-constraints-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologySpreadConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>node<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:3.1</code></pre></div>
    </div>
</div>


<!--
In this case, to match the first constraint, the incoming Pod can only be placed onto "zoneB"; while in terms of the second constraint, the incoming Pod can only be placed onto "node4". Then the results of 2 constraints are ANDed, so the only viable option is to place on "node4".
-->
<p>在这种情况下，为了匹配第一个约束，新的 Pod 只能放置在 &quot;zoneB&quot; 中；而在第二个约束中，
新的 Pod 只能放置在 &quot;node4&quot; 上。最后两个约束的结果加在一起，唯一可行的选择是放置
在 &quot;node4&quot; 上。</p>
<!--
Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:
-->
<p>多个约束之间可能存在冲突。假设有一个跨越 2 个区域的 3 节点集群：</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p4(Pod) --> n3(Node3)
        p5(Pod) --> n3
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n1
        p3(Pod) --> n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
If you apply "two-constraints.yaml" to this cluster, you will notice "mypod" stays in `Pending` state. This is because: to satisfy the first constraint, "mypod" can only be put to "zoneB"; while in terms of the second constraint, "mypod" can only put to "node2". Then a joint result of "zoneB" and "node2" returns nothing.
-->
<p>如果对集群应用 &quot;two-constraints.yaml&quot;，会发现 &quot;mypod&quot; 处于 <code>Pending</code> 状态。
这是因为：为了满足第一个约束，&quot;mypod&quot; 只能放在 &quot;zoneB&quot; 中，而第二个约束要求
&quot;mypod&quot; 只能放在 &quot;node2&quot; 上。Pod 调度无法满足两种约束。</p>
<!--
To overcome this situation, you can either increase the `maxSkew` or modify one of the constraints to use `whenUnsatisfiable: ScheduleAnyway`.
-->
<p>为了克服这种情况，你可以增加 <code>maxSkew</code> 或修改其中一个约束，让其使用
<code>whenUnsatisfiable: ScheduleAnyway</code>。</p>
<!--
### Interaction With Node Affinity and Node Selectors

The scheduler will skip the non-matching nodes from the skew calculations if the incoming Pod has `spec.nodeSelector` or `spec.affinity.nodeAffinity` defined.
-->
<h3 id="interaction-with-node-affinity-and-node-selectors">节点亲和性与节点选择器的相互作用  </h3>
<p>如果 Pod 定义了 <code>spec.nodeSelector</code> 或 <code>spec.affinity.nodeAffinity</code>，
调度器将在偏差计算中跳过不匹配的节点。</p>
<!--
### Example: TopologySpreadConstraints with NodeAffinity

Suppose you have a 5-node cluster ranging from zoneA to zoneC:
-->
<h3 id="示例-topologyspreadconstraints-与-nodeaffinity">示例：TopologySpreadConstraints 与 NodeAffinity</h3>
<p>假设你有一个跨越 zoneA 到 zoneC 的 5 节点集群：</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p3(Pod) --> n3(Node3)
        n4(Node4)
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n2(Node2)
    end

classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneC"
        n5(Node5)
    end

classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n5 k8s;
class zoneC cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
and you know that "zoneC" must be excluded. In this case, you can compose the yaml as below, so that "mypod" will be placed onto "zoneB" instead of "zoneC". Similarly `spec.nodeSelector` is also respected.
-->
<p>而且你知道 &quot;zoneC&quot; 必须被排除在外。在这种情况下，可以按如下方式编写 YAML，
以便将 &quot;mypod&quot; 放置在 &quot;zoneB&quot; 上，而不是 &quot;zoneC&quot; 上。同样，<code>spec.nodeSelector</code>
也要一样处理。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml" download="pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml"><code>pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml')" title="Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologySpreadConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>NotIn<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- zoneC<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:3.1</code></pre></div>
    </div>
</div>


<!--
The scheduler doesn't have prior knowledge of all the zones or other topology domains that a cluster has. They are determined from the existing nodes in the cluster. This could lead to a problem in autoscaled clusters, when a node pool (or node group) is scaled to zero nodes and the user is expecting them to scale up, because, in this case, those topology domains won't be considered until there is at least one node in them.
-->
<p>调度器不会预先知道集群拥有的所有区域和其他拓扑域。拓扑域由集群中存在的节点确定。
在自动伸缩的集群中，如果一个节点池（或节点组）的节点数量为零，
而用户正期望其扩容时，可能会导致调度出现问题。
因为在这种情况下，调度器不会考虑这些拓扑域信息，因为它们是空的，没有节点。</p>
<!--
### Other Noticeable Semantics

There are some implicit conventions worth noting here:
-->
<h3 id="other-noticeable-semantics">其他值得注意的语义  </h3>
<p>这里有一些值得注意的隐式约定：</p>
<!--
- Only the Pods holding the same namespace as the incoming Pod can be matching candidates.

- The scheduler will bypass the nodes without `topologySpreadConstraints[*].topologyKey` present. This implies that:

  1. the Pods located on those nodes do not impact `maxSkew` calculation - in the above example, suppose "node1" does not have label "zone", then the 2 Pods will be disregarded, hence the incoming Pod will be scheduled into "zoneA".
  2. the incoming Pod has no chances to be scheduled onto this kind of nodes - in the above example, suppose a "node5" carrying label `{zone-typo: zoneC}` joins the cluster, it will be bypassed due to the absence of label key "zone".
-->
<ul>
<li>只有与新的 Pod 具有相同命名空间的 Pod 才能作为匹配候选者。</li>
<li>调度器会忽略没有 <code>topologySpreadConstraints[*].topologyKey</code> 的节点。这意味着：
<ol>
<li>
<p>位于这些节点上的 Pod 不影响 <code>maxSkew</code> 的计算。
在上面的例子中，假设 &quot;node1&quot; 没有标签 &quot;zone&quot;，那么 2 个 Pod 将被忽略，
因此传入的 Pod 将被调度到 &quot;zoneA&quot; 中。</p>
</li>
<li>
<p>新的 Pod 没有机会被调度到这类节点上。
在上面的例子中，假设一个带有标签 <code>{zone-typo: zoneC}</code> 的 &quot;node5&quot; 加入到集群，
它将由于没有标签键 &quot;zone&quot; 而被忽略。</p>
</li>
</ol>
</li>
</ul>
<!--
- Be aware of what will happen if the incomingPod’s `topologySpreadConstraints[*].labelSelector` doesn’t match its own labels. In the above example, if we remove the incoming Pod’s labels, it can still be placed onto "zoneB" since the constraints are still satisfied. However, after the placement, the degree of imbalance of the cluster remains unchanged - it’s still zoneA having 2 Pods which hold label {foo:bar}, and zoneB having 1 Pod which holds label {foo:bar}. So if this is not what you expect, we recommend the workload’s `topologySpreadConstraints[*].labelSelector` to match its own labels.
-->
<ul>
<li>注意，如果新 Pod 的 <code>topologySpreadConstraints[*].labelSelector</code> 与自身的
标签不匹配，将会发生什么。
在上面的例子中，如果移除新 Pod 上的标签，Pod 仍然可以调度到 &quot;zoneB&quot;，因为约束仍然满足。
然而，在调度之后，集群的不平衡程度保持不变。zoneA 仍然有 2 个带有 {foo:bar} 标签的 Pod，
zoneB 有 1 个带有 {foo:bar} 标签的 Pod。
因此，如果这不是你所期望的，建议工作负载的 <code>topologySpreadConstraints[*].labelSelector</code>
与其自身的标签匹配。</li>
</ul>
<!--
### Cluster-level default constraints

It is possible to set default topology spread constraints for a cluster. Default
topology spread constraints are applied to a Pod if, and only if:

- It doesn't define any constraints in its `.spec.topologySpreadConstraints`.
- It belongs to a service, replication controller, replica set or stateful set.
-->
<h3 id="cluster-level-default-constraints">集群级别的默认约束  </h3>
<p>为集群设置默认的拓扑分布约束也是可能的。默认拓扑分布约束在且仅在以下条件满足
时才会应用到 Pod 上：</p>
<ul>
<li>Pod 没有在其 <code>.spec.topologySpreadConstraints</code> 设置任何约束；</li>
<li>Pod 隶属于某个服务、副本控制器、ReplicaSet 或 StatefulSet。</li>
</ul>
<!--
Default constraints can be set as part of the `PodTopologySpread` plugin args
in a [scheduling profile](/docs/reference/scheduling/config/#profiles).
The constraints are specified with the same [API above](#api), except that
`labelSelector` must be empty. The selectors are calculated from the services,
replication controllers, replica sets or stateful sets that the Pod belongs to.

An example configuration might look like follows:
-->
<p>你可以在 <a href="/zh/docs/reference/scheduling/config/#profiles">调度方案（Scheduling Profile）</a>
中将默认约束作为 <code>PodTopologySpread</code> 插件参数的一部分来设置。
约束的设置采用<a href="#api">如前所述的 API</a>，只是 <code>labelSelector</code> 必须为空。
选择算符是根据 Pod 所属的服务、副本控制器、ReplicaSet 或 StatefulSet 来设置的。</p>
<p>配置的示例可能看起来像下面这个样子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">profiles</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">schedulerName</span>:<span style="color:#bbb"> </span>default-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pluginConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>PodTopologySpread<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">defaultConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>ScheduleAnyway<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">defaultingType</span>:<span style="color:#bbb"> </span>List<span style="color:#bbb">
</span></code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The score produced by default scheduling constraints might conflict with the
score produced by the
[`SelectorSpread` plugin](/docs/reference/scheduling/config/#scheduling-plugins).
It is recommended that you disable this plugin in the scheduling profile when
using default constraints for `PodTopologySpread`.
-->
<p>默认调度约束所生成的评分可能与
<a href="/zh/docs/reference/scheduling/config/#scheduling-plugins"><code>SelectorSpread</code> 插件</a>
所生成的评分有冲突。
建议你在为 <code>PodTopologySpread</code> 设置默认约束是禁用调度方案中的该插件。
</div>
<!--
#### Internal default constraints
-->
<h4 id="internal-default-constraints">内部默认约束   </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code>
</div>


<!--
With the `DefaultPodTopologySpread` feature gate, enabled by default, the
legacy `SelectorSpread` plugin is disabled.
kube-scheduler uses the following default topology constraints for the
`PodTopologySpread` plugin configuration:
-->
<p>当你使用了默认启用的 <code>DefaultPodTopologySpread</code> 特性门控时，原来的
<code>SelectorSpread</code> 插件会被禁用。
kube-scheduler 会使用下面的默认拓扑约束作为 <code>PodTopologySpread</code> 插件的
配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">defaultConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>ScheduleAnyway<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;topology.kubernetes.io/zone&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>ScheduleAnyway<span style="color:#bbb">
</span></code></pre></div><!--
Also, the legacy `SelectorSpread` plugin, which provides an equivalent behavior,
is disabled.
-->
<p>此外，原来用于提供等同行为的 <code>SelectorSpread</code> 插件也会被禁用。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The `PodTopologySpread` plugin does not score the nodes that don't have
the topology keys specified in the spreading constraints. This might result
in a different default behavior compared to the legacy `SelectorSpread` plugin when
using the default topology constraints.
-->
<p>对于分布约束中所指定的拓扑键而言，<code>PodTopologySpread</code> 插件不会为不包含这些主键的节点评分。
这可能导致在使用默认拓扑约束时，其行为与原来的 <code>SelectorSpread</code> 插件的默认行为不同，</p>
<!--
If your nodes are not expected to have **both** `kubernetes.io/hostname` and
`topology.kubernetes.io/zone` labels set, define your own constraints
instead of using the Kubernetes defaults.
-->
<p>如果你的节点不会 <strong>同时</strong> 设置 <code>kubernetes.io/hostname</code> 和
<code>topology.kubernetes.io/zone</code> 标签，你应该定义自己的约束而不是使用
Kubernetes 的默认约束。</p>

</div>
<!--
If you don't want to use the default Pod spreading constraints for your cluster,
you can disable those defaults by setting `defaultingType` to `List` and leaving
empty `defaultConstraints` in the `PodTopologySpread` plugin configuration:
-->
<p>如果你不想为集群使用默认的 Pod 分布约束，你可以通过设置 <code>defaultingType</code> 参数为 <code>List</code>
并将 <code>PodTopologySpread</code> 插件配置中的 <code>defaultConstraints</code> 参数置空来禁用默认 Pod 分布约束。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">profiles</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">schedulerName</span>:<span style="color:#bbb"> </span>default-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pluginConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>PodTopologySpread<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">defaultConstraints</span>:<span style="color:#bbb"> </span>[]<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">defaultingType</span>:<span style="color:#bbb"> </span>List<span style="color:#bbb">
</span></code></pre></div><!--
## Comparison with PodAffinity/PodAntiAffinity

In Kubernetes, directives related to "Affinity" control how Pods are
scheduled - more packed or more scattered.
-->
<h2 id="与-podaffinity-podantiaffinity-相比较">与 PodAffinity/PodAntiAffinity 相比较</h2>
<p>在 Kubernetes 中，与“亲和性”相关的指令控制 Pod 的调度方式（更密集或更分散）。</p>
<!--
- For `PodAffinity`, you can try to pack any number of Pods into qualifying
  topology domain(s)
- For `PodAntiAffinity`, only one Pod can be scheduled into a
  single topology domain.
-->
<ul>
<li>对于 <code>PodAffinity</code>，你可以尝试将任意数量的 Pod 集中到符合条件的拓扑域中。</li>
<li>对于 <code>PodAntiAffinity</code>，只能将一个 Pod 调度到某个拓扑域中。</li>
</ul>
<!--
For finer control, you can specify topology spread constraints to distribute
Pods across different topology domains - to achieve either high availability or
cost-saving. This can also help on rolling update workloads and scaling out
replicas smoothly. See
[Motivation](https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation)
for more details.
-->
<p>要实现更细粒度的控制，你可以设置拓扑分布约束来将 Pod 分布到不同的拓扑域下，
从而实现高可用性或节省成本。这也有助于工作负载的滚动更新和平稳地扩展副本规模。
有关详细信息，请参考
<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20190221-pod-topology-spread.md#motivation">动机</a>文档。</p>
<!--
## Known Limitations

- There's no guarantee that the constraints remain satisfied when Pods are removed. For example, scaling down a Deployment may result in imbalanced Pods distribution.
You can use [Descheduler](https://github.com/kubernetes-sigs/descheduler) to rebalance the Pods distribution.

- Pods matched on tainted nodes are respected. See [Issue 80921](https://github.com/kubernetes/kubernetes/issues/80921)
-->
<h2 id="已知局限性">已知局限性</h2>
<ul>
<li>
<p>当 Pod 被移除时，无法保证约束仍被满足。例如，缩减某 Deployment 的规模时，
Pod 的分布可能不再均衡。
你可以使用 <a href="https://github.com/kubernetes-sigs/descheduler">Descheduler</a>
来重新实现 Pod 分布的均衡。</p>
</li>
<li>
<p>具有污点的节点上匹配的 Pods 也会被统计。
参考 <a href="https://github.com/kubernetes/kubernetes/issues/80921">Issue 80921</a>。</p>
</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
- [Blog: Introducing PodTopologySpread](https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/)
  explains `maxSkew` in details, as well as bringing up some advanced usage examples.
-->
<ul>
<li><a href="https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/">博客: PodTopologySpread介绍</a>
详细解释了 <code>maxSkew</code>，并给出了一些高级的使用示例。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4aaf43c715cd764bc8ed4436f3537e68">1.4 - 干扰（Disruptions）</h1>
    
	<!--
reviewers:
- erictune
- foxish
- davidopp
title: Disruptions
content_type: concept
weight: 60
-->
<!-- overview -->
<!--
This guide is for application owners who want to build
highly available applications, and thus need to understand
what types of Disruptions can happen to Pods.
-->
<p>本指南针对的是希望构建高可用性应用程序的应用所有者，他们有必要了解可能发生在 Pod 上的干扰类型。</p>
<!--
It is also for Cluster Administrators who want to perform automated
cluster actions, like upgrading and autoscaling clusters.
-->
<p>文档同样适用于想要执行自动化集群操作（例如升级和自动扩展集群）的集群管理员。</p>
<!-- body -->
<!--
## Voluntary and Involuntary Disruptions

Pods do not disappear until someone (a person or a controller) destroys them, or
there is an unavoidable hardware or system software error.
-->
<h2 id="voluntary-and-involuntary-disruptions">自愿干扰和非自愿干扰    </h2>
<p>Pod 不会消失，除非有人（用户或控制器）将其销毁，或者出现了不可避免的硬件或软件系统错误。</p>
<!--
We call these unavoidable cases *involuntary disruptions* to
an application.  Examples are:
-->
<p>我们把这些不可避免的情况称为应用的<em>非自愿干扰（Involuntary Disruptions）</em>。例如：</p>
<!--
- a hardware failure of the physical machine backing the node
- cluster administrator deletes VM (instance) by mistake
- cloud provider or hypervisor failure makes VM disappear
- a kernel panic
- the node disappears from the cluster due to cluster network partition
- eviction of a pod due to the node being [out-of-resources](/docs/concepts/scheduling-eviction/node-pressure-eviction/).
-->
<ul>
<li>节点下层物理机的硬件故障</li>
<li>集群管理员错误地删除虚拟机（实例）</li>
<li>云提供商或虚拟机管理程序中的故障导致的虚拟机消失</li>
<li>内核错误</li>
<li>节点由于集群网络隔离从集群中消失</li>
<li>由于节点<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/">资源不足</a>导致 pod 被驱逐。</li>
</ul>
<!--
Except for the out-of-resources condition, all these conditions
should be familiar to most users; they are not specific
to Kubernetes.
-->
<p>除了资源不足的情况，大多数用户应该都熟悉这些情况；它们不是特定于 Kubernetes 的。</p>
<!--
We call other cases *voluntary disruptions*.  These include both
actions initiated by the application owner and those initiated by a Cluster
Administrator.  Typical application owner actions include:
-->
<p>我们称其他情况为<em>自愿干扰（Voluntary Disruptions）</em>。
包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者的操
作包括：</p>
<!--
- deleting the deployment or other controller that manages the pod
- updating a deployment's pod template causing a restart
- directly deleting a pod (e.g. by accident)
-->
<ul>
<li>删除 Deployment 或其他管理 Pod 的控制器</li>
<li>更新了 Deployment 的 Pod 模板导致 Pod 重启</li>
<li>直接删除 Pod（例如，因为误操作）</li>
</ul>
<!--
Cluster Administrator actions include:

- [Draining a node](/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.
- Draining a node from a cluster to scale the cluster down (learn about
[Cluster Autoscaling](https://github.com/kubernetes/autoscaler/#readme)
).
- Removing a pod from a node to permit something else to fit on that node.
-->
<p>集群管理员操作包括：</p>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/safely-drain-node/">排空（drain）节点</a>进行修复或升级。</li>
<li>从集群中排空节点以缩小集群（了解<a href="https://github.com/kubernetes/autoscaler/#readme">集群自动扩缩</a>）。</li>
<li>从节点中移除一个 Pod，以允许其他 Pod 使用该节点。</li>
</ul>
<!--
These actions might be taken directly by the cluster administrator, or by automation
run by the cluster administrator, or by your cluster hosting provider.
-->
<p>这些操作可能由集群管理员直接执行，也可能由集群管理员所使用的自动化工具执行，或者由集群托管提供商自动执行。</p>
<!--
Ask your cluster administrator or consult your cloud provider or distribution documentation
to determine if any sources of voluntary disruptions are enabled for your cluster.
If none are enabled, you can skip creating Pod Disruption Budgets.
-->
<p>咨询集群管理员或联系云提供商，或者查询发布文档，以确定是否为集群启用了任何资源干扰源。
如果没有启用，可以不用创建 Pod Disruption Budgets（Pod 干扰预算）</p>
<!--
Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example,
deleting deployments or pods bypasses Pod Disruption Budgets.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 并非所有的自愿干扰都会受到 Pod 干扰预算的限制。
例如，删除 Deployment 或 Pod 的删除操作就会跳过 Pod 干扰预算检查。
</div>

<!--
## Dealing with Disruptions

Here are some ways to mitigate involuntary disruptions:
-->
<h2 id="处理干扰">处理干扰</h2>
<p>以下是减轻非自愿干扰的一些方法：</p>
<!--
- Ensure your pod [requests the resources](/docs/tasks/configure-pod-container/assign-cpu-ram-container) it needs.
- Replicate your application if you need higher availability.  (Learn about running replicated
[stateless](/docs/tasks/run-application/run-stateless-application-deployment/)
and [stateful](/docs/tasks/run-application/run-replicated-stateful-application/) applications.)
- For even higher availability when running replicated applications,
  spread applications across racks (using
  [anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity))
  or across zones (if using a
  [multi-zone cluster](/docs/setup/multiple-zones).)
-->
<ul>
<li>确保 Pod 在请求中给出<a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">所需资源</a>。</li>
<li>如果需要更高的可用性，请复制应用程序。
（了解有关运行多副本的<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">无状态</a>
和<a href="/zh/docs/tasks/run-application/run-replicated-stateful-application/">有状态</a>应用程序的信息。）</li>
<li>为了在运行复制应用程序时获得更高的可用性，请跨机架（使用
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">反亲和性</a>
或跨区域（如果使用<a href="/zh/docs/setup/best-practices/multiple-zones/">多区域集群</a>）扩展应用程序。</li>
</ul>
<!--
The frequency of voluntary disruptions varies.  On a basic Kubernetes cluster, there are
no automated voluntary disruptions (only user-triggered ones).  However, your cluster administrator or hosting provider
may run some additional services which cause voluntary disruptions. For example,
rolling out node software updates can cause voluntary disruptions. Also, some implementations
of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.
Your cluster administrator or hosting provider should have documented what level of voluntary
disruptions, if any, to expect. Certain configuration options, such as
[using PriorityClasses](/docs/concepts/scheduling-eviction/pod-priority-preemption/)
in your pod spec can also cause voluntary (and involuntary) disruptions.
-->
<p>自愿干扰的频率各不相同。在一个基本的 Kubernetes 集群中，没有自愿干扰（只有用户触发的干扰）。
然而，集群管理员或托管提供商可能运行一些可能导致自愿干扰的额外服务。例如，节点软
更新可能导致自愿干扰。另外，集群（节点）自动缩放的某些
实现可能导致碎片整理和紧缩节点的自愿干扰。集群
管理员或托管提供商应该已经记录了各级别的自愿干扰（如果有的话）。
有些配置选项，例如在 pod spec 中
<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">使用 PriorityClasses</a>
也会产生自愿（和非自愿）的干扰。</p>
<!--
Kubernetes offers features to help run highly available applications at the same
time as frequent voluntary disruptions.  We call this set of features
*Disruption Budgets*.
-->
<p>Kubernetes 提供特性来满足在出现频繁自愿干扰的同时运行高可用的应用程序。我们称这些特性为
<em>干扰预算（Disruption Budget）</em>。</p>
<!--
## Pod disruption budgets

Kubernetes offers features to help you run highly available applications even when you
introduce frequent voluntary disruptions.

An Application Owner can create a `PodDisruptionBudget` object (PDB) for each application.
A PDB limits the number of pods of a replicated application that are down simultaneously from
voluntary disruptions.  For example, a quorum-based application would
like to ensure that the number of replicas running is never brought below the
number needed for a quorum. A web front end might want to
ensure that the number of replicas serving load never falls below a certain
percentage of the total.
-->
<h2 id="pod-disruption-budgets">干扰预算  </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>


<p>即使你会经常引入自愿性干扰，Kubernetes 也能够支持你运行高度可用的应用。</p>
<p>应用程序所有者可以为每个应用程序创建 <code>PodDisruptionBudget</code> 对象（PDB）。
PDB 将限制在同一时间因自愿干扰导致的复制应用程序中宕机的 pod 数量。
例如，基于票选机制的应用程序希望确保运行的副本数永远不会低于仲裁所需的数量。
Web 前端可能希望确保提供负载的副本数量永远不会低于总数的某个百分比。</p>
<!--
Cluster managers and hosting providers should use tools which
respect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#eviction-api)
instead of directly deleting pods or deployments.  Examples are the `kubectl drain` command
and the Kubernetes-on-GCE cluster upgrade script (`cluster/gce/upgrade.sh`).
-->
<p>集群管理员和托管提供商应该使用遵循 PodDisruptionBudgets 的接口
（通过调用<a href="/zh/docs/tasks/administer-cluster/safely-drain-node/#the-eviction-api">Eviction API</a>），
而不是直接删除 Pod 或 Deployment。</p>
<!--
For example, the `kubectl drain` subcommand lets you mark a node as going out of
service. When you run `kubectl drain`, the tool tries to evict all of the Pods on
the Node you'are taking out of service. The eviction request may be temporarily rejected,
and the tool periodically retries all failed requests until all pods
are terminated, or until a configurable timeout is reached.
-->
<p>例如，<code>kubectl drain</code> 命令可以用来标记某个节点即将停止服务。
运行 <code>kubectl drain</code> 命令时，工具会尝试驱逐机器上的所有 Pod。
<code>kubectl</code> 所提交的驱逐请求可能会暂时被拒绝，所以该工具会定时重试失败的请求，
直到所有的 Pod 都被终止，或者达到配置的超时时间。</p>
<!--
A PDB specifies the number of replicas that an application can tolerate having, relative to how
many it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is
supposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,
then the Eviction API will allow voluntary disruption of one, but not two pods, at a time.
-->
<p>PDB 指定应用程序可以容忍的副本数量（相当于应该有多少副本）。
例如，具有 <code>.spec.replicas: 5</code> 的 Deployment 在任何时间都应该有 5 个 Pod。
如果 PDB 允许其在某一时刻有 4 个副本，那么驱逐 API 将允许同一时刻仅有一个而不是两个 Pod 自愿干扰。</p>
<!--
The group of pods that comprise the application is specified using a label selector, the same
as the one used by the application's controller (deployment, stateful-set, etc).
-->
<p>使用标签选择器来指定构成应用程序的一组 Pod，这与应用程序的控制器（Deployment，StatefulSet 等）
选择 Pod 的逻辑一样。</p>
<!--
The "intended" number of pods is computed from the `.spec.replicas` of the pods controller.
The controller is discovered from the pods using the `.metadata.ownerReferences` of the object.
-->
<p>Pod 控制器的 <code>.spec.replicas</code> 计算“预期的” Pod 数量。
根据 Pod 对象的 <code>.metadata.ownerReferences</code> 字段来发现控制器。</p>
<!--
[Involuntary disruptions](#voluntary-and-involuntary-disruptions) cannot be prevented by PDBs; however they
do count against the budget.
-->
<p>PDB 无法防止<a href="#voluntary-and-involuntary-disruptions">非自愿干扰</a>；
但它们确实计入预算。</p>
<!--
Pods which are deleted or unavailable due to a rolling upgrade to an application do count
against the disruption budget, but controllers (like deployment and stateful-set)
are not limited by PDBs when doing rolling upgrades - the handling of failures
during application updates is configured in spec for the specific workload resource.
-->
<p>由于应用程序的滚动升级而被删除或不可用的 Pod 确实会计入干扰预算，
但是控制器（如 Deployment 和 StatefulSet）在进行滚动升级时不受 PDB
的限制。应用程序更新期间的故障处理方式是在对应的工作负载资源的 <code>spec</code> 中配置的。</p>
<!--
When a pod is evicted using the eviction API, it is gracefully
[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination),
hornoring the
`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core).
-->
<p>当使用驱逐 API 驱逐 Pod 时，Pod 会被体面地
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">终止</a>，期间会
参考 <a href="/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core">PodSpec</a>
中的 <code>terminationGracePeriodSeconds</code> 配置值。</p>
<!--
## PDB Example

Consider a cluster with 3 nodes, `node-1` through `node-3`.
The cluster is running several applications.  One of them has 3 replicas initially called
`pod-a`, `pod-b`, and `pod-c`.  Another, unrelated pod without a PDB, called `pod-x`, is also shown.
Initially, the pods are laid out as follows:
-->
<h2 id="pdb-example">PDB 例子  </h2>
<p>假设集群有 3 个节点，<code>node-1</code> 到 <code>node-3</code>。集群上运行了一些应用。
其中一个应用有 3 个副本，分别是 <code>pod-a</code>，<code>pod-b</code> 和 <code>pod-c</code>。
另外，还有一个不带 PDB 的无关 pod <code>pod-x</code> 也同样显示出来。
最初，所有的 Pod 分布如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1</th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">pod-a  <em>available</em></td>
<td style="text-align:center">pod-b <em>available</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
</tr>
<tr>
<td style="text-align:center">pod-x  <em>available</em></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<!--
All 3 pods are part of a deployment, and they collectively have a PDB which requires
there be at least 2 of the 3 pods to be available at all times.
-->
<p>3 个 Pod 都是 deployment 的一部分，并且共同拥有同一个 PDB，要求 3 个 Pod 中至少有 2 个 Pod 始终处于可用状态。</p>
<!--
For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.
The cluster administrator first tries to drain `node-1` using the `kubectl drain` command.
That tool tries to evict `pod-a` and `pod-x`.  This succeeds immediately.
Both pods go into the `terminating` state at the same time.
This puts the cluster in this state:
-->
<p>例如，假设集群管理员想要重启系统，升级内核版本来修复内核中的权限。
集群管理员首先使用 <code>kubectl drain</code> 命令尝试排空 <code>node-1</code> 节点。
命令尝试驱逐 <code>pod-a</code> 和 <code>pod-x</code>。操作立即就成功了。
两个 Pod 同时进入 <code>terminating</code> 状态。这时的集群处于下面的状态：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1 <em>draining</em></th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">pod-a  <em>terminating</em></td>
<td style="text-align:center">pod-b <em>available</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
</tr>
<tr>
<td style="text-align:center">pod-x  <em>terminating</em></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<!--
The deployment notices that one of the pods is terminating, so it creates a replacement
called `pod-d`.  Since `node-1` is cordoned, it lands on another node.  Something has
also created `pod-y` as a replacement for `pod-x`.
-->
<p>Deployment 控制器观察到其中一个 Pod 正在终止，因此它创建了一个替代 Pod <code>pod-d</code>。
由于 <code>node-1</code> 被封锁（cordon），<code>pod-d</code> 落在另一个节点上。
同样其他控制器也创建了 <code>pod-y</code> 作为 <code>pod-x</code> 的替代品。</p>
<!--
(Note: for a StatefulSet, `pod-a`, which would be called something like `pod-0`, would need
to terminate completely before its replacement, which is also called `pod-0` but has a
different UID, could be created.  Otherwise, the example applies to a StatefulSet as well.)
-->
<p>（注意：对于 StatefulSet 来说，<code>pod-a</code>（也称为 <code>pod-0</code>）需要在替换 Pod 创建之前完全终止，
替代它的也称为 <code>pod-0</code>，但是具有不同的 UID。除此之外，此示例也适用于 StatefulSet。）</p>
<!--
Now the cluster is in this state:
-->
<p>当前集群的状态如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1 <em>draining</em></th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">pod-a  <em>terminating</em></td>
<td style="text-align:center">pod-b <em>available</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
</tr>
<tr>
<td style="text-align:center">pod-x  <em>terminating</em></td>
<td style="text-align:center">pod-d <em>starting</em></td>
<td style="text-align:center">pod-y</td>
</tr>
</tbody>
</table>
<!--
At some point, the pods terminate, and the cluster looks like this:
-->
<p>在某一时刻，Pod 被终止，集群如下所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1 <em>drained</em></th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-b <em>available</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-d <em>starting</em></td>
<td style="text-align:center">pod-y</td>
</tr>
</tbody>
</table>
<!--
At this point, if an impatient cluster administrator tries to drain `node-2` or
`node-3`, the drain command will block, because there are only 2 available
pods for the deployment, and its PDB requires at least 2.  After some time passes, `pod-d` becomes available.
-->
<p>此时，如果一个急躁的集群管理员试图排空（drain）<code>node-2</code> 或 <code>node-3</code>，drain 命令将被阻塞，
因为对于 Deployment 来说只有 2 个可用的 Pod，并且它的 PDB 至少需要 2 个。
经过一段时间，<code>pod-d</code> 变得可用。</p>
<!--
The cluster state now looks like this:
-->
<p>集群状态如下所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1 <em>drained</em></th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-b <em>available</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-d <em>available</em></td>
<td style="text-align:center">pod-y</td>
</tr>
</tbody>
</table>
<!--
Now, the cluster administrator tries to drain `node-2`.
The drain command will try to evict the two pods in some order, say
`pod-b` first and then `pod-d`.  It will succeed at evicting `pod-b`.
But, when it tries to evict `pod-d`, it will be refused because that would leave only
one pod available for the deployment.
-->
<p>现在，集群管理员试图排空（drain）<code>node-2</code>。
drain 命令将尝试按照某种顺序驱逐两个 Pod，假设先是 <code>pod-b</code>，然后是 <code>pod-d</code>。
命令成功驱逐 <code>pod-b</code>，但是当它尝试驱逐 <code>pod-d</code>时将被拒绝，因为对于
Deployment 来说只剩一个可用的 Pod 了。</p>
<!--
The deployment creates a replacement for `pod-b` called `pod-e`.
Because there are not enough resources in the cluster to schedule
`pod-e` the drain will again block.  The cluster may end up in this
state:
-->
<p>Deployment 创建 <code>pod-b</code> 的替代 Pod <code>pod-e</code>。
因为集群中没有足够的资源来调度 <code>pod-e</code>，drain 命令再次阻塞。集群最终将是下面这种状态：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1 <em>drained</em></th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
<th style="text-align:center"><em>no node</em></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-b <em>terminating</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
<td style="text-align:center">pod-e <em>pending</em></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-d <em>available</em></td>
<td style="text-align:center">pod-y</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<!--
At this point, the cluster administrator needs to
add a node back to the cluster to proceed with the upgrade.
-->
<p>此时，集群管理员需要增加一个节点到集群中以继续升级操作。</p>
<!--
You can see how Kubernetes varies the rate at which disruptions
can happen, according to:
-->
<p>可以看到 Kubernetes 如何改变干扰发生的速率，根据：</p>
<!--
- how many replicas an application needs
- how long it takes to gracefully shutdown an instance
- how long it takes a new instance to start up
- the type of controller
- the cluster's resource capacity
-->
<ul>
<li>应用程序需要多少个副本</li>
<li>优雅关闭应用实例需要多长时间</li>
<li>启动应用新实例需要多长时间</li>
<li>控制器的类型</li>
<li>集群的资源能力</li>
</ul>
<!--
## Separating Cluster Owner and Application Owner Roles

Often, it is useful to think of the Cluster Manager
and Application Owner as separate roles with limited knowledge
of each other.   This separation of responsibilities
may make sense in these scenarios:
-->
<h2 id="分离集群所有者和应用所有者角色">分离集群所有者和应用所有者角色</h2>
<p>通常，将集群管理者和应用所有者视为彼此了解有限的独立角色是很有用的。这种责任分离在下面这些场景下是有意义的：</p>
<!--
- when there are many application teams sharing a Kubernetes cluster, and
  there is natural specialization of roles
- when third-party tools or services are used to automate cluster management
-->
<ul>
<li>当有许多应用程序团队共用一个 Kubernetes 集群，并且有自然的专业角色</li>
<li>当第三方工具或服务用于集群自动化管理</li>
</ul>
<!--
Pod Disruption Budgets support this separation of roles by providing an
interface between the roles.
-->
<p>Pod 干扰预算通过在角色之间提供接口来支持这种分离。</p>
<!--
If you do not have such a separation of responsibilities in your organization,
you may not need to use Pod Disruption Budgets.
-->
<p>如果你的组织中没有这样的责任分离，则可能不需要使用 Pod 干扰预算。</p>
<!--
## How to perform Disruptive Actions on your Cluster

If you are a Cluster Administrator, and you need to perform a disruptive action on all
the nodes in your cluster, such as a node or system software upgrade, here are some options:
-->
<h2 id="如何在集群上执行干扰性操作">如何在集群上执行干扰性操作</h2>
<p>如果你是集群管理员，并且需要对集群中的所有节点执行干扰操作，例如节点或系统软件升级，则可以使用以下选项</p>
<!--
- Accept downtime during the upgrade.
- Failover to another complete replica cluster.
   -  No downtime, but may be costly both for the duplicated nodes
     and for human effort to orchestrate the switchover.
- Write disruption tolerant applications and use PDBs.
   - No downtime.
   - Minimal resource duplication.
   - Allows more automation of cluster administration.
   - Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary
     disruptions largely overlaps with work to support autoscaling and tolerating
     involuntary disruptions.
-->
<ul>
<li>接受升级期间的停机时间。</li>
<li>故障转移到另一个完整的副本集群。
<ul>
<li>没有停机时间，但是对于重复的节点和人工协调成本可能是昂贵的。</li>
</ul>
</li>
<li>编写可容忍干扰的应用程序和使用 PDB。
<ul>
<li>不停机。</li>
<li>最小的资源重复。</li>
<li>允许更多的集群管理自动化。</li>
<li>编写可容忍干扰的应用程序是棘手的，但对于支持容忍自愿干扰所做的工作，和支持自动扩缩和容忍非
自愿干扰所做工作相比，有大量的重叠</li>
</ul>
</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).
* Learn more about [draining nodes](/docs/tasks/administer-cluster/safely-drain-node/)
* Learn about [updating a deployment](/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)
  including steps to maintain its availability during the rollout.
-->
<ul>
<li>参考<a href="/zh/docs/tasks/run-application/configure-pdb/">配置 Pod 干扰预算</a>中的方法来保护你的应用。</li>
<li>进一步了解<a href="/zh/docs/tasks/administer-cluster/safely-drain-node/">排空节点</a>的信息。</li>
<li>了解<a href="/zh/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">更新 Deployment</a>
的过程，包括如何在其进程中维持应用的可用性</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-53a1005011e1bda2ce81819aad7c8b32">1.5 - 临时容器</h1>
    
	<!--
title: Ephemeral Containers
content_type: concept
weight: 80
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!--
This page provides an overview of ephemeral containers: a special type of container
that runs temporarily in an existing <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> to
accomplish user-initiated actions such as troubleshooting. You use ephemeral
containers to inspect services rather than to build applications.
-->
<p>本页面概述了临时容器：一种特殊的容器，该容器在现有
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
中临时运行，以便完成用户发起的操作，例如故障排查。
你会使用临时容器来检查服务，而不是用它来构建应用程序。</p>
<!-- body -->
<!--
## Understanding ephemeral containers

<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> are the fundamental building
block of Kubernetes applications. Since Pods are intended to be disposable and
replaceable, you cannot add a container to a Pod once it has been created.
Instead, you usually delete and replace Pods in a controlled fashion using
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='deployments'>deployments</a>.
-->
<h2 id="understanding-ephemeral-containers">了解临时容器  </h2>
<p><a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 是 Kubernetes 应用程序的基本构建块。
由于 Pod 是一次性且可替换的，因此一旦 Pod 创建，就无法将容器加入到 Pod 中。
取而代之的是，通常使用 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>
以受控的方式来删除并替换 Pod。</p>
<!--
Sometimes it's necessary to inspect the state of an existing Pod, however, for
example to troubleshoot a hard-to-reproduce bug. In these cases you can run
an ephemeral container in an existing Pod to inspect its state and run
arbitrary commands.
-->
<p>有时有必要检查现有 Pod 的状态。例如，对于难以复现的故障进行排查。
在这些场景中，可以在现有 Pod 中运行临时容器来检查其状态并运行任意命令。</p>
<!--
### What is an ephemeral container?

Ephemeral containers differ from other containers in that they lack guarantees
for resources or execution, and they will never be automatically restarted, so
they are not appropriate for building applications.  Ephemeral containers are
described using the same `ContainerSpec` as regular containers, but many fields
are incompatible and disallowed for ephemeral containers.
-->
<h3 id="what-is-an-ephemeral-container">什么是临时容器？   </h3>
<p>临时容器与其他容器的不同之处在于，它们缺少对资源或执行的保证，并且永远不会自动重启，
因此不适用于构建应用程序。
临时容器使用与常规容器相同的 <code>ContainerSpec</code> 节来描述，但许多字段是不兼容和不允许的。</p>
<!--
- Ephemeral containers may not have ports, so fields such as `ports`,
  `livenessProbe`, `readinessProbe` are disallowed.
- Pod resource allocations are immutable, so setting `resources` is disallowed.
- For a complete list of allowed fields, see the [EphemeralContainer reference
  documentation](/docs/reference/generated/kubernetes-api/v1.23/#ephemeralcontainer-v1-core).
-->
<ul>
<li>临时容器没有端口配置，因此像 <code>ports</code>，<code>livenessProbe</code>，<code>readinessProbe</code>
这样的字段是不允许的。</li>
<li>Pod 资源分配是不可变的，因此 <code>resources</code> 配置是不允许的。</li>
<li>有关允许字段的完整列表，请参见
<a href="/docs/reference/generated/kubernetes-api/v1.23/#ephemeralcontainer-v1-core">EphemeralContainer 参考文档</a>。</li>
</ul>
<!--
Ephemeral containers are created using a special `ephemeralcontainers` handler
in the API rather than by adding them directly to `pod.spec`, so it's not
possible to add an ephemeral container using `kubectl edit`.
-->
<p>临时容器是使用 API 中的一种特殊的 <code>ephemeralcontainers</code> 处理器进行创建的，
而不是直接添加到 <code>pod.spec</code> 段，因此无法使用 <code>kubectl edit</code> 来添加一个临时容器。</p>
<!--
Like regular containers, you may not change or remove an ephemeral container
after you have added it to a Pod.
-->
<p>与常规容器一样，将临时容器添加到 Pod 后，将不能更改或删除临时容器。</p>
<!--
## Uses for ephemeral containers

Ephemeral containers are useful for interactive troubleshooting when `kubectl
exec` is insufficient because a container has crashed or a container image
doesn't include debugging utilities.
-->
<h2 id="uses-for-ephemeral-containers">临时容器的用途  </h2>
<p>当由于容器崩溃或容器镜像不包含调试工具而导致 <code>kubectl exec</code> 无用时，
临时容器对于交互式故障排查很有用。</p>
<!--
In particular, [distroless images](https://github.com/GoogleContainerTools/distroless)
enable you to deploy minimal container images that reduce attack surface
and exposure to bugs and vulnerabilities. Since distroless images do not include a
shell or any debugging utilities, it's difficult to troubleshoot distroless
images using `kubectl exec` alone.
-->
<p>尤其是，<a href="https://github.com/GoogleContainerTools/distroless">Distroless 镜像</a>
允许用户部署最小的容器镜像，从而减少攻击面并减少故障和漏洞的暴露。
由于 distroless 镜像不包含 Shell 或任何的调试工具，因此很难单独使用
<code>kubectl exec</code> 命令进行故障排查。</p>
<!--
When using ephemeral containers, it's helpful to enable [process namespace
sharing](/docs/tasks/configure-pod-container/share-process-namespace/) so
you can view processes in other containers.
-->
<p>使用临时容器时，启用
<a href="/zh/docs/tasks/configure-pod-container/share-process-namespace/">进程名字空间共享</a>
很有帮助，可以查看其他容器中的进程。</p>
<p>What's next</p>
<!--
* Learn how to [debug pods using ephemeral containers](/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container).
-->
<ul>
<li>了解如何<a href="/zh/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container">使用临时调试容器来进行调试</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-89637410cacae45a36ab1cc278c482eb">2 - 工作负载资源</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-a2dc0393e0c4079e1c504b6429844e86">2.1 - Deployments</h1>
    
	<!--
title: Deployments
feature:
  title: Automated rollouts and rollbacks
  description: >
    Kubernetes progressively rolls out changes to your application or its configuration, while monitoring application health to ensure it doesn't kill all your instances at the same time. If something goes wrong, Kubernetes will rollback the change for you. Take advantage of a growing ecosystem of deployment solutions.

content_type: concept
weight: 10
-->
<!-- overview -->
<!--
A _Deployment_ provides declarative updates for [Pods](/docs/concepts/workloads/pods/pod/) and
[ReplicaSets](/docs/concepts/workloads/controllers/replicaset/).
-->
<p>一个 Deployment 为 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
和 <a class='glossary-tooltip' title='ReplicaSet 是下一代副本控制器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/replicaset/' target='_blank' aria-label='ReplicaSet'>ReplicaSet</a>
提供声明式的更新能力。</p>
<!--
You describe a _desired state_ in a Deployment, and the Deployment <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.
-->
<p>你负责描述 Deployment 中的 <em>目标状态</em>，而 Deployment <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a>
以受控速率更改实际状态，
使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment，
并通过新的 Deployment 收养其资源。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.
-->
<p>不要管理 Deployment 所拥有的 ReplicaSet 。
如果存在下面未覆盖的使用场景，请考虑在 Kubernetes 仓库中提出 Issue。
</div>
<!-- body -->
<!--
## Use Case

The following are typical use cases for Deployments:
-->
<h2 id="用例">用例</h2>
<p>以下是 Deployments 的典型用例：</p>
<!--
* [Create a Deployment to rollout a ReplicaSet](#creating-a-deployment). The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.
* [Declare the new state of the Pods](#updating-a-deployment) by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.
-->
<ul>
<li><a href="#creating-a-deployment">创建 Deployment 以将 ReplicaSet 上线</a>。 ReplicaSet 在后台创建 Pods。
检查 ReplicaSet 的上线状态，查看其是否成功。</li>
<li>通过更新 Deployment 的 PodTemplateSpec，<a href="#updating-a-deployment">声明 Pod 的新状态</a> 。
新的 ReplicaSet 会被创建，Deployment 以受控速率将 Pod 从旧 ReplicaSet 迁移到新 ReplicaSet。
每个新的 ReplicaSet 都会更新 Deployment 的修订版本。</li>
</ul>
<!--
* [Rollback to an earlier Deployment revision](#rolling-back-a-deployment) if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.
* [Scale up the Deployment to facilitate more load](#scaling-a-deployment).
* [Pause the Deployment](#pausing-and-resuming-a-deployment) to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.
* [Use the status of the Deployment](#deployment-status) as an indicator that a rollout has stuck.
* [Clean up older ReplicaSets](#clean-up-policy) that you don't need anymore.
-->
<ul>
<li>如果 Deployment 的当前状态不稳定，<a href="#rolling-back-a-deployment">回滚到较早的 Deployment 版本</a>。
每次回滚都会更新 Deployment 的修订版本。</li>
<li><a href="#scaling-a-deployment">扩大 Deployment 规模以承担更多负载</a>。</li>
<li><a href="#pausing-and-resuming-a-deployment">暂停 Deployment </a> 以应用对 PodTemplateSpec 所作的多项修改，
然后恢复其执行以启动新的上线版本。</li>
<li><a href="#deployment-status">使用 Deployment 状态</a> 来判定上线过程是否出现停滞。</li>
<li><a href="#clean-up-policy">清理较旧的不再需要的 ReplicaSet</a> 。</li>
</ul>
<!--
 ## Creating a Deployment

The following is an example of a Deployment. It creates a ReplicaSet to bring up three `nginx` Pods:
-->
<h2 id="creating-a-deployment">创建 Deployment </h2>
<p>下面是一个 Deployment 示例。其中创建了一个 ReplicaSet，负责启动三个 <code>nginx</code> Pods：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/nginx-deployment.yaml" download="controllers/nginx-deployment.yaml"><code>controllers/nginx-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-nginx-deployment-yaml')" title="Copy controllers/nginx-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-nginx-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In this example:
-->
<p>在该例中：</p>
<!--
 * A Deployment named `nginx-deployment` is created, indicated by the `.metadata.name` field.
 * The Deployment creates three replicated Pods, indicated by the `replicas` field.
-->
<ul>
<li>创建名为 <code>nginx-deployment</code>（由 <code>.metadata.name</code> 字段标明）的 Deployment。</li>
<li>该 Deployment 创建三个（由 <code>replicas</code> 字段标明）Pod 副本。</li>
</ul>
<!--
* The `selector` field defines how the Deployment finds which Pods to manage.
  In this case, you select a label that is defined in the Pod template (`app: nginx`).
  However, more sophisticated selection rules are possible,
  as long as the Pod template itself satisfies the rule.
-->
<ul>
<li>
<p><code>selector</code> 字段定义 Deployment 如何查找要管理的 Pods。
在这里，你选择在 Pod 模板中定义的标签（<code>app: nginx</code>）。
不过，更复杂的选择规则是也可能的，只要 Pod 模板本身满足所给规则即可。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
  The `spec.selector.matchLabels` field is a map of {key,value} pairs.
  A single {key,value} in the `matchLabels` map is equivalent to an element of `matchExpressions`,
  whose `key` field is "key", the `operator` is "In", and the `values` array contains only "value".
  All of the requirements, from both `matchLabels` and `matchExpressions`, must be satisfied in order to match.
  -->
<p><code>spec.selector.matchLabels</code> 字段是 <code>{key,value}</code> 键值对映射。
在 <code>matchLabels</code> 映射中的每个 <code>{key,value}</code> 映射等效于 <code>matchExpressions</code> 中的一个元素，
即其 <code>key</code> 字段是 “key”，<code>operator</code> 为 “In”，<code>values</code> 数组仅包含 “value”。
在 <code>matchLabels</code> 和 <code>matchExpressions</code> 中给出的所有条件都必须满足才能匹配。
</div>
</li>
</ul>
<!--
* The `template` field contains the following sub-fields:
  * The Pods are labeled `app: nginx`using the `.meatadata.labels` field.
  * The Pod template's specification, or `.template.spec` field, indicates that
    the Pods run one container, `nginx`, which runs the `nginx`
    [Docker Hub](https://hub.docker.com/) image at version 1.14.2.
  * Create one container and name it `nginx` using the `.spec.template.spec.containers[0].name` field.
-->
<ul>
<li><code>template</code> 字段包含以下子字段：
<ul>
<li>Pod 被使用 <code>.metadata.labels</code> 字段打上 <code>app: nginx</code> 标签。</li>
<li>Pod 模板规约（即 <code>.template.spec</code> 字段）指示 Pods 运行一个 <code>nginx</code> 容器，
该容器运行版本为 1.14.2 的 <code>nginx</code> <a href="https://hub.docker.com/">Docker Hub</a>镜像。</li>
<li>创建一个容器并使用 <code>.spec.template.spec.containers[0].name</code> 字段将其命名为 <code>nginx</code>。</li>
</ul>
</li>
</ul>
<!--
Before you begin, make sure your Kubernetes cluster is up and running.
Follow the steps given below to create the above Deployment:
-->
<p>开始之前，请确保的 Kubernetes 集群已启动并运行。
按照以下步骤创建上述 Deployment ：</p>
<!--
1. Create the Deployment by running the following command:
-->
<ol>
<li>
<p>通过运行以下命令创建 Deployment ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
</code></pre></div></li>
</ol>
<!--
 2. Run `kubectl get deployments` to check if the Deployment was created.

    If the Deployment is still being created, the output is similar to the following:
-->
<ol start="2">
<li>
<p>运行 <code>kubectl get deployments</code> 检查 Deployment 是否已创建。
如果仍在创建 Deployment，则输出类似于：</p>
<pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/3     0            0           1s
</code></pre><!--
When you inspect the Deployments in your cluster, the following fields are displayed:
-->
<p>在检查集群中的 Deployment 时，所显示的字段有：</p>
<!--
* `NAME` lists the names of the Deployments in the cluster.
* `READY` displays how many replicas of the application are available to your users. It follows the pattern ready/desired.
* `UP-TO-DATE` displays the number of replicas that have been updated to achieve the desired state.
* `AVAILABLE` displays how many replicas of the application are available to your users.
* `AGE` displays the amount of time that the application has been running.
-->
<ul>
<li><code>NAME</code> 列出了集群中 Deployment 的名称。</li>
<li><code>READY</code> 显示应用程序的可用的“副本”数。显示的模式是“就绪个数/期望个数”。</li>
<li><code>UP-TO-DATE</code> 显示为了达到期望状态已经更新的副本数。</li>
<li><code>AVAILABLE</code> 显示应用可供用户使用的副本数。</li>
<li><code>AGE</code> 显示应用程序运行的时间。</li>
</ul>
<!--
Notice how the number of desired replicas is 3 according to `.spec.replicas` field.
-->
<p>请注意期望副本数是根据 <code>.spec.replicas</code> 字段设置 3。</p>
</li>
</ol>
<!--
3. To see the Deployment rollout status, run `kubectl rollout status deployment/nginx-deployment`.

   The output is similar to:
-->
<ol start="3">
<li>
<p>要查看 Deployment 上线状态，运行 <code>kubectl rollout status deployment/nginx-deployment</code>。</p>
<p>输出类似于：</p>
<pre><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment &quot;nginx-deployment&quot; successfully rolled out
</code></pre></li>
</ol>
<!--
4. Run the `kubectl get deployments` again a few seconds later. The output is similar to this:
-->
<ol start="4">
<li>
<p>几秒钟后再次运行 <code>kubectl get deployments</code>。输出类似于：</p>
<pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           18s
</code></pre><!--
Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.
-->
<p>注意 Deployment 已创建全部三个副本，并且所有副本都是最新的（它们包含最新的 Pod 模板）
并且可用。</p>
</li>
</ol>
<!--
5. To see the ReplicaSet (`rs`) created by the Deployment, run `kubectl get rs`. The output is similar to this:
-->
<ol start="5">
<li>
<p>要查看 Deployment 创建的 ReplicaSet（<code>rs</code>），运行 <code>kubectl get rs</code>。
输出类似于：</p>
<pre><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75675f5897   3         3         3       18s
</code></pre><!--
ReplicaSet output shows the following fields:

* `NAME` lists the names of the ReplicaSets in the namespace.
* `DESIRED` displays the desired number of _replicas_ of the application, which you define when you create the Deployment. This is the _desired state_.
* `CURRENT` displays how many replicas are currently running.
* `READY` displays how many replicas of the application are available to your users.
* `AGE` displays the amount of time that the application has been running.
-->
<p>ReplicaSet 输出中包含以下字段：</p>
<ul>
<li><code>NAME</code> 列出名字空间中 ReplicaSet 的名称；</li>
<li><code>DESIRED</code> 显示应用的期望副本个数，即在创建 Deployment 时所定义的值。
此为期望状态；</li>
<li><code>CURRENT</code> 显示当前运行状态中的副本个数；</li>
<li><code>READY</code> 显示应用中有多少副本可以为用户提供服务；</li>
<li><code>AGE</code> 显示应用已经运行的时间长度。</li>
</ul>
<!--
Notice that the name of the ReplicaSet is always formatted as `[DEPLOYMENT-NAME]-[RANDOM-STRING]`. The random string is
randomly generated and uses the `pod-template-hash` as a seed.
-->
<p>注意 ReplicaSet 的名称始终被格式化为<code>[Deployment名称]-[随机字符串]</code>。
其中的随机字符串是使用 <code>pod-template-hash</code> 作为种子随机生成的。</p>
</li>
</ol>
<!--
6. To see the labels automatically generated for each Pod, run `kubectl get pods -show-labels`.
   The following output is returned:
-->
<ol start="6">
<li>
<p>要查看每个 Pod 自动生成的标签，运行 <code>kubectl get pods --show-labels</code>。返回以下输出：</p>
<pre><code>NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
</code></pre><!--
The created ReplicaSet ensures that there are three `nginx` Pods.
-->
<p>所创建的 ReplicaSet 确保总是存在三个 <code>nginx</code> Pod。</p>
</li>
</ol>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
You must specify an appropriate selector and Pod template labels in a Deployment
(in this case, `app: nginx`).

Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.
-->
<p>你必须在 Deployment 中指定适当的选择算符和 Pod 模板标签（在本例中为 <code>app: nginx</code>）。
标签或者选择算符不要与其他控制器（包括其他 Deployment 和 StatefulSet）重叠。
Kubernetes 不会阻止你这样做，但是如果多个控制器具有重叠的选择算符，
它们可能会发生冲突执行难以预料的操作。
</div>
<!--
 ### Pod-template-hash label
-->
<h3 id="pod-template-hash-标签">Pod-template-hash 标签</h3>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Do not change this label.
-->
<p>不要更改此标签。
</div>
<!--
The `pod-template-hash` label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.
-->
<p>Deployment 控制器将 <code>pod-template-hash</code> 标签添加到 Deployment
所创建或收留的每个 ReplicaSet 。</p>
<!--
This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the `PodTemplate` of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,
and in any existing Pods that the ReplicaSet might have.
-->
<p>此标签可确保 Deployment 的子 ReplicaSets 不重叠。
标签是通过对 ReplicaSet 的 <code>PodTemplate</code> 进行哈希处理。
所生成的哈希值被添加到 ReplicaSet 选择算符、Pod 模板标签，并存在于在 ReplicaSet
可能拥有的任何现有 Pod 中。</p>
<!--
 ## Updating a Deployment
-->
<h2 id="updating-a-deployment">更新 Deployment  </h2>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
 A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, `.spec.template`)
is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.
-->
<p>仅当 Deployment Pod 模板（即 <code>.spec.template</code>）发生改变时，例如模板的标签或容器镜像被更新，
才会触发 Deployment 上线。其他更新（如对 Deployment 执行扩缩容的操作）不会触发上线动作。
</div>
<!--
Follow the steps given below to update your Deployment:
-->
<p>按照以下步骤更新 Deployment：</p>
<!--
1. Let's update the nginx Pods to use the `nginx:1.16.1` image instead of the `nginx:1.14.2` image.
-->
<ol>
<li>
<p>先来更新 nginx Pod 以使用 <code>nginx:1.16.1</code> 镜像，而不是 <code>nginx:1.14.2</code> 镜像。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image deployment.v1.apps/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.16.1
</code></pre></div><!--
or use the following command:
-->
<p>或者使用下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.16.1
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>deployment/nginx-deployment image updated
</code></pre><!--
Alternatively, you can `edit` the Deployment and change `.spec.template.spec.containers[0].image` from `nginx:1.14.2` to `nginx:1.16.1`:
-->
<p>或者，可以对 Deployment 执行 <code>edit</code> 操作并将 <code>.spec.template.spec.containers[0].image</code> 从
<code>nginx:1.14.2</code> 更改至 <code>nginx:1.16.1</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment/nginx-deployment edited
</code></pre></li>
</ol>
<!--
2. To see the rollout status, run:
-->
<ol start="2">
<li>
<p>要查看上线状态，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
</code></pre><!-- or -->
<p>或者</p>
<pre><code>deployment &quot;nginx-deployment&quot; successfully rolled out
</code></pre></li>
</ol>
<!--
Get more details on your updated Deployment:
-->
<p>获取关于已更新的 Deployment 的更多信息：</p>
<!--
* After the rollout succeeds, you can view the Deployment by running `kubectl get deployments`.
  The output is similar to this:
-->
<ul>
<li>
<p>在上线成功后，可以通过运行 <code>kubectl get deployments</code> 来查看 Deployment：
输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#b44">NAME               READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span style="color:#b44">nginx-deployment   3/3     3            3           36s</span>
</code></pre></div></li>
</ul>
<!--
* Run `kubectl get rs` to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it
up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.
-->
<ul>
<li>
<p>运行 <code>kubectl get rs</code> 以查看 Deployment 通过创建新的 ReplicaSet 并将其扩容到
3 个副本并将旧 ReplicaSet 缩容到 0 个副本完成了 Pod 的更新操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s
</code></pre></li>
</ul>
<!--
* Running `get pods` should now show only the new Pods:
-->
<ul>
<li>
<p>现在运行 <code>get pods</code> 应仅显示新的 Pods:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
</code></pre><!--
Next time you want to update these Pods, you only need to update the Deployment's Pod template again.

Deployment ensures that only a certain number of Pods are down while they are being updated. By default,
it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).
-->
<p>下次要更新这些 Pods 时，只需再次更新 Deployment Pod 模板即可。</p>
<p>Deployment 可确保在更新时仅关闭一定数量的 Pod。默认情况下，它确保至少所需 Pods
75% 处于运行状态（最大不可用比例为 25%）。</p>
<!--
Deployment also ensures that only a certain number of Pods are created above the desired number of Pods.
By default, it ensures that at most 25% of the desired number of Pods are up (25% max surge).
-->
<p>Deployment 还确保仅所创建 Pod 数量只可能比期望 Pods 数高一点点。
默认情况下，它可确保启动的 Pod 个数比期望个数最多多出 25%（最大峰值 25%）。</p>
<!--   
For example, if you look at the above Deployment closely, you will see that it first created a new Pod,
then deleted some old Pods, and created new ones. It does not kill old Pods until a sufficient number of
new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.
It makes sure that at least 2 Pods are available and that at max 4 Pods in total are available. In case of
  a Deployment with 4 replicas, the number of Pods would be between 3 and 5.
-->
<p>例如，如果仔细查看上述 Deployment ，将看到它首先创建了一个新的 Pod，然后删除了一些旧的 Pods，
并创建了新的 Pods。它不会杀死老 Pods，直到有足够的数量新的 Pods 已经出现。
在足够数量的旧 Pods 被杀死前并没有创建新 Pods。它确保至少 2 个 Pod 可用，
同时最多总共 4 个 Pod 可用。
当 Deployment 设置为 4 个副本时，Pod 的个数会介于 3 和 5 之间。</p>
</li>
</ul>
<!--
* Get details of your Deployment:
-->
<ul>
<li>
<p>获取 Deployment 的更多信息</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployments
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
   Containers:
    nginx:
      Image:        nginx:1.16.1
      Port:         80/TCP
      Environment:  &lt;none&gt;
      Mounts:       &lt;none&gt;
    Volumes:        &lt;none&gt;
  Conditions:
    Type           Status  Reason
    ----           ------  ------
    Available      True    MinimumReplicasAvailable
    Progressing    True    NewReplicaSetAvailable
  OldReplicaSets:  &lt;none&gt;
  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
  Events:
    Type    Reason             Age   From                   Message
    ----    ------             ----  ----                   -------
    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
</code></pre><!--
Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)
and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet
(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet
to 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.
It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.
Finally, you'll have 3 available replicas
in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.
-->
<p>可以看到，当第一次创建 Deployment 时，它创建了一个 ReplicaSet（<code>nginx-deployment-2035384211</code>）
并将其直接扩容至 3 个副本。更新 Deployment 时，它创建了一个新的 ReplicaSet
（nginx-deployment-1564180365），并将其扩容为 1，等待其就绪；然后将旧 ReplicaSet 缩容到 2，
将新的 ReplicaSet 扩容到 2 以便至少有 3 个 Pod 可用且最多创建 4 个 Pod。
然后，它使用相同的滚动更新策略继续对新的 ReplicaSet 扩容并对旧的 ReplicaSet 缩容。
最后，你将有 3 个可用的副本在新的 ReplicaSet 中，旧 ReplicaSet 将缩容到 0。</p>
</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Kubernetes doesn't count terminating Pods when calculating the number of `availableReplicas`, which must be between
`replicas - maxUnavailable` and `replicas + maxSurge`. As a result, you might notice that there are more Pods than
expected during a rollout, and that the total resources consumed by the Deployment is more than `replicas + maxSurge`
until the `terminationGracePeriodSeconds` of the terminating Pods expires.
-->
<p>Kubernetes 在计算 <code>availableReplicas</code> 数值时不考虑终止过程中的 Pod，
<code>availableReplicas</code> 的值一定介于 <code>replicas - maxUnavailable</code> 和 <code>replicas + maxSurge</code> 之间。
因此，你可能在上线期间看到 Pod 个数比预期的多，Deployment 所消耗的总的资源也大于
<code>replicas + maxSurge</code> 个 Pod 所用的资源，直到被终止的 Pod 所设置的
<code>terminationGracePeriodSeconds</code> 到期为止。
</div>
<!--
### Rollover (aka multiple updates in-flight)

Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up
the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels
match `.spec.selector` but whose template does not match `.spec.template` are scaled down. Eventually, the new
ReplicaSet is scaled to `.spec.replicas` and all old ReplicaSets is scaled to 0.
-->
<h3 id="翻转-多-deployment-动态更新">翻转（多 Deployment 动态更新）</h3>
<p>Deployment 控制器每次注意到新的 Deployment 时，都会创建一个 ReplicaSet 以启动所需的 Pods。
如果更新了 Deployment，则控制标签匹配 <code>.spec.selector</code> 但模板不匹配 <code>.spec.template</code> 的
Pods 的现有 ReplicaSet 被缩容。最终，新的 ReplicaSet 缩放为 <code>.spec.replicas</code> 个副本，
所有旧 ReplicaSets 缩放为 0 个副本。</p>
<!--
If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet
as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously
- it will add it to its list of old ReplicaSets and start scaling it down.
-->
<p>当 Deployment 正在上线时被更新，Deployment 会针对更新创建一个新的 ReplicaSet
并开始对其扩容，之前正在被扩容的 ReplicaSet 会被翻转，添加到旧 ReplicaSets 列表
并开始缩容。</p>
<!--
For example, suppose you create a Deployment to create 5 replicas of `nginx:1.14.2`,
but then update the Deployment to create 5 replicas of `nginx:1.16.1`, when only 3
replicas of `nginx:1.7.9` had been created. In that case, the Deployment immediately starts
killing the 3 `nginx:1.7.9` Pods that it had created, and starts creating
`nginx:1.9.1` Pods. It does not wait for the 5 replicas of `nginx:1.14.2` to be created
before changing course.
-->
<p>例如，假定你在创建一个 Deployment 以生成 <code>nginx:1.14.2</code> 的 5 个副本，但接下来
更新 Deployment 以创建 5 个 <code>nginx:1.16.1</code> 的副本，而此时只有 3 个<code>nginx:1.14.2</code>
副本已创建。在这种情况下，Deployment 会立即开始杀死 3 个 <code>nginx:1.14.2</code> Pods，
并开始创建 <code>nginx:1.16.1</code> Pods。它不会等待 <code>nginx:1.14.2</code> 的 5
个副本都创建完成后才开始执行变更动作。</p>
<!--
### Label selector updates

 It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.
In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped
all of the implications.
-->
<h3 id="label-selector-updates">更改标签选择算符  </h3>
<p>通常不鼓励更新标签选择算符。建议你提前规划选择算符。
在任何情况下，如果需要更新标签选择算符，请格外小心，
并确保自己了解这背后可能发生的所有事情。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In API version `apps/v1`, a Deployment's label selector is immutable after it gets created.
-->
<p>在 API 版本 <code>apps/v1</code> 中，Deployment 标签选择算符在创建后是不可变的。
</div>
<!--
 * Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,
otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does
not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and
creating a new ReplicaSet.
* Selector updates changes the existing value in a selector key - result in the same behavior as additions.
* Selector removals removes an existing key from the Deployment selector - do not require any changes in the
Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the
removed label still exists in any existing Pods and ReplicaSets.
-->
<ul>
<li>添加选择算符时要求使用新标签更新 Deployment 规约中的 Pod 模板标签，否则将返回验证错误。
此更改是非重叠的，也就是说新的选择算符不会选择使用旧选择算符所创建的 ReplicaSet 和 Pod，
这会导致创建新的 ReplicaSet 时所有旧 ReplicaSet 都会被孤立。</li>
<li>选择算符的更新如果更改了某个算符的键名，这会导致与添加算符时相同的行为。</li>
<li>删除选择算符的操作会删除从 Deployment 选择算符中删除现有算符。
此操作不需要更改 Pod 模板标签。现有 ReplicaSet 不会被孤立，也不会因此创建新的 ReplicaSet，
但请注意已删除的标签仍然存在于现有的 Pod 和 ReplicaSet 中。</li>
</ul>
<!--
## Rolling Back a Deployment
-->
<h2 id="rolling-back-a-deployment">回滚 Deployment</h2>
<!--
 Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.
By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want
(you can change that by modifying revision history limit).
-->
<p>有时，你可能想要回滚 Deployment；例如，当 Deployment 不稳定时（例如进入反复崩溃状态）。
默认情况下，Deployment 的所有上线记录都保留在系统中，以便可以随时回滚
（你可以通过修改修订历史记录限制来更改这一约束）。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
A Deployment's revision is created when a Deployment's rollout is triggered. This means that the
new revision is created if and only if the Deployment's Pod template (`.spec.template`) is changed,
for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,
do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.
This means that when you roll back to an earlier revision, only the Deployment's Pod template part is
rolled back.
-->
<p>Deployment 被触发上线时，系统就会创建 Deployment 的新的修订版本。
这意味着仅当 Deployment 的 Pod 模板（<code>.spec.template</code>）发生更改时，才会创建新修订版本
-- 例如，模板的标签或容器镜像发生变化。
其他更新，如 Deployment 的扩缩容操作不会创建 Deployment 修订版本。
这是为了方便同时执行手动缩放或自动缩放。
换言之，当你回滚到较早的修订版本时，只有 Deployment 的 Pod 模板部分会被回滚。
</div>
<!--
* Suppose that you made a typo while updating the Deployment, by putting the image name as `nginx:1.161` instead of `nginx:1.16.1`:
-->
<ul>
<li>
<p>假设你在更新 Deployment 时犯了一个拼写错误，将镜像名称命名设置为
<code>nginx:1.161</code> 而不是 <code>nginx:1.16.1</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.161 --record<span style="color:#666">=</span><span style="color:#a2f">true</span>
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>deployment/nginx-deployment image updated
</code></pre></li>
</ul>
<!--
* The rollout gets stuck. You can verify it by checking the rollout status:
-->
<ul>
<li>
<p>此上线进程会出现停滞。你可以通过检查上线状态来验证：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status deployment/nginx-deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
</code></pre></li>
</ul>
<!--
* Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,
[read more here](#deployment-status).
-->
<ul>
<li>按 Ctrl-C 停止上述上线状态观测。有关上线停滞的详细信息，<a href="#deployment-status">参考这里</a>。</li>
</ul>
<!--
 * You see that the number of old replicas (`nginx-deployment-1564180365` and `nginx-deployment-2035384211`) is 2, and new replicas (nginx-deployment-3066724191) is 1.
-->
<ul>
<li>
<p>你可以看到旧的副本有两个（<code>nginx-deployment-1564180365</code> 和 <code>nginx-deployment-2035384211</code>），
新的副本有 1 个（<code>nginx-deployment-3066724191</code>）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   <span style="color:#666">3</span>         <span style="color:#666">3</span>         <span style="color:#666">3</span>       25s
nginx-deployment-2035384211   <span style="color:#666">0</span>         <span style="color:#666">0</span>         <span style="color:#666">0</span>       36s
nginx-deployment-3066724191   <span style="color:#666">1</span>         <span style="color:#666">1</span>         <span style="color:#666">0</span>       6s
</code></pre></div></li>
</ul>
<!--
* Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.
-->
<ul>
<li>
<p>查看所创建的 Pod，你会注意到新 ReplicaSet 所创建的 1 个 Pod 卡顿在镜像拉取循环中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            <span style="color:#666">0</span>          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            <span style="color:#666">0</span>          25s
nginx-deployment-1564180365-hysrc   1/1       Running            <span style="color:#666">0</span>          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   <span style="color:#666">0</span>          6s
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
  The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (`maxUnavailable` specifically) that you have specified. Kubernetes by default sets the value to 25%.
  -->
<p>Deployment 控制器自动停止有问题的上线过程，并停止对新的 ReplicaSet 扩容。
这行为取决于所指定的 rollingUpdate 参数（具体为 <code>maxUnavailable</code>）。
默认情况下，Kubernetes 将此值设置为 25%。
</div>
</li>
</ul>
<!--
* Get the description of the Deployment:
-->
<ul>
<li>
<p>获取 Deployment 描述信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.91
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubobjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
</code></pre><!--
To fix this, you need to rollback to a previous revision of Deployment that is stable.
-->
<p>要解决此问题，需要回滚到以前稳定的 Deployment 版本。</p>
</li>
</ul>
<!--
### Checking Rollout History of a Deployment

Follow the steps given below to check the rollout history:
-->
<h3 id="检查-deployment-上线历史">检查 Deployment 上线历史</h3>
<p>按照如下步骤检查回滚历史：</p>
<!--
 1. First, check the revisions of this Deployment:
-->
<ol>
<li>
<p>首先，检查 Deployment 修订历史：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout <span style="color:#a2f">history</span> deployment/nginx-deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>deployments &quot;nginx-deployment&quot;
REVISION    CHANGE-CAUSE
1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml
2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161
</code></pre><!--
`CHANGE-CAUSE` is copied from the Deployment annotation `kubernetes.io/change-cause` to its revisions upon creation. You can specify the`CHANGE-CAUSE` message by:
-->
<p><code>CHANGE-CAUSE</code> 的内容是从 Deployment 的 <code>kubernetes.io/change-cause</code> 注解复制过来的。
复制动作发生在修订版本创建时。你可以通过以下方式设置 <code>CHANGE-CAUSE</code> 消息：</p>
<!--
* Annotating the Deployment with `kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.9.1"`
* Manually editing the manifest of the resource.
-->
<ul>
<li>使用 <code>kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=&quot;image updated to 1.16.1&quot;</code>
为 Deployment 添加注解。</li>
<li>手动编辑资源的清单。</li>
</ul>
</li>
</ol>
<!--
2. To see the details of each revision, run:
-->
<ol start="2">
<li>
<p>要查看修订历史的详细信息，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout <span style="color:#a2f">history</span> deployment/nginx-deployment --revision<span style="color:#666">=</span><span style="color:#666">2</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployments &quot;nginx-deployment&quot; revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
  Containers:
   nginx:
    Image:      nginx:1.16.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      &lt;none&gt;
  No volumes.
</code></pre></li>
</ol>
<!--
### Rolling Back to a Previous Revision
Follow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.
-->
<h3 id="rolling-back-to-a-previous-revision">回滚到之前的修订版本  </h3>
<p>按照下面给出的步骤将 Deployment 从当前版本回滚到以前的版本（即版本 2）。</p>
<!--
 1. Now you've decided to undo the current rollout and rollback to the previous revision:
-->
<ol>
<li>
<p>假定现在你已决定撤消当前上线并回滚到以前的修订版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout undo deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment
</code></pre><!--
Alternatively, you can rollback to a specific revision by specifying it with `-to-revision`:
-->
<p>或者，你也可以通过使用 <code>--to-revision</code> 来回滚到特定修订版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout undo deployment/nginx-deployment --to-revision<span style="color:#666">=</span><span style="color:#666">2</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment
</code></pre><!--
For more details about rollout related commands, read [`kubectl rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout).
-->
<p>与回滚相关的指令的更详细信息，请参考
<a href="/docs/reference/generated/kubectl/kubectl-commands#rollout"><code>kubectl rollout</code></a>。</p>
<!--
The Deployment is now rolled back to a previous stable revision. As you can see, a `DeploymentRollback` event
for rolling back to revision 2 is generated from Deployment controller.
-->
<p>现在，Deployment 正在回滚到以前的稳定版本。正如你所看到的，Deployment
控制器生成了回滚到修订版本 2 的 <code>DeploymentRollback</code> 事件。</p>
</li>
</ol>
<!--
 2. Check if the rollback was successful and the Deployment is running as expected, run:
-->
<ol start="2">
<li>
<p>检查回滚是否成功以及 Deployment 是否正在运行，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30m
</code></pre></li>
</ol>
<!--
3. Get the description of the Deployment:
-->
<ol start="3">
<li>
<p>获取 Deployment 描述信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployment nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
                        kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.16.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &quot;nginx-deployment&quot; to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
</code></pre></li>
</ol>
<!--
## Scaling a Deployment

You can scale a Deployment by using the following command:
-->
<h2 id="scaling-a-deployment">缩放 Deployment  </h2>
<p>你可以使用如下指令缩放 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale deployment/nginx-deployment --replicas<span style="color:#666">=</span><span style="color:#666">10</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment scaled
</code></pre><!--
Assuming [horizontal Pod autoscaling](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) is enabled
in your cluster, you can setup an autoscaler for your Deployment and choose the minimum and maximum number of
Pods you want to run based on the CPU utilization of your existing Pods.
-->
<p>假设集群启用了<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">Pod 的水平自动缩放</a>，
你可以为 Deployment 设置自动缩放器，并基于现有 Pod 的 CPU 利用率选择要运行的
Pod 个数下限和上限。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl autoscale deployment/nginx-deployment --min<span style="color:#666">=</span><span style="color:#666">10</span> --max<span style="color:#666">=</span><span style="color:#666">15</span> --cpu-percent<span style="color:#666">=</span><span style="color:#666">80</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment scaled
</code></pre><!--
### Proportional scaling

RollingUpdate Deployments support running multiple versions of an application at the same time. When you
or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress
or paused), the Deployment controller balances the additional replicas in the existing active
ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called *proportional scaling*.
-->
<h3 id="proportional-scaling">比例缩放 </h3>
<p>RollingUpdate 的 Deployment 支持同时运行应用程序的多个版本。
当自动缩放器缩放处于上线进程（仍在进行中或暂停）中的 RollingUpdate Deployment 时，
Deployment 控制器会平衡现有的活跃状态的 ReplicaSets（含 Pods 的 ReplicaSets）中的额外副本，
以降低风险。这称为 <em>比例缩放（Proportional Scaling）</em>。</p>
<!--
For example, you are running a Deployment with 10 replicas, [maxSurge](#max-surge)=3, and [maxUnavailable](#max-unavailable)=2.
-->
<p>例如，你正在运行一个 10 个副本的 Deployment，其
<a href="#max-surge">maxSurge</a>=3，<a href="#max-unavailable">maxUnavailable</a>=2。</p>
<!--
* Ensure that the 10 replicas in your Deployment are running.
-->
<ul>
<li>
<p>确保 Deployment 的这 10 个副本都在运行。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deploy
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s
</code></pre></li>
</ul>
<!--
* You update to a new image which happens to be unresolvable from inside the cluster.
-->
<ul>
<li>
<p>更新 Deployment 使用新镜像，碰巧该镜像无法从集群内部解析。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:sometag
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment image updated
</code></pre></li>
</ul>
<!--
* The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the
`maxUnavailable` requirement that you mentioned above. Check out the rollout status:
-->
<ul>
<li>
<p>镜像更新使用 ReplicaSet <code>nginx-deployment-1989198191</code> 启动新的上线过程，
但由于上面提到的 <code>maxUnavailable</code> 要求，该进程被阻塞了。检查上线状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
</code></pre></li>
</ul>
<!--
* Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas
to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using
proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you
spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the
most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the
ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.
-->
<ul>
<li>然后，出现了新的 Deployment 扩缩请求。自动缩放器将 Deployment 副本增加到 15。
Deployment 控制器需要决定在何处添加 5 个新副本。如果未使用比例缩放，所有 5 个副本
都将添加到新的 ReplicaSet 中。使用比例缩放时，可以将额外的副本分布到所有 ReplicaSet。
较大比例的副本会被添加到拥有最多副本的 ReplicaSet，而较低比例的副本会进入到
副本较少的 ReplicaSet。所有剩下的副本都会添加到副本最多的 ReplicaSet。
具有零副本的 ReplicaSets 不会被扩容。</li>
</ul>
<!--
In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the
new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming
the new replicas become healthy. To confirm this, run:
-->
<p>在上面的示例中，3 个副本被添加到旧 ReplicaSet 中，2 个副本被添加到新 ReplicaSet。
假定新的副本都很健康，上线过程最终应将所有副本迁移到新的 ReplicaSet 中。
要确认这一点，请运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deploy
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
</code></pre><!--
The rollout status confirms how the replicas were added to each ReplicaSet.
-->
<p>上线状态确认了副本是如何被添加到每个 ReplicaSet 的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   <span style="color:#666">7</span>         <span style="color:#666">7</span>         <span style="color:#666">0</span>         7m
nginx-deployment-618515232    <span style="color:#666">11</span>        <span style="color:#666">11</span>        <span style="color:#666">11</span>        7m
</code></pre></div><!--
## Pausing and Resuming a rollout of a Deployment {#pausing-and-resuming-a-deployment}

When you update a Deployment, or plan to, you can pause rollouts
for that Deployment before you trigger one or more updates. When
you're ready to apply those changes, you resume rollouts for the
Deployment. This approach allows you to
apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.
-->
<h2 id="pausing-and-resuming-a-deployment">暂停、恢复 Deployment 的上线过程 </h2>
<p>在你更新一个 Deployment 的时候，或者计划更新它的时候，
你可以在触发一个或多个更新之前暂停 Deployment 的上线过程。
当你准备行应用这些变更时，你可以重新恢复 Deployment 上线过程。
这样做使得你能够在暂停和恢复执行之间应用多个修补程序，而不会触发不必要的上线操作。</p>
<!--
* For example, with a Deployment that was created:

  Get the Deployment details:
-->
<ul>
<li>
<p>例如，对于一个刚刚创建的 Deployment：</p>
<p>获取该 Deployment 信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deploy
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m
</code></pre><!--
Get the rollout status:
-->
<p>获取上线状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
</code></pre></li>
</ul>
<!--
* Pause by running the following command:
-->
<ul>
<li>
<p>使用如下指令暂停上线：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout pause deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">deployment.apps/nginx-deployment paused
</code></pre></div></li>
</ul>
<!--
* Then update the image of the Deployment:
-->
<ul>
<li>
<p>接下来更新 Deployment 镜像：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.16.1
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment image updated
</code></pre></li>
</ul>
<!--
 * Notice that no new rollout started:
-->
<ul>
<li>
<p>注意没有新的上线被触发：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout <span style="color:#a2f">history</span> deployment/nginx-deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>deployments &quot;nginx&quot;
REVISION  CHANGE-CAUSE
1   &lt;none&gt;
</code></pre></li>
</ul>
<!--
* Get the rollout status to verify that the existing ReplicaSet has not changed:
-->
<ul>
<li>
<p>获取上线状态验证现有的 ReplicaSet 没有被更改：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   <span style="color:#666">3</span>         <span style="color:#666">3</span>         <span style="color:#666">3</span>         2m
</code></pre></div></li>
</ul>
<!--
* You can make as many updates as you wish, for example, update the resources that will be used:
-->
<ul>
<li>
<p>你可以根据需要执行很多更新操作，例如，可以要使用的资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> resources deployment/nginx-deployment -c<span style="color:#666">=</span>nginx --limits<span style="color:#666">=</span><span style="color:#b8860b">cpu</span><span style="color:#666">=</span>200m,memory<span style="color:#666">=</span>512Mi
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment resource requirements updated
</code></pre><!--
The initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to
the Deployment will not have any effect as long as the Deployment is paused.
-->
<p>暂停 Deployment 上线之前的初始状态将继续发挥作用，但新的更新在 Deployment
上线被暂停期间不会产生任何效果。</p>
</li>
</ul>
<!--
* Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:
-->
<ul>
<li>
<p>最终，恢复 Deployment 上线并观察新的 ReplicaSet 的创建过程，其中包含了所应用的所有更新：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout resume deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于这样：</p>
<pre><code>deployment.apps/nginx-deployment resumed
</code></pre></li>
</ul>
<!--
* Watch the status of the rollout until it's done.
-->
<ul>
<li>
<p>观察上线的状态，直到完成。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs -w
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
</code></pre></li>
</ul>
<!--
* Get the status of the latest rollout:
-->
<ul>
<li>
<p>获取最近上线的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
</code></pre></li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
You cannot rollback a paused Deployment until you resume it.
-->
<p>你不可以回滚处于暂停状态的 Deployment，除非先恢复其执行状态。
</div>
<!--
## Deployment status

A Deployment enters various states during its lifecycle. It can be [progressing](#progressing-deployment) while
rolling out a new ReplicaSet, it can be [complete](#complete-deployment), or it can [fail to progress](#failed-deployment).
-->
<h2 id="deployment-status">Deployment 状态</h2>
<p>Deployment 的生命周期中会有许多状态。上线新的 ReplicaSet 期间可能处于
<a href="#progressing-deployment">Progressing（进行中）</a>，可能是
<a href="#complete-deployment">Complete（已完成）</a>，也可能是
<a href="#failed-deployment">Failed（失败）</a>以至于无法继续进行。</p>
<!--
### Progressing Deployment

Kubernetes marks a Deployment as _progressing_ when one of the following tasks is performed:
-->
<h3 id="progressing-deployment">进行中的 Deployment </h3>
<p>执行下面的任务期间，Kubernetes 标记 Deployment 为 <em>进行中（Progressing）</em>：</p>
<!--
* The Deployment creates a new ReplicaSet.
* The Deployment is scaling up its newest ReplicaSet.
* The Deployment is scaling down its older ReplicaSet(s).
* New Pods become ready or available (ready for at least [MinReadySeconds](#min-ready-seconds)).
-->
<ul>
<li>Deployment 创建新的 ReplicaSet</li>
<li>Deployment 正在为其最新的 ReplicaSet 扩容</li>
<li>Deployment 正在为其旧有的 ReplicaSet(s) 缩容</li>
<li>新的 Pods 已经就绪或者可用（就绪至少持续了 <a href="#min-ready-seconds">MinReadySeconds</a> 秒）。</li>
</ul>
<!--
When the rollout becomes “progressing”, the Deployment controller adds a condition with the following
attributes to the Deployment's `.status.conditions`:
-->
<p>当上线过程进入“Progressing”状态时，Deployment 控制器会向 Deployment 的
<code>.status.conditions</code> 中添加包含下面属性的状况条目：</p>
<ul>
<li><code>type: Progressing</code></li>
<li><code>status: &quot;True&quot;</code></li>
<li><code>reason: NewReplicaSetCreated</code> | <code>reason: FoundNewReplicaSet</code> | <code>reason: ReplicaSetUpdated</code></li>
</ul>
<!--
You can monitor the progress for a Deployment by using `kubectl rollout status`.
-->
<p>你可以使用 <code>kubectl rollout status</code> 监视 Deployment 的进度。</p>
<!--
### Complete Deployment

Kubernetes marks a Deployment as _complete_ when it has the following characteristics:
-->
<h3 id="complete-deployment">完成的 Deployment   </h3>
<p>当 Deployment 具有以下特征时，Kubernetes 将其标记为 <em>完成（Complete）</em>：</p>
<!--
* All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any
updates you've requested have been completed.
* All of the replicas associated with the Deployment are available.
* No old replicas for the Deployment are running.
-->
<ul>
<li>与 Deployment 关联的所有副本都已更新到指定的最新版本，这意味着之前请求的所有更新都已完成。</li>
<li>与 Deployment 关联的所有副本都可用。</li>
<li>未运行 Deployment 的旧副本。</li>
</ul>
<!--
When the rollout becomes “complete”, the Deployment controller sets a condition with the following
attributes to the Deployment's `.status.conditions`:
-->
<p>当上线过程进入“Complete”状态时，Deployment 控制器会向 Deployment 的
<code>.status.conditions</code> 中添加包含下面属性的状况条目：</p>
<ul>
<li><code>type: Progressing</code></li>
<li><code>status: &quot;True&quot;</code></li>
<li><code>reason: NewReplicaSetAvailable</code></li>
</ul>
<!--
This `Progressing` condition will retain a status value of `"True"` until a new rollout
is initiated. The condition holds even when availability of replicas changes (which
does instead affect the `Available` condition).
-->
<p>这一 <code>Progressing</code> 状况的状态值会持续为 <code>&quot;True&quot;</code>，直至新的上线动作被触发。
即使副本的可用状态发生变化（进而影响 <code>Available</code> 状况），<code>Progressing</code> 状况的值也不会变化。</p>
<!--
You can check if a Deployment has completed by using `kubectl rollout status`. If the rollout completed
successfully, `kubectl rollout status` returns a zero exit code.
-->
<p>你可以使用 <code>kubectl rollout status</code> 检查 Deployment 是否已完成。
如果上线成功完成，<code>kubectl rollout status</code> 返回退出代码 0。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status deployment/nginx-deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Waiting <span style="color:#a2f;font-weight:bold">for</span> rollout to finish: <span style="color:#666">2</span> of <span style="color:#666">3</span> updated replicas are available...
deployment <span style="color:#b44">&#34;nginx-deployment&#34;</span> successfully rolled out
</code></pre></div><p>从 <code>kubectl rollout</code> 命令获得的返回状态为 0（成功）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ <span style="color:#a2f">echo</span> <span style="color:#b8860b">$?</span>
</code></pre></div><pre><code>0
</code></pre><!--
### Failed Deployment
-->
<h3 id="failed-deployment">失败的 Deployment  </h3>
<!--
Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur
due to some of the following factors:
-->
<p>你的 Deployment 可能会在尝试部署其最新的 ReplicaSet 受挫，一直处于未完成状态。
造成此情况一些可能因素如下：</p>
<!--
 * Insufficient quota
* Readiness probe failures
* Image pull errors
* Insufficient permissions
* Limit ranges
* Application runtime misconfiguration
-->
<ul>
<li>配额（Quota）不足</li>
<li>就绪探测（Readiness Probe）失败</li>
<li>镜像拉取错误</li>
<li>权限不足</li>
<li>限制范围（Limit Ranges）问题</li>
<li>应用程序运行时的配置错误</li>
</ul>
<!--
One way you can detect this condition is to specify a deadline parameter in your Deployment spec:
([`.spec.progressDeadlineSeconds`](#progress-deadline-seconds)). `.spec.progressDeadlineSeconds` denotes the
number of seconds the Deployment controller waits before indicating (in the Deployment status) that the
Deployment progress has stalled.
-->
<p>检测此状况的一种方法是在 Deployment 规约中指定截止时间参数：
（[<code>.spec.progressDeadlineSeconds</code>]（#progress-deadline-seconds））。
<code>.spec.progressDeadlineSeconds</code> 给出的是一个秒数值，Deployment 控制器在（通过 Deployment 状态）
标示 Deployment 进展停滞之前，需要等待所给的时长。</p>
<!--
The following `kubectl` command sets the spec with `progressDeadlineSeconds` to make the controller report
lack of progress for a Deployment after 10 minutes:
-->
<p>以下 <code>kubectl</code> 命令设置规约中的 <code>progressDeadlineSeconds</code>，从而告知控制器
在 10 分钟后报告 Deployment 没有进展：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch deployment/nginx-deployment -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;progressDeadlineSeconds&#34;:600}}&#39;</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment patched
</code></pre><!--
Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following
attributes to the Deployment's `.status.conditions`:
-->
<p>超过截止时间后，Deployment 控制器将添加具有以下属性的 Deployment 状况到
Deployment 的 <code>.status.conditions</code> 中：</p>
<ul>
<li>Type=Progressing</li>
<li>Status=False</li>
<li>Reason=ProgressDeadlineExceeded</li>
</ul>
<!--
This condition can also fail early and is then set to status value of `"False"` due to reasons as `ReplicaSetCreateError`.
Also, the deadline is not taken into account anymore once the Deployment rollout completes.
-->
<p>这一状况也可能会比较早地失败，因而其状态值被设置为 <code>&quot;False&quot;</code>，
其原因为 <code>ReplicaSetCreateError</code>。
一旦 Deployment 上线完成，就不再考虑其期限。</p>
<!--
See the [Kubernetes API conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties) for more information on status conditions.
-->
<p>参考
<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties">Kubernetes API Conventions</a>
获取更多状态状况相关的信息。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
 Kubernetes takes no action on a stalled Deployment other than to report a status condition with
`Reason=ProgressDeadlineExceeded`. Higher level orchestrators can take advantage of it and act accordingly, for
example, rollback the Deployment to its previous version.
-->
<p>除了报告 <code>Reason=ProgressDeadlineExceeded</code> 状态之外，Kubernetes 对已停止的
Deployment 不执行任何操作。更高级别的编排器可以利用这一设计并相应地采取行动。
例如，将 Deployment 回滚到其以前的版本。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline. You can
safely pause a Deployment in the middle of a rollout and resume without triggering the condition for
exceeding the deadline.
-->
<p>如果你暂停了某个 Deployment 上线，Kubernetes 不再根据指定的截止时间检查 Deployment 进展。
你可以在上线过程中间安全地暂停 Deployment 再恢复其执行，这样做不会导致超出最后时限的问题。
</div>
<!--
You may experience transient errors with your Deployments, either due to a low timeout that you have set or
due to any other kind of error that can be treated as transient. For example, let's suppose you have
insufficient quota. If you describe the Deployment you will notice the following section:
-->
<p>Deployment 可能会出现瞬时性的错误，可能因为设置的超时时间过短，
也可能因为其他可认为是临时性的问题。例如，假定所遇到的问题是配额不足。
如果描述 Deployment，你将会注意到以下部分：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployment nginx-deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>&lt;...&gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&lt;...&gt;
</code></pre><!--
If you run `kubectl get deployment nginx-deployment -o yaml`, the Deployment status is similar to this:
-->
<p>如果运行 <code>kubectl get deployment nginx-deployment -o yaml</code>，Deployment 状态输出
将类似于这样：</p>
<pre><code>status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set &quot;nginx-deployment-4262182780&quot; is progressing.
    reason: ReplicaSetUpdated
    status: &quot;True&quot;
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &quot;True&quot;
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: 'Error creating: pods &quot;nginx-deployment-4262182780-&quot; is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2'
    reason: FailedCreate
    status: &quot;True&quot;
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
</code></pre><!--
Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the
reason for the Progressing condition:
-->
<p>最终，一旦超过 Deployment 进度限期，Kubernetes 将更新状态和进度状况的原因：</p>
<pre><code>Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
</code></pre><!--
You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other
controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota
conditions and the Deployment controller then completes the Deployment rollout, you'll see the
Deployment's status update with a successful condition (`Status=True` and `Reason=NewReplicaSetAvailable`).
-->
<p>可以通过缩容 Deployment 或者缩容其他运行状态的控制器，或者直接在命名空间中增加配额
来解决配额不足的问题。如果配额条件满足，Deployment 控制器完成了 Deployment 上线操作，
Deployment 状态会更新为成功状况（<code>Status=True</code> and <code>Reason=NewReplicaSetAvailable</code>）。</p>
<pre><code>Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
</code></pre><!--
`type: Available` with `status: "True"` means that your Deployment has minimum availability. Minimum availability is dictated
by the parameters specified in the deployment strategy. `type: Progressing` with `status: "True"` means that your Deployment
is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum
required new replicas are available (see the Reason of the condition for the particulars - in our case
`reason: NewReplicaSetAvailable` means that the Deployment is complete).
-->
<p><code>type: Available</code> 加上 <code>status: True</code> 意味着 Deployment 具有最低可用性。
最低可用性由 Deployment 策略中的参数指定。
<code>type: Progressing</code> 加上 <code>status: True</code> 表示 Deployment 处于上线过程中，并且正在运行，
或者已成功完成进度，最小所需新副本处于可用。
请参阅对应状况的 Reason 了解相关细节。
在我们的案例中 <code>reason: NewReplicaSetAvailable</code> 表示 Deployment 已完成。</p>
<!--
You can check if a Deployment has failed to progress by using `kubectl rollout status`. `kubectl rollout status`
returns a non-zero exit code if the Deployment has exceeded the progression deadline.
-->
<p>你可以使用 <code>kubectl rollout status</code> 检查 Deployment 是否未能取得进展。
如果 Deployment 已超过进度限期，<code>kubectl rollout status</code> 返回非零退出代码。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment &quot;nginx&quot; exceeded its progress deadline
</code></pre><!--
and the exit status from `kubectl rollout` is 1 (indicating an error):
-->
<p><code>kubectl rollout</code> 命令的退出状态为 1（表明发生了错误）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ <span style="color:#a2f">echo</span> <span style="color:#b8860b">$?</span>
</code></pre></div><pre><code>1
</code></pre><!--
### Operating on a failed deployment

All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back
to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment
Pod template.
-->
<h3 id="operating-on-a-failed-deployment">对失败 Deployment 的操作  </h3>
<p>可应用于已完成的 Deployment 的所有操作也适用于失败的 Deployment。
你可以对其执行扩缩容、回滚到以前的修订版本等操作，或者在需要对 Deployment 的
Pod 模板应用多项调整时，将 Deployment 暂停。</p>
<!--
## Clean up Policy

You can set `.spec.revisionHistoryLimit` field in a Deployment to specify how many old ReplicaSets for
this Deployment you want to retain. The rest will be garbage-collected in the background. By default,
it is 10.
-->
<h2 id="clean-up-policy">清理策略  </h2>
<p>你可以在 Deployment 中设置 <code>.spec.revisionHistoryLimit</code> 字段以指定保留此
Deployment 的多少个旧有 ReplicaSet。其余的 ReplicaSet 将在后台被垃圾回收。
默认情况下，此值为 10。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment
thus that Deployment will not be able to roll back.
-->
<p>显式将此字段设置为 0 将导致 Deployment 的所有历史记录被清空，因此 Deployment 将无法回滚。
</div>
<!--
## Canary Deployment

If you want to roll out releases to a subset of users or servers using the Deployment, you
can create multiple Deployments, one for each release, following the canary pattern described in
[managing resources](/docs/concepts/cluster-administration/manage-deployment/#canary-deployments).
-->
<h2 id="canary-deployment">金丝雀部署</h2>
<p>如果要使用 Deployment 向用户子集或服务器子集上线版本，
则可以遵循<a href="/zh/docs/concepts/cluster-administration/manage-deployment/#canary-deployments">资源管理</a>
所描述的金丝雀模式，创建多个 Deployment，每个版本一个。</p>
<!--
## Writing a Deployment Spec

As with all other Kubernetes configs, a Deployment needs `apiVersion`, `kind`, and `metadata` fields.
For general information about working with config files, see
[deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/),
configuring containers, and [using kubectl to manage resources](/docs/concepts/overview/working-with-objects/object-management/) documents.
-->
<h2 id="writing-a-deployment-spec">编写 Deployment 规约      </h2>
<p>同其他 Kubernetes 配置一样， Deployment 需要 <code>apiVersion</code>，<code>kind</code> 和 <code>metadata</code> 字段。
有关配置文件的其他信息，请参考<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">部署 Deployment</a>、
配置容器和<a href="/zh/docs/concepts/overview/working-with-objects/object-management/">使用 kubectl 管理资源</a>等相关文档。</p>
<!--
The name of a Deployment object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A Deployment also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
-->
<p>Deployment 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。
Deployment 还需要 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> 部分</a>。</p>
<!--
### Pod Template

The `.spec.template` and `.spec.selector` are the only required field of the `.spec`.
-->
<h3 id="pod-template">Pod 模板    </h3>
<p><code>.spec</code> 中只有 <code>.spec.template</code> 和 <code>.spec.selector</code> 是必需的字段。</p>
<!--

The `.spec.template` is a [Pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>, except it is nested and does not have an `apiVersion` or `kind`.
-->
<p><code>.spec.template</code> 是一个 <a href="/zh/docs/concepts/workloads/pods/#pod-templates">Pod 模板</a>。
它和 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 的语法规则完全相同。
只是这里它是嵌套的，因此不需要 <code>apiVersion</code> 或 <code>kind</code>。</p>
<!--
In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [selector](#selector)).
-->
<p>除了 Pod 的必填字段外，Deployment 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。
对于标签，请确保不要与其他控制器重叠。请参考<a href="#selector">选择算符</a>。</p>
<!--
Only a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is
allowed, which is the default if not specified.
-->
<p>只有 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>.spec.template.spec.restartPolicy</code></a>
等于 <code>Always</code> 才是被允许的，这也是在没有指定时的默认设置。</p>
<!--
### Replicas

`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.
-->
<h3 id="replicas">副本  </h3>
<p><code>.spec.replicas</code> 是指定所需 Pod 的可选字段。它的默认值是1。</p>
<!--
Should you manually scale a Deployment, example via `kubectl scale deployment
deployment --replicas=X`, and then you update that Deployment based on a manifest
(for example: by running `kubectl apply -f deployment.yaml`),
then applying that manifest overwrites the manual scaling that you previously did.
-->
<p>如果你对某个 Deployment 执行了手动扩缩操作（例如，通过
<code>kubectl scale deployment deployment --replicas=X</code>），
之后基于清单对 Deployment 执行了更新操作（例如通过运行
<code>kubectl apply -f deployment.yaml</code>），那么通过应用清单而完成的更新会覆盖之前手动扩缩所作的变更。</p>
<!--
If a [HorizontalPodAutoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/) (or any
similar API for horizontal scaling) is managing scaling for a Deployment, don't set `.spec.replicas`.
-->
<p>如果一个 <a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale/">HorizontalPodAutoscaler</a>
（或者其他执行水平扩缩操作的类似 API）在管理 Deployment 的扩缩，
则不要设置 <code>.spec.replicas</code>。</p>
<!--
Instead, allow the Kubernetes
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> to manage the
`.spec.replicas` field automatically.
-->
<p>恰恰相反，应该允许 Kubernetes
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a>来自动管理
<code>.spec.replicas</code> 字段。</p>
<!--
### Selector

`.spec.selector` is an required field that specifies a [label selector](/docs/concepts/overview/working-with-objects/labels/)
for the Pods targeted by this Deployment.

`.spec.selector` must match `.spec.template.metadata.labels`, or it will be rejected by the API.
-->
<h3 id="selector">选择算符  </h3>
<p><code>.spec.selector</code> 是指定本 Deployment 的 Pod
<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签选择算符</a>的必需字段。</p>
<p><code>.spec.selector</code> 必须匹配 <code>.spec.template.metadata.labels</code>，否则请求会被 API 拒绝。</p>
<!--
In API version `apps/v1`, `.spec.selector` and `.metadata.labels` do not default to `.spec.template.metadata.labels` if not set. So they must be set explicitly. Also note that `.spec.selector` is immutable after creation of the Deployment in `apps/v1`.
-->
<p>在 API <code>apps/v1</code>版本中，<code>.spec.selector</code> 和 <code>.metadata.labels</code> 如果没有设置的话，
不会被默认设置为 <code>.spec.template.metadata.labels</code>，所以需要明确进行设置。
同时在 <code>apps/v1</code>版本中，Deployment 创建后 <code>.spec.selector</code> 是不可变的。</p>
<!--
A Deployment may terminate Pods whose labels match the selector if their template is different
from `.spec.template` or if the total number of such Pods exceeds `.spec.replicas`. It brings up new
Pods with `.spec.template` if the number of Pods is less than the desired number.
-->
<p>当 Pod 的标签和选择算符匹配，但其模板和 <code>.spec.template</code> 不同时，或者此类 Pod
的总数超过 <code>.spec.replicas</code> 的设置时，Deployment 会终结之。
如果 Pods 总数未达到期望值，Deployment 会基于 <code>.spec.template</code> 创建新的 Pod。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
You should not create other Pods whose labels match this selector, either directly, by creating
another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you
do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.
-->
<p>你不应直接创建与此选择算符匹配的 Pod，也不应通过创建另一个 Deployment 或者类似于
ReplicaSet 或 ReplicationController 这类控制器来创建标签与此选择算符匹配的 Pod。
如果这样做，第一个 Deployment 会认为它创建了这些 Pod。
Kubernetes 不会阻止你这么做。
</div>
<!--
If you have multiple controllers that have overlapping selectors, the controllers will fight with each
other and won't behave correctly.
-->
<p>如果有多个控制器的选择算符发生重叠，则控制器之间会因冲突而无法正常工作。</p>
<!--
### Strategy

`.spec.strategy` specifies the strategy used to replace old Pods by new ones.
`.spec.strategy.type` can be "Recreate" or "RollingUpdate". "RollingUpdate" is
the default value.
-->
<h3 id="strategy">策略  </h3>
<p><code>.spec.strategy</code> 策略指定用于用新 Pods 替换旧 Pods 的策略。
<code>.spec.strategy.type</code> 可以是 “Recreate” 或 “RollingUpdate”。“RollingUpdate” 是默认值。</p>
<!--
#### Recreate Deployment

All existing Pods are killed before new ones are created when `.spec.strategy.type==Recreate`.
-->
<h4 id="recreate-deployment">重新创建 Deployment  </h4>
<p>如果 <code>.spec.strategy.type==Recreate</code>，在创建新 Pods 之前，所有现有的 Pods 会被杀死。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods 
of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new 
revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the 
replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an 
"at most" guarantee for your Pods, you should consider using a 
[StatefulSet](/docs/concepts/workloads/controllers/statefulset/).
-->
<p>这只会确保为了升级而创建新 Pod 之前其他 Pod 都已终止。如果你升级一个 Deployment，
所有旧版本的 Pod 都会立即被终止。控制器等待这些 Pod 被成功移除之后，
才会创建新版本的 Pod。如果你手动删除一个 Pod，其生命周期是由 ReplicaSet 来控制的，
后者会立即创建一个替换 Pod（即使旧的 Pod 仍然处于 Terminating 状态）。
如果你需要一种“最多 n 个”的 Pod 个数保证，你需要考虑使用
<a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>。
</div>
<!--
#### Rolling Update Deployment

The Deployment updates Pods in a rolling update
fashion when `.spec.strategy.type==RollingUpdate`. You can specify `maxUnavailable` and `maxSurge` to control
the rolling update process.
-->
<h4 id="rolling-update-deployment">滚动更新 Deployment  </h4>
<p>Deployment 会在 <code>.spec.strategy.type==RollingUpdate</code>时，采取
滚动更新的方式更新 Pods。你可以指定 <code>maxUnavailable</code> 和 <code>maxSurge</code> 来控制滚动更新
过程。</p>
<!--
##### Max Unavailable
-->
<h5 id="max-unavailable">最大不可用  </h5>
<!--
`.spec.strategy.rollingUpdate.maxUnavailable` is an optional field that specifies the maximum number
of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)
or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by
rounding down. The value cannot be 0 if `.spec.strategy.rollingUpdate.maxSurge` is 0. The default value is 25%.
-->
<p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> 是一个可选字段，用来指定
更新过程中不可用的 Pod 的个数上限。该值可以是绝对数字（例如，5），也可以是所需
Pods 的百分比（例如，10%）。百分比值会转换成绝对数并去除小数部分。
如果 <code>.spec.strategy.rollingUpdate.maxSurge</code> 为 0，则此值不能为 0。
默认值为 25%。</p>
<!--
For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired
Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled
down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available
at all times during the update is at least 70% of the desired Pods.
-->
<p>例如，当此值设置为 30% 时，滚动更新开始时会立即将旧 ReplicaSet 缩容到期望 Pod 个数的70%。
新 Pod 准备就绪后，可以继续缩容旧有的 ReplicaSet，然后对新的 ReplicaSet 扩容，
确保在更新期间可用的 Pods 总数在任何时候都至少为所需的 Pod 个数的 70%。</p>
<!--
##### Max Surge

 `.spec.strategy.rollingUpdate.maxSurge` is an optional field that specifies the maximum number of Pods
that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a
percentage of desired Pods (for example, 10%). The value cannot be 0 if `MaxUnavailable` is 0. The absolute number
is calculated from the percentage by rounding up. The default value is 25%.
-->
<h5 id="max-surge">最大峰值  </h5>
<p><code>.spec.strategy.rollingUpdate.maxSurge</code> 是一个可选字段，用来指定可以创建的超出期望
Pod 个数的 Pod 数量。此值可以是绝对数（例如，5）或所需 Pods 的百分比（例如，10%）。
如果 <code>MaxUnavailable</code> 为 0，则此值不能为 0。百分比值会通过向上取整转换为绝对数。
此字段的默认值为 25%。</p>
<!--
For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the
rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired
Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the
total number of Pods running at any time during the update is at most 130% of desired Pods.
-->
<p>例如，当此值为 30% 时，启动滚动更新后，会立即对新的 ReplicaSet 扩容，同时保证新旧 Pod
的总数不超过所需 Pod 总数的 130%。一旦旧 Pods 被杀死，新的 ReplicaSet 可以进一步扩容，
同时确保更新期间的任何时候运行中的 Pods 总数最多为所需 Pods 总数的 130%。</p>
<!--
 ### Progress Deadline Seconds

 `.spec.progressDeadlineSeconds` is an optional field that specifies the number of seconds you want
to wait for your Deployment to progress before the system reports back that the Deployment has
[failed progressing](#failed-deployment) - surfaced as a condition with `type: Progressing`, `status: False`.
and `reason: ProgressDeadlineExceeded` in the status of the resource. The Deployment controller will keep
retrying the Deployment. In the future, once automatic rollback will be implemented, the Deployment
controller will roll back a Deployment as soon as it observes such a condition.
-->
<h3 id="progress-deadline-seconds">进度期限秒数   </h3>
<p><code>.spec.progressDeadlineSeconds</code> 是一个可选字段，用于指定系统在报告 Deployment
<a href="#failed-deployment">进展失败</a> 之前等待 Deployment 取得进展的秒数。
这类报告会在资源状态中体现为 <code>type: Progressing</code>、<code>status: False</code>、
<code>reason: ProgressDeadlineExceeded</code>。Deployment 控制器将持续重试 Deployment。
将来，一旦实现了自动回滚，Deployment 控制器将在探测到这样的条件时立即回滚 Deployment。</p>
<!--
If specified, this field needs to be greater than `.spec.minReadySeconds`.
-->
<p>如果指定，则此字段值需要大于 <code>.spec.minReadySeconds</code> 取值。</p>
<!--
### Min Ready Seconds

 `.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be ready without any of its containers crashing, for it to be considered available.
This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).
-->
<h3 id="min-ready-seconds">最短就绪时间   </h3>
<p><code>.spec.minReadySeconds</code> 是一个可选字段，用于指定新创建的 Pod
在没有任意容器崩溃情况下的最小就绪时间，
只有超出这个时间 Pod 才被视为可用。默认值为 0（Pod 在准备就绪后立即将被视为可用）。
要了解何时 Pod 被视为就绪，
可参考<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">容器探针</a>。</p>
<!--
### Revision History Limit

A Deployment's revision history is stored in the ReplicaSets it controls.
-->
<h3 id="修订历史限制">修订历史限制</h3>
<p>Deployment 的修订历史记录存储在它所控制的 ReplicaSets 中。</p>
<!--
`.spec.revisionHistoryLimit` is an optional field that specifies the number of old ReplicaSets to retain
to allow rollback. These old ReplicaSets consume resources in `etcd` and crowd the output of `kubectl get rs`. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.
-->
<p><code>.spec.revisionHistoryLimit</code> 是一个可选字段，用来设定出于会滚目的所要保留的旧 ReplicaSet 数量。
这些旧 ReplicaSet 会消耗 etcd 中的资源，并占用 <code>kubectl get rs</code> 的输出。
每个 Deployment 修订版本的配置都存储在其 ReplicaSets 中；因此，一旦删除了旧的 ReplicaSet，
将失去回滚到 Deployment 的对应修订版本的能力。
默认情况下，系统保留 10 个旧 ReplicaSet，但其理想值取决于新 Deployment 的频率和稳定性。</p>
<!--
More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.
In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.
-->
<p>更具体地说，将此字段设置为 0 意味着将清理所有具有 0 个副本的旧 ReplicaSet。
在这种情况下，无法撤消新的 Deployment 上线，因为它的修订历史被清除了。</p>
<!--
### Paused

`.spec.paused` is an optional boolean field for pausing and resuming a Deployment. The only difference between
a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused
Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when
it is created.
-->
<h3 id="paused">paused（暂停的） </h3>
<p><code>.spec.paused</code> 是用于暂停和恢复 Deployment 的可选布尔字段。
暂停的 Deployment 和未暂停的 Deployment 的唯一区别是，Deployment 处于暂停状态时，
PodTemplateSpec 的任何修改都不会触发新的上线。
Deployment 在创建时是默认不会处于暂停状态。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods).
* [Run a Stateless Application Using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/).
* `Deployment` is a top-level resource in the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for deployments.
* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how
  you can use it to manage application availability during disruptions.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pod</a>。</li>
<li><a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">使用 Deployment 运行一个无状态应用</a>。</li>
<li><code>Deployment</code> 是 Kubernetes REST API 中的一个顶层资源。
阅读 





<a href=""></a>
对象定义，以了解 Deployment 的 API 细节。</li>
<li>阅读 <a href="/zh/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>
了解如何使用它来在可能出现干扰的情况下管理应用的可用性。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d459b930218774655fa7fd1620625539">2.2 - ReplicaSet</h1>
    
	<!--
reviewers:
- Kashomon
- bprashanth
- madhusudancs
title: ReplicaSet
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often
used to guarantee the availability of a specified number of identical Pods.
-->
<p>ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。
因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。</p>
<!-- body -->
<!--
## How a ReplicaSet works

A ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number
of replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods
it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating
and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod
template.
-->
<h2 id="how-a-replicaset-works">ReplicaSet 的工作原理</h2>
<p>ReplicaSet 是通过一组字段来定义的，包括一个用来识别可获得的 Pod
的集合的选择算符、一个用来标明应该维护的副本个数的数值、一个用来指定应该创建新 Pod
以满足副本个数条件时要使用的 Pod 模板等等。
每个 ReplicaSet 都通过根据需要创建和 删除 Pod 以使得副本个数达到期望值，
进而实现其存在价值。当 ReplicaSet 需要创建新的 Pod 时，会使用所提供的 Pod 模板。</p>
<!--
A ReplicaSet is linked to its Pods via the Pods' [metadata.ownerReferences](/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents)
field, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning
ReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet
knows of the state of the Pods it is maintaining and plans accordingly.
-->
<p>ReplicaSet 通过 Pod 上的
<a href="/zh/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents">metadata.ownerReferences</a>
字段连接到附属 Pod，该字段给出当前对象的属主资源。
ReplicaSet 所获得的 Pod 都在其 ownerReferences 字段中包含了属主 ReplicaSet
的标识信息。正是通过这一连接，ReplicaSet 知道它所维护的 Pod 集合的状态，
并据此计划其操作行为。</p>
<!--
A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no OwnerReference or the
OwnerReference is not a <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> and it matches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.
-->
<p>ReplicaSet 使用其选择算符来辨识要获得的 Pod 集合。如果某个 Pod 没有
OwnerReference 或者其 OwnerReference 不是一个
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>，且其匹配到
某 ReplicaSet 的选择算符，则该 Pod 立即被此 ReplicaSet 获得。</p>
<!--
## When to use a ReplicaSet

A ReplicaSet ensures that a specified number of pod replicas are running at any given
time. However, a Deployment is a higher-level concept that manages ReplicaSets and
provides declarative updates to pods along with a lot of other useful features.
Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless
you require custom update orchestration or don't require updates at all.

This actually means that you may never need to manipulate ReplicaSet objects:
use a Deployment instead, and define your application in the spec section.
-->
<h2 id="何时使用-replicaset">何时使用 ReplicaSet</h2>
<p>ReplicaSet 确保任何时间都有指定数量的 Pod 副本在运行。
然而，Deployment 是一个更高级的概念，它管理 ReplicaSet，并向 Pod
提供声明式的更新以及许多其他有用的功能。
因此，我们建议使用 Deployment 而不是直接使用 ReplicaSet，除非
你需要自定义更新业务流程或根本不需要更新。</p>
<p>这实际上意味着，你可能永远不需要操作 ReplicaSet 对象：而是使用
Deployment，并在 spec 部分定义你的应用。</p>
<!--
## Example
-->
<h2 id="示例">示例</h2>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/frontend.yaml" download="controllers/frontend.yaml"><code>controllers/frontend.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-frontend-yaml')" title="Copy controllers/frontend.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-frontend-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># modify replicas according to your case</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-redis<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google_samples/gb-frontend:v3<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Saving this manifest into `frontend.yaml` and submitting it to a Kubernetes cluster should
create the defined ReplicaSet and the pods that it manages.
-->
<p>将此清单保存到 <code>frontend.yaml</code> 中，并将其提交到 Kubernetes 集群，
应该就能创建 yaml 文件所定义的 ReplicaSet 及其管理的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</code></pre></div><!--
You can then get the current ReplicaSets deployed:
-->
<p>你可以看到当前被部署的 ReplicaSet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
And see the frontend one you created:
-->
<p>并看到你所创建的前端：</p>
<pre><code>NAME       DESIRED   CURRENT   READY   AGE
frontend   3         3         3       6s
</code></pre><!--
You can also check on the state of the ReplicaSet:
-->
<p>你也可以查看 ReplicaSet 的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe rs/frontend
</code></pre></div><!--
And you will see output similar to:
-->
<p>你会看到类似如下的输出：</p>
<pre><code>Name:		frontend
Namespace:	default
Selector:	tier=frontend
Labels:		app=guestbook
		tier=frontend
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;ReplicaSet&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;labels&quot;:{&quot;app&quot;:&quot;guestbook&quot;,&quot;tier&quot;:&quot;frontend&quot;},&quot;name&quot;:&quot;frontend&quot;,...
Replicas:	3 current / 3 desired
Pods Status:	3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       tier=frontend
  Containers:
   php-redis:
    Image:      gcr.io/google_samples/gb-frontend:v3
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts
</code></pre><!--
And lastly you can check for the Pods brought up:
-->
<p>最后可以查看启动了的 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
You should see Pod information similar to:
-->
<p>你会看到类似如下的 Pod 信息：</p>
<pre><code>NAME             READY   STATUS    RESTARTS   AGE
frontend-b2zdv   1/1     Running   0          6m36s
frontend-vcmts   1/1     Running   0          6m36s
frontend-wtsmm   1/1     Running   0          6m36s
</code></pre><!--
You can also verify that the owner reference of these pods is set to the frontend ReplicaSet.
To do this, get the yaml of one of the Pods running:
-->
<p>你也可以查看 Pods 的属主引用被设置为前端的 ReplicaSet。
要实现这点，可取回运行中的 Pods 之一的 YAML：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods frontend-b2zdv -o yaml
</code></pre></div><!--
The output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:
-->
<p>输出将类似这样，frontend ReplicaSet 的信息被设置在 metadata 的
<code>ownerReferences</code> 字段中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2020-02-12T07:06:16Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">generateName</span>:<span style="color:#bbb"> </span>frontend-<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend-b2zdv<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ownerReferences</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">blockOwnerDeletion</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">controller</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
## Non-Template Pod acquisitions

While you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have
labels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited
to owning Pods specified by its template - it can acquire other Pods in the manner specified in the previous sections.
-->
<h2 id="非模板-pod-的获得">非模板 Pod 的获得</h2>
<!--
While you can create bare Pods with no problems, it is strongly recommended to
make sure that the bare Pods do not have labels which match the selector of
one of your ReplicaSets. The reason for this is because a ReplicaSet is not
limited to owning Pods specified by its template - it can acquire other Pods
in the manner specified in the previous sections.

Take the previous frontend ReplicaSet example, and the Pods specified in the
following manifest:
-->
<p>尽管你完全可以直接创建裸的 Pods，强烈建议你确保这些裸的 Pods 并不包含可能与你
的某个 ReplicaSet 的选择算符相匹配的标签。原因在于 ReplicaSet 并不仅限于拥有
在其模板中设置的 Pods，它还可以像前面小节中所描述的那样获得其他 Pods。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-rs.yaml" download="pods/pod-rs.yaml"><code>pods/pod-rs.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-rs-yaml')" title="Copy pods/pod-rs.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-rs-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/hello-app:2.0<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/hello-app:1.0<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend
ReplicaSet, they will immediately be acquired by it.

Suppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to
fulfill its replica count requirement:
-->
<p>由于这些 Pod 没有控制器（Controller，或其他对象）作为其属主引用，并且
其标签与 frontend ReplicaSet 的选择算符匹配，它们会立即被该 ReplicaSet
获取。</p>
<p>假定你在 frontend ReplicaSet 已经被部署之后创建 Pods，并且你已经在 ReplicaSet
中设置了其初始的 Pod 副本数以满足其副本计数需要：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</code></pre></div><!--
The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over
its desired count.

Fetching the Pods:
-->
<p>新的 Pods 会被该 ReplicaSet 获取，并立即被 ReplicaSet 终止，因为
它们的存在会使得 ReplicaSet 中 Pod 个数超出其期望值。</p>
<p>取回 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The output shows that the new Pods are either already terminated, or in the process of being terminated:
-->
<p>输出显示新的 Pods 或者已经被终止，或者处于终止过程中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME             READY   STATUS        RESTARTS   AGE
frontend-b2zdv   1/1     Running       <span style="color:#666">0</span>          10m
frontend-vcmts   1/1     Running       <span style="color:#666">0</span>          10m
frontend-wtsmm   1/1     Running       <span style="color:#666">0</span>          10m
pod1             0/1     Terminating   <span style="color:#666">0</span>          1s
pod2             0/1     Terminating   <span style="color:#666">0</span>          1s
</code></pre></div><!--
If you create the Pods first:
-->
<p>如果你先行创建 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</code></pre></div><!--
And then create the ReplicaSet however:
-->
<p>之后再创建 ReplicaSet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</code></pre></div><!--
You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the
number of its new Pods and the original matches its desired count. As fetching the Pods:
-->
<p>你会看到 ReplicaSet 已经获得了该 Pods，并仅根据其规约创建新的 Pods，直到
新的 Pods 和原来的 Pods 的总数达到其预期个数。
这时取回 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
Will reveal in its output:
-->
<p>将会生成下面的输出：</p>
<pre><code>NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   0          9s
pod1             1/1     Running   0          36s
pod2             1/1     Running   0          36s
</code></pre><p>采用这种方式，一个 ReplicaSet 中可以包含异质的 Pods 集合。</p>
<!--
## Writing a ReplicaSet Spec

As with all other Kubernetes API objects, a ReplicaSet needs the `apiVersion`, `kind`, and `metadata` fields.
For ReplicaSets, the `kind` is always a ReplicaSet.

The name of a ReplicaSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A ReplicaSet also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status).
-->
<h2 id="编写-replicaset-的-spec">编写 ReplicaSet 的 spec</h2>
<p>与所有其他 Kubernetes API 对象一样，ReplicaSet 也需要 <code>apiVersion</code>、<code>kind</code>、和 <code>metadata</code> 字段。
对于 ReplicaSets 而言，其 <code>kind</code> 始终是 ReplicaSet。</p>
<p>ReplicaSet 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<p>ReplicaSet 也需要 <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status"><code>.spec</code></a>
部分。</p>
<!--
### Pod Template

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates) which is also
required to have labels in place. In our `frontend.yaml` example we had one label: `tier: frontend`.
Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.

For the template's [restart policy](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) field,
`.spec.template.spec.restartPolicy`, the only allowed value is `Always`, which is the default.
-->
<h3 id="pod-模版">Pod 模版</h3>
<p><code>.spec.template</code> 是一个<a href="/zh/docs/concepts/workloads/pods/#pod-templates">Pod 模版</a>，
要求设置标签。在 <code>frontend.yaml</code> 示例中，我们指定了标签 <code>tier: frontend</code>。
注意不要将标签与其他控制器的选择算符重叠，否则那些控制器会尝试收养此 Pod。</p>
<p>对于模板的<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">重启策略</a>
字段，<code>.spec.template.spec.restartPolicy</code>，唯一允许的取值是 <code>Always</code>，这也是默认值.</p>
<!--
### Pod Selector

The `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/). As discussed
[earlier](#how-a-replicaset-works) these are the labels used to identify potential Pods to acquire. In our
`frontend.yaml` example, the selector was:

```yaml
matchLabels:
  tier: frontend
```

In the ReplicaSet, `.spec.template.metadata.labels` must match `spec.selector`, or it will
be rejected by the API.
-->
<h3 id="pod-selector">Pod 选择算符  </h3>
<p><code>.spec.selector</code> 字段是一个<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签选择算符</a>。
如前文中<a href="#how-a-replicaset-works">所讨论的</a>，这些是用来标识要被获取的 Pods
的标签。在签名的 <code>frontend.yaml</code> 示例中，选择算符为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></code></pre></div><p>在 ReplicaSet 中，<code>.spec.template.metadata.labels</code> 的值必须与 <code>spec.selector</code> 值
相匹配，否则该配置会被 API 拒绝。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
For 2 ReplicaSets specifying the same `.spec.selector` but different `.spec.template.metadata.labels` and `.spec.template.spec` fields, each ReplicaSet ignores the Pods created by the other ReplicaSet.
-->
<p>对于设置了相同的 <code>.spec.selector</code>，但
<code>.spec.template.metadata.labels</code> 和 <code>.spec.template.spec</code> 字段不同的
两个 ReplicaSet 而言，每个 ReplicaSet 都会忽略被另一个 ReplicaSet 所
创建的 Pods。
</div>
<!--
### Replicas

You can specify how many Pods should run concurrently by setting `.spec.replicas`. The ReplicaSet will create/delete
its Pods to match this number.

If you do not specify `.spec.replicas`, then it defaults to 1.
-->
<h3 id="replicas">Replicas</h3>
<p>你可以通过设置 <code>.spec.replicas</code> 来指定要同时运行的 Pod 个数。
ReplicaSet 创建、删除 Pods 以与此值匹配。</p>
<p>如果你没有指定 <code>.spec.replicas</code>, 那么默认值为 1。</p>
<!--
## Working with ReplicaSets

### Deleting a ReplicaSet and its Pods

To delete a ReplicaSet and all of its Pods, use [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete). The [Garbage collector](/docs/concepts/workloads/controllers/garbage-collection/) automatically deletes all of the dependent Pods by default.

When using the REST API or the `client-go` library, you must set `propagationPolicy` to `Background` or `Foreground` in delete option. e.g. :
-->
<h2 id="使用-replicasets">使用 ReplicaSets</h2>
<h3 id="删除-replicaset-和它的-pod">删除 ReplicaSet 和它的 Pod</h3>
<p>要删除 ReplicaSet 和它的所有 Pod，使用
<a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a> 命令。
默认情况下，<a href="/zh/docs/concepts/workloads/controllers/garbage-collection/">垃圾收集器</a>
自动删除所有依赖的 Pod。</p>
<p>当使用 REST API 或 <code>client-go</code> 库时，你必须在删除选项中将 <code>propagationPolicy</code>
设置为 <code>Background</code> 或 <code>Foreground</code>。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
curl -X DELETE  <span style="color:#b44">&#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   -d <span style="color:#b44">&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   -H <span style="color:#b44">&#34;Content-Type: application/json&#34;</span>
</code></pre></div><!--
### Deleting just a ReplicaSet

You can delete a ReplicaSet without affecting any of its Pods using [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete) with the `-cascade=orphan` option.
When using the REST API or the `client-go` library, you must set `propagationPolicy` to `Orphan`.
For example:
-->
<h3 id="只删除-replicaset">只删除 ReplicaSet</h3>
<p>你可以只删除 ReplicaSet 而不影响它的 Pods，方法是使用
<a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>
命令并设置 <code>--cascade=orphan</code> 选项。</p>
<p>当使用 REST API 或 <code>client-go</code> 库时，你必须将 <code>propagationPolicy</code> 设置为 <code>Orphan</code>。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
curl -X DELETE  <span style="color:#b44">&#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -d <span style="color:#b44">&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -H <span style="color:#b44">&#34;Content-Type: application/json&#34;</span>
</code></pre></div><!--
Once the original is deleted, you can create a new ReplicaSet to replace it.  As long
as the old and new `.spec.selector` are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update Pods to a new spec in a controlled way, use a
[Deployment](/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), as ReplicaSets do not support a rolling update directly.
-->
<p>一旦删除了原来的 ReplicaSet，就可以创建一个新的来替换它。
由于新旧 ReplicaSet 的 <code>.spec.selector</code> 是相同的，新的 ReplicaSet 将接管老的 Pod。
但是，它不会努力使现有的 Pod 与新的、不同的 Pod 模板匹配。
若想要以可控的方式更新 Pod 的规约，可以使用
<a href="/zh/docs/concepts/workloads/controllers/deployment/#creating-a-deployment">Deployment</a>
资源，因为 ReplicaSet 并不直接支持滚动更新。</p>
<!--
### Isolating pods from a ReplicaSet

Pods may be removed from a ReplicaSet's target set by changing their labels. This technique may be used to remove pods 
from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (
  assuming that the number of replicas is not also changed).

-->
<h3 id="将-pod-从-replicaset-中隔离">将 Pod 从 ReplicaSet 中隔离</h3>
<p>可以通过改变标签来从 ReplicaSet 的目标集中移除 Pod。
这种技术可以用来从服务中去除 Pod，以便进行排错、数据恢复等。
以这种方式移除的 Pod 将被自动替换（假设副本的数量没有改变）。</p>
<!--
### Scaling a ReplicaSet

A ReplicaSet can be easily scaled up or down by simply updating the `.spec.replicas` field. The ReplicaSet controller
ensures that a desired number of pods with a matching label selector are available and operational.
-->
<h3 id="缩放-repliaset">缩放 RepliaSet</h3>
<p>通过更新 <code>.spec.replicas</code> 字段，ReplicaSet 可以被轻松的进行缩放。ReplicaSet
控制器能确保匹配标签选择器的数量的 Pod 是可用的和可操作的。</p>
<!--
When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to
prioritize scaling down pods based on the following general algorithm:
-->
<p>在降低集合规模时，ReplicaSet 控制器通过对可用的 Pods 进行排序来优先选择
要被删除的 Pods。其一般性算法如下：</p>
<!--
 1. Pending (and unschedulable) pods are scaled down first
 2. If `controller.kubernetes.io/pod-deletion-cost` annotation is set, then
    the pod with the lower value will come first.
 3. Pods on nodes with more replicas come before pods on nodes with fewer replicas.
 4. If the pods' creation times differ, the pod that was created more recently
    comes before the older pod (the creation times are bucketed on an integer log scale
    when the `LogarithmicScaleDown` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled)
-->
<ol>
<li>首先选择剔除悬决（Pending，且不可调度）的 Pods</li>
<li>如果设置了 <code>controller.kubernetes.io/pod-deletion-cost</code> 注解，则注解值
较小的优先被裁减掉</li>
<li>所处节点上副本个数较多的 Pod 优先于所处节点上副本较少者</li>
<li>如果 Pod 的创建时间不同，最近创建的 Pod 优先于早前创建的 Pod 被裁减。
（当 <code>LogarithmicScaleDown</code> 这一
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
被启用时，创建时间是按整数幂级来分组的）。</li>
</ol>
<p>如果以上比较结果都相同，则随机选择。</p>
<!--
### Pod deletion cost 
-->
<h3 id="pod-deletion-cost">Pod 删除开销  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
Using the [`controller.kubernetes.io/pod-deletion-cost`](/docs/reference/labels-annotations-taints/#pod-deletion-cost) 
annotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.
-->
<p>通过使用 <a href="/zh/docs/reference/labels-annotations-taints/#pod-deletion-cost"><code>controller.kubernetes.io/pod-deletion-cost</code></a>
注解，用户可以对 ReplicaSet 缩容时要先删除哪些 Pods 设置偏好。</p>
<!--
The annotation should be set on the pod, the range is [-2147483647, 2147483647]. It represents the cost of
deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion
cost are preferred to be deleted before pods with higher deletion cost. 
-->
<p>此注解要设置到 Pod 上，取值范围为 [-2147483647, 2147483647]。
所代表的的是删除同一 ReplicaSet 中其他 Pod 相比较而言的开销。
删除开销较小的 Pods 比删除开销较高的 Pods 更容易被删除。</p>
<!--
The implicit value for this annotation for pods that don't set it is 0; negative values are permitted.
Invalid values will be rejected by the API server.
-->
<p>Pods 如果未设置此注解，则隐含的设置值为 0。负值也是可接受的。
如果注解值非法，API 服务器会拒绝对应的 Pod。</p>
<!--
This feature is beta and enabled by default. You can disable it using the
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`PodDeletionCost` in both kube-apiserver and kube-controller-manager.
-->
<p>此功能特性处于 Beta 阶段，默认被禁用。你可以通过为 kube-apiserver 和
kube-controller-manager 设置
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>PodDeletionCost</code> 来启用此功能。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
- This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.
- Users should avoid updating the annotation frequently, such as updating it based on a metric value,
  because doing so will generate a significant number of pod updates on the apiserver.
-->
<ul>
<li>此机制实施时仅是尽力而为，并不能对 Pod 的删除顺序作出任何保证；</li>
<li>用户应避免频繁更新注解值，例如根据某观测度量值来更新此注解值是应该避免的。
这样做会在 API 服务器上产生大量的 Pod 更新操作。</li>
</ul>

</div>
<!--
#### Example Use Case

The different pods of an application could have different utilization levels. On scale down, the application 
may prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application
should update `controller.kubernetes.io/pod-deletion-cost` once before issuing a scale down (setting the 
annotation to a value proportional to pod utilization level). This works if the application itself controls
the down scaling; for example, the driver pod of a Spark deployment.
-->
<h4 id="使用场景示例">使用场景示例</h4>
<p>同一应用的不同 Pods 可能其利用率是不同的。在对应用执行缩容操作时，可能
希望移除利用率较低的 Pods。为了避免频繁更新 Pods，应用应该在执行缩容
操作之前更新一次 <code>controller.kubernetes.io/pod-deletion-cost</code> 注解值
（将注解值设置为一个与其 Pod 利用率对应的值）。
如果应用自身控制器缩容操作时（例如 Spark 部署的驱动 Pod），这种机制
是可以起作用的。</p>
<!--
### ReplicaSet as an Horizontal Pod Autoscaler Target

A ReplicaSet can also be a target for
[Horizontal Pod Autoscalers (HPA)](/docs/tasks/run-application/horizontal-pod-autoscale/). That is,
a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting
the ReplicaSet we created in the previous example.
-->
<h3 id="replicaset-作为水平的-pod-自动缩放器目标">ReplicaSet 作为水平的 Pod 自动缩放器目标</h3>
<p>ReplicaSet 也可以作为
<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale/">水平的 Pod 缩放器 (HPA)</a>
的目标。也就是说，ReplicaSet 可以被 HPA 自动缩放。
以下是 HPA 以我们在前一个示例中创建的副本集为目标的示例。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/hpa-rs.yaml" download="controllers/hpa-rs.yaml"><code>controllers/hpa-rs.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-hpa-rs-yaml')" title="Copy controllers/hpa-rs.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-hpa-rs-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>autoscaling/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>HorizontalPodAutoscaler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend-scaler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleTargetRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">maxReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">targetCPUUtilizationPercentage</span>:<span style="color:#bbb"> </span><span style="color:#666">50</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Saving this manifest into `hpa-rs.yaml` and submitting it to a Kubernetes cluster should
create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage
of the replicated pods.
-->
<p>将这个列表保存到 <code>hpa-rs.yaml</code> 并提交到 Kubernetes 集群，就能创建它所定义的
HPA，进而就能根据复制的 Pod 的 CPU 利用率对目标 ReplicaSet进行自动缩放。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
</code></pre></div><!--
Alternatively, you can use the `kubectl autoscale` command to accomplish the same
(and it's easier!)
-->
<p>或者，可以使用 <code>kubectl autoscale</code> 命令完成相同的操作。 (而且它更简单！)</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl autoscale rs frontend --max<span style="color:#666">=</span><span style="color:#666">10</span> --min<span style="color:#666">=</span><span style="color:#666">3</span> --cpu-percent<span style="color:#666">=</span><span style="color:#666">50</span>
</code></pre></div><!--
## Alternatives to ReplicaSet

### Deployment (Recommended)

[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is an object which can own ReplicaSets and update
them and their Pods via declarative, server-side rolling updates.
While ReplicaSets can be used independently, today they're  mainly used by Deployments as a mechanism to orchestrate Pod
creation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that
they create. Deployments own and manage their ReplicaSets.
As such, it is recommended to use Deployments when you want ReplicaSets.
-->
<h2 id="replicaset-的替代方案">ReplicaSet 的替代方案</h2>
<h3 id="deployment-推荐">Deployment （推荐）</h3>
<p><a href="/zh/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> 是一个
可以拥有 ReplicaSet 并使用声明式方式在服务器端完成对 Pods 滚动更新的对象。
尽管 ReplicaSet 可以独立使用，目前它们的主要用途是提供给 Deployment 作为
编排 Pod 创建、删除和更新的一种机制。当使用 Deployment 时，你不必关心
如何管理它所创建的 ReplicaSet，Deployment 拥有并管理其 ReplicaSet。
因此，建议你在需要 ReplicaSet 时使用 Deployment。</p>
<!--
### Bare Pods

Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your application requires only a single Pod. Think of it similarly to a process supervisor, only it supervises multiple Pods across multiple nodes instead of individual processes on a single node. A ReplicaSet delegates local container restarts to some agent on the node (for example, Kubelet or Docker).
-->
<h3 id="裸-pod">裸 Pod</h3>
<p>与用户直接创建 Pod 的情况不同，ReplicaSet 会替换那些由于某些原因被删除或被终止的
Pod，例如在节点故障或破坏性的节点维护（如内核升级）的情况下。
因为这个原因，我们建议你使用 ReplicaSet，即使应用程序只需要一个 Pod。
想像一下，ReplicaSet 类似于进程监视器，只不过它在多个节点上监视多个 Pod，
而不是在单个节点上监视单个进程。
ReplicaSet 将本地容器重启的任务委托给了节点上的某个代理（例如，Kubelet 或 Docker）去完成。</p>
<!--
### Job

Use a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicaSet for Pods that are expected to terminate on their own
(that is, batch jobs).
-->
<h3 id="job">Job</h3>
<p>使用<a href="/zh/docs/concepts/workloads/controllers/job/"><code>Job</code></a> 代替ReplicaSet，
可以用于那些期望自行终止的 Pod。</p>
<!--
### DaemonSet

Use a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicaSet for pods that provide a
machine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.
-->
<h3 id="daemonset">DaemonSet</h3>
<p>对于管理那些提供主机级别功能（如主机监控和主机日志）的容器，
就要用 <a href="/zh/docs/concepts/workloads/controllers/daemonset/"><code>DaemonSet</code></a>
而不用 ReplicaSet。
这些 Pod 的寿命与主机寿命有关：这些 Pod 需要先于主机上的其他 Pod 运行，
并且在机器准备重新启动/关闭时安全地终止。</p>
<h3 id="replicationcontroller">ReplicationController</h3>
<!--
ReplicaSets are the successors to [_ReplicationControllers_](/docs/concepts/workloads/controllers/replicationcontroller/).
The two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based
selector requirements as described in the [labels user guide](/docs/concepts/overview/working-with-objects/labels/#label-selectors).
As such, ReplicaSets are preferred over ReplicationControllers
-->
<p>ReplicaSet 是 <a href="/zh/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>
的后继者。二者目的相同且行为类似，只是 ReplicationController 不支持
<a href="/zh/docs/concepts/overview/working-with-objects/labels/#label-selectors">标签用户指南</a>
中讨论的基于集合的选择算符需求。
因此，相比于 ReplicationController，应优先考虑 ReplicaSet。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods).
* Learn about [Deployments](/docs/concepts/workloads/controllers/deployment/).
* [Run a Stateless Application Using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/),
  which relies on ReplicaSets to work.
* `ReplicaSet` is a top-level resource in the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for replica sets.
* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how
  you can use it to manage application availability during disruptions.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。</li>
<li>了解 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployments</a>。</li>
<li><a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">使用 Deployment 运行一个无状态应用</a>，它依赖于 ReplicaSet。</li>
<li><code>ReplicaSet</code> 是 Kubernetes REST API 中的顶级资源。阅读 





<a href=""></a>
对象定义理解关于该资源的 API。</li>
<li>阅读<a href="/zh/docs/concepts/workloads/pods/disruptions/">Pod 干扰预算（Disruption Budget）</a>，了解如何在干扰下运行高度可用的应用。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-6d72299952c37ca8cc61b416e5bdbcd4">2.3 - StatefulSets</h1>
    
	<!--
title: StatefulSets
content_type: concept
weight: 40
-->
<!-- overview -->
<!--
StatefulSet is the workload API object used to manage stateful applications.
-->
<p>StatefulSet 是用来管理有状态应用的工作负载 API 对象。</p>
<!--
---
title: StatefulSet
id: statefulset
date: 2018-04-12
full_link: /docs/concepts/workloads/controllers/statefulset/
short_description: >
  Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.

aka: 
tags:
- fundamental
- core-object
- workload
- storage
---
-->
<!--
 Manages the deployment and scaling of a set of <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>, *and provides guarantees about the ordering and uniqueness* of these Pods.
-->
<p>StatefulSet 用来管理某 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 集合的部署和扩缩，
并为这些 Pod 提供持久存储和持久标识符。</p>
<!--
Like a <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable&#58; each has a persistent identifier that it maintains across any rescheduling.
-->
<p>和 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a> 类似，
StatefulSet 管理基于相同容器规约的一组 Pod。但和 Deployment 不同的是，
StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID。这些 Pod 是基于相同的规约来创建的，
但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。</p>
<!--
If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.
-->
<p>如果希望使用存储卷为工作负载提供持久存储，可以使用 StatefulSet 作为解决方案的一部分。
尽管 StatefulSet 中的单个 Pod 仍可能出现故障，
但持久的 Pod 标识符使得将现有卷与替换已失败 Pod 的新 Pod 相匹配变得更加容易。</p>
<!-- body -->
<!--
## Using StatefulSets

StatefulSets are valuable for applications that require one or more of the
following.
-->
<h2 id="使用-statefulsets">使用 StatefulSets</h2>
<p>StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值：</p>
<!--
* Stable, unique network identifiers.
* Stable, persistent storage.
* Ordered, graceful deployment and scaling.
* Ordered, automated rolling updates.
-->
<ul>
<li>稳定的、唯一的网络标识符。</li>
<li>稳定的、持久的存储。</li>
<li>有序的、优雅的部署和缩放。</li>
<li>有序的、自动的滚动更新。</li>
</ul>
<!--
In the above, stable is synonymous with persistence across Pod (re)scheduling.
If an application doesn't require any stable identifiers or ordered deployment,
deletion, or scaling, you should deploy your application using a workload object
that provides a set of stateless replicas.
[Deployment](/docs/concepts/workloads/controllers/deployment/) or
[ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) may be better suited to your stateless needs.
-->
<p>在上面描述中，“稳定的”意味着 Pod 调度或重调度的整个过程是有持久性的。
如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用
由一组无状态的副本控制器提供的工作负载来部署应用程序，比如
<a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a> 或者
<a href="/zh/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>
可能更适用于你的无状态应用部署需要。</p>
<!--
## Limitations
-->
<h2 id="limitations">限制 </h2>
<!--
* The storage for a given Pod must either be provisioned by a [PersistentVolume Provisioner](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md) based on the requested `storage class`, or pre-provisioned by an admin.
* Deleting and/or scaling a StatefulSet down will *not* delete the volumes associated with the StatefulSet. This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources.
* StatefulSets currently require a [Headless Service](/docs/concepts/services-networking/service/#headless-services) to be responsible for the network identity of the Pods. You are responsible for creating this Service.
* StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.
* When using [Rolling Updates](#rolling-updates) with the default
  [Pod Management Policy](#pod-management-policies) (`OrderedReady`),
  it's possible to get into a broken state that requires
  [manual intervention to repair](#forced-rollback).
-->
<ul>
<li>给定 Pod 的存储必须由
<a href="https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md">PersistentVolume 驱动</a>
基于所请求的 <code>storage class</code> 来提供，或者由管理员预先提供。</li>
<li>删除或者收缩 StatefulSet 并<em>不会</em>删除它关联的存储卷。
这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。</li>
<li>StatefulSet 当前需要<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>
来负责 Pod 的网络标识。你需要负责创建此服务。</li>
<li>当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。
为了实现 StatefulSet 中的 Pod 可以有序地且体面地终止，可以在删除之前将 StatefulSet
缩放为 0。</li>
<li>在默认 <a href="#pod-management-policies">Pod 管理策略</a>(<code>OrderedReady</code>) 时使用
<a href="#rolling-updates">滚动更新</a>，可能进入需要<a href="#forced-rollback">人工干预</a>
才能修复的损坏状态。</li>
</ul>
<!--
## Components
The example below demonstrates the components of a StatefulSet.
-->
<h2 id="components">组件 </h2>
<p>下面的示例演示了 StatefulSet 的组件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># has to match .spec.template.metadata.labels</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;nginx&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># by default is 1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># has to match .spec.selector.matchLabels</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/nginx-slim:0.8<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;my-storage-class&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></code></pre></div><!--
In the above example:

* A Headless Service, named `nginx`, is used to control the network domain.
* The StatefulSet, named `web`, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.
* The `volumeClaimTemplates` will provide stable storage using [PersistentVolumes](/docs/concepts/storage/persistent-volumes/) provisioned by a PersistentVolume Provisioner.

The name of a StatefulSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

-->
<p>上述例子中：</p>
<ul>
<li>名为 <code>nginx</code> 的 Headless Service 用来控制网络域名。</li>
<li>名为 <code>web</code> 的 StatefulSet 有一个 Spec，它表明将在独立的 3 个 Pod 副本中启动 nginx 容器。</li>
<li><code>volumeClaimTemplates</code> 将通过 PersistentVolumes 驱动提供的
<a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a> 来提供稳定的存储。</li>
</ul>
<p>StatefulSet 的命名需要遵循<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>规范。</p>
<!--
## Pod Selector
-->
<h2 id="pod-selector">Pod 选择算符    </h2>
<!--
You must set the `.spec.selector` field of a StatefulSet to match the labels of its `.spec.template.metadata.labels`. Prior to Kubernetes 1.8, the `.spec.selector` field was defaulted when omitted. In 1.8 and later versions, failing to specify a matching Pod Selector will result in a validation error during StatefulSet creation.
-->
<p>你必须设置 StatefulSet 的 <code>.spec.selector</code> 字段，使之匹配其在
<code>.spec.template.metadata.labels</code> 中设置的标签。在 Kubernetes 1.8 版本之前，
被忽略 <code>.spec.selector</code> 字段会获得默认设置值。
在 1.8 和以后的版本中，未指定匹配的 Pod 选择器将在创建 StatefulSet 期间导致验证错误。</p>
<!--
## Pod Identity

StatefulSet Pods have a unique identity that is comprised of an ordinal, a
stable network identity, and stable storage. The identity sticks to the Pod,
regardless of which node it's (re)scheduled on.
-->
<h2 id="pod-identity">Pod 标识  </h2>
<p>StatefulSet Pod 具有唯一的标识，该标识包括顺序标识、稳定的网络标识和稳定的存储。
该标识和 Pod 是绑定的，不管它被调度在哪个节点上。</p>
<!--
### Ordinal Index

For a StatefulSet with N replicas, each Pod in the StatefulSet will be
assigned an integer ordinal, from 0 up through N-1, that is unique over the Set.
-->
<h3 id="ordinal-index">有序索引  </h3>
<p>对于具有 N 个副本的 StatefulSet，StatefulSet 中的每个 Pod 将被分配一个整数序号，
从 0 到 N-1，该序号在 StatefulSet 上是唯一的。</p>
<!--
### Stable Network ID

Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet
and the ordinal of the Pod. The pattern for the constructed hostname
is `$(statefulset name)-$(ordinal)`. The example above will create three Pods
named `web-0,web-1,web-2`.
A StatefulSet can use a [Headless Service](/docs/concepts/services-networking/service/#headless-services)
to control the domain of its Pods. The domain managed by this Service takes the form:
`$(service name).$(namespace).svc.cluster.local`, where "cluster.local" is the
cluster domain.
As each Pod is created, it gets a matching DNS subdomain, taking the form:
`$(podname).$(governing service domain)`, where the governing service is defined
by the `serviceName` field on the StatefulSet.
-->
<h3 id="stable-network-id">稳定的网络 ID  </h3>
<p>StatefulSet 中的每个 Pod 根据 StatefulSet 的名称和 Pod 的序号派生出它的主机名。
组合主机名的格式为<code>$(StatefulSet 名称)-$(序号)</code>。
上例将会创建三个名称分别为 <code>web-0、web-1、web-2</code> 的 Pod。
StatefulSet 可以使用 <a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>
控制它的 Pod 的网络域。管理域的这个服务的格式为：
<code>$(服务名称).$(命名空间).svc.cluster.local</code>，其中 <code>cluster.local</code> 是集群域。
一旦每个 Pod 创建成功，就会得到一个匹配的 DNS 子域，格式为：
<code>$(pod 名称).$(所属服务的 DNS 域名)</code>，其中所属服务由 StatefulSet 的 <code>serviceName</code> 域来设定。</p>
<!--
Depending on how DNS is configured in your cluster, you may not be able to look up the DNS
name for a newly-run Pod immediately. This behavior can occur when other clients in the
cluster have already sent queries for the hostname of the Pod before it was created.
Negative caching (normal in DNS) means that the results of previous failed lookups are
remembered and reused, even after the Pod is running, for at least a few seconds.

If you need to discover Pods promptly after they are created, you have a few options:

- Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.
- Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the config map for CoreDNS, which currently caches for 30 seconds).


As mentioned in the [limitations](#limitations) section, you are responsible for
creating the [Headless Service](/docs/concepts/services-networking/service/#headless-services)
responsible for the network identity of the pods.

-->
<p>取决于集群域内部 DNS 的配置，有可能无法查询一个刚刚启动的 Pod 的 DNS 命名。
当集群内其他客户端在 Pod 创建完成前发出 Pod 主机名查询时，就会发生这种情况。
负缓存 (在 DNS 中较为常见) 意味着之前失败的查询结果会被记录和重用至少若干秒钟，
即使 Pod 已经正常运行了也是如此。</p>
<p>如果需要在 Pod 被创建之后及时发现它们，有以下选项：</p>
<ul>
<li>直接查询 Kubernetes API（比如，利用 watch 机制）而不是依赖于 DNS 查询</li>
<li>缩短 Kubernetes DNS 驱动的缓存时长（通常这意味着修改 CoreDNS 的 ConfigMap，目前缓存时长为 30 秒）</li>
</ul>
<p>正如<a href="#limitations">限制</a>中所述，你需要负责创建<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>
以便为 Pod 提供网络标识。</p>
<!--
Here are some examples of choices for Cluster Domain, Service name,
StatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.


Cluster Domain | Service (ns/name) | StatefulSet (ns/name)  | StatefulSet Domain  | Pod DNS | Pod Hostname |
-------------- | ----------------- | ----------------- | -------------- | ------- | ------------ |
 cluster.local | default/nginx     | default/web       | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |
 cluster.local | foo/nginx         | foo/web           | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |
 kube.local    | foo/nginx         | foo/web           | nginx.foo.svc.kube.local        | web-{0..N-1}.nginx.foo.svc.kube.local        | web-{0..N-1} |

-->
<p>下面给出一些选择集群域、服务名、StatefulSet 名、及其怎样影响 StatefulSet 的 Pod 上的 DNS 名称的示例：</p>
<table>
<thead>
<tr>
<th>集群域名</th>
<th>服务（名字空间/名字）</th>
<th>StatefulSet（名字空间/名字）</th>
<th>StatefulSet 域名</th>
<th>Pod DNS</th>
<th>Pod 主机名</th>
</tr>
</thead>
<tbody>
<tr>
<td>cluster.local</td>
<td>default/nginx</td>
<td>default/web</td>
<td>nginx.default.svc.cluster.local</td>
<td>web-{0..N-1}.nginx.default.svc.cluster.local</td>
<td>web-{0..N-1}</td>
</tr>
<tr>
<td>cluster.local</td>
<td>foo/nginx</td>
<td>foo/web</td>
<td>nginx.foo.svc.cluster.local</td>
<td>web-{0..N-1}.nginx.foo.svc.cluster.local</td>
<td>web-{0..N-1}</td>
</tr>
<tr>
<td>kube.local</td>
<td>foo/nginx</td>
<td>foo/web</td>
<td>nginx.foo.svc.kube.local</td>
<td>web-{0..N-1}.nginx.foo.svc.kube.local</td>
<td>web-{0..N-1}</td>
</tr>
</tbody>
</table>
<!--
Cluster Domain will be set to `cluster.local` unless
[otherwise configured](/docs/concepts/services-networking/dns-pod-service/#how-it-works).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 集群域会被设置为 <code>cluster.local</code>，除非有<a href="/zh/docs/concepts/services-networking/dns-pod-service/">其他配置</a>。
</div>
<!--
### Stable Storage

For each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one PersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume with a StorageClass of `my-storage-class` and 1 Gib of provisioned storage. If no StorageClass
is specified, then the default StorageClass will be used. When a Pod is (re)scheduled
onto a node, its `volumeMounts` mount the PersistentVolumes associated with its
PersistentVolume Claims. Note that, the PersistentVolumes associated with the
Pods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.
This must be done manually.
-->
<h3 id="stable-storage">稳定的存储 </h3>
<p>对于 StatefulSet 中定义的每个 VolumeClaimTemplate，每个 Pod 接收到一个 PersistentVolumeClaim。在上面的 nginx 示例中，每个 Pod 将会得到基于 StorageClass <code>my-storage-class</code> 提供的
1 Gib 的 PersistentVolume。
如果没有声明 StorageClass，就会使用默认的 StorageClass。
当一个 Pod 被调度（重新调度）到节点上时，它的 <code>volumeMounts</code> 会挂载与其
PersistentVolumeClaims 相关联的 PersistentVolume。
请注意，当 Pod 或者 StatefulSet 被删除时，与 PersistentVolumeClaims 相关联的
PersistentVolume 并不会被删除。要删除它必须通过手动方式来完成。</p>
<!--
### Pod Name Label

When the StatefulSet <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> creates a Pod,
it adds a label, `statefulset.kubernetes.io/pod-name`, that is set to the name of
the Pod. This label allows you to attach a Service to a specific Pod in
the StatefulSet.
-->
<h3 id="pod-name-label">Pod 名称标签  </h3>
<p>当 StatefulSet <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> 创建 Pod 时，
它会添加一个标签 <code>statefulset.kubernetes.io/pod-name</code>，该标签值设置为 Pod 名称。
这个标签允许你给 StatefulSet 中的特定 Pod 绑定一个 Service。</p>
<!--
## Deployment and Scaling Guarantees

* For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.
* When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.
* Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.
* Before a Pod is terminated, all of its successors must be completely shutdown.
-->
<h2 id="deployment-and-scaling-guarantees">部署和扩缩保证  </h2>
<ul>
<li>对于包含 N 个 副本的 StatefulSet，当部署 Pod 时，它们是依次创建的，顺序为 <code>0..N-1</code>。</li>
<li>当删除 Pod 时，它们是逆序终止的，顺序为 <code>N-1..0</code>。</li>
<li>在将缩放操作应用到 Pod 之前，它前面的所有 Pod 必须是 Running 和 Ready 状态。</li>
<li>在 Pod 终止之前，所有的继任者必须完全关闭。</li>
</ul>
<!--
The StatefulSet should not specify a `pod.Spec.TerminationGracePeriodSeconds` of 0. This practice is unsafe and strongly discouraged. For further explanation, please refer to [force deleting StatefulSet Pods](/docs/tasks/run-application/force-delete-stateful-set-pod/).
-->
<p>StatefulSet 不应将 <code>pod.Spec.TerminationGracePeriodSeconds</code> 设置为 0。
这种做法是不安全的，要强烈阻止。更多的解释请参考
<a href="/zh/docs/tasks/run-application/force-delete-stateful-set-pod/">强制删除 StatefulSet Pod</a>。</p>
<!--
When the nginx example above is created, three Pods will be deployed in the order
web-0, web-1, web-2. web-1 will not be deployed before web-0 is
[Running and Ready](/docs/user-guide/pod-states/), and web-2 will not be deployed until
web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before
web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and
becomes Running and Ready.
-->
<p>在上面的 nginx 示例被创建后，会按照 web-0、web-1、web-2 的顺序部署三个 Pod。
在 web-0 进入 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">Running 和 Ready</a>
状态前不会部署 web-1。在 web-1 进入 Running 和 Ready 状态前不会部署 web-2。
如果 web-1 已经处于 Running 和 Ready 状态，而 web-2 尚未部署，在此期间发生了
web-0 运行失败，那么 web-2 将不会被部署，要等到 web-0 部署完成并进入 Running 和
Ready 状态后，才会部署 web-2。</p>
<!--
If a user were to scale the deployed example by patching the StatefulSet such that
`replicas=1`, web-2 would be terminated first. web-1 would not be terminated until web-2
is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and
is completely shutdown, but prior to web-1's termination, web-1 would not be terminated
until web-0 is Running and Ready.
-->
<p>如果用户想将示例中的 StatefulSet 收缩为 <code>replicas=1</code>，首先被终止的是 web-2。
在 web-2 没有被完全停止和删除前，web-1 不会被终止。
当 web-2 已被终止和删除、web-1 尚未被终止，如果在此期间发生 web-0 运行失败，
那么就不会终止 web-1，必须等到 web-0 进入 Running 和 Ready 状态后才会终止 web-1。</p>
<!--
### Pod Management Policies

In Kubernetes 1.7 and later, StatefulSet allows you to relax its ordering guarantees while
preserving its uniqueness and identity guarantees via its `.spec.podManagementPolicy` field.
-->
<h3 id="pod-management-policies">Pod 管理策略</h3>
<p>在 Kubernetes 1.7 及以后的版本中，StatefulSet 允许你放宽其排序保证，
同时通过它的 <code>.spec.podManagementPolicy</code> 域保持其唯一性和身份保证。</p>
<!--
#### OrderedReady Pod Management

`OrderedReady` pod management is the default for StatefulSets. It implements the behavior
described [above](#deployment-and-scaling-guarantees).
-->
<h4 id="orderedready-pod-管理">OrderedReady Pod 管理</h4>
<p><code>OrderedReady</code> Pod 管理是 StatefulSet 的默认设置。它实现了
<a href="#deployment-and-scaling-guarantees">上面</a>描述的功能。</p>
<!--
#### Parallel Pod Management

`Parallel` pod management tells the StatefulSet controller to launch or
terminate all Pods in parallel, and to not wait for Pods to become Running
and Ready or completely terminated prior to launching or terminating another
Pod. This option only affects the behavior for scaling operations. Updates are not affected.

-->
<h4 id="parallel-pod-management">并行 Pod 管理  </h4>
<p><code>Parallel</code> Pod 管理让 StatefulSet 控制器并行的启动或终止所有的 Pod，
启动或者终止其他 Pod 前，无需等待 Pod 进入 Running 和 ready 或者完全停止状态。
这个选项只会影响伸缩操作的行为，更新则不会被影响。</p>
<!--
## Update Strategies

A StatefulSet's `.spec.updateStrategy` field allows you to configure
and disable automated rolling updates for containers, labels, resource request/limits, and
annotations for the Pods in a StatefulSet. There are two possible values:
-->
<h2 id="update-strategies">更新策略 </h2>
<p>StatefulSet 的 <code>.spec.updateStrategy</code> 字段让
你可以配置和禁用掉自动滚动更新 Pod 的容器、标签、资源请求或限制、以及注解。
有两个允许的值：</p>
<!--
`OnDelete`
: When a StatefulSet's `.spec.updateStrategy.type` is set to `OnDelete`,
  the StatefulSet controller will not automatically update the Pods in a
  StatefulSet. Users must manually delete Pods to cause the controller to
  create new Pods that reflect modifications made to a StatefulSet's `.spec.template`.

`RollingUpdate`
: The `RollingUpdate` update strategy implements automated, rolling update for the Pods in a StatefulSet. This is the default update strategy.
-->
<dl>
<dt><code>OnDelete</code></dt>
<dd>当 StatefulSet 的 <code>.spec.updateStrategy.type</code> 设置为 <code>OnDelete</code> 时，
它的控制器将不会自动更新 StatefulSet 中的 Pod。
用户必须手动删除 Pod 以便让控制器创建新的 Pod，以此来对 StatefulSet 的
<code>.spec.template</code> 的变动作出反应。</dd>
<dt><code>RollingUpdate</code></dt>
<dd><code>RollingUpdate</code> 更新策略对 StatefulSet 中的 Pod 执行自动的滚动更新。这是默认的更新策略。</dd>
</dl>
<!--
## Rolling Updates

When a StatefulSet's `.spec.updateStrategy.type` is set to `RollingUpdate`, the
StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed
in the same order as Pod termination (from the largest ordinal to the smallest), updating
each Pod one at a time.

The Kubernetes control plane waits until an updated Pod is Running and Ready prior
to updating its predecessor. If you have set `.spec.minReadySeconds` (see [Minimum Ready Seconds](#minimum-ready-seconds)), the control plane additionally waits that amount of time after the Pod turns ready, before moving on.
-->
<h2 id="rolling-updates">滚动更新</h2>
<p>当 StatefulSet 的 <code>.spec.updateStrategy.type</code> 被设置为 <code>RollingUpdate</code> 时，
StatefulSet 控制器会删除和重建 StatefulSet 中的每个 Pod。
它将按照与 Pod 终止相同的顺序（从最大序号到最小序号）进行，每次更新一个 Pod。</p>
<p>Kubernetes 控制面会等到被更新的 Pod 进入 Running 和 Ready 状态，然后再更新其前身。
如果你设置了 <code>.spec.minReadySeconds</code>（查看<a href="#minimum-ready-seconds">最短就绪秒数</a>），控制面在 Pod 就绪后会额外等待一定的时间再执行下一步。</p>
<!--
### Partitioned rolling updates {#partitions}

The `RollingUpdate` update strategy can be partitioned, by specifying a
`.spec.updateStrategy.rollingUpdate.partition`. If a partition is specified, all Pods with an
ordinal that is greater than or equal to the partition will be updated when the StatefulSet's
`.spec.template` is updated. All Pods with an ordinal that is less than the partition will not
be updated, and, even if they are deleted, they will be recreated at the previous version. If a
StatefulSet's `.spec.updateStrategy.rollingUpdate.partition` is greater than its `.spec.replicas`,
updates to its `.spec.template` will not be propagated to its Pods.
In most cases you will not need to use a partition, but they are useful if you want to stage an
update, roll out a canary, or perform a phased roll out.
-->
<h3 id="partitions">分区滚动更新  </h3>
<p>通过声明 <code>.spec.updateStrategy.rollingUpdate.partition</code> 的方式，<code>RollingUpdate</code>
更新策略可以实现分区。
如果声明了一个分区，当 StatefulSet 的 <code>.spec.template</code> 被更新时，
所有序号大于等于该分区序号的 Pod 都会被更新。
所有序号小于该分区序号的 Pod 都不会被更新，并且，即使他们被删除也会依据之前的版本进行重建。
如果 StatefulSet 的 <code>.spec.updateStrategy.rollingUpdate.partition</code> 大于它的
<code>.spec.replicas</code>，对它的 <code>.spec.template</code> 的更新将不会传递到它的 Pod。
在大多数情况下，你不需要使用分区，但如果你希望进行阶段更新、执行金丝雀或执行
分阶段上线，则这些分区会非常有用。</p>
<!--
### Forced Rollback

When using [Rolling Updates](#rolling-updates) with the default
[Pod Management Policy](#pod-management-policies) (`OrderedReady`),
it's possible to get into a broken state that requires manual intervention to repair.

If you update the Pod template to a configuration that never becomes Running and
Ready (for example, due to a bad binary or application-level configuration error),
StatefulSet will stop the rollout and wait.
-->
<h3 id="forced-rollback">强制回滚</h3>
<p>在默认 <a href="#pod-management-policies">Pod 管理策略</a>(<code>OrderedReady</code>) 下使用
<a href="#rolling-updates">滚动更新</a> ，可能进入需要人工干预才能修复的损坏状态。</p>
<p>如果更新后 Pod 模板配置进入无法运行或就绪的状态（例如，由于错误的二进制文件
或应用程序级配置错误），StatefulSet 将停止回滚并等待。</p>
<!--
In this state, it's not enough to revert the Pod template to a good configuration.
Due to a [known issue](https://github.com/kubernetes/kubernetes/issues/67250),
StatefulSet will continue to wait for the broken Pod to become Ready
(which never happens) before it will attempt to revert it back to the working
configuration.

After reverting the template, you must also delete any Pods that StatefulSet had
already attempted to run with the bad configuration.
StatefulSet will then begin to recreate the Pods using the reverted template.
-->
<p>在这种状态下，仅将 Pod 模板还原为正确的配置是不够的。由于
<a href="https://github.com/kubernetes/kubernetes/issues/67250">已知问题</a>，StatefulSet
将继续等待损坏状态的 Pod 准备就绪（永远不会发生），然后再尝试将其恢复为正常工作配置。</p>
<p>恢复模板后，还必须删除 StatefulSet 尝试使用错误的配置来运行的 Pod。这样，
StatefulSet 才会开始使用被还原的模板来重新创建 Pod。</p>
<!--
### Minimum ready seconds

`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be ready without any of its containers crashing, for it to be considered available.
This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).

Please note that this field only works if you enable the `StatefulSetMinReadySeconds` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).
-->
<h3 id="minimum-ready-seconds">最短就绪秒数  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>


<p><code>.spec.minReadySeconds</code> 是一个可选字段，用于指定新创建的 Pod 就绪（没有任何容器崩溃）后被认为可用的最小秒数。
默认值是 0（Pod 就绪时就被认为可用）。要了解 Pod 何时被认为已就绪，请参阅<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">容器探针</a>。</p>
<p>请注意只有当你启用 <code>StatefulSetMinReadySeconds</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>时，该字段才会生效。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods).
* Find out how to use StatefulSets
  * Follow an example of [deploying a stateful application](/docs/tutorials/stateful-application/basic-stateful-set/).
  * Follow an example of [deploying Cassandra with Stateful Sets](/docs/tutorials/stateful-application/cassandra/).
  * Follow an example of [running a replicated stateful application](/docs/tasks/run-application/run-replicated-stateful-application/).
  * Learn how to [scale a StatefulSet](/docs/tasks/run-application/scale-stateful-set/).
  * Learn what's involved when you [delete a StatefulSet](/docs/tasks/run-application/delete-stateful-set/).
  * Learn how to [configure a Pod to use a volume for storage](/docs/tasks/configure-pod-container/configure-volume-storage/).
  * Learn how to [configure a Pod to use a PersistentVolume for storage](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).
* `StatefulSet` is a top-level resource in the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for stateful sets.
* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how
  you can use it to manage application availability during disruptions.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。</li>
<li>了解如何使用 StatefulSet
<ul>
<li>跟随示例<a href="/zh/docs/tutorials/stateful-application/basic-stateful-set/">部署有状态应用</a>。</li>
<li>跟随示例<a href="/zh/docs/tutorials/stateful-application/cassandra/">使用 StatefulSet 部署 Cassandra</a>。</li>
<li>跟随示例<a href="/zh/docs/tasks/run-application/run-replicated-stateful-application/">运行多副本的有状态应用程序</a>。</li>
<li>了解如何<a href="/zh/docs/tasks/run-application/scale-stateful-set/">扩缩 StatefulSet</a>。</li>
<li>了解<a href="/zh/docs/tasks/run-application/delete-stateful-set/">删除 StatefulSet</a>涉及到的操作。</li>
<li>了解如何<a href="/zh/docs/tasks/configure-pod-container/configure-volume-storage/">配置 Pod 以使用卷进行存储</a>。</li>
<li>了解如何<a href="/zh/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">配置 Pod 以使用 PersistentVolume 作为存储</a>。</li>
</ul>
</li>
<li><code>StatefulSet</code> 是 Kubernetes REST API 中的顶级资源。阅读 





<a href=""></a>
对象定义理解关于该资源的 API。</li>
<li>阅读<a href="/zh/docs/concepts/workloads/pods/disruptions/">Pod 干扰预算（Disruption Budget）</a>，了解如何在干扰下运行高度可用的应用。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-41600eb8b6631c88848156f381e9d588">2.4 - DaemonSet</h1>
    
	<!--
title: DaemonSet
content_type: concept
weight: 50
--->
<!-- overview -->
<!--
A _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the
cluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage
collected.  Deleting a DaemonSet will clean up the Pods it created.
--->
<p><em>DaemonSet</em> 确保全部（或者某些）节点上运行一个 Pod 的副本。
当有节点加入集群时， 也会为他们新增一个 Pod 。
当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。</p>
<!--
Some typical uses of a DaemonSet are:

- running a cluster storage daemon on every node
- running a logs collection daemon on every node
- running a node monitoring daemon on every node
-->
<p>DaemonSet 的一些典型用法：</p>
<ul>
<li>在每个节点上运行集群守护进程</li>
<li>在每个节点上运行日志收集守护进程</li>
<li>在每个节点上运行监控守护进程</li>
</ul>
<!--
In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.
A more complex setup might use multiple DaemonSets for a single type of daemon, but with
different flags and/or different memory and cpu requests for different hardware types.
-->
<p>一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个 DaemonSet。
一个稍微复杂的用法是为同一种守护进程部署多个 DaemonSet；每个具有不同的标志，
并且对不同硬件类型具有不同的内存、CPU 要求。</p>
<!-- body -->
<!--
## Writing a DaemonSet Spec

### Create a DaemonSet
-->
<h2 id="writing-a-daemon-set-spec">编写 DaemonSet Spec  </h2>
<h3 id="create-a-daemon-set">创建 DaemonSet  </h3>
<!--
You can describe a DaemonSet in a YAML file. For example, the `daemonset.yaml` file below
describes a DaemonSet that runs the fluentd-elasticsearch Docker image:
-->
<p>你可以在 YAML 文件中描述 DaemonSet。
例如，下面的 daemonset.yaml 文件描述了一个运行 fluentd-elasticsearch Docker 镜像的 DaemonSet：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/daemonset.yaml" download="controllers/daemonset.yaml"><code>controllers/daemonset.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-daemonset-yaml')" title="Copy controllers/daemonset.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-daemonset-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>fluentd-logging<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this toleration is to have the daemonset runnable on master nodes</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># remove it if your masters can&#39;t run pods</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>node-role.kubernetes.io/master<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>Exists<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>quay.io/fluentd_elasticsearch/fluentd:v2.5.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>100m<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlibdockercontainers<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/docker/containers<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlibdockercontainers<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/lib/docker/containers<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create a DaemonSet based on the YAML file:
-->
<p>基于 YAML 文件创建 DaemonSet：</p>
<pre><code>kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
</code></pre><!--
### Required Fields

As with all other Kubernetes config, a DaemonSet needs `apiVersion`, `kind`, and `metadata` fields.  For
general information about working with config files, see
[running stateless applications](/docs/tasks/run-application/run-stateless-application-deployment/)
and [object management using kubectl](/docs/concepts/overview/working-with-objects/object-management/).

The name of a DaemonSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A DaemonSet also needs a
[`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)
section.
-->
<h3 id="required-fields">必需字段  </h3>
<p>和所有其他 Kubernetes 配置一样，DaemonSet 需要 <code>apiVersion</code>、<code>kind</code> 和 <code>metadata</code> 字段。
有关配置文件的基本信息，参见
<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">部署应用</a>、
<a href="/zh/docs/tasks/">配置容器</a>和
<a href="/zh/docs/concepts/overview/working-with-objects/object-management/">使用 kubectl 进行对象管理</a>
文档。</p>
<p>DaemonSet 对象的名称必须是一个合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<p>DaemonSet 也需要一个 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code></a> 配置段。</p>
<!--
### Pod Template

The `.spec.template` is one of the required fields in `.spec`.

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates).
It has exactly the same schema as a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>,
except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate
labels (see [pod selector](#pod-selector)).

A Pod Template in a DaemonSet must have a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)
 equal to `Always`, or be unspecified, which defaults to `Always`.
-->
<h3 id="pod-template">Pod 模板  </h3>
<p><code>.spec</code> 中唯一必需的字段是 <code>.spec.template</code>。</p>
<p><code>.spec.template</code> 是一个 <a href="/zh/docs/concepts/workloads/pods/#pod-templates">Pod 模板</a>。
除了它是嵌套的，因而不具有 <code>apiVersion</code> 或 <code>kind</code> 字段之外，它与
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 具有相同的 schema。</p>
<p>除了 Pod 必需字段外，在 DaemonSet 中的 Pod 模板必须指定合理的标签（查看 <a href="#pod-selector">Pod 选择算符</a>）。</p>
<p>在 DaemonSet 中的 Pod 模板必须具有一个值为 <code>Always</code> 的
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>RestartPolicy</code></a>。
当该值未指定时，默认是 <code>Always</code>。</p>
<!--
### Pod Selector

The `.spec.selector` field is a pod selector.  It works the same as the `.spec.selector` of
a [Job](/docs/concepts/jobs/run-to-completion-finite-workloads/).

You must specify a pod selector that matches the labels of the
`.spec.template`.
Also, once a DaemonSet is created,
its `.spec.selector` can not be mutated. Mutating the pod selector can lead to the
unintentional orphaning of Pods, and it was found to be confusing to users.
-->
<h3 id="pod-selector">Pod 选择算符    </h3>
<p><code>.spec.selector</code> 字段表示 Pod 选择算符，它与
<a href="/zh/docs/concepts/workloads/controllers/job/">Job</a> 的 <code>.spec.selector</code> 的作用是相同的。</p>
<p>你必须指定与 <code>.spec.template</code> 的标签匹配的 Pod 选择算符。
此外，一旦创建了 DaemonSet，它的 <code>.spec.selector</code> 就不能修改。
修改 Pod 选择算符可能导致 Pod 意外悬浮，并且这对用户来说是费解的。</p>
<!--
The `.spec.selector` is an object consisting of two fields:
-->
<p><code>spec.selector</code> 是一个对象，如下两个字段组成：</p>
<!--
* `matchLabels` - works the same as the `.spec.selector` of a
  [ReplicationController](/docs/concepts/workloads/controllers/replicationcontroller/).
* `matchExpressions` - allows to build more sophisticated selectors by specifying key,
  list of values and an operator that relates the key and values.
-->
<ul>
<li><code>matchLabels</code> - 与 <a href="/zh/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>
的 <code>.spec.selector</code> 的作用相同。</li>
<li><code>matchExpressions</code> - 允许构建更加复杂的选择器，可以通过指定 key、value
列表以及将 key 和 value 列表关联起来的 operator。</li>
</ul>
<!--
When the two are specified the result is ANDed.
-->
<p>当上述两个字段都指定时，结果会按逻辑与（AND）操作处理。</p>
<!--
The `.spec.selector` must match the `.spec.template.metadata.labels`.
Config with these two not matching will be rejected by the API.
-->
<p><code>.spec.selector</code> 必须与 <code>.spec.template.metadata.labels</code> 相匹配。
如果配置中这两个字段不匹配，则会被 API 拒绝。</p>
<!--
### Running Pods on Only Some Nodes

If you specify a `.spec.template.spec.nodeSelector`, then the DaemonSet controller will
create Pods on nodes which match that [node selector](/docs/concepts/scheduling-eviction/assign-pod-node/).
Likewise if you specify a `.spec.template.spec.affinity`,
then DaemonSet controller will create Pods on nodes which match that
[node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/).
If you do not specify either, then the DaemonSet controller will create Pods on all nodes.
-->
<h3 id="running-pods-on-only-some-nodes">仅在某些节点上运行 Pod  </h3>
<p>如果指定了 <code>.spec.template.spec.nodeSelector</code>，DaemonSet 控制器将在能够与
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">Node 选择算符</a> 匹配的节点上创建 Pod。
类似这种情况，可以指定 <code>.spec.template.spec.affinity</code>，之后 DaemonSet 控制器
将在能够与<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">节点亲和性</a>
匹配的节点上创建 Pod。
如果根本就没有指定，则 DaemonSet Controller 将在所有节点上创建 Pod。</p>
<!--
## How Daemon Pods are Scheduled

### Scheduled by default scheduler
-->
<h2 id="how-daemon-pods-are-scheduled">Daemon Pods 是如何被调度的  </h2>
<h3 id="scheduled-by-default-scheduler">通过默认调度器调度  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.17 [stable]</code>
</div>


<!--
A DaemonSet ensures that all eligible nodes run a copy of a Pod. Normally, the
node that a Pod runs on is selected by the Kubernetes scheduler. However,
DaemonSet pods are created and scheduled by the DaemonSet controller instead.
That introduces the following issues:

* Inconsistent Pod behavior: Normal Pods waiting to be scheduled are created
  and in `Pending` state, but DaemonSet pods are not created in `Pending`
  state. This is confusing to the user.
* [Pod preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/)
  is handled by default scheduler. When preemption is enabled, the DaemonSet controller
  will make scheduling decisions without considering pod priority and preemption.
-->
<p>DaemonSet 确保所有符合条件的节点都运行该 Pod 的一个副本。
通常，运行 Pod 的节点由 Kubernetes 调度器选择。
不过，DaemonSet Pods 由 DaemonSet 控制器创建和调度。这就带来了以下问题：</p>
<ul>
<li>Pod 行为的不一致性：正常 Pod 在被创建后等待调度时处于 <code>Pending</code> 状态，
DaemonSet Pods 创建后不会处于 <code>Pending</code> 状态下。这使用户感到困惑。</li>
<li><a href="/zh/docs/concepts/configuration/pod-priority-preemption/">Pod 抢占</a>
由默认调度器处理。启用抢占后，DaemonSet 控制器将在不考虑 Pod 优先级和抢占
的情况下制定调度决策。</li>
</ul>
<!--
`ScheduleDaemonSetPods` allows you to schedule DaemonSets using the default
scheduler instead of the DaemonSet controller, by adding the `NodeAffinity` term
to the DaemonSet pods, instead of the `.spec.nodeName` term. The default
scheduler is then used to bind the pod to the target host. If node affinity of
the DaemonSet pod already exists, it is replaced (the original node affinity was
taken into account before selecting the target host). The DaemonSet controller only
performs these operations when creating or modifying DaemonSet pods, and no
changes are made to the `spec.template` of the DaemonSet.
-->
<p><code>ScheduleDaemonSetPods</code> 允许您使用默认调度器而不是 DaemonSet 控制器来调度 DaemonSets，
方法是将 <code>NodeAffinity</code> 条件而不是 <code>.spec.nodeName</code> 条件添加到 DaemonSet Pods。
默认调度器接下来将 Pod 绑定到目标主机。
如果 DaemonSet Pod 的节点亲和性配置已存在，则被替换
（原始的节点亲和性配置在选择目标主机之前被考虑）。
DaemonSet 控制器仅在创建或修改 DaemonSet Pod 时执行这些操作，
并且不会更改 DaemonSet 的 <code>spec.template</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">matchFields</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>metadata.name<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- target-host-name<span style="color:#bbb">
</span></code></pre></div><!--
In addition, `node.kubernetes.io/unschedulable:NoSchedule` toleration is added
automatically to DaemonSet Pods. The default scheduler ignores
`unschedulable` Nodes when scheduling DaemonSet Pods.
-->
<p>此外，系统会自动添加 <code>node.kubernetes.io/unschedulable：NoSchedule</code> 容忍度到
DaemonSet Pods。在调度 DaemonSet Pod 时，默认调度器会忽略 <code>unschedulable</code> 节点。</p>
<!--
### Taints and Tolerations

Although Daemon Pods respect
[taints and tolerations](/docs/concepts/configuration/taint-and-toleration),
the following tolerations are added to DaemonSet Pods automatically according to
the related features.
-->
<h3 id="taint-and-toleration">污点和容忍度  </h3>
<p>尽管 Daemon Pods 遵循<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration">污点和容忍度</a>
规则，根据相关特性，控制器会自动将以下容忍度添加到 DaemonSet Pod：</p>
<table>
<thead>
<tr>
<th>容忍度键名</th>
<th>效果</th>
<th>版本</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>node.kubernetes.io/not-ready</code></td>
<td>NoExecute</td>
<td>1.13+</td>
<td>当出现类似网络断开的情况导致节点问题时，DaemonSet Pod 不会被逐出。</td>
</tr>
<tr>
<td><code>node.kubernetes.io/unreachable</code></td>
<td>NoExecute</td>
<td>1.13+</td>
<td>当出现类似于网络断开的情况导致节点问题时，DaemonSet Pod 不会被逐出。</td>
</tr>
<tr>
<td><code>node.kubernetes.io/disk-pressure</code></td>
<td>NoSchedule</td>
<td>1.8+</td>
<td>DaemonSet Pod 被默认调度器调度时能够容忍磁盘压力属性。</td>
</tr>
<tr>
<td><code>node.kubernetes.io/memory-pressure</code></td>
<td>NoSchedule</td>
<td>1.8+</td>
<td>DaemonSet Pod 被默认调度器调度时能够容忍内存压力属性。</td>
</tr>
<tr>
<td><code>node.kubernetes.io/unschedulable</code></td>
<td>NoSchedule</td>
<td>1.12+</td>
<td>DaemonSet Pod 能够容忍默认调度器所设置的 <code>unschedulable</code> 属性.</td>
</tr>
<tr>
<td><code>node.kubernetes.io/network-unavailable</code></td>
<td>NoSchedule</td>
<td>1.12+</td>
<td>DaemonSet 在使用宿主网络时，能够容忍默认调度器所设置的 <code>network-unavailable</code> 属性。</td>
</tr>
</tbody>
</table>
<!--
## Communicating with Daemon Pods
-->
<!--
Some possible patterns for communicating with Pods in a DaemonSet are:

- **Push**: Pods in the DaemonSet are configured to send updates to another service, such
  as a stats database.  They do not have clients.
- **NodeIP and Known Port**: Pods in the DaemonSet can use a `hostPort`, so that the pods
  are reachable via the node IPs.
  Clients know the list of node IPs somehow, and know the port by convention.
- **DNS**: Create a [headless service](/docs/concepts/services-networking/service/#headless-services)
  with the same pod selector, and then discover DaemonSets using the `endpoints`
  resource or retrieve multiple A records from DNS.
- **Service**: Create a service with the same Pod selector, and use the service to reach a
  daemon on a random node. (No way to reach specific node.)
-->
<h2 id="communicating-with-daemon-pods">与 Daemon Pods 通信  </h2>
<p>与 DaemonSet 中的 Pod 进行通信的几种可能模式如下：</p>
<ul>
<li>
<p><strong>推送（Push）</strong>：配置 DaemonSet 中的 Pod，将更新发送到另一个服务，例如统计数据库。
这些服务没有客户端。</p>
</li>
<li>
<p><strong>NodeIP 和已知端口</strong>：DaemonSet 中的 Pod 可以使用 <code>hostPort</code>，从而可以通过节点 IP
访问到 Pod。客户端能通过某种方法获取节点 IP 列表，并且基于此也可以获取到相应的端口。</p>
</li>
<li>
<p><strong>DNS</strong>：创建具有相同 Pod 选择算符的
<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>，
通过使用 <code>endpoints</code> 资源或从 DNS 中检索到多个 A 记录来发现 DaemonSet。</p>
</li>
<li>
<p><strong>Service</strong>：创建具有相同 Pod 选择算符的服务，并使用该服务随机访问到某个节点上的
守护进程（没有办法访问到特定节点）。</p>
</li>
</ul>
<!--
## Updating a DaemonSet

If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete
Pods from newly not-matching nodes.

You can modify the Pods that a DaemonSet creates.  However, Pods do not allow all
fields to be updated.  Also, the DaemonSet controller will use the original template the next
time a node (even with the same name) is created.
-->
<h2 id="updating-a-daemon-set">更新 DaemonSet  </h2>
<p>如果节点的标签被修改，DaemonSet 将立刻向新匹配上的节点添加 Pod，
同时删除不匹配的节点上的 Pod。</p>
<p>你可以修改 DaemonSet 创建的 Pod。不过并非 Pod 的所有字段都可更新。
下次当某节点（即使具有相同的名称）被创建时，DaemonSet 控制器还会使用最初的模板。</p>
<!--
You can delete a DaemonSet.  If you specify `--cascade=orphan` with `kubectl`, then the Pods
will be left on the nodes.  If you subsequently create a new DaemonSet with the same selector,
the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces
them according to its `updateStrategy`.

You can [perform a rolling update](/docs/tasks/manage-daemon/update-daemon-set/) on a DaemonSet.
-->
<p>您可以删除一个 DaemonSet。如果使用 <code>kubectl</code> 并指定 <code>--cascade=orphan</code> 选项，
则 Pod 将被保留在节点上。接下来如果创建使用相同选择算符的新 DaemonSet，
新的 DaemonSet 会收养已有的 Pod。
如果有 Pod 需要被替换，DaemonSet 会根据其 <code>updateStrategy</code> 来替换。</p>
<p>你可以对 DaemonSet <a href="/zh/docs/tasks/manage-daemon/update-daemon-set/">执行滚动更新</a>操作。</p>
<!--
## Alternatives to DaemonSet

### Init Scripts
-->
<h2 id="alternatives-to-daemon-set">DaemonSet 的替代方案  </h2>
<h3 id="init-scripts">init 脚本  </h3>
<!--
It is certainly possible to run daemon processes by directly starting them on a node (e.g. using
`init`, `upstartd`, or `systemd`).  This is perfectly fine.  However, there are several advantages to
running such processes via a DaemonSet:

- Ability to monitor and manage logs for daemons in the same way as applications.
- Same config language and tools (e.g. Pod templates, `kubectl`) for daemons and applications.
- Running daemons in containers with resource limits increases isolation between daemons from app
  containers.  However, this can also be accomplished by running the daemons in a container but not in a Pod
  (e.g. start directly via Docker).
-->
<p>直接在节点上启动守护进程（例如使用 <code>init</code>、<code>upstartd</code> 或 <code>systemd</code>）的做法当然是可行的。
不过，基于 DaemonSet 来运行这些进程有如下一些好处：</p>
<ul>
<li>
<p>像所运行的其他应用一样，DaemonSet 具备为守护进程提供监控和日志管理的能力。</p>
</li>
<li>
<p>为守护进程和应用所使用的配置语言和工具（如 Pod 模板、<code>kubectl</code>）是相同的。</p>
</li>
<li>
<p>在资源受限的容器中运行守护进程能够增加守护进程和应用容器的隔离性。
然而，这一点也可以通过在容器中运行守护进程但却不在 Pod 中运行之来实现。
例如，直接基于 Docker 启动。</p>
</li>
</ul>
<!--
### Bare Pods

It is possible to create Pods directly which specify a particular node to run on.  However,
a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of
node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should
use a DaemonSet rather than creating individual Pods.
-->
<h3 id="bare-pods">裸 Pod  </h3>
<p>直接创建 Pod并指定其运行在特定的节点上也是可以的。
然而，DaemonSet 能够替换由于任何原因（例如节点失败、例行节点维护、内核升级）
而被删除或终止的 Pod。
由于这个原因，你应该使用 DaemonSet 而不是单独创建 Pod。</p>
<!--
### Static Pods

It is possible to create Pods by writing a file to a certain directory watched by Kubelet.  These
are called [static pods](/docs/tasks/configure-pod-container/static-pod/).
Unlike DaemonSet, static Pods cannot be managed with kubectl
or other Kubernetes API clients.  Static Pods do not depend on the apiserver, making them useful
in cluster bootstrapping cases.  Also, static Pods may be deprecated in the future.
-->
<h3 id="static-pods">静态 Pod  </h3>
<p>通过在一个指定的、受 <code>kubelet</code> 监视的目录下编写文件来创建 Pod 也是可行的。
这类 Pod 被称为<a href="/zh/docs/tasks/configure-pod-container/static-pod/">静态 Pod</a>。
不像 DaemonSet，静态 Pod 不受 <code>kubectl</code> 和其它 Kubernetes API 客户端管理。
静态 Pod 不依赖于 API 服务器，这使得它们在启动引导新集群的情况下非常有用。
此外，静态 Pod 在将来可能会被废弃。</p>
<!--
### Deployments

DaemonSets are similar to [Deployments](/docs/concepts/workloads/controllers/deployment/) in that
they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,
storage servers).

Use a Deployment for stateless services, like frontends, where scaling up and down the
number of replicas and rolling out updates are more important than controlling exactly which host
the Pod runs on.  Use a DaemonSet when it is important that a copy of a Pod always run on
all or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.

For example, [network plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.
-->
<h3 id="deployments">Deployments</h3>
<p>DaemonSet 与 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployments</a> 非常类似，
它们都能创建 Pod，并且 Pod 中的进程都不希望被终止（例如，Web 服务器、存储服务器）。</p>
<p>建议为无状态的服务使用 Deployments，比如前端服务。
对这些服务而言，对副本的数量进行扩缩容、平滑升级，比精确控制 Pod 运行在某个主机上要重要得多。
当需要 Pod 副本总是运行在全部或特定主机上，并且当该 DaemonSet 提供了节点级别的功能（允许其他 Pod 在该特定节点上正确运行）时，
应该使用 DaemonSet。</p>
<p>例如，<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件</a>通常包含一个以 DaemonSet 运行的组件。
这个 DaemonSet 组件确保它所在的节点的集群网络正常工作。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods).
  * Learn about [static Pods](#static-pods), which are useful for running Kubernetes
    <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> components.
* Find out how to use DaemonSets
  * [Perform a rolling update on a DaemonSet](/docs/tasks/manage-daemon/update-daemon-set/)
  * [Perform a rollback on a DaemonSet](/docs/tasks/manage-daemon/rollback-daemon-set/)
    (for example, if a roll out didn't work how you expected).
* Understand [how Kubernetes assigns Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/).
* Learn about [device plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) and
  [add ons](/docs/concepts/cluster-administration/addons/), which often run as DaemonSets.
* `DaemonSet` is a top-level resource in the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for daemon sets.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。
<ul>
<li>了解<a href="#static-pods">静态 Pod</a>，这对运行 Kubernetes <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a>组件有帮助。</li>
</ul>
</li>
<li>了解如何使用 DaemonSet
<ul>
<li><a href="/zh/docs/tasks/manage-daemon/update-daemon-set/">对 DaemonSet 执行滚动更新</a></li>
<li><a href="/zh/docs/tasks/manage-daemon/rollback-daemon-set/">对 DaemonSet 执行回滚</a>（例如：新的版本没有达到你的预期）</li>
</ul>
</li>
<li>理解<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">Kubernetes 如何将 Pod 分配给节点</a>。</li>
<li>了解<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">设备插件</a>和
<a href="/zh/docs/concepts/cluster-administration/addons/">扩展（Addons）</a>，它们常以 DaemonSet 运行。</li>
<li><code>DaemonSet</code> 是 Kubernetes REST API 中的顶级资源。阅读 





<a href=""></a>
对象定义理解关于该资源的 API。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-cc7cc3c4907039d9f863162e20bfbbef">2.5 - Jobs</h1>
    
	<!--
reviewers:
- erictune
- soltysh
title: Jobs
content_type: concept
feature:
  title: Batch execution
  description: >
    In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.
weight: 50
-->
<!-- overview -->
<!--
A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.
As pods successfully complete, the Job tracks the successful completions.  When a specified number
of successful completions is reached, the task (ie, Job) is complete.  Deleting a Job will clean up
the Pods it created. Suspending a Job will delete its active Pods until the Job
is resumed again.

A simple case is to create one Job object in order to reliably run one Pod to completion.
The Job object will start a new Pod if the first Pod fails or is deleted (for example
due to a node hardware failure or a node reboot).

You can also use a Job to run multiple Pods in parallel.
-->
<p>Job 会创建一个或者多个 Pods，并将继续重试 Pods 的执行，直到指定数量的 Pods 成功终止。
随着 Pods 成功结束，Job 跟踪记录成功完成的 Pods 个数。
当数量达到指定的成功个数阈值时，任务（即 Job）结束。
删除 Job 的操作会清除所创建的全部 Pods。
挂起 Job 的操作会删除 Job 的所有活跃 Pod，直到 Job 被再次恢复执行。</p>
<p>一种简单的使用场景下，你会创建一个 Job 对象以便以一种可靠的方式运行某 Pod 直到完成。
当第一个 Pod 失败或者被删除（比如因为节点硬件失效或者重启）时，Job
对象会启动一个新的 Pod。</p>
<p>你也可以使用 Job 以并行的方式运行多个 Pod。</p>
<!-- body -->
<!--
## Running an example Job

Here is an example Job config.  It computes π to 2000 places and prints it out.
It takes around 10s to complete.
-->
<h2 id="running-an-example-job">运行示例 Job    </h2>
<p>下面是一个 Job 配置示例。它负责计算 π 到小数点后 2000 位，并将结果打印出来。
此计算大约需要 10 秒钟完成。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/job.yaml" download="controllers/job.yaml"><code>controllers/job.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-job-yaml')" title="Copy controllers/job.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-job-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>perl<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;perl&#34;</span>,<span style="color:#bbb">  </span><span style="color:#b44">&#34;-Mbignum=bpi&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-wle&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;print bpi(2000)&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">backoffLimit</span>:<span style="color:#bbb"> </span><span style="color:#666">4</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--You can run the example with this command:-->
<p>你可以使用下面的命令来运行此示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
</code></pre></div><p>输出类似于：</p>
<pre><code>job.batch/pi created
</code></pre><!-- Check on the status of the Job with `kubectl`: -->
<p>使用 <code>kubectl</code> 来检查 Job 的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe jobs/pi
</code></pre></div><p>输出类似于：</p>
<pre><code>Name:           pi
Namespace:      default
Selector:       controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                job-name=pi
Annotations:    kubectl.kubernetes.io/last-applied-configuration:
                  {&quot;apiVersion&quot;:&quot;batch/v1&quot;,&quot;kind&quot;:&quot;Job&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;pi&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;backoffLimit&quot;:4,&quot;template&quot;:...
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           job-name=pi
  Containers:
   pi:
    Image:      perl
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7
</code></pre><!--
To view completed Pods of a Job, use `kubectl get pods`.

To list all the Pods that belong to a Job in a machine readable form, you can use a command like this:
-->
<p>要查看 Job 对应的已完成的 Pods，可以执行 <code>kubectl get pods</code>。</p>
<p>要以机器可读的方式列举隶属于某 Job 的全部 Pods，你可以使用类似下面这条命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">pods</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get pods --selector<span style="color:#666">=</span>job-name<span style="color:#666">=</span>pi --output<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.items[*].metadata.name}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>
<span style="color:#a2f">echo</span> <span style="color:#b8860b">$pods</span>
</code></pre></div><p>输出类似于：</p>
<pre><code>pi-5rwd7
</code></pre><!--
Here, the selector is the same as the selector for the Job.  The `-output=jsonpath` option specifies an expression
with the name from each Pod in the returned list.

View the standard output of one of the pods:
-->
<p>这里，选择算符与 Job 的选择算符相同。<code>--output=jsonpath</code> 选项给出了一个表达式，
用来从返回的列表中提取每个 Pod 的 name 字段。</p>
<p>查看其中一个 Pod 的标准输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs <span style="color:#b8860b">$pods</span>
</code></pre></div><!--The output is similar to this:-->
<p>输出类似于：</p>
<pre><code>3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
</code></pre><!--
## Writing a Job spec

As with all other Kubernetes config, a Job needs `apiVersion`, `kind`, and `metadata` fields.
Its name must be a valid [DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A Job also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
-->
<h2 id="编写-job-规约">编写 Job 规约</h2>
<p>与 Kubernetes 中其他资源的配置类似，Job 也需要 <code>apiVersion</code>、<code>kind</code> 和 <code>metadata</code> 字段。
Job 的名字必须是合法的 <a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<p>Job 配置还需要一个<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> 节</a>。</p>
<!--
### Pod Template

The `.spec.template` is the only required field of the `.spec`.

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>, except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a pod template in a Job must specify appropriate
labels (see [pod selector](#pod-selector)) and an appropriate restart policy.

Only a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Never` or `OnFailure` is allowed.
-->
<h3 id="pod-模版">Pod 模版</h3>
<p>Job 的 <code>.spec</code> 中只有 <code>.spec.template</code> 是必需的字段。</p>
<p>字段 <code>.spec.template</code> 的值是一个 <a href="/zh/docs/concepts/workloads/pods/#pod-templates">Pod 模版</a>。
其定义规范与 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
完全相同，只是其中不再需要 <code>apiVersion</code> 或 <code>kind</code> 字段。</p>
<p>除了作为 Pod 所必需的字段之外，Job 中的 Pod 模版必需设置合适的标签
（参见<a href="#pod-selector">Pod 选择算符</a>）和合适的重启策略。</p>
<p>Job 中 Pod 的 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>RestartPolicy</code></a>
只能设置为 <code>Never</code> 或 <code>OnFailure</code> 之一。</p>
<!--
### Pod selector

The `.spec.selector` field is optional.  In almost all cases you should not specify it.
See section [specifying your own pod selector](#specifying-your-own-pod-selector).
-->
<h3 id="pod-selector">Pod 选择算符  </h3>
<p>字段 <code>.spec.selector</code> 是可选的。在绝大多数场合，你都不需要为其赋值。
参阅<a href="#specifying-your-own-pod-selector">设置自己的 Pod 选择算符</a>.</p>
<!--
### Parallel execution for Jobs {#parallel-jobs}

There are three main types of task suitable to run as a Job:
-->
<h3 id="parallel-jobs">Job 的并行执行</h3>
<p>适合以 Job 形式来运行的任务主要有三种：</p>
<!--
1. Non-parallel Jobs
   - normally, only one Pod is started, unless the Pod fails.
   - the Job is complete as soon as its Pod terminates successfully.
1. Parallel Jobs with a *fixed completion count*:
   - specify a non-zero positive value for `.spec.completions`.
   - the Job represents the overall task, and is complete when there are `.spec.completions` successful Pods.
   - when using `.spec.completionMode="Indexed"`, each Pod gets a different index in the range 0 to `.spec.completions-1`.
1. Parallel Jobs with a *work queue*:
   - do not specify `.spec.completions`, default to `.spec.parallelism`.
   - the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.
   - each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done.
   - when _any_ Pod from the Job terminates with success, no new Pods are created.
   - once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success.
   - once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output.  They should all be in the process of exiting.
-->
<ol>
<li>非并行 Job：
<ul>
<li>通常只启动一个 Pod，除非该 Pod 失败。</li>
<li>当 Pod 成功终止时，立即视 Job 为完成状态。</li>
</ul>
</li>
<li>具有 <em>确定完成计数</em> 的并行 Job：
<ul>
<li><code>.spec.completions</code> 字段设置为非 0 的正数值。</li>
<li>Job 用来代表整个任务，当成功的 Pod 个数达到 <code>.spec.completions</code> 时，Job 被视为完成。</li>
<li>当使用 <code>.spec.completionMode=&quot;Indexed&quot;</code> 时，每个 Pod 都会获得一个不同的
索引值，介于 0 和 <code>.spec.completions-1</code> 之间。</li>
</ul>
</li>
<li>带 <em>工作队列</em> 的并行 Job：
<ul>
<li>不设置 <code>spec.completions</code>，默认值为 <code>.spec.parallelism</code>。</li>
<li>多个 Pod 之间必须相互协调，或者借助外部服务确定每个 Pod 要处理哪个工作条目。
例如，任一 Pod 都可以从工作队列中取走最多 N 个工作条目。</li>
<li>每个 Pod 都可以独立确定是否其它 Pod 都已完成，进而确定 Job 是否完成。</li>
<li>当 Job 中 <em>任何</em> Pod 成功终止，不再创建新 Pod。</li>
<li>一旦至少 1 个 Pod 成功完成，并且所有 Pod 都已终止，即可宣告 Job 成功完成。</li>
<li>一旦任何 Pod 成功退出，任何其它 Pod 都不应再对此任务执行任何操作或生成任何输出。
所有 Pod 都应启动退出过程。</li>
</ul>
</li>
</ol>
<!--
For a _non-parallel_ Job, you can leave both `.spec.completions` and `.spec.parallelism` unset.  When both are
unset, both are defaulted to 1.

For a _fixed completion count_ Job, you should set `.spec.completions` to the number of completions needed.
You can set `.spec.parallelism`, or leave it unset and it will default to 1.

For a _work queue_ Job, you must leave `.spec.completions` unset, and set `.spec.parallelism` to
a non-negative integer.

For more information about how to make use of the different types of job, see the [job patterns](#job-patterns) section.
-->
<p>对于 <em>非并行</em> 的 Job，你可以不设置 <code>spec.completions</code> 和 <code>spec.parallelism</code>。
这两个属性都不设置时，均取默认值 1。</p>
<p>对于 <em>确定完成计数</em> 类型的 Job，你应该设置 <code>.spec.completions</code> 为所需要的完成个数。
你可以设置 <code>.spec.parallelism</code>，也可以不设置。其默认值为 1。</p>
<p>对于一个 <em>工作队列</em> Job，你不可以设置 <code>.spec.completions</code>，但要将<code>.spec.parallelism</code>
设置为一个非负整数。</p>
<p>关于如何利用不同类型的 Job 的更多信息，请参见 <a href="#job-patterns">Job 模式</a>一节。</p>
<!--
#### Controlling parallelism

The requested parallelism (`.spec.parallelism`) can be set to any non-negative value.
If it is unspecified, it defaults to 1.
If it is specified as 0, then the Job is effectively paused until it is increased.

Actual parallelism (number of pods running at any instant) may be more or less than requested
parallelism, for a variety of reasons:
-->
<h4 id="controlling-parallelism">控制并行性  </h4>
<p>并行性请求（<code>.spec.parallelism</code>）可以设置为任何非负整数。
如果未设置，则默认为 1。
如果设置为 0，则 Job 相当于启动之后便被暂停，直到此值被增加。</p>
<p>实际并行性（在任意时刻运行状态的 Pods 个数）可能比并行性请求略大或略小，
原因如下：</p>
<!--
- For _fixed completion count_ Jobs, the actual number of pods running in parallel will not exceed the number of
  remaining completions.   Higher values of `.spec.parallelism` are effectively ignored.
- For _work queue_ Jobs, no new Pods are started after any Pod has succeeded - remaining Pods are allowed to complete, however.
- If the Job <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> has not had time to react.
- If the Job controller failed to create Pods for any reason (lack of `ResourceQuota`, lack of permission, etc.),
  then there may be fewer pods than requested.
- The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.
- When a Pod is gracefully shut down, it takes time to stop.
-->
<ul>
<li>对于 <em>确定完成计数</em> Job，实际上并行执行的 Pods 个数不会超出剩余的完成数。
如果 <code>.spec.parallelism</code> 值较高，会被忽略。</li>
<li>对于 <em>工作队列</em> Job，有任何 Job 成功结束之后，不会有新的 Pod 启动。
不过，剩下的 Pods 允许执行完毕。</li>
<li>如果 Job <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a> 没有来得及作出响应，或者</li>
<li>如果 Job 控制器因为任何原因（例如，缺少 <code>ResourceQuota</code> 或者没有权限）无法创建 Pods。
Pods 个数可能比请求的数目小。</li>
<li>Job 控制器可能会因为之前同一 Job 中 Pod 失效次数过多而压制新 Pod 的创建。</li>
<li>当 Pod 处于体面终止进程中，需要一定时间才能停止。</li>
</ul>
<!--
### Completion mode
-->
<h3 id="completion-mode">完成模式  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
Jobs with _fixed completion count_ - that is, jobs that have non null
`.spec.completions` - can have a completion mode that is specified in `.spec.completionMode`:
-->
<p>带有 <em>确定完成计数</em> 的 Job，即 <code>.spec.completions</code> 不为 null 的 Job，
都可以在其 <code>.spec.completionMode</code> 中设置完成模式：</p>
<!--
- `NonIndexed` (default): the Job is considered complete when there have been
  `.spec.completions` successfully completed Pods. In other words, each Pod
  completion is homologous to each other. Note that Jobs that have null
  `.spec.completions` are implicitly `NonIndexed`.
- `Indexed`: the Pods of a Job get an associated completion index from 0 to
  `.spec.completions-1`. The index is available through three mechanisms:
  - The Pod annotation `batch.kubernetes.io/job-completion-index`.
  - As part of the Pod hostname, following the pattern `$(job-name)-$(index)`.
    When you use an Indexed Job in combination with a
    <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a>, Pods within the Job can use
    the deterministic hostnames to address each other via DNS.
  - From the containarized task, in the environment variable `JOB_COMPLETION_INDEX`.

  The Job is considered complete when there is one successfully completed Pod
  for each index. For more information about how to use this mode, see
  [Indexed Job for Parallel Processing with Static Work Assignment](/docs/tasks/job/indexed-parallel-processing-static/).
  Note that, although rare, more than one Pod could be started for the same
  index, but only one of them will count towards the completion count.
-->
<ul>
<li>
<p><code>NonIndexed</code>（默认值）：当成功完成的 Pod 个数达到 <code>.spec.completions</code> 所
设值时认为 Job 已经完成。换言之，每个 Job 完成事件都是独立无关且同质的。
要注意的是，当 <code>.spec.completions</code> 取值为 null 时，Job 被隐式处理为 <code>NonIndexed</code>。</p>
</li>
<li>
<p><code>Indexed</code>：Job 的 Pod 会获得对应的完成索引，取值为 0 到 <code>.spec.completions-1</code>。
该索引可以通过三种方式获取：</p>
<ul>
<li>Pod 注解 <code>batch.kubernetes.io/job-completion-index</code>。</li>
<li>作为 Pod 主机名的一部分，遵循模式 <code>$(job-name)-$(index)</code>。
当你同时使用带索引的 Job（Indexed Job）与 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a>，
Job 中的 Pods 可以通过 DNS 使用确切的主机名互相寻址。</li>
<li>对于容器化的任务，在环境变量 <code>JOB_COMPLETION_INDEX</code> 中。</li>
</ul>
<p>当每个索引都对应一个完成完成的 Pod 时，Job 被认为是已完成的。
关于如何使用这种模式的更多信息，可参阅
<a href="/zh/docs/tasks/job/indexed-parallel-processing-static/">用带索引的 Job 执行基于静态任务分配的并行处理</a>。
需要注意的是，对同一索引值可能被启动的 Pod 不止一个，尽管这种情况很少发生。
这时，只有一个会被记入完成计数中。</p>
</li>
</ul>
<!--
## Handling Pod and container failures

A container in a Pod may fail for a number of reasons, such as because the process in it exited with
a non-zero exit code, or the container was killed for exceeding a memory limit, etc.  If this
happens, and the `.spec.template.spec.restartPolicy = "OnFailure"`, then the Pod stays
on the node, but the container is re-run.  Therefore, your program needs to handle the case when it is
restarted locally, or else specify `.spec.template.spec.restartPolicy = "Never"`.
See [pod lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/#example-states) for more information on `restartPolicy`.
-->
<h2 id="处理-pod-和容器失效">处理 Pod 和容器失效</h2>
<p>Pod 中的容器可能因为多种不同原因失效，例如因为其中的进程退出时返回值非零，
或者容器因为超出内存约束而被杀死等等。
如果发生这类事件，并且 <code>.spec.template.spec.restartPolicy = &quot;OnFailure&quot;</code>，
Pod 则继续留在当前节点，但容器会被重新运行。
因此，你的程序需要能够处理在本地被重启的情况，或者要设置
<code>.spec.template.spec.restartPolicy = &quot;Never&quot;</code>。
关于 <code>restartPolicy</code> 的更多信息，可参阅
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#example-states">Pod 生命周期</a>。</p>
<!--
An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node
(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the
`.spec.template.spec.restartPolicy = "Never"`.  When a Pod fails, then the Job controller
starts a new Pod.  This means that your application needs to handle the case when it is restarted in a new
pod.  In particular, it needs to handle temporary files, locks, incomplete output and the like
caused by previous runs.
-->
<p>整个 Pod 也可能会失败，且原因各不相同。
例如，当 Pod 启动时，节点失效（被升级、被重启、被删除等）或者其中的容器失败而
<code>.spec.template.spec.restartPolicy = &quot;Never&quot;</code>。
当 Pod 失败时，Job 控制器会启动一个新的 Pod。
这意味着，你的应用需要处理在一个新 Pod 中被重启的情况。
尤其是应用需要处理之前运行所产生的临时文件、锁、不完整的输出等问题。</p>
<!--
Note that even if you specify `.spec.parallelism = 1` and `.spec.completions = 1` and
`.spec.template.spec.restartPolicy = "Never"`, the same program may
sometimes be started twice.

If you do specify `.spec.parallelism` and `.spec.completions` both greater than 1, then there may be
multiple pods running at once.  Therefore, your pods must also be tolerant of concurrency.
-->
<p>注意，即使你将 <code>.spec.parallelism</code> 设置为 1，且将 <code>.spec.completions</code> 设置为
1，并且 <code>.spec.template.spec.restartPolicy</code> 设置为 &quot;Never&quot;，同一程序仍然有可能被启动两次。</p>
<p>如果你确实将 <code>.spec.parallelism</code> 和 <code>.spec.completions</code> 都设置为比 1 大的值，
那就有可能同时出现多个 Pod 运行的情况。
为此，你的 Pod 也必须能够处理并发性问题。</p>
<!--
### Pod backoff failure policy

There are situations where you want to fail a Job after some amount of retries
due to a logical error in configuration etc.
To do so, set `.spec.backoffLimit` to specify the number of retries before
considering a Job as failed. The back-off limit is set by default to 6. Failed
Pods associated with the Job are recreated by the Job controller with an
exponential back-off delay (10s, 20s, 40s ...) capped at six minutes. The
back-off count is reset when a Job's Pod is deleted or successful without any
other Pods for the Job failing around that time.
-->
<h3 id="pod-回退失效策略">Pod 回退失效策略</h3>
<p>在有些情形下，你可能希望 Job 在经历若干次重试之后直接进入失败状态，因为这很
可能意味着遇到了配置错误。
为了实现这点，可以将 <code>.spec.backoffLimit</code> 设置为视 Job 为失败之前的重试次数。
失效回退的限制值默认为 6。
与 Job 相关的失效的 Pod 会被 Job 控制器重建，回退重试时间将会按指数增长
（从 10 秒、20 秒到 40 秒）最多至 6 分钟。
当 Job 的 Pod 被删除时，或者 Pod 成功时没有其它 Pod 处于失败状态，失效回退的次数也会被重置（为 0）。</p>
<!--
If your job has `restartPolicy = "OnFailure"`, keep in mind that your Pod running the Job
will be terminated once the job backoff limit has been reached. This can make debugging the Job's executable more difficult. We suggest setting
`restartPolicy = "Never"` when debugging the Job or using a logging system to ensure output
from failed Jobs is not lost inadvertently.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果你的 Job 的 <code>restartPolicy</code> 被设置为 &quot;OnFailure&quot;，就要注意运行该 Job 的 Pod
会在 Job 到达失效回退次数上限时自动被终止。
这会使得调试 Job 中可执行文件的工作变得非常棘手。
我们建议在调试 Job 时将 <code>restartPolicy</code> 设置为 &quot;Never&quot;，
或者使用日志系统来确保失效 Jobs 的输出不会意外遗失。
</div>
<!--
## Job termination and cleanup

When a Job completes, no more Pods are created, but the Pods are [usually](#pod-backoff-failure-policy) not deleted either.
Keeping them around
allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.
The job object also remains after it is completed so that you can view its status.  It is up to the user to delete
old jobs after noting their status.  Delete the job with `kubectl` (e.g. `kubectl delete jobs/pi` or `kubectl delete -f ./job.yaml`). When you delete the job using `kubectl`, all the pods it created are deleted too.
-->
<h2 id="job-终止与清理">Job 终止与清理</h2>
<p>Job 完成时不会再创建新的 Pod，不过已有的 Pod <a href="#pod-backoff-failure-policy">通常</a>也不会被删除。
保留这些 Pod 使得你可以查看已完成的 Pod 的日志输出，以便检查错误、警告
或者其它诊断性输出。
Job 完成时 Job 对象也一样被保留下来，这样你就可以查看它的状态。
在查看了 Job 状态之后删除老的 Job 的操作留给了用户自己。
你可以使用 <code>kubectl</code> 来删除 Job（例如，<code>kubectl delete jobs/pi</code>
或者 <code>kubectl delete -f ./job.yaml</code>）。
当使用 <code>kubectl</code> 来删除 Job 时，该 Job 所创建的 Pods 也会被删除。</p>
<!--
By default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`) or a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the
`.spec.backoffLimit` described above. Once `.spec.backoffLimit` has been reached the Job will be marked as failed and any running Pods will be terminated.

Another way to terminate a Job is by setting an active deadline.
Do this by setting the `.spec.activeDeadlineSeconds` field of the Job to a number of seconds.
The `activeDeadlineSeconds` applies to the duration of the job, no matter how many Pods are created.
Once a Job reaches `activeDeadlineSeconds`, all of its running Pods are terminated and the Job status will become `type: Failed` with `reason: DeadlineExceeded`.
-->
<p>默认情况下，Job 会持续运行，除非某个 Pod 失败（<code>restartPolicy=Never</code>）
或者某个容器出错退出（<code>restartPolicy=OnFailure</code>）。
这时，Job 基于前述的 <code>spec.backoffLimit</code> 来决定是否以及如何重试。
一旦重试次数到达 <code>.spec.backoffLimit</code> 所设的上限，Job 会被标记为失败，
其中运行的 Pods 都会被终止。</p>
<p>终止 Job 的另一种方式是设置一个活跃期限。
你可以为 Job 的 <code>.spec.activeDeadlineSeconds</code> 设置一个秒数值。
该值适用于 Job 的整个生命期，无论 Job 创建了多少个 Pod。
一旦 Job 运行时间达到 <code>activeDeadlineSeconds</code> 秒，其所有运行中的 Pod
都会被终止，并且 Job 的状态更新为 <code>type: Failed</code>
及 <code>reason: DeadlineExceeded</code>。</p>
<!--
Note that a Job's `.spec.activeDeadlineSeconds` takes precedence over its `.spec.backoffLimit`. Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it reaches the time limit specified by `activeDeadlineSeconds`, even if the `backoffLimit` is not yet reached.

Example:
-->
<p>注意 Job 的 <code>.spec.activeDeadlineSeconds</code> 优先级高于其 <code>.spec.backoffLimit</code> 设置。
因此，如果一个 Job 正在重试一个或多个失效的 Pod，该 Job 一旦到达
<code>activeDeadlineSeconds</code> 所设的时限即不再部署额外的 Pod，即使其重试次数还未
达到 <code>backoffLimit</code> 所设的限制。</p>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi-with-timeout<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">backoffLimit</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">activeDeadlineSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>perl<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;perl&#34;</span>,<span style="color:#bbb">  </span><span style="color:#b44">&#34;-Mbignum=bpi&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-wle&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;print bpi(2000)&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div><!--
Note that both the Job spec and the [Pod template spec](/docs/concepts/workloads/pods/init-containers/#detailed-behavior) within the Job have an `activeDeadlineSeconds` field. Ensure that you set this field at the proper level.

Keep in mind that the `restartPolicy` applies to the Pod, and not to the Job itself: there is no automatic Job restart once the Job status is `type: Failed`.
That is, the Job termination mechanisms activated with `.spec.activeDeadlineSeconds` and `.spec.backoffLimit` result in a permanent Job failure that requires manual intervention to resolve.
-->
<p>注意 Job 规约和 Job 中的
<a href="/zh/docs/concepts/workloads/pods/init-containers/#detailed-behavior">Pod 模版规约</a>
都有 <code>activeDeadlineSeconds</code> 字段。
请确保你在合适的层次设置正确的字段。</p>
<p>还要注意的是，<code>restartPolicy</code> 对应的是 Pod，而不是 Job 本身：
一旦 Job 状态变为 <code>type: Failed</code>，就不会再发生 Job 重启的动作。
换言之，由 <code>.spec.activeDeadlineSeconds</code> 和 <code>.spec.backoffLimit</code> 所触发的 Job 终结机制
都会导致 Job 永久性的失败，而这类状态都需要手工干预才能解决。</p>
<!--
## Clean up finished jobs automatically

Finished Jobs are usually no longer needed in the system. Keeping them around in
the system will put pressure on the API server. If the Jobs are managed directly
by a higher level controller, such as
[CronJobs](/docs/concepts/workloads/controllers/cron-jobs/), the Jobs can be
cleaned up by CronJobs based on the specified capacity-based cleanup policy.

### TTL mechanism for finished Jobs
-->
<h2 id="clean-up-finished-jobs-automatically">自动清理完成的 Job  </h2>
<p>完成的 Job 通常不需要留存在系统中。在系统中一直保留它们会给 API
服务器带来额外的压力。
如果 Job 由某种更高级别的控制器来管理，例如
<a href="/zh/docs/concepts/workloads/controllers/cron-jobs/">CronJobs</a>，
则 Job 可以被 CronJob 基于特定的根据容量裁定的清理策略清理掉。</p>
<h3 id="ttl-mechanisms-for-finished-jobs">已完成 Job 的 TTL 机制 </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code>
</div>


<!--
Another way to clean up finished Jobs (either `Complete` or `Failed`)
automatically is to use a TTL mechanism provided by a
[TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) for
finished resources, by specifying the `.spec.ttlSecondsAfterFinished` field of
the Job.

When the TTL controller cleans up the Job, it will delete the Job cascadingly,
i.e. delete its dependent objects, such as Pods, together with the Job. Note
that when the Job is deleted, its lifecycle guarantees, such as finalizers, will
be honored.

For example:
-->
<p>自动清理已完成 Job （状态为 <code>Complete</code> 或 <code>Failed</code>）的另一种方式是使用由
<a href="/zh/docs/concepts/workloads/controllers/ttlafterfinished/">TTL 控制器</a>所提供
的 TTL 机制。
通过设置 Job 的 <code>.spec.ttlSecondsAfterFinished</code> 字段，可以让该控制器清理掉
已结束的资源。</p>
<p>TTL 控制器清理 Job 时，会级联式地删除 Job 对象。
换言之，它会删除所有依赖的对象，包括 Pod 及 Job 本身。
注意，当 Job 被删除时，系统会考虑其生命周期保障，例如其 Finalizers。</p>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi-with-ttl<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ttlSecondsAfterFinished</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>perl<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;perl&#34;</span>,<span style="color:#bbb">  </span><span style="color:#b44">&#34;-Mbignum=bpi&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-wle&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;print bpi(2000)&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div><!--
The Job `pi-with-ttl` will be eligible to be automatically deleted, `100`
seconds after it finishes.

If the field is set to `0`, the Job will be eligible to be automatically deleted
immediately after it finishes. If the field is unset, this Job won't be cleaned
up by the TTL controller after it finishes.
-->
<p>Job <code>pi-with-ttl</code> 在结束 100 秒之后，可以成为被自动删除的对象。</p>
<p>如果该字段设置为 <code>0</code>，Job 在结束之后立即成为可被自动删除的对象。
如果该字段没有设置，Job 不会在结束之后被 TTL 控制器自动清除。</p>
<!--
## Job patterns

The Job object can be used to support reliable parallel execution of Pods.  The Job object is not
designed to support closely-communicating parallel processes, as commonly found in scientific
computing.  It does support parallel processing of a set of independent but related *work items*.
These might be emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in a
NoSQL database to scan, and so on.
-->
<h2 id="job-patterns">Job 模式 </h2>
<p>Job 对象可以用来支持多个 Pod 的可靠的并发执行。
Job 对象不是设计用来支持相互通信的并行进程的，后者一般在科学计算中应用较多。
Job 的确能够支持对一组相互独立而又有所关联的 <em>工作条目</em> 的并行处理。
这类工作条目可能是要发送的电子邮件、要渲染的视频帧、要编解码的文件、NoSQL
数据库中要扫描的主键范围等等。</p>
<!--
In a complex system, there may be multiple different sets of work items.  Here we are just
considering one set of work items that the user wants to manage together &mdash; a *batch job*.

There are several different patterns for parallel computation, each with strengths and weaknesses.
The tradeoffs are:
-->
<p>在一个复杂系统中，可能存在多个不同的工作条目集合。这里我们仅考虑用户希望一起管理的
工作条目集合之一 — <em>批处理作业</em>。</p>
<p>并行计算的模式有好多种，每种都有自己的强项和弱点。这里要权衡的因素有：</p>
<!--
- One Job object for each work item, vs. a single Job object for all work items.  The latter is
  better for large numbers of work items.  The former creates some overhead for the user and for the
  system to manage large numbers of Job objects.
- Number of pods created equals number of work items, vs. each Pod can process multiple work items.
  The former typically requires less modification to existing code and containers.  The latter
  is better for large numbers of work items, for similar reasons to the previous bullet.
- Several approaches use a work queue.  This requires running a queue service,
  and modifications to the existing program or container to make it use the work queue.
  Other approaches are easier to adapt to an existing containerised application.
-->
<ul>
<li>每个工作条目对应一个 Job 或者所有工作条目对应同一 Job 对象。
后者更适合处理大量工作条目的场景；
前者会给用户带来一些额外的负担，而且需要系统管理大量的 Job 对象。</li>
<li>创建与工作条目相等的 Pod 或者令每个 Pod 可以处理多个工作条目。
前者通常不需要对现有代码和容器做较大改动；
后者则更适合工作条目数量较大的场合，原因同上。</li>
<li>有几种技术都会用到工作队列。这意味着需要运行一个队列服务，并修改现有程序或容器
使之能够利用该工作队列。
与之比较，其他方案在修改现有容器化应用以适应需求方面可能更容易一些。</li>
</ul>
<!--
The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.
The pattern names are also links to examples and more detailed description.
-->
<p>下面是对这些权衡的汇总，列 2 到 4 对应上面的权衡比较。
模式的名称对应了相关示例和更详细描述的链接。</p>
<table>
<thead>
<tr>
<th>模式</th>
<th style="text-align:center">单个 Job 对象</th>
<th style="text-align:center">Pods 数少于工作条目数？</th>
<th style="text-align:center">直接使用应用无需修改?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="/zh/docs/tasks/job/coarse-parallel-processing-work-queue/">每工作条目一 Pod 的队列</a></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center">有时</td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/fine-parallel-processing-work-queue/">Pod 数量可变的队列</a></td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/indexed-parallel-processing-static">静态任务分派的带索引的 Job</a></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/parallel-processing-expansion/">Job 模版扩展</a></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
</tbody>
</table>
<!--
When you specify completions with `.spec.completions`, each Pod created by the Job controller
has an identical [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).  This means that
all pods for a task will have the same command line and the same
image, the same volumes, and (almost) the same environment variables.  These patterns
are different ways to arrange for pods to work on different things.

This table shows the required settings for `.spec.parallelism` and `.spec.completions` for each of the patterns.
Here, `W` is the number of work items.
-->
<p>当你使用 <code>.spec.completions</code> 来设置完成数时，Job 控制器所创建的每个 Pod
使用完全相同的 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>spec</code></a>。
这意味着任务的所有 Pod 都有相同的命令行，都使用相同的镜像和数据卷，甚至连
环境变量都（几乎）相同。
这些模式是让每个 Pod 执行不同工作的几种不同形式。</p>
<p>下表显示的是每种模式下 <code>.spec.parallelism</code> 和 <code>.spec.completions</code> 所需要的设置。
其中，<code>W</code> 表示的是工作条目的个数。</p>
<table>
<thead>
<tr>
<th>模式</th>
<th style="text-align:center"><code>.spec.completions</code></th>
<th style="text-align:center"><code>.spec.parallelism</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="/zh/docs/tasks/job/coarse-parallel-processing-work-queue/">每工作条目一 Pod 的队列</a></td>
<td style="text-align:center">W</td>
<td style="text-align:center">任意值</td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/fine-parallel-processing-work-queue/">Pod 个数可变的队列</a></td>
<td style="text-align:center">1</td>
<td style="text-align:center">任意值</td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/indexed-parallel-processing-static">静态任务分派的带索引的 Job</a></td>
<td style="text-align:center">W</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/parallel-processing-expansion/">Job 模版扩展</a></td>
<td style="text-align:center">1</td>
<td style="text-align:center">应该为 1</td>
</tr>
</tbody>
</table>
<!--
## Advanced usage

### Suspending a Job
-->
<h2 id="advanced-usage">高级用法  </h2>
<h3 id="suspending-a-job">挂起 Job  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In Kubernetes version 1.21, this feature was in alpha, which required additional
steps to enable this feature; make sure to read the [right documentation for the
version of Kubernetes you're using](/docs/home/supported-doc-versions/).
-->
<p>该特性在 Kubernetes 1.21 版本中是 Alpha 阶段，启用该特性需要额外的步骤；
请确保你正在阅读<a href="/zh/docs/home/supported-doc-versions/">与集群版本一致的文档</a>。
</div>
<!--
When a Job is created, the Job controller will immediately begin creating Pods
to satisfy the Job's requirements and will continue to do so until the Job is
complete. However, you may want to temporarily suspend a Job's execution and
resume it later, or start Jobs in suspended state and have a custom controller
decide later when to start them.
-->
<p>Job 被创建时，Job 控制器会马上开始执行 Pod 创建操作以满足 Job 的需求，
并持续执行此操作直到 Job 完成为止。
不过你可能想要暂时挂起 Job 执行，或启动处于挂起状态的job，
并拥有一个自定义控制器以后再决定什么时候开始。</p>
<!-- 
To suspend a Job, you can update the `.spec.suspend` field of
the Job to true; later, when you want to resume it again, update it to false.
Creating a Job with `.spec.suspend` set to true will create it in the suspended
state.
-->
<p>要挂起一个 Job，你可以更新 <code>.spec.suspend</code> 字段为 true，
之后，当你希望恢复其执行时，将其更新为 false。
创建一个 <code>.spec.suspend</code> 被设置为 true 的 Job 本质上会将其创建为被挂起状态。</p>
<!--
When a Job is resumed from suspension, its `.status.startTime` field will be
reset to the current time. This means that the `.spec.activeDeadlineSeconds`
timer will be stopped and reset when a Job is suspended and resumed.
-->
<p>当 Job 被从挂起状态恢复执行时，其 <code>.status.startTime</code> 字段会被重置为
当前的时间。这意味着 <code>.spec.activeDeadlineSeconds</code> 计时器会在 Job 挂起时
被停止，并在 Job 恢复执行时复位。</p>
<!--
Remember that suspending a Job will delete all active Pods. When the Job is
suspended, your [Pods will be terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)
with a SIGTERM signal. The Pod's graceful termination period will be honored and
your Pod must handle this signal in this period. This may involve saving
progress for later or undoing changes. Pods terminated this way will not count
towards the Job's `completions` count.
-->
<p>要记住的是，挂起 Job 会删除其所有活跃的 Pod。当 Job 被挂起时，你的 Pod 会
收到 SIGTERM 信号而被<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">终止</a>。
Pod 的体面终止期限会被考虑，不过 Pod 自身也必须在此期限之内处理完信号。
处理逻辑可能包括保存进度以便将来恢复，或者取消已经做出的变更等等。
Pod 以这种形式终止时，不会被记入 Job 的 <code>completions</code> 计数。</p>
<!--
An example Job definition in the suspended state can be like so:
-->
<p>处于被挂起状态的 Job 的定义示例可能是这样子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get job myjob -o yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myjob<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">suspend</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">parallelism</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">completions</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>...<span style="color:#bbb">
</span></code></pre></div><!--
The Job's status can be used to determine if a Job is suspended or has been
suspended in the past:
-->
<p>Job 的 <code>status</code> 可以用来确定 Job 是否被挂起，或者曾经被挂起。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get jobs/myjob -o yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="">apiVersion:</span> <span style="">batch/v</span><span style="color:#666">1</span>
<span style="">kind:</span> <span style="">Job</span>
<span style="">#</span> <span style="">.metadata</span> <span style="">and</span> <span style="">.spec</span> <span style="">omitted</span>
<span style="">status:</span>
  <span style="">conditions:</span>
  <span style="">-</span> <span style="">lastProbeTime:</span> <span style="color:#b44">&#34;2021-02-05T13:14:33Z&#34;</span>
    <span style="">lastTransitionTime:</span> <span style="color:#b44">&#34;2021-02-05T13:14:33Z&#34;</span>
    <span style="">status:</span> <span style="color:#b44">&#34;True&#34;</span>
    <span style="">type:</span> <span style="">Suspended</span>
  <span style="">startTime:</span> <span style="color:#b44">&#34;2021-02-05T13:13:48Z&#34;</span>
</code></pre></div><!--
The Job condition of type "Suspended" with status "True" means the Job is
suspended; the `lastTransitionTime` field can be used to determine how long the
Job has been suspended for. If the status of that condition is "False", then the
Job was previously suspended and is now running. If such a condition does not
exist in the Job's status, the Job has never been stopped.

Events are also created when the Job is suspended and resumed:
-->
<p>Job 的 &quot;Suspended&quot; 类型的状况在状态值为 &quot;True&quot; 时意味着 Job 正被
挂起；<code>lastTransitionTime</code> 字段可被用来确定 Job 被挂起的时长。
如果此状况字段的取值为 &quot;False&quot;，则 Job 之前被挂起且现在在运行。
如果 &quot;Suspended&quot; 状况在 <code>status</code> 字段中不存在，则意味着 Job 从未
被停止执行。</p>
<p>当 Job 被挂起和恢复执行时，也会生成事件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe jobs/myjob
</code></pre></div><pre><code>Name:           myjob
...
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl
  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl
  Normal  Suspended         11m   job-controller  Job suspended
  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44
  Normal  Resumed           3s    job-controller  Job resumed
</code></pre><!--
The last four events, particularly the "Suspended" and "Resumed" events, are
directly a result of toggling the `.spec.suspend` field. In the time between
these two events, we see that no Pods were created, but Pod creation restarted
as soon as the Job was resumed.
-->
<p>最后四个事件，特别是 &quot;Suspended&quot; 和 &quot;Resumed&quot; 事件，都是因为 <code>.spec.suspend</code>
字段值被改来改去造成的。在这两个事件之间，我们看到没有 Pod 被创建，不过当
Job 被恢复执行时，Pod 创建操作立即被重启执行。</p>
<!--
### Mutable Scheduling Directives
-->
<h3 id="mutable-scheduling-directives">可变调度指令</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!--
In order to use this behavior, you must enable the `JobMutableNodeSchedulingDirectives`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
on the [API server](/docs/reference/command-line-tools-reference/kube-apiserver/).
It is enabled by default.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 为了使用此功能，你必须在 <a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">API 服务器</a>上启用
<code>JobMutableNodeSchedulingDirectives</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
默认情况下启用。
</div>
<!--
In most cases a parallel job will want the pods to run with constraints, 
like all in the same zone, or all either on GPU model x or y but not a mix of both.
-->
<p>在大多数情况下，并行作业会希望 Pod 在一定约束条件下运行，
比如所有的 Pod 都在同一个区域，或者所有的 Pod 都在 GPU 型号 x 或 y 上，而不是两者的混合。</p>
<!--
The [suspend](#suspending-a-job) field is the first step towards achieving those semantics. Suspend allows a 
custom queue controller to decide when a job should start; However, once a job is unsuspended,
a custom queue controller has no influence on where the pods of a job will actually land.
-->
<p><a href="#suspend-a-job">suspend</a> 字段是实现这些语义的第一步。
suspend 允许自定义队列控制器，以决定工作何时开始；然而，一旦工作被取消暂停，
自定义队列控制器对 Job 中 Pods 的实际放置位置没有影响。</p>
<!--
This feature allows updating a Job's scheduling directives before it starts, which gives custom queue
controllers the ability to influence pod placement while at the same time offloading actual 
pod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never 
been unsuspended before.
-->
<p>此特性允许在 Job 开始之前更新调度指令，从而为定制队列提供影响 Pod
放置的能力，同时将 Pod 与节点间的分配关系留给 kube-scheduler 决定。
这一特性仅适用于之前从未被暂停过的、已暂停的 Job。
控制器能够影响 Pod 放置，同时参考实际
pod-to-node 分配给 kube-scheduler。这仅适用于从未暂停的 Jobs。</p>
<!--
The fields in a Job's pod template that can be updated are node affinity, node selector, 
tolerations, labels and annotations.
-->
<p>Job 的 Pod 模板中可以更新的字段是节点亲和性、节点选择器、容忍、标签和注解。</p>
<!--
### Specifying your own Pod selector {#specifying-your-own-pod-selector}

Normally, when you create a Job object, you do not specify `.spec.selector`.
The system defaulting logic adds this field when the Job is created.
It picks a selector value that will not overlap with any other jobs.

However, in some cases, you might need to override this automatically set selector.
To do this, you can specify the `.spec.selector` of the Job.
-->
<h3 id="specifying-your-own-pod-selector">指定你自己的 Pod 选择算符</h3>
<p>通常，当你创建一个 Job 对象时，你不会设置 <code>.spec.selector</code>。
系统的默认值填充逻辑会在创建 Job 时添加此字段。
它会选择一个不会与任何其他 Job 重叠的选择算符设置。</p>
<p>不过，有些场合下，你可能需要重载这个自动设置的选择算符。
为了实现这点，你可以手动设置 Job 的 <code>spec.selector</code> 字段。</p>
<!--
Be very careful when doing this.  If you specify a label selector which is not
unique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated
job may be deleted, or this Job may count other Pods as completing it, or one or both
Jobs may refuse to create Pods or run to completion.  If a non-unique selector is
chosen, then other controllers (e.g. ReplicationController) and their Pods may behave
in unpredictable ways too.  Kubernetes will not stop you from making a mistake when
specifying `.spec.selector`.
-->
<p>做这个操作时请务必小心。
如果你所设定的标签选择算符并不唯一针对 Job 对应的 Pod 集合，甚或该算符还能匹配
其他无关的 Pod，这些无关的 Job 的 Pod 可能会被删除。
或者当前 Job 会将另外一些 Pod 当作是完成自身工作的 Pods，
又或者两个 Job 之一或者二者同时都拒绝创建 Pod，无法运行至完成状态。
如果所设置的算符不具有唯一性，其他控制器（如 RC 副本控制器）及其所管理的 Pod
集合可能会变得行为不可预测。
Kubernetes 不会在你设置 <code>.spec.selector</code> 时尝试阻止你犯这类错误。</p>
<!--
Here is an example of a case when you might want to use this feature.

Say Job `old` is already running.  You want existing Pods
to keep running, but you want the rest of the Pods it creates
to use a different pod template and for the Job to have a new name.
You cannot update the Job because these fields are not updatable.
Therefore, you delete Job `old` but _leave its pods
running_, using `kubectl delete jobs/old --cascade=orphan`.
Before deleting it, you make a note of what selector it uses:
-->
<p>下面是一个示例场景，在这种场景下你可能会使用刚刚讲述的特性。</p>
<p>假定名为 <code>old</code> 的 Job 已经处于运行状态。
你希望已有的 Pod 继续运行，但你希望 Job 接下来要创建的其他 Pod
使用一个不同的 Pod 模版，甚至希望 Job 的名字也发生变化。
你无法更新现有的 Job，因为这些字段都是不可更新的。
因此，你会删除 <code>old</code> Job，但 <em>允许该 Job 的 Pod 集合继续运行</em>。
这是通过 <code>kubectl delete jobs/old --cascade=orphan</code> 实现的。
在删除之前，我们先记下该 Job 所使用的选择算符。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get job old -o yaml
</code></pre></div><p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>old<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">controller-uid</span>:<span style="color:#bbb"> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!--
Then you create a new Job with name `new` and you explicitly specify the same selector.
Since the existing Pods have label `controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`,
they are controlled by Job `new` as well.

You need to specify `manualSelector: true` in the new Job since you are not using
the selector that the system normally generates for you automatically.
-->
<p>接下来你会创建名为 <code>new</code> 的新 Job，并显式地为其设置相同的选择算符。
由于现有 Pod 都具有标签 <code>controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002</code>，
它们也会被名为 <code>new</code> 的 Job 所控制。</p>
<p>你需要在新 Job 中设置 <code>manualSelector: true</code>，因为你并未使用系统通常自动为你
生成的选择算符。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>new<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">manualSelector</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">controller-uid</span>:<span style="color:#bbb"> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!--
The new Job itself will have a different uid from `a8f3d00d-c6d2-11e5-9f87-42010af00002`.  Setting
`manualSelector: true` tells the system that you know what you are doing and to allow this
mismatch.
-->
<p>新的 Job 自身会有一个不同于 <code>a8f3d00d-c6d2-11e5-9f87-42010af00002</code> 的唯一 ID。
设置 <code>manualSelector: true</code> 是在告诉系统你知道自己在干什么并要求系统允许这种不匹配
的存在。</p>
<!--
### Job tracking with finalizers

In order to use this behavior, you must enable the `JobTrackingWithFinalizers`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
on the [API server](/docs/reference/command-line-tools-reference/kube-apiserver/)
and the [controller manager](/docs/reference/command-line-tools-reference/kube-controller-manager/).
It is enabled by default.

When enabled, the control plane tracks new Jobs using the behavior described
below. Jobs created before the feature was enabled are unaffected. As a user,
the only difference you would see is that the control plane tracking of Job
completion is more accurate.
-->
<h3 id="job-tracking-with-finalizers">使用 Finalizer 追踪 Job  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>要使用该行为，你必须为 <a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">API 服务器</a>
和<a href="/zh/docs/reference/command-line-tools-reference/kube-controller-manager/">控制器管理器</a>
启用 <code>JobTrackingWithFinalizers</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
默认是启用的。</p>
<p>启用后，控制面基于下述行为追踪新的 Job。在启用该特性之前创建的 Job 不受影响。
作为用户，你会看到的唯一区别是控制面对 Job 完成情况的跟踪更加准确。</p>

</div>
<!--
When this feature isn't enabled, the Job <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a>
relies on counting the Pods that exist in the cluster to track the Job status,
that is, to keep the counters for `succeeded` and `failed` Pods.
However, Pods can be removed for a number of reasons, including:
- The garbage collector that removes orphan Pods when a Node goes down.
- The garbage collector that removes finished Pods (in `Succeeded` or `Failed`
  phase) after a threshold.
- Human intervention to delete Pods belonging to a Job.
- An external controller (not provided as part of Kubernetes) that removes or
  replaces Pods.
-->
<p>该功能未启用时，Job <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> 依靠计算集群中存在的 Pod 来跟踪作业状态。
也就是说，维持一个统计 <code>succeeded</code> 和 <code>failed</code> 的 Pod 的计数器。
然而，Pod 可以因为一些原因被移除，包括：</p>
<ul>
<li>当一个节点宕机时，垃圾收集器会删除孤立（Orphan）Pod。</li>
<li>垃圾收集器在某个阈值后删除已完成的 Pod（处于 <code>Succeeded</code> 或 <code>Failed</code> 阶段）。</li>
<li>人工干预删除 Job 的 Pod。</li>
<li>一个外部控制器（不包含于 Kubernetes）来删除或取代 Pod。</li>
</ul>
<!--
If you enable the `JobTrackingWithFinalizers` feature for your cluster, the
control plane keeps track of the Pods that belong to any Job and notices if any
such Pod is removed from the API server. To do that, the Job controller creates Pods with
the finalizer `batch.kubernetes.io/job-tracking`. The controller removes the
finalizer only after the Pod has been accounted for in the Job status, allowing
the Pod to be removed by other controllers or users.

The Job controller uses the new algorithm for new Jobs only. Jobs created
before the feature is enabled are unaffected. You can determine if the Job
controller is tracking a Job using Pod finalizers by checking if the Job has the
annotation `batch.kubernetes.io/job-tracking`. You should **not** manually add
or remove this annotation from Jobs.
-->
<p>如果你为你的集群启用了 <code>JobTrackingWithFinalizers</code> 特性，控制面会跟踪属于任何 Job 的 Pod。
并注意是否有任何这样的 Pod 被从 API 服务器上删除。
为了实现这一点，Job 控制器创建的 Pod 带有 Finalizer <code>batch.kubernetes.io/job-tracking</code>。
控制器只有在 Pod 被记入 Job 状态后才会移除 Finalizer，允许 Pod 可以被其他控制器或用户删除。</p>
<p>Job 控制器只对新的 Job 使用新的算法。在启用该特性之前创建的 Job 不受影响。
你可以根据检查 Job 是否含有 <code>batch.kubernetes.io/job-tracking</code> 注解，来确定 Job 控制器是否正在使用 Pod Finalizer 追踪 Job。
你<strong>不</strong>应该给 Job 手动添加或删除该注解。</p>
<!--
## Alternatives

### Bare Pods

When the node that a Pod is running on reboots or fails, the pod is terminated
and will not be restarted.  However, a Job will create new Pods to replace terminated ones.
For this reason, we recommend that you use a Job rather than a bare Pod, even if your application
requires only a single Pod.
-->
<h2 id="alternatives">替代方案 </h2>
<h3 id="bare-pods">裸 Pod </h3>
<p>当 Pod 运行所在的节点重启或者失败，Pod 会被终止并且不会被重启。
Job 会重新创建新的 Pod 来替代已终止的 Pod。
因为这个原因，我们建议你使用 Job 而不是独立的裸 Pod，
即使你的应用仅需要一个 Pod。</p>
<!--
### Replication Controller

Jobs are complementary to [Replication Controllers](/docs/user-guide/replication-controller).
A Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job
manages Pods that are expected to terminate (e.g. batch tasks).

As discussed in [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/), `Job` is *only* appropriate
for pods with `RestartPolicy` equal to `OnFailure` or `Never`.
(Note: If `RestartPolicy` is not set, the default value is `Always`.)
-->
<h3 id="replication-controller">副本控制器   </h3>
<p>Job 与<a href="/zh/docs/concepts/workloads/controllers/replicationcontroller/">副本控制器</a>是彼此互补的。
副本控制器管理的是那些不希望被终止的 Pod （例如，Web 服务器），
Job 管理的是那些希望被终止的 Pod（例如，批处理作业）。</p>
<p>正如在 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">Pod 生命期</a> 中讨论的，
<code>Job</code> 仅适合于 <code>restartPolicy</code> 设置为 <code>OnFailure</code> 或 <code>Never</code> 的 Pod。
注意：如果 <code>restartPolicy</code> 未设置，其默认值是 <code>Always</code>。</p>
<!--
### Single Job starts controller Pod

Another pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort
of custom controller for those Pods.  This allows the most flexibility, but may be somewhat
complicated to get started with and offers less integration with Kubernetes.
-->
<h3 id="单个-job-启动控制器-pod">单个 Job 启动控制器 Pod</h3>
<p>另一种模式是用唯一的 Job 来创建 Pod，而该 Pod 负责启动其他 Pod，因此扮演了一种
后启动 Pod 的控制器的角色。
这种模式的灵活性更高，但是有时候可能会把事情搞得很复杂，很难入门，
并且与 Kubernetes 的集成度很低。</p>
<!--
One example of this pattern would be a Job which starts a Pod which runs a script that in turn
starts a Spark master controller (see [spark example](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md)), runs a spark
driver, and then cleans up.

An advantage of this approach is that the overall process gets the completion guarantee of a Job
object, but maintains complete control over what Pods are created and how work is assigned to them.
-->
<p>这种模式的实例之一是用 Job 来启动一个运行脚本的 Pod，脚本负责启动 Spark
主控制器（参见 <a href="https://github.com/kubernetes/examples/tree/master/staging/spark/README.md">Spark 示例</a>），
运行 Spark 驱动，之后完成清理工作。</p>
<p>这种方法的优点之一是整个过程得到了 Job 对象的完成保障，
同时维持了对创建哪些 Pod、如何向其分派工作的完全控制能力，</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods).
* Read about different ways of running Jobs:
   * [Coarse Parallel Processing Using a Work Queue](/docs/tasks/job/coarse-parallel-processing-work-queue/)
   * [Fine Parallel Processing Using a Work Queue](/docs/tasks/job/fine-parallel-processing-work-queue/)
   * Use an [indexed Job for parallel processing with static work assignment](/docs/tasks/job/indexed-parallel-processing-static/) (beta)
   * Create multiple Jobs based on a template: [Parallel Processing using Expansions](/docs/tasks/job/parallel-processing-expansion/)
* Follow the links within [Clean up finished jobs automatically](#clean-up-finished-jobs-automatically)
  to learn more about how your cluster can clean up completed and / or failed tasks.
* `Job` is part of the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for jobs.
* Read about [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/), which you
  can use to define a series of Jobs that will run based on a schedule, similar to
  the Unix tool `cron`.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。</li>
<li>了解运行 Job 的不同的方式：
<ul>
<li><a href="/zh/docs/tasks/job/coarse-parallel-processing-work-queue/">使用工作队列进行粗粒度并行处理</a></li>
<li><a href="/zh/docs/tasks/job/fine-parallel-processing-work-queue/">使用工作队列进行精细的并行处理</a></li>
<li><a href="/zh/docs/tasks/job/indexed-parallel-processing-static/">使用索引作业完成静态工作分配下的并行处理</a>（Beta 阶段）</li>
<li>基于一个模板运行多个 Job：<a href="/zh/docs/tasks/job/parallel-processing-expansion/">使用展开的方式进行并行处理</a></li>
</ul>
</li>
<li>跟随<a href="#clean-up-finished-jobs-automatically">自动清理完成的 Job</a> 文中的链接，了解你的集群如何清理完成和失败的任务。</li>
<li><code>Job</code> 是 Kubernetes REST API 的一部分。阅读 





<a href=""></a>
对象定义理解关于该资源的 API。</li>
<li>阅读 <a href="/zh/docs/concepts/workloads/controllers/cron-jobs/"><code>CronJob</code></a>，它允许你定义一系列定期运行的 Job，类似于 Unix 工具 <code>cron</code>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4de50a37ebb6f2340484192126cb7a04">2.6 - 已完成 Job 的自动清理</h1>
    
	<!--
title: Automatic Clean-up for Finished Jobs
content_type: concept
weight: 70
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code>
</div>


<!--
TTL-after-finished <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> provides a 
TTL (time to live) mechanism to limit the lifetime of resource objects that 
have finished execution. TTL controller only handles 
<a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Jobs'>Jobs</a>.
-->
<p>TTL-after-finished <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a> 提供了一种 TTL 机制来限制已完成执行的资源对象的生命周期。
TTL 控制器目前只处理 <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a>。</p>
<!-- body -->
<!--
## TTL-after-finished Controller

The TTL-after-finished controller is only supported for Jobs. A cluster operator can use this feature to clean
up finished Jobs (either `Complete` or `Failed`) automatically by specifying the
`.spec.ttlSecondsAfterFinished` field of a Job, as in this
[example](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).
-->
<h2 id="ttl-after-finished-控制器">TTL-after-finished 控制器</h2>
<p>TTL-after-finished 控制器只支持 Job。集群操作员可以通过指定 Job 的 <code>.spec.ttlSecondsAfterFinished</code>
字段来自动清理已结束的作业（<code>Complete</code> 或 <code>Failed</code>），如
<a href="/zh/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">示例</a>
所示。</p>
<!--
The TTL-after-finished controller will assume that a job is eligible to be cleaned up
TTL seconds after the job has finished, in other words, when the TTL has expired. When the
TTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete
its dependent objects together with it. Note that when the job is deleted,
its lifecycle guarantees, such as finalizers, will be honored.
-->
<p>TTL-after-finished 控制器假设作业能在执行完成后的 TTL 秒内被清理，也就是当 TTL 过期后。
当 TTL 控制器清理作业时，它将做级联删除操作，即删除资源对象的同时也删除其依赖对象。
注意，当资源被删除时，由该资源的生命周期保证其终结器（Finalizers）等被执行。</p>
<!--
The TTL seconds can be set at any time. Here are some examples for setting the
`.spec.ttlSecondsAfterFinished` field of a Job:
-->
<p>可以随时设置 TTL 秒。以下是设置 Job 的 <code>.spec.ttlSecondsAfterFinished</code> 字段的一些示例：</p>
<!--
* Specify this field in the job manifest, so that a Job can be cleaned up
  automatically some time after it finishes.
* Set this field of existing, already finished jobs, to adopt this new feature.
* Use a
  [mutating admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)
  to set this field dynamically at job creation time. Cluster administrators can
  use this to enforce a TTL policy for finished jobs.
* Use a
  [mutating admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)
  to set this field dynamically after the job has finished, and choose
  different TTL values based on job status, labels, etc.
-->
<ul>
<li>在作业清单（manifest）中指定此字段，以便 Job 在完成后的某个时间被自动清除。</li>
<li>将此字段设置为现有的、已完成的作业，以采用此新功能。</li>
<li>在创建作业时使用 <a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">mutating admission webhook</a>
动态设置该字段。集群管理员可以使用它对完成的作业强制执行 TTL 策略。</li>
<li>使用 <a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">mutating admission webhook</a>
在作业完成后动态设置该字段，并根据作业状态、标签等选择不同的 TTL 值。</li>
</ul>
<!--
## Caveat

### Updating TTL Seconds

Note that the TTL period, e.g. `.spec.ttlSecondsAfterFinished` field of Jobs,
can be modified after the job is created or has finished. However, once the
Job becomes eligible to be deleted (when the TTL has expired), the system won't
guarantee that the Jobs will be kept, even if an update to extend the TTL
returns a successful API response.
-->
<h2 id="警告">警告</h2>
<h3 id="更新-ttl-秒数">更新 TTL 秒数</h3>
<p>请注意，在创建 Job 或已经执行结束后，仍可以修改其 TTL 周期，例如 Job 的
<code>.spec.ttlSecondsAfterFinished</code> 字段。
但是一旦 Job 变为可被删除状态（当其 TTL 已过期时），即使您通过 API 增加其 TTL
时长得到了成功的响应，系统也不保证 Job 将被保留。</p>
<!--
### Time Skew

Because TTL-after-finished controller uses timestamps stored in the Kubernetes resources to
determine whether the TTL has expired or not, this feature is sensitive to time
skew in the cluster, which may cause TTL-after-finished controller to clean up resource objects
at the wrong time.
-->
<h3 id="time-skew">时间偏差 </h3>
<p>由于 TTL-after-finished 控制器使用存储在 Kubernetes 资源中的时间戳来确定 TTL 是否已过期，
因此该功能对集群中的时间偏差很敏感，这可能导致 TTL-after-finished 控制器在错误的时间清理资源对象。</p>
<!--
Clocks aren't always correct, but the difference should be
very small. Please be aware of this risk when setting a non-zero TTL.
-->
<p>时钟并不总是如此正确，但差异应该很小。
设置非零 TTL 时请注意避免这种风险。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* [Clean up Jobs automatically](/docs/concepts/workloads/controllers/jobs-run-to-completion/#clean-up-finished-jobs-automatically)
* [Design doc](https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md)
-->
<ul>
<li><a href="/zh/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">自动清理 Job</a></li>
<li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md">设计文档</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2e4cec01c525b45eccd6010e21cc76d9">2.7 - CronJob</h1>
    
	<!--
title: CronJob
content_type: concept
weight: 80
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>


<!--
A _CronJob_ creates <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Jobs'>Jobs</a> on a repeating schedule.

One CronJob object is like one line of a _crontab_ (cron table) file. It runs a job periodically
on a given schedule, written in [Cron](https://en.wikipedia.org/wiki/Cron) format.
-->
<p><em>CronJob</em> 创建基于时隔重复调度的 <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Jobs'>Jobs</a>。</p>
<p>一个 CronJob 对象就像 <em>crontab</em> (cron table) 文件中的一行。
它用 <a href="https://en.wikipedia.org/wiki/Cron">Cron</a> 格式进行编写，
并周期性地在给定的调度时间执行 Job。</p>
<!--
All **CronJob** `schedule:` times are based on the timezone of the

If your control plane runs the kube-controller-manager in Pods or bare
containers, the timezone set for the kube-controller-manager container determines the timezone
that the cron job controller uses.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p>所有 <strong>CronJob</strong> 的 <code>schedule:</code> 时间都是基于
<a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>.
的时区。</p>
<p>如果你的控制平面在 Pod 或是裸容器中运行了 kube-controller-manager，
那么为该容器所设置的时区将会决定 Cron Job 的控制器所使用的时区。</p>

</div>

<!--
The [v1 CronJob API](/docs/reference/kubernetes-api/workload-resources/cron-job-v1/)
does not officially support setting timezone as explained above.

Setting variables such as `CRON_TZ` or `TZ` is not officially supported by the Kubernetes project.
`CRON_TZ` or `TZ` is an implementation detail of the internal library being used
for parsing and calculating the next Job creation time. Any usage of it is not
recommended in a production cluster.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p>如 <a href="/zh/docs/reference/kubernetes-api/workload-resources/cron-job-v1/">v1 CronJob API</a> 所述，官方并不支持设置时区。</p>
<p>Kubernetes 项目官方并不支持设置如 <code>CRON_TZ</code> 或者 <code>TZ</code> 等变量。
<code>CRON_TZ</code> 或者 <code>TZ</code> 是用于解析和计算下一个 Job 创建时间所使用的内部库中一个实现细节。
不建议在生产集群中使用它。</p>

</div>

<!--
When creating the manifest for a CronJob resource, make sure the name you provide
is a valid [DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
The name must be no longer than 52 characters. This is because the CronJob controller will automatically
append 11 characters to the job name provided and there is a constraint that the
maximum length of a Job name is no more than 63 characters.
-->
<p>为 CronJob 资源创建清单时，请确保所提供的名称是一个合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>.
名称不能超过 52 个字符。
这是因为 CronJob 控制器将自动在提供的 Job 名称后附加 11 个字符，并且存在一个限制，
即 Job 名称的最大长度不能超过 63 个字符。</p>
<!-- body -->
<!--
## CronJob

CronJobs are meant for performing regular scheduled actions such as backups,
report generation, and so on. Each of those tasks should be configured to recur
indefinitely (for example: once a day / week / month); you can define the point
in time within that interval when the job should start.
-->
<h2 id="cronjob">CronJob</h2>
<p>CronJob 用于执行周期性的动作，例如备份、报告生成等。
这些任务中的每一个都应该配置为周期性重复的（例如：每天/每周/每月一次）；
你可以定义任务开始执行的时间间隔。</p>
<!--
### Example

This example CronJob manifest prints the current time and a hello message every minute:
-->
<h3 id="示例">示例</h3>
<p>下面的 CronJob 示例清单会在每分钟打印出当前时间和问候消息：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/job/cronjob.yaml" download="application/job/cronjob.yaml"><code>application/job/cronjob.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-job-cronjob-yaml')" title="Copy application/job/cronjob.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-job-cronjob-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronJob<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">schedule</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;* * * * *&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">jobTemplate</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- /bin/sh<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- date; echo Hello from the Kubernetes cluster<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<p><a href="/zh/docs/tasks/job/automated-tasks-with-cron-jobs/">使用 CronJob 运行自动化任务</a>
一文会为你详细讲解此例。</p>
<!--
### Cron schedule syntax
-->
<h3 id="cron-时间表语法">Cron 时间表语法</h3>
<pre><code># ┌───────────── 分钟 (0 - 59)
# │ ┌───────────── 小时 (0 - 23)
# │ │ ┌───────────── 月的某天 (1 - 31)
# │ │ │ ┌───────────── 月份 (1 - 12)
# │ │ │ │ ┌───────────── 周的某天 (0 - 6)（周日到周一；在某些系统上，7 也是星期日）
# │ │ │ │ │                          或者是 sun，mon，tue，web，thu，fri，sat
# │ │ │ │ │
# │ │ │ │ │
# * * * * *
</code></pre><!-- 
| Entry 	| Description   | Equivalent to |
| ------------- | ------------- |-------------  |
| @yearly (or @annually) | Run once a year at midnight of 1 January | 0 0 1 1 * |
| @monthly               | Run once a month at midnight of the first day of the month | 0 0 1 * * |
| @weekly                | Run once a week at midnight on Sunday morning | 0 0 * * 0 |
| @daily (or @midnight)  | Run once a day at midnight | 0 0 * * * |
| @hourly                | Run once an hour at the beginning of the hour | 0 * * * * |
-->
<table>
<thead>
<tr>
<th>输入</th>
<th>描述</th>
<th>相当于</th>
</tr>
</thead>
<tbody>
<tr>
<td>@yearly (or @annually)</td>
<td>每年 1 月 1 日的午夜运行一次</td>
<td>0 0 1 1 *</td>
</tr>
<tr>
<td>@monthly</td>
<td>每月第一天的午夜运行一次</td>
<td>0 0 1 * *</td>
</tr>
<tr>
<td>@weekly</td>
<td>每周的周日午夜运行一次</td>
<td>0 0 * * 0</td>
</tr>
<tr>
<td>@daily (or @midnight)</td>
<td>每天午夜运行一次</td>
<td>0 0 * * *</td>
</tr>
<tr>
<td>@hourly</td>
<td>每小时的开始一次</td>
<td>0 * * * *</td>
</tr>
</tbody>
</table>
<!--  
For example, the line below states that the task must be started every Friday at midnight, as well as on the 13th of each month at midnight:
-->
<p>例如，下面这行指出必须在每个星期五的午夜以及每个月 13 号的午夜开始任务：</p>
<p><code>0 0 13 * 5</code></p>
<!--  
To generate CronJob schedule expressions, you can also use web tools like [crontab.guru](https://crontab.guru/).
-->
<p>要生成 CronJob 时间表表达式，你还可以使用 <a href="https://crontab.guru/">crontab.guru</a> 之类的 Web 工具。</p>
<!--
## CronJob Limitations

A cron job creates a job object _about_ once per execution time of its schedule. We say "about" because there
are certain circumstances where two jobs might be created, or no job might be created. We attempt to make these rare,
but do not completely prevent them. Therefore, jobs should be _idempotent_.
-->
<h2 id="cron-job-limitations">CronJob 限制 </h2>
<p>CronJob 根据其计划编排，在每次该执行任务的时候大约会创建一个 Job。
我们之所以说 &quot;大约&quot;，是因为在某些情况下，可能会创建两个 Job，或者不会创建任何 Job。
我们试图使这些情况尽量少发生，但不能完全杜绝。因此，Job 应该是 <em>幂等的</em>。</p>
<!--
If `startingDeadlineSeconds` is set to a large value or left unset (the default)
and if `concurrencyPolicy` is set to `Allow`, the jobs will always run
at least once.
-->
<p>如果 <code>startingDeadlineSeconds</code> 设置为很大的数值或未设置（默认），并且
<code>concurrencyPolicy</code> 设置为 <code>Allow</code>，则作业将始终至少运行一次。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
If `startingDeadlineSeconds` is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.
-->
<p>如果 <code>startingDeadlineSeconds</code> 的设置值低于 10 秒钟，CronJob 可能无法被调度。
这是因为 CronJob 控制器每 10 秒钟执行一次检查。
</div>

<!--
For every CronJob, the CronJob <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs the error
-->
<p>对于每个 CronJob，CronJob <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a>
检查从上一次调度的时间点到现在所错过了调度次数。如果错过的调度次数超过 100 次，
那么它就不会启动这个任务，并记录这个错误:</p>
<pre><code>Cannot determine if job needs to be started. Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
</code></pre><!--
It is important to note that if the `startingDeadlineSeconds` field is set (not `nil`), the controller counts how many missed jobs occurred from the value of `startingDeadlineSeconds` until now rather than from the last scheduled time until now. For example, if `startingDeadlineSeconds` is `200`, the controller counts how many missed jobs occurred in the last 200 seconds.
-->
<p>需要注意的是，如果 <code>startingDeadlineSeconds</code> 字段非空，则控制器会统计从
<code>startingDeadlineSeconds</code> 设置的值到现在而不是从上一个计划时间到现在错过了多少次 Job。
例如，如果 <code>startingDeadlineSeconds</code> 是 <code>200</code>，则控制器会统计在过去 200 秒中错过了多少次 Job。</p>
<!--
A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, If `concurrencyPolicy` is set to `Forbid` and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.
-->
<p>如果未能在调度时间内创建 CronJob，则计为错过。
例如，如果 <code>concurrencyPolicy</code> 被设置为 <code>Forbid</code>，并且当前有一个调度仍在运行的情况下，
试图调度的 CronJob 将被计算为错过。</p>
<!--
For example, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its
`startingDeadlineSeconds` field is not set. If the CronJob controller happens to
be down from `08:29:00` to `10:21:00`, the job will not start as the number of missed jobs which missed their schedule is greater than 100.
-->
<p>例如，假设一个 CronJob 被设置为从 <code>08:30:00</code> 开始每隔一分钟创建一个新的 Job，
并且它的 <code>startingDeadlineSeconds</code> 字段未被设置。如果 CronJob 控制器从
<code>08:29:00</code> 到 <code>10:21:00</code> 终止运行，则该 Job 将不会启动，因为其错过的调度
次数超过了 100。</p>
<!--
To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its
`startingDeadlineSeconds` is set to 200 seconds. If the CronJob controller happens to
be down for the same period as the previous example (`08:29:00` to `10:21:00`,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (ie, 3 missed schedules), rather than from the last scheduled time until now.
-->
<p>为了进一步阐述这个概念，假设将 CronJob 设置为从 <code>08:30:00</code> 开始每隔一分钟创建一个新的 Job，
并将其 <code>startingDeadlineSeconds</code> 字段设置为 200 秒。
如果 CronJob 控制器恰好在与上一个示例相同的时间段（<code>08:29:00</code> 到 <code>10:21:00</code>）终止运行，
则 Job 仍将从 <code>10:22:00</code> 开始。
造成这种情况的原因是控制器现在检查在最近 200 秒（即 3 个错过的调度）中发生了多少次错过的
Job 调度，而不是从现在为止的最后一个调度时间开始。</p>
<!--
The CronJob is only responsible for creating Jobs that match its schedule, and
the Job in turn is responsible for the management of the Pods it represents.
-->
<p>CronJob 仅负责创建与其调度时间相匹配的 Job，而 Job 又负责管理其代表的 Pod。</p>
<!--
## Controller version {#new-controller}

Starting with Kubernetes v1.21 the second version of the CronJob controller
is the default implementation. To disable the default CronJob controller
and use the original CronJob controller instead, one pass the `CronJobControllerV2`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
flag to the <a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>,
and set this flag to `false`. For example:
-->
<h2 id="new-controller">控制器版本  </h2>
<p>从 Kubernetes v1.21 版本开始，CronJob 控制器的第二个版本被用作默认实现。
要禁用此默认 CronJob 控制器而使用原来的 CronJob 控制器，请在
<a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>
中设置<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>CronJobControllerV2</code>，将此标志设置为 <code>false</code>。例如：</p>
<pre><code>--feature-gates=&quot;CronJobControllerV2=false&quot;
</code></pre><h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods/) and
  [Jobs](/docs/concepts/workloads/controllers/job/), two concepts
  that CronJobs rely upon.
* Read about the [format](https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format)
  of CronJob `.spec.schedule` fields.
* For instructions on creating and working with CronJobs, and for an example
  of a CronJob manifest,
  see [Running automated tasks with CronJobs](/docs/tasks/job/automated-tasks-with-cron-jobs/).
* For instructions to clean up failed or completed jobs automatically,
  see [Clean up Jobs automatically](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)
* `CronJob` is part of the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for Kubernetes cron jobs.
-->
<ul>
<li>了解 CronJob 所依赖的 <a href="/zh/docs/concepts/workloads/pods/">Pods</a> 与 <a href="/zh/docs/concepts/workloads/controllers/job/">Job</a> 的概念。</li>
<li>阅读 CronJob <code>.spec.schedule</code> 字段的<a href="https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format">格式</a>。</li>
<li>有关创建和使用 CronJob 的说明及示例规约文件，请参见
<a href="/zh/docs/tasks/job/automated-tasks-with-cron-jobs/">使用 CronJob 运行自动化任务</a>。</li>
<li>有关自动清理失败或完成作业的说明，请参阅<a href="/zh/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">自动清理作业</a></li>
<li><code>CronJob</code> 是 Kubernetes REST API 的一部分，
阅读 





<a href=""></a>
对象定义以了解关于该资源的 API。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-27f1331d515d95f76aa1156088b4ad91">2.8 - ReplicationController</h1>
    
	<!--
reviewers:
- bprashanth
- janetkuo
title: ReplicationController
feature:
  title: Self-healing
  anchor: How a ReplicationController Works
  description: >
    Restarts containers that fail, replaces and reschedules containers when nodes die, kills containers that don't respond to your user-defined health check, and doesn't advertise them to clients until they are ready to serve.

content_type: concept
weight: 90
-->
<!-- overview -->
<!--
A [`Deployment`](/docs/concepts/workloads/controllers/deployment/) that configures a [`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is now the recommended way to set up replication.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 现在推荐使用配置 <a href="/zh/docs/concepts/workloads/controllers/replicaset/"><code>ReplicaSet</code></a> 的
<a href="/zh/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> 来建立副本管理机制。
</div>
<!--
A _ReplicationController_ ensures that a specified number of pod replicas are running at any one
time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is
always up and available.
-->
<p><em>ReplicationController</em> 确保在任何时候都有特定数量的 Pod 副本处于运行状态。
换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。</p>
<!-- body -->
<!--
## How a ReplicationController Works

If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the
ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a
ReplicationController are automatically replaced if they fail, are deleted, or are terminated.
For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.
For this reason, you should use a ReplicationController even if your application requires
only a single pod. A ReplicationController is similar to a process supervisor,
but instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods
across multiple nodes.
-->
<h2 id="replicationcontroller-如何工作">ReplicationController 如何工作</h2>
<p>当 Pod 数量过多时，ReplicationController 会终止多余的 Pod。当 Pod 数量太少时，ReplicationController 将会启动新的 Pod。
与手动创建的 Pod 不同，由 ReplicationController 创建的 Pod 在失败、被删除或被终止时会被自动替换。
例如，在中断性维护（如内核升级）之后，你的 Pod 会在节点上重新创建。
因此，即使你的应用程序只需要一个 Pod，你也应该使用 ReplicationController 创建 Pod。
ReplicationController 类似于进程管理器，但是 ReplicationController 不是监控单个节点上的单个进程，而是监控跨多个节点的多个 Pod。</p>
<!--
ReplicationController is often abbreviated to "rc" in discussion, and as a shortcut in
kubectl commands.

A simple case is to create one ReplicationController object to reliably run one instance of
a Pod indefinitely.  A more complex use case is to run several identical replicas of a replicated
service, such as web servers.
-->
<p>在讨论中，ReplicationController 通常缩写为 &quot;rc&quot;，并作为 kubectl 命令的快捷方式。</p>
<p>一个简单的示例是创建一个 ReplicationController 对象来可靠地无限期地运行 Pod 的一个实例。
更复杂的用例是运行一个多副本服务（如 web 服务器）的若干相同副本。</p>
<!--
## Running an example ReplicationController

This example ReplicationController config runs three copies of the nginx web server.
-->
<h2 id="运行一个示例-replicationcontroller">运行一个示例 ReplicationController</h2>
<p>这个示例 ReplicationController 配置运行 nginx Web 服务器的三个副本。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/replication.yaml" download="controllers/replication.yaml"><code>controllers/replication.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-replication-yaml')" title="Copy controllers/replication.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-replication-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ReplicationController<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Run the example job by downloading the example file and then running this command:
-->
<p>通过下载示例文件并运行以下命令来运行示例任务:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/controllers/replication.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>replicationcontroller/nginx created
</code></pre><!--
Check on the status of the ReplicationController using this command:
-->
<p>使用以下命令检查 ReplicationController 的状态:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe replicationcontrollers/nginx
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &lt;none&gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
</code></pre><!--
Here, three pods are created, but none is running yet, perhaps because the image is being pulled.
A little later, the same command may show:
-->
<p>在这里，创建了三个 Pod，但没有一个 Pod 正在运行，这可能是因为正在拉取镜像。
稍后，相同的命令可能会显示：</p>
<pre><code>Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
</code></pre><!--
To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:
-->
<p>要以机器可读的形式列出属于 ReplicationController 的所有 Pod，可以使用如下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">pods</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get pods --selector<span style="color:#666">=</span><span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx --output<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">={</span>.items..metadata.name<span style="color:#666">}</span><span style="color:#a2f;font-weight:bold">)</span>
<span style="color:#a2f">echo</span> <span style="color:#b8860b">$pods</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>nginx-3ntk0 nginx-4ok8v nginx-qrm3m
</code></pre><!--
Here, the selector is the same as the selector for the ReplicationController (seen in the
`kubectl describe` output), and in a different form in `replication.yaml`.  The `--output=jsonpath` option
specifies an expression with the name from each pod in the returned list.
-->
<p>这里，选择算符与 ReplicationController 的选择算符相同（参见 <code>kubectl describe</code> 输出），并以不同的形式出现在 <code>replication.yaml</code> 中。
<code>--output=jsonpath</code> 选项指定了一个表达式，仅从返回列表中的每个 Pod 中获取名称。</p>
<!--
## Writing a ReplicationController Spec

As with all other Kubernetes config, a ReplicationController needs `apiVersion`, `kind`, and `metadata` fields.
For general information about working with configuration files, see [object management](/docs/concepts/overview/working-with-objects/object-management/).

A ReplicationController also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
-->
<h2 id="编写一个-replicationcontroller-规约">编写一个 ReplicationController 规约</h2>
<p>与所有其它 Kubernetes 配置一样，ReplicationController 需要 <code>apiVersion</code>、
<code>kind</code> 和 <code>metadata</code> 字段。
有关使用配置文件的常规信息，参考
<a href="/zh/docs/concepts/overview/working-with-objects/object-management/">对象管理</a>。</p>
<p>ReplicationController 也需要一个 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> 部分</a>。</p>
<!--
### Pod Template

The `.spec.template` is the only required field of the `.spec`.

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/pod-overview/#pod-templates). It has exactly the same schema as a [pod](/docs/concepts/workloads/pods/pod/), except it is nested and does not have an `apiVersion` or `kind`.
-->
<h3 id="pod-template">Pod 模板 </h3>
<p><code>.spec.template</code> 是 <code>.spec</code> 的唯一必需字段。</p>
<p><code>.spec.template</code> 是一个 <a href="/zh/docs/concepts/workloads/pods/#pod-templates">Pod 模板</a>。
它的模式与 <a href="/zh/docs/concepts/workloads/pods/">Pod</a> 完全相同，只是它是嵌套的，没有 <code>apiVersion</code> 或 <code>kind</code> 属性。</p>
<!--
In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [pod selector](#pod-selector).

Only a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is allowed, which is the default if not specified.

For local container restarts, ReplicationControllers delegate to an agent on the node,
for example the [Kubelet](/docs/admin/kubelet/) or Docker.
-->
<p>除了 Pod 所需的字段外，ReplicationController 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。
对于标签，请确保不与其他控制器重叠。参考 <a href="#pod-selector">Pod 选择算符</a>。</p>
<p>只允许 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>.spec.template.spec.restartPolicy</code></a> 等于 <code>Always</code>，如果没有指定，这是默认值。</p>
<p>对于本地容器重启，ReplicationController 委托给节点上的代理，
例如 <a href="/zh/docs/reference/command-line-tools-reference/kubelet/">Kubelet</a> 或 Docker。</p>
<!--
### Labels on the ReplicationController

The ReplicationController can itself have labels (`.metadata.labels`).  Typically, you
would set these the same as the `.spec.template.metadata.labels`; if `.metadata.labels` is not specified
then it defaults to  `.spec.template.metadata.labels`. However, they are allowed to be
different, and the `.metadata.labels` do not affect the behavior of the ReplicationController.
-->
<h3 id="replicationcontroller-上的标签">ReplicationController 上的标签</h3>
<p>ReplicationController 本身可以有标签 （<code>.metadata.labels</code>）。
通常，你可以将这些设置为 <code>.spec.template.metadata.labels</code>；
如果没有指定 <code>.metadata.labels</code> 那么它默认为 <code>.spec.template.metadata.labels</code>。<br>
但是，Kubernetes 允许它们是不同的，<code>.metadata.labels</code> 不会影响 ReplicationController 的行为。</p>
<!--
### Pod Selector

The `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors). A ReplicationController
manages all the pods with labels that match the selector. It does not distinguish
between pods that it created or deleted and pods that another person or process created or
deleted. This allows the ReplicationController to be replaced without affecting the running pods.
-->
<h3 id="pod-selector">Pod 选择算符</h3>
<p><code>.spec.selector</code> 字段是一个<a href="/zh/docs/concepts/overview/working-with-objects/labels/#label-selectors">标签选择算符</a>。
ReplicationController 管理标签与选择算符匹配的所有 Pod。
它不区分它创建或删除的 Pod 和其他人或进程创建或删除的 Pod。
这允许在不影响正在运行的 Pod 的情况下替换 ReplicationController。</p>
<!--
If specified, the `.spec.template.metadata.labels` must be equal to the `.spec.selector`, or it will
be rejected by the API.  If `.spec.selector` is unspecified, it will be defaulted to
`.spec.template.metadata.labels`.
-->
<p>如果指定了 <code>.spec.template.metadata.labels</code>，它必须和 <code>.spec.selector</code> 相同，否则它将被 API 拒绝。
如果没有指定 <code>.spec.selector</code>，它将默认为 <code>.spec.template.metadata.labels</code>。</p>
<!--
Also you should not normally create any pods whose labels match this selector, either directly, with
another ReplicationController, or with another controller such as Job. If you do so, the
ReplicationController thinks that it created the other pods.  Kubernetes does not stop you
from doing this.

If you do end up with multiple controllers that have overlapping selectors, you
will have to manage the deletion yourself (see [below](#working-with-replicationcontrollers)).
-->
<p>另外，通常不应直接使用另一个 ReplicationController 或另一个控制器（例如 Job）
来创建其标签与该选择算符匹配的任何 Pod。如果这样做，ReplicationController 会认为它创建了这些 Pod。
Kubernetes 并没有阻止你这样做。</p>
<p>如果你的确创建了多个控制器并且其选择算符之间存在重叠，那么你将不得不自己管理删除操作（参考<a href="#working-with-replicationcontrollers">后文</a>）。</p>
<!--
### Multiple Replicas

You can specify how many pods should run concurrently by setting `.spec.replicas` to the number
of pods you would like to have running concurrently.  The number running at any time may be higher
or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully
shutdown, and a replacement starts early.

If you do not specify `.spec.replicas`, then it defaults to 1.
-->
<h3 id="多个副本">多个副本</h3>
<p>你可以通过设置 <code>.spec.replicas</code> 来指定应该同时运行多少个 Pod。
在任何时候，处于运行状态的 Pod 个数都可能高于或者低于设定值。例如，副本个数刚刚被增加或减少时，或者一个 Pod 处于优雅终止过程中而其替代副本已经提前开始创建时。</p>
<p>如果你没有指定 <code>.spec.replicas</code> ，那么它默认是 1。</p>
<!--
## Working with ReplicationControllers

### Deleting a ReplicationController and its Pods

To delete a ReplicationController and all its pods, use [`kubectl
delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  Kubectl will scale the ReplicationController to zero and wait
for it to delete each pod before deleting the ReplicationController itself.  If this kubectl
command is interrupted, it can be restarted.

When using the REST API or [client library](/docs/reference/using-api/client-libraries), you need to do the steps explicitly (scale replicas to
0, wait for pod deletions, then delete the ReplicationController).
-->
<h2 id="working-with-replicationcontrollers">使用 ReplicationController</h2>
<h3 id="删除一个-replicationcontroller-以及它的-pod">删除一个 ReplicationController 以及它的 Pod</h3>
<p>要删除一个 ReplicationController 以及它的 Pod，使用
<a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>。
kubectl 将 ReplicationController 缩放为 0 并等待以便在删除 ReplicationController 本身之前删除每个 Pod。
如果这个 kubectl 命令被中断，可以重新启动它。</p>
<p>当使用 REST API 或<a href="/zh/docs/reference/using-api/client-libraries">客户端库</a>时，你需要明确地执行这些步骤（缩放副本为 0、
等待 Pod 删除，之后删除 ReplicationController 资源）。</p>
<!--
### Deleting only a ReplicationController

You can delete a ReplicationController without affecting any of its pods.

Using kubectl, specify the `--cascade=orphan` option to [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).

When using the REST API or [client library](/docs/reference/using-api/client-libraries), you can delete the ReplicationController object.
-->
<h3 id="只删除-replicationcontroller">只删除 ReplicationController</h3>
<p>你可以删除一个 ReplicationController 而不影响它的任何 Pod。</p>
<p>使用 kubectl，为 <a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a> 指定 <code>--cascade=orphan</code> 选项。</p>
<p>当使用 REST API 或客户端库(/zh/docs/reference/using-api/client-libraries)时，只需删除 ReplicationController 对象。</p>
<!--
Once the original is deleted, you can create a new ReplicationController to replace it.  As long
as the old and new `.spec.selector` are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update pods to a new spec in a controlled way, use a [rolling update](#rolling-updates).
-->
<p>一旦原始对象被删除，你可以创建一个新的 ReplicationController 来替换它。
只要新的和旧的 <code>.spec.selector</code> 相同，那么新的控制器将领养旧的 Pod。
但是，它不会做出任何努力使现有的 Pod 匹配新的、不同的 Pod 模板。
如果希望以受控方式更新 Pod 以使用新的 spec，请执行<a href="#rolling-updates">滚动更新</a>操作。</p>
<!--
### Isolating pods from a ReplicationController

Pods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).
-->
<h3 id="从-replicationcontroller-中隔离-pod">从 ReplicationController 中隔离 Pod</h3>
<p>通过更改 Pod 的标签，可以从 ReplicationController 的目标中删除 Pod。
此技术可用于从服务中删除 Pod 以进行调试、数据恢复等。以这种方式删除的 Pod
将被自动替换（假设复制副本的数量也没有更改）。</p>
<!--
## Common usage patterns
-->
<h2 id="常见的使用模式">常见的使用模式</h2>
<!--
### Rescheduling

As mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).
-->
<h3 id="rescheduling">重新调度  </h3>
<p>如上所述，无论你想要继续运行 1 个 Pod 还是 1000 个 Pod，一个 ReplicationController 都将确保存在指定数量的 Pod，即使在节点故障或 Pod 终止(例如，由于另一个控制代理的操作)的情况下也是如此。</p>
<!--
### Scaling

The ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by updating the `replicas` field.
-->
<h3 id="scaling">扩缩容  </h3>
<p>通过设置 <code>replicas</code> 字段，ReplicationController 可以允许扩容或缩容副本的数量。
你可以手动或通过自动缩放控制代理来控制 ReplicationController 执行此操作。</p>
<!--
### Rolling updates

The ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.

As explained in [#1353](http://issue.k8s.io/1353), the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.
-->
<h3 id="rolling-updates">滚动更新</h3>
<p>ReplicationController 的设计目的是通过逐个替换 Pod 以方便滚动更新服务。</p>
<p>如 <a href="https://issue.k8s.io/1353">#1353</a> PR 中所述，建议的方法是使用 1 个副本创建一个新的 ReplicationController，
逐个扩容新的（+1）和缩容旧的（-1）控制器，然后在旧的控制器达到 0 个副本后将其删除。
这一方法能够实现可控的 Pod 集合更新，即使存在意外失效的状况。</p>
<!--
Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.

The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.

Rolling update is implemented in the client tool
[`kubectl rolling-update`](/docs/reference/generated/kubectl/kubectl-commands#rolling-update). Visit [`kubectl rolling-update` task](/docs/tasks/run-application/rolling-update-replication-controller/) for more concrete examples.
-->
<p>理想情况下，滚动更新控制器将考虑应用程序的就绪情况，并确保在任何给定时间都有足够数量的 Pod 有效地提供服务。</p>
<p>这两个 ReplicationController 将需要创建至少具有一个不同标签的 Pod，比如 Pod 主要容器的镜像标签，因为通常是镜像更新触发滚动更新。</p>
<p>滚动更新是在客户端工具 <a href="/docs/reference/generated/kubectl/kubectl-commands#rolling-update"><code>kubectl rolling-update</code></a>
中实现的。访问 <a href="/zh/docs/tasks/run-application/rolling-update-replication-controller/"><code>kubectl rolling-update</code> 任务</a>以获得更多的具体示例。</p>
<!--
### Multiple release tracks

In addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.

For instance, a service might target all pods with `tier in (frontend), environment in (prod)`.  Now say you have 10 replicated pods that make up this tier.  But you want to be able to 'canary' a new version of this component.  You could set up a ReplicationController with `replicas` set to 9 for the bulk of the replicas, with labels `tier=frontend, environment=prod, track=stable`, and another ReplicationController with `replicas` set to 1 for the canary, with labels `tier=frontend, environment=prod, track=canary`.  Now the service is covering both the canary and non-canary pods.  But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.
-->
<h3 id="多个版本跟踪">多个版本跟踪</h3>
<p>除了在滚动更新过程中运行应用程序的多个版本之外，通常还会使用多个版本跟踪来长时间，
甚至持续运行多个版本。这些跟踪将根据标签加以区分。</p>
<p>例如，一个服务可能把具有 <code>tier in (frontend), environment in (prod)</code> 的所有 Pod 作为目标。
现在假设你有 10 个副本的 Pod 组成了这个层。但是你希望能够 <code>canary</code> （<code>金丝雀</code>）发布这个组件的新版本。
你可以为大部分副本设置一个 ReplicationController，其中 <code>replicas</code> 设置为 9，
标签为 <code>tier=frontend, environment=prod, track=stable</code> 而为 <code>canary</code>
设置另一个 ReplicationController，其中 <code>replicas</code> 设置为 1，
标签为 <code>tier=frontend, environment=prod, track=canary</code>。
现在这个服务覆盖了 <code>canary</code> 和非 <code>canary</code> Pod。但你可以单独处理
ReplicationController，以测试、监控结果等。</p>
<!--
### Using ReplicationControllers with Services

Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic
goes to the old version, and some goes to the new version.

A ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.
-->
<h3 id="和服务一起使用-replicationcontroller">和服务一起使用 ReplicationController</h3>
<p>多个 ReplicationController 可以位于一个服务的后面，例如，一部分流量流向旧版本，
一部分流量流向新版本。</p>
<p>一个 ReplicationController 永远不会自行终止，但它不会像服务那样长时间存活。
服务可以由多个 ReplicationController 控制的 Pod 组成，并且在服务的生命周期内
（例如，为了执行 Pod 更新而运行服务），可以创建和销毁许多 ReplicationController。
服务本身和它们的客户端都应该忽略负责维护服务 Pod 的 ReplicationController 的存在。</p>
<!--
## Writing programs for Replication

Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the [RabbitMQ work queues](https://www.rabbitmq.com/tutorials/tutorial-two-python.html), as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.
-->
<h2 id="编写多副本的应用">编写多副本的应用</h2>
<p>由 ReplicationController 创建的 Pod 是可替换的，语义上是相同的，
尽管随着时间的推移，它们的配置可能会变得异构。
这显然适合于多副本的无状态服务器，但是 ReplicationController 也可以用于维护主选、
分片和工作池应用程序的可用性。
这样的应用程序应该使用动态的工作分配机制，例如
<a href="https://www.rabbitmq.com/tutorials/tutorial-two-python.html">RabbitMQ 工作队列</a>，
而不是静态的或者一次性定制每个 Pod 的配置，这被认为是一种反模式。
执行的任何 Pod 定制，例如资源的垂直自动调整大小（例如，CPU 或内存），
都应该由另一个在线控制器进程执行，这与 ReplicationController 本身没什么不同。</p>
<!--
## Responsibilities of the ReplicationController

The ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, [readiness](http://issue.k8s.io/620) and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.
-->
<h2 id="replicationcontroller-的职责">ReplicationController 的职责</h2>
<p>ReplicationController 仅确保所需的 Pod 数量与其标签选择算符匹配，并且是可操作的。
目前，它的计数中只排除终止的 Pod。
未来，可能会考虑系统提供的<a href="https://issue.k8s.io/620">就绪状态</a>和其他信息，
我们可能会对替换策略添加更多控制，
我们计划发出事件，这些事件可以被外部客户端用来实现任意复杂的替换和/或缩减策略。</p>
<!--
The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in [#492](http://issue.k8s.io/492)), which would change its `replicas` field. We will not add scheduling policies (for example, [spreading](http://issue.k8s.io/367#issuecomment-48428019)) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation ([#170](http://issue.k8s.io/170)).
-->
<p>ReplicationController 永远被限制在这个狭隘的职责范围内。
它本身既不执行就绪态探测，也不执行活跃性探测。
它不负责执行自动缩放，而是由外部自动缩放器控制（如
<a href="https://issue.k8s.io/492">#492</a> 中所述），后者负责更改其 <code>replicas</code> 字段值。
我们不会向 ReplicationController 添加调度策略(例如，
<a href="https://issue.k8s.io/367#issuecomment-48428019">spreading</a>)。
它也不应该验证所控制的 Pod 是否与当前指定的模板匹配，因为这会阻碍自动调整大小和其他自动化过程。
类似地，完成期限、整理依赖关系、配置扩展和其他特性也属于其他地方。
我们甚至计划考虑批量创建 Pod 的机制（查阅 <a href="https://issue.k8s.io/170">#170</a>）。</p>
<!--
The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The "macro" operations currently supported by kubectl (run, scale, rolling-update) are proof-of-concept examples of this. For instance, we could imagine something like [Asgard](https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1) managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.
-->
<p>ReplicationController 旨在成为可组合的构建基元。
我们希望在它和其他补充原语的基础上构建更高级别的 API 或者工具，以便于将来的用户使用。
kubectl 目前支持的 &quot;macro&quot; 操作（运行、缩放、滚动更新）就是这方面的概念示例。
例如，我们可以想象类似于 <a href="https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1">Asgard</a>
的东西管理 ReplicationController、自动定标器、服务、调度策略、金丝雀发布等。</p>
<!--
## API Object

Replication controller is a top-level resource in the Kubernetes REST API. More details about the
API object can be found at:
[ReplicationController API object](/docs/reference/generated/kubernetes-api/v1.23/#replicationcontroller-v1-core).
-->
<h2 id="api-对象">API 对象</h2>
<p>在 Kubernetes REST API 中 Replication controller 是顶级资源。
更多关于 API 对象的详细信息可以在
<a href="/docs/reference/generated/kubernetes-api/v1.23/#replicationcontroller-v1-core">ReplicationController API 对象</a>找到。</p>
<!--
## Alternatives to ReplicationController
### ReplicaSet

[`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is the next-generation ReplicationController that supports the new [set-based label selector](/docs/concepts/overview/working-with-objects/labels/#set-based-requirement).
It’s mainly used by [Deployment](/docs/concepts/workloads/controllers/deployment/) as a mechanism to orchestrate Pod creation, deletion and updates.
Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don’t require updates at all.
-->
<h2 id="replicationcontroller-的替代方案">ReplicationController 的替代方案</h2>
<h3 id="replicaset">ReplicaSet</h3>
<p><a href="/zh/docs/concepts/workloads/controllers/replicaset/"><code>ReplicaSet</code></a> 是下一代 ReplicationController，
支持新的<a href="/zh/docs/concepts/overview/working-with-objects/labels/#set-based-requirement">基于集合的标签选择算符</a>。
它主要被 <a href="/zh/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a>
用来作为一种编排 Pod 创建、删除及更新的机制。
请注意，我们推荐使用 Deployment 而不是直接使用 ReplicaSet，除非
你需要自定义更新编排或根本不需要更新。</p>
<!--
### Deployment (Recommended)

[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is a higher-level API object that updates its underlying Replica Sets and their Pods.
Deployments are recommended if you want the rolling update functionality,
because they are declarative, server-side, and have additional features.
-->
<h3 id="deployment-推荐">Deployment （推荐）</h3>
<p><a href="/zh/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> 是一种更高级别的 API 对象，用于更新其底层 ReplicaSet 及其 Pod。
如果你想要这种滚动更新功能，那么推荐使用 Deployment，因为它们是声明式的、服务端的，并且具有其它特性。</p>
<!--
### Bare Pods

Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node.  A ReplicationController delegates local container restarts to some agent on the node (for example, Kubelet or Docker).
-->
<h3 id="裸-pod">裸 Pod</h3>
<p>与用户直接创建 Pod 的情况不同，ReplicationController 能够替换因某些原因
被删除或被终止的 Pod ，例如在节点故障或中断节点维护的情况下，例如内核升级。
因此，我们建议你使用 ReplicationController，即使你的应用程序只需要一个 Pod。
可以将其看作类似于进程管理器，它只管理跨多个节点的多个 Pod ，而不是单个节点上的单个进程。
ReplicationController 将本地容器重启委托给节点上的某个代理(例如，Kubelet 或 Docker)。</p>
<!--
### Job

Use a [`Job`](/docs/concepts/jobs/run-to-completion-finite-workloads/) instead of a ReplicationController for Pods that are expected to terminate on their own
(that is, batch jobs).
-->
<h3 id="job">Job</h3>
<p>对于预期会自行终止的 Pod (即批处理任务)，使用
<a href="/zh/docs/concepts/workloads/controllers/job/"><code>Job</code></a> 而不是 ReplicationController。</p>
<!--
### DaemonSet

Use a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicationController for pods that provide a
machine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.
-->
<h3 id="daemonset">DaemonSet</h3>
<p>对于提供机器级功能（例如机器监控或机器日志记录）的 Pod，
使用 <a href="/zh/docs/concepts/workloads/controllers/daemonset/"><code>DaemonSet</code></a> 而不是
ReplicationController。
这些 Pod 的生命期与机器的生命期绑定：它们需要在其他 Pod 启动之前在机器上运行，
并且在机器准备重新启动或者关闭时安全地终止。</p>
<!--
## What's next

* Learn about [Pods](/docs/concepts/workloads/pods).
* Learn about [Deployment](/docs/concepts/workloads/controllers/deployment/), the replacement
  for ReplicationController.
* `ReplicationController` is part of the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for replication controllers.
-->
<h2 id="what-s-next">What's next</h2>
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。</li>
<li>了解 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Depolyment</a>，ReplicationController 的替代品。</li>
<li><code>ReplicationController</code> 是 Kubernetes REST API 的一部分，阅读 





<a href=""></a>
对象定义以了解 replication controllers 的 API。</li>
</ul>

</div>



    
	
  

    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      
      <a class="text-white" href="/zh/docs/home/">主页</a>
      
      <a class="text-white" href="/zh/blog/">博客</a>
      
      <a class="text-white" href="/zh/training/">培训</a>
      
      <a class="text-white" href="/zh/partners/">合作伙伴</a>
      
      <a class="text-white" href="/zh/community/">社区</a>
      
      <a class="text-white" href="/zh/case-studies/">案例分析</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" href="https://discuss.kubernetes.io">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/kubernetesio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://youtube.com/kubernetescommunity">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/kubernetes/kubernetes">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://slack.k8s.io">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Contribute" aria-label="Contribute">
    <a class="text-white" target="_blank" href="https://git.k8s.io/community/contributors/guide">
      <i class="fas fa-edit"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" href="https://stackoverflow.com/questions/tagged/kubernetes">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2024 The Kubernetes Authors | Documentation Distributed under <a href="https://git.k8s.io/website/LICENSE" class="light-text">CC BY 4.0</a></small>
        <br/>
        <small class="text-white">Copyright &copy; 2024 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage" class="light-text">Trademark Usage page</a></small>
        <br/>
        <small class="text-white">ICP license: 京ICP备17074266号-3</small>
        
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>










<script src="/js/main.js"></script>






  </body>
</html>
