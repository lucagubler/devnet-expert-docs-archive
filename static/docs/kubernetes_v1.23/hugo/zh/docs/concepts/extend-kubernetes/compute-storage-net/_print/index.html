<!doctype html>
<html lang="zh" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>


<link rel="alternate" hreflang="en" href="http://localhost:1313/docs/concepts/extend-kubernetes/compute-storage-net/">
<link rel="alternate" hreflang="ko" href="http://localhost:1313/ko/docs/concepts/extend-kubernetes/compute-storage-net/">
<link rel="alternate" hreflang="fr" href="http://localhost:1313/fr/docs/concepts/extend-kubernetes/compute-storage-net/">
<link rel="alternate" hreflang="es" href="http://localhost:1313/es/docs/concepts/extend-kubernetes/compute-storage-net/">
<link rel="alternate" hreflang="pt-br" href="http://localhost:1313/pt-br/docs/concepts/extend-kubernetes/compute-storage-net/">
<link rel="alternate" hreflang="id" href="http://localhost:1313/id/docs/concepts/extend-kubernetes/compute-storage-net/">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.87.0" />
<link rel="canonical" type="text/html" href="http://localhost:1313/zh/docs/concepts/extend-kubernetes/compute-storage-net/">
<link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="apple-touch-icon" href="/images/kubernetes-192x192.png">
<title>计算、存储和网络扩展 | Kubernetes</title><meta property="og:title" content="计算、存储和网络扩展" />
<meta property="og:description" content="生产级别的容器编排系统" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/zh/docs/concepts/extend-kubernetes/compute-storage-net/" /><meta property="og:site_name" content="Kubernetes" />

<meta itemprop="name" content="计算、存储和网络扩展">
<meta itemprop="description" content="生产级别的容器编排系统"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="计算、存储和网络扩展"/>
<meta name="twitter:description" content="生产级别的容器编排系统"/>






<link href="/scss/main.css" rel="stylesheet">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "http://localhost:1313/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href="/css/feature-states.css">



<meta name="description" content="">
<meta property="og:description" content="">
<meta name="twitter:description" content="">
<meta property="og:url" content="http://localhost:1313/zh/docs/concepts/extend-kubernetes/compute-storage-net/">
<meta property="og:title" content="计算、存储和网络扩展">
<meta name="twitter:title" content="计算、存储和网络扩展">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">

<script src="/js/script.js"></script>


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/zh/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/zh/docs/" >文档</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/blog/" >Kubernetes 博客</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/training/" >培训</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/partners/" >合作伙伴</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/community/" >社区</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/case-studies/" >案例分析</a>
			</li>
			
			
			
			<li class="nav-item dropdown">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/releases">Release Information</a>
	
	<a class="dropdown-item" href="https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/">v1.23</a>
	
	<a class="dropdown-item" href="https://v1-22.docs.kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/">v1.22</a>
	
	<a class="dropdown-item" href="https://v1-21.docs.kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/">v1.21</a>
	
	<a class="dropdown-item" href="https://v1-20.docs.kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/">v1.20</a>
	
	<a class="dropdown-item" href="https://v1-19.docs.kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/">v1.19</a>
	
</div>
			</li>
			
			
			<li class="nav-item dropdown">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/concepts/extend-kubernetes/compute-storage-net/">English</a>
	
	<a class="dropdown-item" href="/ko/docs/concepts/extend-kubernetes/compute-storage-net/">한국어 Korean</a>
	
	<a class="dropdown-item" href="/fr/docs/concepts/extend-kubernetes/compute-storage-net/">Français</a>
	
	<a class="dropdown-item" href="/es/docs/concepts/extend-kubernetes/compute-storage-net/">Español</a>
	
	<a class="dropdown-item" href="/pt-br/docs/concepts/extend-kubernetes/compute-storage-net/">Português</a>
	
	<a class="dropdown-item" href="/id/docs/concepts/extend-kubernetes/compute-storage-net/">Bahasa Indonesia</a>
	
</div>

			</li>
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">计算、存储和网络扩展</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-1ac2260db9ecccbf0303a899bc27ce6d">网络插件</a></li>


    
  
    
    
	
<li>2: <a href="#pg-53e1ea8892ceca307ba19e8d6a7b8d32">设备插件</a></li>


    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-1ac2260db9ecccbf0303a899bc27ce6d">1 - 网络插件</h1>
    
	<!--
title: Network Plugins
content_type: concept
weight: 10
-->
<!-- overview -->
<!--
Network plugins in Kubernetes come in a few flavors:

* CNI plugins: adhere to the [Container Network Interface](https://github.com/containernetworking/cni) (CNI) specification, designed for interoperability.
  * Kubernetes follows the [v0.4.0](https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md) release of the CNI specification.
* Kubenet plugin: implements basic `cbr0` using the `bridge` and `host-local` CNI plugins
-->
<p>Kubernetes中的网络插件有几种类型：</p>
<ul>
<li>CNI 插件：遵守<a href="https://github.com/containernetworking/cni">容器网络接口（Container Network Interface，CNI）</a>
规范，其设计上偏重互操作性。
<ul>
<li>Kubernetes 遵从 CNI 规范的
<a href="https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md">v0.4.0</a>
版本。</li>
</ul>
</li>
<li>Kubenet 插件：使用 <code>bridge</code> 和 <code>host-local</code> CNI 插件实现了基本的 <code>cbr0</code>。</li>
</ul>
<!-- body -->
<!--
## Installation

The kubelet has a single default network plugin, and a default network common to the entire cluster. It probes for plugins when it starts up, remembers what it finds, and executes the selected plugin at appropriate times in the pod lifecycle (this is only true for Docker, as CRI manages its own CNI plugins). There are two Kubelet command line parameters to keep in mind when using plugins:

* `cni-bin-dir`: Kubelet probes this directory for plugins on startup
* `network-plugin`: The network plugin to use from `cni-bin-dir`.  It must match the name reported by a plugin probed from the plugin directory.  For CNI plugins, this is "cni".
-->
<h2 id="安装">安装</h2>
<p>kubelet 有一个单独的默认网络插件，以及一个对整个集群通用的默认网络。
它在启动时探测插件，记住找到的内容，并在 Pod 生命周期的适当时间执行
所选插件（这仅适用于 Docker，因为 CRI 管理自己的 CNI 插件）。
在使用插件时，需要记住两个 kubelet 命令行参数：</p>
<ul>
<li><code>cni-bin-dir</code>： kubelet 在启动时探测这个目录中的插件</li>
<li><code>network-plugin</code>： 要使用的网络插件来自 <code>cni-bin-dir</code>。
它必须与从插件目录探测到的插件报告的名称匹配。
对于 CNI 插件，其值为 &quot;cni&quot;。</li>
</ul>
<!--
## Network Plugin Requirements

Besides providing the [`NetworkPlugin` interface](https://github.com/kubernetes/kubernetes/tree/v1.23.0/pkg/kubelet/dockershim/network/plugins.go) to configure and clean up pod networking, the plugin may also need specific support for kube-proxy.  The iptables proxy obviously depends on iptables, and the plugin may need to ensure that container traffic is made available to iptables.  For example, if the plugin connects containers to a Linux bridge, the plugin must set the `net/bridge/bridge-nf-call-iptables` sysctl to `1` to ensure that the iptables proxy functions correctly.  If the plugin does not use a Linux bridge (but instead something like Open vSwitch or some other mechanism) it should ensure container traffic is appropriately routed for the proxy.

By default if no kubelet network plugin is specified, the `noop` plugin is used, which sets `net/bridge/bridge-nf-call-iptables=1` to ensure simple configurations (like Docker with a bridge) work correctly with the iptables proxy.
-->
<h2 id="网络插件要求">网络插件要求</h2>
<p>除了提供
<a href="https://github.com/kubernetes/kubernetes/tree/v1.23.0/pkg/kubelet/dockershim/network/plugins.go"><code>NetworkPlugin</code> 接口</a>
来配置和清理 Pod 网络之外，该插件还可能需要对 kube-proxy 的特定支持。
iptables 代理显然依赖于 iptables，插件可能需要确保 iptables 能够监控容器的网络通信。
例如，如果插件将容器连接到 Linux 网桥，插件必须将 <code>net/bridge/bridge-nf-call-iptables</code>
系统参数设置为<code>1</code>，以确保 iptables 代理正常工作。
如果插件不使用 Linux 网桥（而是类似于 Open vSwitch 或者其它一些机制），
它应该确保为代理对容器通信执行正确的路由。</p>
<p>默认情况下，如果未指定 kubelet 网络插件，则使用 <code>noop</code> 插件，
该插件设置 <code>net/bridge/bridge-nf-call-iptables=1</code>，以确保简单的配置
（如带网桥的 Docker ）与 iptables 代理正常工作。</p>
<!--
### CNI

The CNI plugin is selected by passing Kubelet the `--network-plugin=cni` command-line option.  Kubelet reads a file from `--cni-conf-dir` (default `/etc/cni/net.d`) and uses the CNI configuration from that file to set up each pod's network.  The CNI configuration file must match the [CNI specification](https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration), and any required CNI plugins referenced by the configuration must be present in `--cni-bin-dir` (default `/opt/cni/bin`).

If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order.

In addition to the CNI plugin specified by the configuration file, Kubernetes requires the standard CNI [`lo`](https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go) plugin, at minimum version 0.2.0
-->
<h3 id="cni">CNI</h3>
<p>通过给 Kubelet 传递 <code>--network-plugin=cni</code> 命令行选项可以选择 CNI 插件。
Kubelet 从 <code>--cni-conf-dir</code> （默认是 <code>/etc/cni/net.d</code>） 读取文件并使用
该文件中的 CNI 配置来设置各个 Pod 的网络。
CNI 配置文件必须与
<a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration">CNI 规约</a>
匹配，并且配置所引用的所有所需的 CNI 插件都应存在于
<code>--cni-bin-dir</code>（默认是 <code>/opt/cni/bin</code>）下。</p>
<p>如果这个目录中有多个 CNI 配置文件，kubelet 将会使用按文件名的字典顺序排列
的第一个作为配置文件。</p>
<p>除了配置文件指定的 CNI 插件外，Kubernetes 还需要标准的 CNI
<a href="https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go"><code>lo</code></a>
插件，最低版本是0.2.0。</p>
<!--
#### Support hostPort

The CNI networking plugin supports `hostPort`. You can use the official [portmap](https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap)
plugin offered by the CNI plugin team or use your own plugin with portMapping functionality.

If you want to enable `hostPort` support, you must specify `portMappings capability` in your `cni-conf-dir`.
For example:
-->
<h4 id="支持-hostport">支持 hostPort</h4>
<p>CNI 网络插件支持 <code>hostPort</code>。 你可以使用官方
<a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap">portmap</a>
插件，它由 CNI 插件团队提供，或者使用你自己的带有 portMapping 功能的插件。</p>
<p>如果你想要启动 <code>hostPort</code> 支持，则必须在 <code>cni-conf-dir</code> 指定 <code>portMappings capability</code>。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;k8s-pod-network&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;cniVersion&#34;</span>: <span style="color:#b44">&#34;0.3.0&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;plugins&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;calico&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;log_level&#34;</span>: <span style="color:#b44">&#34;info&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;datastore_type&#34;</span>: <span style="color:#b44">&#34;kubernetes&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;nodename&#34;</span>: <span style="color:#b44">&#34;127.0.0.1&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;ipam&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;host-local&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;subnet&#34;</span>: <span style="color:#b44">&#34;usePodCidr&#34;</span>
      },
      <span style="color:#008000;font-weight:bold">&#34;policy&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;k8s&#34;</span>
      },
      <span style="color:#008000;font-weight:bold">&#34;kubernetes&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;kubeconfig&#34;</span>: <span style="color:#b44">&#34;/etc/cni/net.d/calico-kubeconfig&#34;</span>
      }
    },
    {
      <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;portmap&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;capabilities&#34;</span>: {<span style="color:#008000;font-weight:bold">&#34;portMappings&#34;</span>: <span style="color:#a2f;font-weight:bold">true</span>}
    }
  ]
}
</code></pre></div><!--
#### Support traffic shaping

**Experimental Feature**

The CNI networking plugin also supports pod ingress and egress traffic shaping. You can use the official [bandwidth](https://github.com/containernetworking/plugins/tree/master/plugins/meta/bandwidth)
plugin offered by the CNI plugin team or use your own plugin with bandwidth control functionality.

If you want to enable traffic shaping support, you must add the `bandwidth` plugin to your CNI configuration file
(default `/etc/cni/net.d`) and ensure that the binary is included in your CNI bin dir (default `/opt/cni/bin`).
-->
<h4 id="支持流量整形">支持流量整形</h4>
<p><strong>实验功能</strong></p>
<p>CNI 网络插件还支持 pod 入口和出口流量整形。
你可以使用 CNI 插件团队提供的
<a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/bandwidth">bandwidth</a>
插件，也可以使用你自己的具有带宽控制功能的插件。</p>
<p>如果你想要启用流量整形支持，你必须将 <code>bandwidth</code> 插件添加到 CNI 配置文件
（默认是 <code>/etc/cni/net.d</code>）并保证该可执行文件包含在你的 CNI 的 bin
文件夹内 (默认为 <code>/opt/cni/bin</code>)。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;k8s-pod-network&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;cniVersion&#34;</span>: <span style="color:#b44">&#34;0.3.0&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;plugins&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;calico&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;log_level&#34;</span>: <span style="color:#b44">&#34;info&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;datastore_type&#34;</span>: <span style="color:#b44">&#34;kubernetes&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;nodename&#34;</span>: <span style="color:#b44">&#34;127.0.0.1&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;ipam&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;host-local&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;subnet&#34;</span>: <span style="color:#b44">&#34;usePodCidr&#34;</span>
      },
      <span style="color:#008000;font-weight:bold">&#34;policy&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;k8s&#34;</span>
      },
      <span style="color:#008000;font-weight:bold">&#34;kubernetes&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;kubeconfig&#34;</span>: <span style="color:#b44">&#34;/etc/cni/net.d/calico-kubeconfig&#34;</span>
      }
    },
    {
      <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;bandwidth&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;capabilities&#34;</span>: {<span style="color:#008000;font-weight:bold">&#34;bandwidth&#34;</span>: <span style="color:#a2f;font-weight:bold">true</span>}
    }
  ]
}
</code></pre></div><!--
Now you can add the `kubernetes.io/ingress-bandwidth` and `kubernetes.io/egress-bandwidth` annotations to your pod.
For example:
-->
<p>现在，你可以将 <code>kubernetes.io/ingress-bandwidth</code> 和 <code>kubernetes.io/egress-bandwidth</code>
注解添加到 pod 中。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/ingress-bandwidth</span>:<span style="color:#bbb"> </span>1M<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/egress-bandwidth</span>:<span style="color:#bbb"> </span>1M<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
### kubenet

Kubenet is a very basic, simple network plugin, on Linux only.  It does not, of itself, implement more advanced features like cross-node networking or network policy.  It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments.

Kubenet creates a Linux bridge named `cbr0` and creates a veth pair for each pod with the host end of each pair connected to `cbr0`.  The pod end of the pair is assigned an IP address allocated from a range assigned to the node either through configuration or by the controller-manager.  `cbr0` is assigned an MTU matching the smallest MTU of an enabled normal interface on the host.

The plugin requires a few things:

* The standard CNI `bridge`, `lo` and `host-local` plugins are required, at minimum version 0.2.0. Kubenet will first search for them in `/opt/cni/bin`. Specify `cni-bin-dir` to supply additional search path. The first found match will take effect.
* Kubelet must be run with the `--network-plugin=kubenet` argument to enable the plugin
* Kubelet should also be run with the `--non-masquerade-cidr=<clusterCidr>` argument to ensure traffic to IPs outside this range will use IP masquerade.
* The node must be assigned an IP subnet through either the `--pod-cidr` kubelet command-line option or the `--allocate-node-cidrs=true -cluster-cidr=<cidr>` controller-manager command-line options.
-->
<h3 id="kubenet">kubenet</h3>
<p>Kubenet 是一个非常基本的、简单的网络插件，仅适用于 Linux。
它本身并不实现更高级的功能，如跨节点网络或网络策略。
它通常与云驱动一起使用，云驱动为节点间或单节点环境中的通信设置路由规则。</p>
<p>Kubenet 创建名为 <code>cbr0</code> 的网桥，并为每个 pod 创建了一个 veth 对，
每个 Pod 的主机端都连接到 <code>cbr0</code>。
这个 veth 对的 Pod 端会被分配一个 IP 地址，该 IP 地址隶属于节点所被分配的 IP
地址范围内。节点的 IP 地址范围则通过配置或控制器管理器来设置。
<code>cbr0</code> 被分配一个 MTU，该 MTU 匹配主机上已启用的正常接口的最小 MTU。</p>
<p>使用此插件还需要一些其他条件：</p>
<ul>
<li>需要标准的 CNI <code>bridge</code>、<code>lo</code> 以及 <code>host-local</code> 插件，最低版本是0.2.0。
kubenet 首先在 <code>/opt/cni/bin</code> 中搜索它们。 指定 <code>cni-bin-dir</code> 以提供
其它搜索路径。首次找到的匹配将生效。</li>
<li>Kubelet 必须和 <code>--network-plugin=kubenet</code> 参数一起运行，才能启用该插件。</li>
<li>Kubelet 还应该和 <code>--non-masquerade-cidr=&lt;clusterCidr&gt;</code> 参数一起运行，
以确保超出此范围的 IP 流量将使用 IP 伪装。</li>
<li>节点必须被分配一个 IP 子网，通过kubelet 命令行的 <code>--pod-cidr</code> 选项或
控制器管理器的命令行选项 <code>--allocate-node-cidrs=true --cluster-cidr=&lt;cidr&gt;</code>
来设置。</li>
</ul>
<!--
### Customizing the MTU (with kubenet)

The MTU should always be configured correctly to get the best networking performance.  Network plugins will usually try
to infer a sensible MTU, but sometimes the logic will not result in an optimal MTU.  For example, if the
Docker bridge or another interface has a small MTU, kubenet will currently select that MTU.  Or if you are
using IPSEC encapsulation, the MTU must be reduced, and this calculation is out-of-scope for
most network plugins.

Where needed, you can specify the MTU explicitly with the `network-plugin-mtu` kubelet option.  For example,
on AWS the `eth0` MTU is typically 9001, so you might specify `--network-plugin-mtu=9001`.  If you're using IPSEC you
might reduce it to allow for encapsulation overhead e.g. `--network-plugin-mtu=8873`.

This option is provided to the network-plugin; currently **only kubenet supports `network-plugin-mtu`**.
-->
<h3 id="自定义-mtu-使用-kubenet">自定义 MTU（使用 kubenet）</h3>
<p>要获得最佳的网络性能，必须确保 MTU 的取值配置正确。
网络插件通常会尝试推断出一个合理的 MTU，但有时候这个逻辑不会产生一个最优的 MTU。
例如，如果 Docker 网桥或其他接口有一个小的 MTU, kubenet 当前将选择该 MTU。
或者如果你正在使用 IPSEC 封装，则必须减少 MTU，并且这种计算超出了大多数网络插件的能力范围。</p>
<p>如果需要，你可以使用 <code>network-plugin-mtu</code> kubelet 选项显式的指定 MTU。
例如：在 AWS 上 <code>eth0</code> MTU 通常是 9001，因此你可以指定 <code>--network-plugin-mtu=9001</code>。
如果你正在使用 IPSEC ，你可以减少它以允许封装开销，例如 <code>--network-plugin-mtu=8873</code>。</p>
<p>此选项会传递给网络插件； 当前 <strong>仅 kubenet 支持 <code>network-plugin-mtu</code></strong>。</p>
<!--
## Usage Summary

* `--network-plugin=cni` specifies that we use the `cni` network plugin with actual CNI plugin binaries located in `--cni-bin-dir` (default `/opt/cni/bin`) and CNI plugin configuration located in `--cni-conf-dir` (default `/etc/cni/net.d`).
* `--network-plugin=kubenet` specifies that we use the `kubenet` network plugin with CNI `bridge`, `lo` and `host-local` plugins placed in `/opt/cni/bin` or `cni-bin-dir`.
* `--network-plugin-mtu=9001` specifies the MTU to use, currently only used by the `kubenet` network plugin.
-->
<h2 id="用法总结">用法总结</h2>
<ul>
<li><code>--network-plugin=cni</code> 用来表明我们要使用 <code>cni</code> 网络插件，实际的 CNI 插件
可执行文件位于 <code>--cni-bin-dir</code>（默认是 <code>/opt/cni/bin</code>）下， CNI 插件配置位于
<code>--cni-conf-dir</code>（默认是 <code>/etc/cni/net.d</code>）下。</li>
<li><code>--network-plugin=kubenet</code> 用来表明我们要使用 <code>kubenet</code> 网络插件，CNI <code>bridge</code>，<code>lo</code>
和 <code>host-local</code> 插件位于 <code>/opt/cni/bin</code> 或 <code>cni-bin-dir</code> 中。</li>
<li><code>--network-plugin-mtu=9001</code> 指定了我们使用的 MTU，当前仅被 <code>kubenet</code> 网络插件使用。</li>
</ul>
<h2 id="what-s-next">What's next</h2>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-53e1ea8892ceca307ba19e8d6a7b8d32">2 - 设备插件</h1>
    <div class="lead">使用 Kubernetes 设备插件框架来实现适用于 GPU、NIC、FPGA、InfiniBand 以及类似的需要特定于供应商设置的资源的插件。</div>
	<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.10 [beta]</code>
</div>


<!--
Kubernetes provides a [device plugin framework](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md)
that you can use to advertise system hardware resources to the
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='Kubelet'>Kubelet</a>.

Instead of customizing the code for Kubernetes itself, vendors can implement a
device plugin that you deploy either manually or as a <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>.
The targeted devices include GPUs, high-performance NICs, FPGAs, InfiniBand adapters,
and other similar computing resources that may require vendor specific initialization
and setup.
-->
<p>Kubernetes 提供了一个
<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md">设备插件框架</a>，你可以用它来将系统硬件资源发布到 <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='Kubelet'>Kubelet</a>。</p>
<p>供应商可以实现设备插件，由你手动部署或作为 <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>
来部署，而不必定制 Kubernetes 本身的代码。目标设备包括 GPU、高性能 NIC、FPGA、
InfiniBand 适配器以及其他类似的、可能需要特定于供应商的初始化和设置的计算资源。</p>
<!-- body -->
<!--
## Device plugin registration
-->
<h2 id="device-plugin-registration">注册设备插件   </h2>
<!--
The kubelet exports a `Registration` gRPC service:
-->
<p><code>kubelet</code> 提供了一个 <code>Registration</code> 的 gRPC 服务：</p>
<pre><code class="language-gRPC" data-lang="gRPC">service Registration {
 rpc Register(RegisterRequest) returns (Empty) {}
}
</code></pre><!--
A device plugin can register itself with the kubelet through this gRPC service.
During the registration, the device plugin needs to send:

  * The name of its Unix socket.
  * The Device Plugin API version against which it was built.
  * The `ResourceName` it wants to advertise. Here `ResourceName` needs to follow the
    [extended resource naming scheme](/docs/concepts/configuration/manage-resources-containers/#extended-resources)
    as `vendor-domain/resourcetype`.
    (For example, an NVIDIA GPU is advertised as `nvidia.com/gpu`.)

Following a successful registration, the device plugin sends the kubelet the
list of devices it manages, and the kubelet is then in charge of advertising those
resources to the API server as part of the kubelet node status update.
For example, after a device plugin registers `hardware-vendor.example/foo` with the kubelet
and reports two healthy devices on a node, the node status is updated
to advertise that the node has 2 "Foo" devices installed and available.
-->
<p>设备插件可以通过此 gRPC 服务在 kubelet 进行注册。在注册期间，设备插件需要发送下面几样内容：</p>
<ul>
<li>设备插件的 Unix 套接字。</li>
<li>设备插件的 API 版本。</li>
<li><code>ResourceName</code> 是需要公布的。这里 <code>ResourceName</code> 需要遵循
<a href="/zh/docs/concepts/configuration/manage-resources-containers/#extended-resources">扩展资源命名方案</a>，
类似于 <code>vendor-domain/resourcetype</code>。（比如 NVIDIA GPU 就被公布为 <code>nvidia.com/gpu</code>。）</li>
</ul>
<p>成功注册后，设备插件就向 kubelet 发送它所管理的设备列表，然后 kubelet
负责将这些资源发布到 API 服务器，作为 kubelet 节点状态更新的一部分。</p>
<p>比如，设备插件在 kubelet 中注册了 <code>hardware-vendor.example/foo</code> 并报告了
节点上的两个运行状况良好的设备后，节点状态将更新以通告该节点已安装 2 个
&quot;Foo&quot; 设备并且是可用的。</p>
<!--
Then, users can request devices in a
[Container](/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core)
specification as they request other types of resources, with the following limitations:

* Extended resources are only supported as integer resources and cannot be overcommitted.
* Devices cannot be shared among Containers.
-->
<p>然后用户需要请求其他类型的资源的时候，就可以在
<a href="/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core">Container</a>
规范请求这类设备，但是有以下的限制：</p>
<ul>
<li>扩展资源仅可作为整数资源使用，并且不能被过量使用</li>
<li>设备不能在容器之间共享</li>
</ul>
<h3 id="example-pod">示例</h3>
<!--
Suppose a Kubernetes cluster is running a device plugin that advertises resource `hardware-vendor.example/foo`
on certain nodes. Here is an example of a pod requesting this resource to run a demo workload:
-->
<p>假设 Kubernetes 集群正在运行一个设备插件，该插件在一些节点上公布的资源为 <code>hardware-vendor.example/foo</code>。
下面就是一个 Pod 示例，请求此资源以运行一个工作负载的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>demo-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>demo-container-1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:2.0<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">hardware-vendor.example/foo</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 这个 pod 需要两个 hardware-vendor.example/foo 设备</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 而且只能够调度到满足需求的节点上</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 如果该节点中有 2 个以上的设备可用，其余的可供其他 Pod 使用</span><span style="color:#bbb">
</span></code></pre></div><!--
## Device plugin implementation

The general workflow of a device plugin includes the following steps:

* Initialization. During this phase, the device plugin performs vendor specific
  initialization and setup to make sure the devices are in a ready state.

* The plugin starts a gRPC service, with a Unix socket under host path
  `/var/lib/kubelet/device-plugins/`, that implements the following interfaces:
-->
<h2 id="device-plugin-implementation">设备插件的实现   </h2>
<p>设备插件的常规工作流程包括以下几个步骤：</p>
<ul>
<li>
<p>初始化。在这个阶段，设备插件将执行供应商特定的初始化和设置，
以确保设备处于就绪状态。</p>
</li>
<li>
<p>插件使用主机路径 <code>/var/lib/kubelet/device-plugins/</code> 下的 Unix 套接字启动
一个 gRPC 服务，该服务实现以下接口：</p>
<!--

```gRPC
service DevicePlugin {
      // GetDevicePluginOptions returns options to be communicated with Device Manager.
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}

      // ListAndWatch returns a stream of List of Devices
      // Whenever a Device state change or a Device disappears, ListAndWatch
      // returns the new list
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}

      // Allocate is called during container creation so that the Device
      // Plugin can run device specific operations and instruct Kubelet
      // of the steps to make the Device available in the container
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}

      // GetPreferredAllocation returns a preferred set of devices to allocate
      // from a list of available ones. The resulting preferred allocation is not
      // guaranteed to be the allocation ultimately performed by the
      // devicemanager. It is only designed to help the devicemanager make a more
      // informed allocation decision when possible.
      rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}

      // PreStartContainer is called, if indicated by Device Plugin during registeration phase,
      // before each container start. Device plugin can run device specific operations
      // such as resetting the device before making devices available to the container.
      rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}
}
```
-->
<pre><code class="language-gRPC" data-lang="gRPC">service DevicePlugin {
      // GetDevicePluginOptions 返回与设备管理器沟通的选项。
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}

      // ListAndWatch 返回 Device 列表构成的数据流。
      // 当 Device 状态发生变化或者 Device 消失时，ListAndWatch
      // 会返回新的列表。
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}

      // Allocate 在容器创建期间调用，这样设备插件可以运行一些特定于设备的操作，
      // 并告诉 kubelet 如何令 Device 可在容器中访问的所需执行的具体步骤
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}

      // GetPreferredAllocation 从一组可用的设备中返回一些优选的设备用来分配，
      // 所返回的优选分配结果不一定会是设备管理器的最终分配方案。
      // 此接口的设计仅是为了让设备管理器能够在可能的情况下做出更有意义的决定。
      rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}

      // PreStartContainer 在设备插件注册阶段根据需要被调用，调用发生在容器启动之前。
      // 在将设备提供给容器使用之前，设备插件可以运行一些诸如重置设备之类的特定于
      // 具体设备的操作，
      rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}
}
</code></pre><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
  Plugins are not required to provide useful implementations for
  `GetPreferredAllocation()` or `PreStartContainer()`. Flags indicating which
  (if any) of these calls are available should be set in the `DevicePluginOptions`
  message sent back by a call to `GetDevicePluginOptions()`. The `kubelet` will
  always call `GetDevicePluginOptions()` to see which optional functions are
  available, before calling any of them directly.
  -->
<p>插件并非必须为 <code>GetPreferredAllocation()</code> 或 <code>PreStartContainer()</code> 提供有用
的实现逻辑，调用 <code>GetDevicePluginOptions()</code> 时所返回的 <code>DevicePluginOptions</code>
消息中应该设置这些调用是否可用。<code>kubelet</code> 在真正调用这些函数之前，总会调用
<code>GetDevicePluginOptions()</code> 来查看是否存在这些可选的函数。
</div>
</li>
</ul>
<!--
* The plugin registers itself with the kubelet through the Unix socket at host
  path `/var/lib/kubelet/device-plugins/kubelet.sock`.

* After successfully registering itself, the device plugin runs in serving mode, during which it keeps
monitoring device health and reports back to the kubelet upon any device state changes.
It is also responsible for serving `Allocate` gRPC requests. During `Allocate`, the device plugin may
do device-specific preparation; for example, GPU cleanup or QRNG initialization.
If the operations succeed, the device plugin returns an `AllocateResponse` that contains container
runtime configurations for accessing the allocated devices. The kubelet passes this information
to the container runtime.
-->
<ul>
<li>插件通过 Unix socket 在主机路径 <code>/var/lib/kubelet/device-plugins/kubelet.sock</code>
处向 kubelet 注册自身。</li>
<li>成功注册自身后，设备插件将以服务模式运行，在此期间，它将持续监控设备运行状况，
并在设备状态发生任何变化时向 kubelet 报告。它还负责响应 <code>Allocate</code> gRPC 请求。
在 <code>Allocate</code> 期间，设备插件可能还会做一些设备特定的准备；例如 GPU 清理或 QRNG 初始化。
如果操作成功，则设备插件将返回 <code>AllocateResponse</code>，其中包含用于访问被分配的设备容器运行时的配置。
kubelet 将此信息传递到容器运行时。</li>
</ul>
<!--
### Handling kubelet restarts

A device plugin is expected to detect kubelet restarts and re-register itself with the new
kubelet instance. In the current implementation, a new kubelet instance deletes all the existing Unix sockets
under `/var/lib/kubelet/device-plugins` when it starts. A device plugin can monitor the deletion
of its Unix socket and re-register itself upon such an event.
-->
<h3 id="处理-kubelet-重启">处理 kubelet 重启</h3>
<p>设备插件应能监测到 kubelet 重启，并且向新的 kubelet 实例来重新注册自己。
在当前实现中，当 kubelet 重启的时候，新的 kubelet 实例会删除 <code>/var/lib/kubelet/device-plugins</code>
下所有已经存在的 Unix 套接字。
设备插件需要能够监控到它的 Unix 套接字被删除，并且当发生此类事件时重新注册自己。</p>
<!--
## Device plugin deployment

You can deploy a device plugin as a DaemonSet, as a package for your node's operating system,
or manually.

The canonical directory `/var/lib/kubelet/device-plugins` requires privileged access,
so a device plugin must run in a privileged security context.
If you're deploying a device plugin as a DaemonSet, `/var/lib/kubelet/device-plugins`
must be mounted as a <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷（Volume）'>卷（Volume）</a>
in the plugin's
[PodSpec](/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core).

If you choose the DaemonSet approach you can rely on Kubernetes to: place the device plugin's
Pod onto Nodes, to restart the daemon Pod after failure, and to help automate upgrades.
-->
<h2 id="设备插件部署">设备插件部署</h2>
<p>你可以将你的设备插件作为节点操作系统的软件包来部署、作为 DaemonSet 来部署或者手动部署。</p>
<p>规范目录 <code>/var/lib/kubelet/device-plugins</code> 是需要特权访问的，所以设备插件
必须要在被授权的安全的上下文中运行。
如果你将设备插件部署为 DaemonSet，<code>/var/lib/kubelet/device-plugins</code> 目录必须要在插件的
<a href="/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core">PodSpec</a>
中声明作为 <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷（Volume）'>卷（Volume）</a> 被挂载到插件中。</p>
<p>如果你选择 DaemonSet 方法，你可以通过 Kubernetes 进行以下操作：
将设备插件的 Pod 放置在节点上，在出现故障后重新启动守护进程 Pod，来进行自动升级。</p>
<!--
## API compatibility

Kubernetes device plugin support is in beta. The API may change before stabilization,
in incompatible ways. As a project, Kubernetes recommends that device plugin developers:

* Watch for changes in future releases.
* Support multiple versions of the device plugin API for backward/forward compatibility.

If you enable the DevicePlugins feature and run device plugins on nodes that need to be upgraded to
a Kubernetes release with a newer device plugin API version, upgrade your device plugins
to support both versions before upgrading these nodes. Taking that approach will
ensure the continuous functioning of the device allocations during the upgrade.
-->
<h2 id="api-兼容性">API 兼容性</h2>
<p>Kubernetes 设备插件支持还处于 beta 版本。所以在稳定版本出来之前 API 会以不兼容的方式进行更改。
作为一个项目，Kubernetes 建议设备插件开发者：</p>
<ul>
<li>注意未来版本的更改</li>
<li>支持多个版本的设备插件 API，以实现向后/向前兼容性。</li>
</ul>
<p>如果你启用 DevicePlugins 功能，并在需要升级到 Kubernetes 版本来获得较新的设备插件 API
版本的节点上运行设备插件，请在升级这些节点之前先升级设备插件以支持这两个版本。
采用该方法将确保升级期间设备分配的连续运行。</p>
<!--
## Monitoring Device Plugin Resources






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.15 [beta]</code>
</div>



In order to monitor resources provided by device plugins, monitoring agents need to be able to
discover the set of devices that are in-use on the node and obtain metadata to describe which
container the metric should be associated with. [Prometheus](https://prometheus.io/) metrics
exposed by device monitoring agents should follow the
[Kubernetes Instrumentation Guidelines](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/instrumentation.md),
identifying containers using `pod`, `namespace`, and `container` prometheus labels.
-->
<h2 id="监控设备插件资源">监控设备插件资源</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.15 [beta]</code>
</div>


<p>为了监控设备插件提供的资源，监控代理程序需要能够发现节点上正在使用的设备，
并获取元数据来描述哪个指标与容器相关联。
设备监控代理暴露给 <a href="https://prometheus.io/">Prometheus</a> 的指标应该遵循
<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/instrumentation.md">Kubernetes Instrumentation Guidelines</a>，
使用 <code>pod</code>、<code>namespace</code> 和 <code>container</code> 标签来标识容器。</p>
<!--
The kubelet provides a gRPC service to enable discovery of in-use devices, and to provide metadata
for these devices:
-->
<p>kubelet 提供了 gRPC 服务来使得正在使用中的设备被发现，并且还未这些设备提供了元数据：</p>
<pre><code class="language-gRPC" data-lang="gRPC">// PodResourcesLister 是一个由 kubelet 提供的服务，用来提供供节点上 
// Pods 和容器使用的节点资源的信息
service PodResourcesLister {
    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
    rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}
}
</code></pre><h3 id="grpc-endpoint-list"><code>List</code> gRPC 端点</h3>
<!--
The `List` endpoint provides information on resources of running pods, with details such as the
id of exclusively allocated CPUs, device id as it was reported by device plugins and id of
the NUMA node where these devices are allocated. Also, for NUMA-based machines, it contains
the information about memory and hugepages reserved for a container.
-->
<p>这一 <code>List</code> 端点提供运行中 Pods 的资源信息，包括类似独占式分配的
CPU ID、设备插件所报告的设备 ID 以及这些设备分配所处的 NUMA 节点 ID。
此外，对于基于 NUMA 的机器，它还会包含为容器保留的内存和大页的信息。</p>
<pre><code class="language-gRPC" data-lang="gRPC">// ListPodResourcesResponse 是 List 函数的响应
message ListPodResourcesResponse {
    repeated PodResources pod_resources = 1;
}

// PodResources 包含关于分配给 Pod 的节点资源的信息
message PodResources {
    string name = 1;
    string namespace = 2;
    repeated ContainerResources containers = 3;
}

// ContainerResources 包含分配给容器的资源的信息
message ContainerResources {
    string name = 1;
    repeated ContainerDevices devices = 2;
    repeated int64 cpu_ids = 3;
    repeated ContainerMemory memory = 4;
}

// ContainerMemory 包含分配给容器的内存和大页信息
message ContainerMemory {
    string memory_type = 1;
    uint64 size = 2;
    TopologyInfo topology = 3;
}

// Topology 描述资源的硬件拓扑结构
message TopologyInfo {
        repeated NUMANode nodes = 1;
}

// NUMA 代表的是 NUMA 节点
message NUMANode {
        int64 ID = 1;
}

// ContainerDevices 包含分配给容器的设备信息
message ContainerDevices {
    string resource_name = 1;
    repeated string device_ids = 2;
    TopologyInfo topology = 3;
}
</code></pre><!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>cpu_ids in the <code>ContainerResources</code> in the <code>List</code> endpoint correspond to exclusive CPUs allocated
to a partilar container. If the goal is to evaluate CPUs that belong to the shared pool, the <code>List</code>
endpoint needs to be used in conjunction with the <code>GetAllocatableResources</code> endpoint as explained
below:</p>
<ol>
<li>Call <code>GetAllocatableResources</code> to get a list of all the allocatable CPUs</li>
<li>Call <code>GetCpuIds</code> on all <code>ContainerResources</code> in the system</li>
<li>Subtract out all of the CPUs from the <code>GetCpuIds</code> calls from the <code>GetAllocatableResources</code> call</li>
</ol>

</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p><code>List</code> 端点中的 <code>ContainerResources</code> 中的 cpu_ids 对应于分配给某个容器的专属 CPU。
如果要统计共享池中的 CPU，<code>List</code> 端点需要与 <code>GetAllocatableResources</code> 端点一起使用，如下所述:</p>
<ol>
<li>调用 <code>GetAllocatableResources</code> 获取所有可用的 CPUs。</li>
<li>在系统中所有的 <code>ContainerResources</code> 上调用 <code>GetCpuIds</code>。</li>
<li>用 <code>GetAllocatableResources</code> 获取的 CPU 数减去 <code>GetCpuIds</code> 获取的 CPU 数。</li>
</ol>

</div>
<h3 id="grpc-endpoint-getallocatableresources"><code>GetAllocatableResources</code> gRPC 端点</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <code>GetAllocatableResources</code> should only be used to evaluate <a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">allocatable</a>
resources on a node. If the goal is to evaluate free/unallocated resources it should be used in
conjunction with the List() endpoint. The result obtained by <code>GetAllocatableResources</code> would remain
the same unless the underlying resources exposed to kubelet change. This happens rarely but when
it does (for example: hotplug/hotunplug, device health changes), client is expected to call
<code>GetAlloctableResources</code> endpoint.
However, calling <code>GetAllocatableResources</code> endpoint is not sufficient in case of cpu and/or memory
update and Kubelet needs to be restarted to reflect the correct resource capacity and allocatable.
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <code>GetAllocatableResources</code> 应该仅被用于评估一个节点上的<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">可分配的</a>
资源。如果目标是评估空闲/未分配的资源，此调用应该与 List() 端点一起使用。
除非暴露给 kubelet 的底层资源发生变化 否则 <code>GetAllocatableResources</code> 得到的结果将保持不变。
这种情况很少发生，但当发生时（例如：热插拔，设备健康状况改变），客户端应该调用 <code>GetAlloctableResources</code> 端点。
然而，调用 <code>GetAllocatableResources</code> 端点在 cpu、内存被更新的情况下是不够的，
Kubelet 需要重新启动以获取正确的资源容量和可分配的资源。
</div>
<!--
GetAllocatableResources provides information on resources initially available on the worker node.
It provides more information than kubelet exports to APIServer.
-->
<p>端点 <code>GetAllocatableResources</code> 提供最初在工作节点上可用的资源的信息。
此端点所提供的信息比导出给 API 服务器的信息更丰富。</p>
<pre><code class="language-gRPC" data-lang="gRPC">// AllocatableResourcesResponses 包含 kubelet 所了解到的所有设备的信息
message AllocatableResourcesResponse {
    repeated ContainerDevices devices = 1;
    repeated int64 cpu_ids = 2;
    repeated ContainerMemory memory = 3;
}

</code></pre><!--
Starting from Kubernetes v1.23, the `GetAllocatableResources` is enabled by default.
You can disable it by turning off the
`KubeletPodResourcesGetAllocatable` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).

Preceding Kubernetes v1.23, to enable this feature `kubelet` must be started with the following flag:

`--feature-gates=KubeletPodResourcesGetAllocatable=true`
-->
<p>从 Kubernetes v1.23 开始，<code>GetAllocatableResources</code> 被默认启用。
你可以通过关闭 <code>KubeletPodResourcesGetAllocatable</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a> 来禁用。</p>
<p>在 Kubernetes v1.23 之前，要启用这一功能，<code>kubelet</code> 必须用以下标志启动：</p>
<p><code>--feature-gates=KubeletPodResourcesGetAllocatable=true</code></p>
<!--
`ContainerDevices` do expose the topology information declaring to which NUMA cells the device is affine.
The NUMA cells are identified using a opaque integer ID, which value is consistent to what device
plugins report [when they register themselves to the kubelet](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager).
-->
<p><code>ContainerDevices</code> 会向外提供各个设备所隶属的 NUMA 单元这类拓扑信息。
NUMA 单元通过一个整数 ID 来标识，其取值与设备插件所报告的一致。
<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">设备插件注册到 kubelet 时</a>
会报告这类信息。</p>
<!--
The gRPC service is served over a unix socket at `/var/lib/kubelet/pod-resources/kubelet.sock`.
Monitoring agents for device plugin resources can be deployed as a daemon, or as a DaemonSet.
The canonical directory `/var/lib/kubelet/pod-resources` requires privileged access, so monitoring
agents must run in a privileged security context.  If a device monitoring agent is running as a
DaemonSet, `/var/lib/kubelet/pod-resources` must be mounted as a
<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷（Volume）'>卷（Volume）</a> in the device monitoring agent's
[PodSpec](/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core).

Support for the "PodResourcesLister service" requires `KubeletPodResources` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be enabled.
It is enabled by default starting with Kubernetes 1.15 and is v1 since Kubernetes 1.20.
-->
<p>gRPC 服务通过 <code>/var/lib/kubelet/pod-resources/kubelet.sock</code> 的 UNIX 套接字来提供服务。
设备插件资源的监控代理程序可以部署为守护进程或者 DaemonSet。
规范的路径 <code>/var/lib/kubelet/pod-resources</code> 需要特权来进入，
所以监控代理程序必须要在获得授权的安全的上下文中运行。
如果设备监控代理以 DaemonSet 形式运行，必须要在插件的
<a href="/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core">PodSpec</a>
中声明将 <code>/var/lib/kubelet/pod-resources</code> 目录以
<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>的形式被挂载到设备监控代理中。</p>
<p>对“PodResourcesLister 服务”的支持要求启用 <code>KubeletPodResources</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
从 Kubernetes 1.15 开始默认启用，自从 Kubernetes 1.20 开始为 v1。</p>
<!--
## Device Plugin integration with the Topology Manager






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [alpha]</code>
</div>




The Topology Manager is a Kubelet component that allows resources to be co-ordinated in a Topology aligned manner. In order to do this, the Device Plugin API was extended to include a `TopologyInfo` struct.
-->
<h2 id="设备插件与拓扑管理器的集成">设备插件与拓扑管理器的集成</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<p>拓扑管理器是 Kubelet 的一个组件，它允许以拓扑对齐方式来调度资源。
为了做到这一点，设备插件 API 进行了扩展来包括一个 <code>TopologyInfo</code> 结构体。</p>
<pre><code class="language-gRPC" data-lang="gRPC">message TopologyInfo {
 repeated NUMANode nodes = 1;
}

message NUMANode {
    int64 ID = 1;
}
</code></pre><!--
Device Plugins that wish to leverage the Topology Manager can send back a populated TopologyInfo struct as part of the device registration, along with the device IDs and the health of the device. The device manager will then use this information to consult with the Topology Manager and make resource assignment decisions.

`TopologyInfo` supports a `nodes` field that is either `nil` (the default) or a list of NUMA nodes. This lets the Device Plugin publish that can span NUMA nodes.

An example `TopologyInfo` struct populated for a device by a Device Plugin:

```
pluginapi.Device{ID: "25102017", Health: pluginapi.Healthy, Topology:&pluginapi.TopologyInfo{Nodes: []*pluginapi.NUMANode{&pluginapi.NUMANode{ID: 0,},}}}
```
-->
<p>设备插件希望拓扑管理器可以将填充的 TopologyInfo 结构体作为设备注册的一部分以及设备 ID
和设备的运行状况发送回去。然后设备管理器将使用此信息来咨询拓扑管理器并做出资源分配决策。</p>
<p><code>TopologyInfo</code> 支持定义 <code>nodes</code> 字段，允许为 <code>nil</code>（默认）或者是一个 NUMA 节点的列表。
这样就可以使设备插件可以跨越 NUMA 节点去发布。</p>
<p>下面是一个由设备插件为设备填充 <code>TopologyInfo</code> 结构体的示例：</p>
<pre><code>pluginapi.Device{ID: &quot;25102017&quot;, Health: pluginapi.Healthy, Topology:&amp;pluginapi.TopologyInfo{Nodes: []*pluginapi.NUMANode{&amp;pluginapi.NUMANode{ID: 0,},}}}
</code></pre><!--
## Device plugin examples {#examples}

Here are some examples of device plugin implementations:

* The [AMD GPU device plugin](https://github.com/RadeonOpenCompute/k8s-device-plugin)
* The [Intel device plugins](https://github.com/intel/intel-device-plugins-for-kubernetes) for Intel GPU, FPGA and QuickAssist devices
* The [KubeVirt device plugins](https://github.com/kubevirt/kubernetes-device-plugins) for hardware-assisted virtualization
* The [NVIDIA GPU device plugin](https://github.com/NVIDIA/k8s-device-plugin)
    * Requires [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) 2.0, which allows you to run GPU-enabled Docker containers.
* The [NVIDIA GPU device plugin for Container-Optimized OS](https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu)
* The [RDMA device plugin](https://github.com/hustcat/k8s-rdma-device-plugin)
* The [Solarflare device plugin](https://github.com/vikaschoudhary16/sfc-device-plugin)
* The [SR-IOV Network device plugin](https://github.com/intel/sriov-network-device-plugin)
* The [Xilinx FPGA device plugins](https://github.com/Xilinx/FPGA_as_a_Service/tree/master/k8s-fpga-device-plugin) for Xilinx FPGA devices
-->
<h2 id="examples">设备插件示例</h2>
<p>下面是一些设备插件实现的示例：</p>
<ul>
<li><a href="https://github.com/RadeonOpenCompute/k8s-device-plugin">AMD GPU 设备插件</a></li>
<li><a href="https://github.com/intel/intel-device-plugins-for-kubernetes">Intel 设备插件</a> 支持 Intel GPU、FPGA 和 QuickAssist 设备</li>
<li><a href="https://github.com/kubevirt/kubernetes-device-plugins">KubeVirt 设备插件</a> 用于硬件辅助的虚拟化</li>
<li>The <a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA GPU 设备插件</a></li>
<li>需要 <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a> 2.0，以允许运行 Docker 容器的时候启用 GPU。</li>
<li><a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu">为 Container-Optimized OS 所提供的 NVIDIA GPU 设备插件</a></li>
<li><a href="https://github.com/hustcat/k8s-rdma-device-plugin">RDMA 设备插件</a></li>
<li><a href="https://github.com/collabora/k8s-socketcan">SocketCAN 设备插件</a></li>
<li><a href="https://github.com/vikaschoudhary16/sfc-device-plugin">Solarflare 设备插件</a></li>
<li><a href="https://github.com/intel/sriov-network-device-plugin">SR-IOV 网络设备插件</a></li>
<li><a href="https://github.com/Xilinx/FPGA_as_a_Service/tree/master/k8s-fpga-device-plugin">Xilinx FPGA 设备插件</a></li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [scheduling GPU resources](/docs/tasks/manage-gpus/scheduling-gpus/) using device plugins
* Learn about [advertising extended resources](/docs/tasks/administer-cluster/extended-resource-node/) on a node
* Read about using [hardware acceleration for TLS ingress](https://kubernetes.io/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/) with Kubernetes
* Learn about the [Topology Manager] (/docs/tasks/adminster-cluster/topology-manager/)
-->
<ul>
<li>查看<a href="/zh/docs/tasks/manage-gpus/scheduling-gpus/">调度 GPU 资源</a> 来学习使用设备插件</li>
<li>查看在上如何<a href="/zh/docs/tasks/administer-cluster/extended-resource-node/">公布节点上的扩展资源</a></li>
<li>阅读如何在 Kubernetes 中使用 <a href="https://kubernetes.io/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/">TLS Ingress 的硬件加速</a></li>
<li>学习<a href="/zh/docs/tasks/administer-cluster/topology-manager/">拓扑管理器</a></li>
</ul>

</div>



    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      
      <a class="text-white" href="/zh/docs/home/">主页</a>
      
      <a class="text-white" href="/zh/blog/">博客</a>
      
      <a class="text-white" href="/zh/training/">培训</a>
      
      <a class="text-white" href="/zh/partners/">合作伙伴</a>
      
      <a class="text-white" href="/zh/community/">社区</a>
      
      <a class="text-white" href="/zh/case-studies/">案例分析</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" href="https://discuss.kubernetes.io">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/kubernetesio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://youtube.com/kubernetescommunity">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/kubernetes/kubernetes">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://slack.k8s.io">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Contribute" aria-label="Contribute">
    <a class="text-white" target="_blank" href="https://git.k8s.io/community/contributors/guide">
      <i class="fas fa-edit"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" href="https://stackoverflow.com/questions/tagged/kubernetes">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2024 The Kubernetes Authors | Documentation Distributed under <a href="https://git.k8s.io/website/LICENSE" class="light-text">CC BY 4.0</a></small>
        <br/>
        <small class="text-white">Copyright &copy; 2024 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage" class="light-text">Trademark Usage page</a></small>
        <br/>
        <small class="text-white">ICP license: 京ICP备17074266号-3</small>
        
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>










<script src="/js/main.js"></script>






  </body>
</html>
