<!doctype html>
<html lang="zh" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>


<link rel="alternate" hreflang="en" href="http://localhost:1313/docs/concepts/policy/">
<link rel="alternate" hreflang="ko" href="http://localhost:1313/ko/docs/concepts/policy/">
<link rel="alternate" hreflang="ja" href="http://localhost:1313/ja/docs/concepts/policy/">
<link rel="alternate" hreflang="fr" href="http://localhost:1313/fr/docs/concepts/policy/">
<link rel="alternate" hreflang="de" href="http://localhost:1313/de/docs/concepts/policy/">
<link rel="alternate" hreflang="es" href="http://localhost:1313/es/docs/concepts/policy/">
<link rel="alternate" hreflang="id" href="http://localhost:1313/id/docs/concepts/policy/">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.87.0" />
<link rel="canonical" type="text/html" href="http://localhost:1313/zh/docs/concepts/policy/">
<link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="apple-touch-icon" href="/images/kubernetes-192x192.png">
<title>策略 | Kubernetes</title><meta property="og:title" content="策略" />
<meta property="og:description" content="可配置的、可应用到一组资源的策略。" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/zh/docs/concepts/policy/" /><meta property="og:site_name" content="Kubernetes" />

<meta itemprop="name" content="策略">
<meta itemprop="description" content="可配置的、可应用到一组资源的策略。"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="策略"/>
<meta name="twitter:description" content="可配置的、可应用到一组资源的策略。"/>






<link href="/scss/main.css" rel="stylesheet">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "http://localhost:1313/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href="/css/feature-states.css">



<meta name="description" content="可配置的、可应用到一组资源的策略。">
<meta property="og:description" content="可配置的、可应用到一组资源的策略。">
<meta name="twitter:description" content="可配置的、可应用到一组资源的策略。">
<meta property="og:url" content="http://localhost:1313/zh/docs/concepts/policy/">
<meta property="og:title" content="策略">
<meta name="twitter:title" content="策略">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">

<script src="/js/script.js"></script>


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/zh/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/zh/docs/" >文档</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/blog/" >Kubernetes 博客</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/training/" >培训</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/partners/" >合作伙伴</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/community/" >社区</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/case-studies/" >案例分析</a>
			</li>
			
			
			
			<li class="nav-item dropdown">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/releases">Release Information</a>
	
	<a class="dropdown-item" href="https://kubernetes.io/zh/docs/concepts/policy/">v1.23</a>
	
	<a class="dropdown-item" href="https://v1-22.docs.kubernetes.io/zh/docs/concepts/policy/">v1.22</a>
	
	<a class="dropdown-item" href="https://v1-21.docs.kubernetes.io/zh/docs/concepts/policy/">v1.21</a>
	
	<a class="dropdown-item" href="https://v1-20.docs.kubernetes.io/zh/docs/concepts/policy/">v1.20</a>
	
	<a class="dropdown-item" href="https://v1-19.docs.kubernetes.io/zh/docs/concepts/policy/">v1.19</a>
	
</div>
			</li>
			
			
			<li class="nav-item dropdown">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/concepts/policy/">English</a>
	
	<a class="dropdown-item" href="/ko/docs/concepts/policy/">한국어 Korean</a>
	
	<a class="dropdown-item" href="/ja/docs/concepts/policy/">日本語 Japanese</a>
	
	<a class="dropdown-item" href="/fr/docs/concepts/policy/">Français</a>
	
	<a class="dropdown-item" href="/de/docs/concepts/policy/">Deutsch</a>
	
	<a class="dropdown-item" href="/es/docs/concepts/policy/">Español</a>
	
	<a class="dropdown-item" href="/id/docs/concepts/policy/">Bahasa Indonesia</a>
	
</div>

			</li>
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/zh/docs/concepts/policy/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">策略</h1>
<div class="lead">可配置的、可应用到一组资源的策略。</div>




    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-a935ff8c59eb116b43494255cc67f69a">限制范围</a></li>


    
  
    
    
	
<li>2: <a href="#pg-94ddc6e901c30f256138db11d09f05a3">资源配额</a></li>


    
  
    
    
	
<li>3: <a href="#pg-7352434db5f5954d2f7656b46fe5a324">进程 ID 约束与预留</a></li>


    
  
    
    
	
<li>4: <a href="#pg-b528c4464c030f3f044124b38d778f04">节点资源管理器</a></li>


    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-a935ff8c59eb116b43494255cc67f69a">1 - 限制范围</h1>
    
	<!-- overview -->
<!--
By default, containers run with unbounded [compute resources](/docs/concepts/configuration/manage-resources-containers/) on a Kubernetes cluster.
With resource quotas, cluster administrators can restrict resource consumption and creation on a <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespace'>namespace</a> basis.
Within a namespace, a Pod or Container can consume as much CPU and memory as defined by the namespace's resource quota. There is a concern that one Pod or Container could monopolize all available resources. A LimitRange is a policy to constrain resource allocations (to Pods or Containers) in a namespace.
-->
<p>默认情况下， Kubernetes 集群上的容器运行使用的<a href="/zh/docs/concepts/configuration/manage-resources-containers/">计算资源</a>没有限制。
使用资源配额，集群管理员可以以<a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>为单位，限制其资源的使用与创建。
在命名空间中，一个 Pod 或 Container 最多能够使用命名空间的资源配额所定义的 CPU 和内存用量。
有人担心，一个 Pod 或 Container 会垄断所有可用的资源。
LimitRange 是在命名空间内限制资源分配（给多个 Pod 或 Container）的策略对象。</p>
<!-- body -->
<!--
A _LimitRange_ provides constraints that can:

- Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.
- Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.
- Enforce a ratio between request and limit for a resource in a namespace.
- Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.
-->
<p>一个 <em>LimitRange（限制范围）</em> 对象提供的限制能够做到：</p>
<ul>
<li>在一个命名空间中实施对每个 Pod 或 Container 最小和最大的资源使用量的限制。</li>
<li>在一个命名空间中实施对每个 PersistentVolumeClaim 能申请的最小和最大的存储空间大小的限制。</li>
<li>在一个命名空间中实施对一种资源的申请值和限制值的比值的控制。</li>
<li>设置一个命名空间中对计算资源的默认申请/限制值，并且自动的在运行时注入到多个 Container 中。</li>
</ul>
<!--
## Enabling LimitRange

LimitRange support has been enabled by default since Kubernetes 1.10.

LimitRange support is enabled by default for many Kubernetes distributions.
-->
<h2 id="启用-limitrange">启用 LimitRange</h2>
<p>对 LimitRange 的支持自 Kubernetes 1.10 版本默认启用。</p>
<p>LimitRange 支持在很多 Kubernetes 发行版本中也是默认启用的。</p>
<!--
The name of a LimitRange object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>LimitRange 的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
### Overview of Limit Range

- The administrator creates one `LimitRange` in one namespace.
- Users create resources like Pods, Containers, and PersistentVolumeClaims in the namespace.
- The `LimitRanger` admission controller enforces defaults and limits for all Pods and Containers that do not set compute resource requirements and tracks usage to ensure it does not exceed resource minimum, maximum and ratio defined in any LimitRange present in the namespace.
- If creating or updating a resource (Pod, Container, PersistentVolumeClaim) that violates a LimitRange constraint, the request to the API server will fail with an HTTP status code `403 FORBIDDEN` and a message explaining the constraint that have been violated.
- If a LimitRange is activated in a namespace for compute resources like `cpu` and `memory`, users must specify
  requests or limits for those values. Otherwise, the system may reject Pod creation.
- LimitRange validations occurs only at Pod Admission stage, not on Running Pods.
-->
<h3 id="限制范围总览">限制范围总览</h3>
<ul>
<li>管理员在一个命名空间内创建一个 <code>LimitRange</code> 对象。</li>
<li>用户在命名空间内创建 Pod ，Container 和 PersistentVolumeClaim 等资源。</li>
<li><code>LimitRanger</code> 准入控制器对所有没有设置计算资源需求的 Pod 和 Container 设置默认值与限制值，
并跟踪其使用量以保证没有超出命名空间中存在的任意 LimitRange 对象中的最小、最大资源使用量以及使用量比值。</li>
<li>若创建或更新资源（Pod、 Container、PersistentVolumeClaim）违反了 LimitRange 的约束，
向 API 服务器的请求会失败，并返回 HTTP 状态码 <code>403 FORBIDDEN</code> 与描述哪一项约束被违反的消息。</li>
<li>若命名空间中的 LimitRange 启用了对 <code>cpu</code> 和 <code>memory</code> 的限制，
用户必须指定这些值的需求使用量与限制使用量。否则，系统将会拒绝创建 Pod。</li>
<li>LimitRange 的验证仅在 Pod 准入阶段进行，不对正在运行的 Pod 进行验证。</li>
</ul>
<!--
Examples of policies that could be created using limit range are:

- In a 2 node cluster with a capacity of 8 GiB RAM and 16 cores, constrain Pods in a namespace to request 100m of CPU with a max limit of 500m for CPU and request 200Mi for Memory with a max limit of 600Mi for Memory.
- Define default CPU limit and request to 150m and memory default request to 300Mi for Containers started with no cpu and memory requests in their specs.
-->
<p>能够使用限制范围创建的策略示例有：</p>
<ul>
<li>在一个有两个节点，8 GiB 内存与16个核的集群中，限制一个命名空间的 Pod 申请
100m 单位，最大 500m 单位的 CPU，以及申请 200Mi，最大 600Mi 的内存。</li>
<li>为 spec 中没有 cpu 和内存需求值的 Container 定义默认 CPU 限制值与需求值
150m，内存默认需求值 300Mi。</li>
</ul>
<!--
In the case where the total limits of the namespace is less than the sum of the limits of the Pods/Containers,
there may be contention for resources. In this case, the Containers or Pods will not be created.
-->
<p>在命名空间的总限制值小于 Pod 或 Container 的限制值的总和的情况下，可能会产生资源竞争。
在这种情况下，将不会创建 Container 或 Pod。</p>
<!--
Neither contention nor changes to a LimitRange will affect already created resources.
-->
<p>竞争和对 LimitRange 的改变都不会影响任何已经创建了的资源。</p>
<h2 id="what-s-next">What's next</h2>
<!--
See [LimitRanger design doc](https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md) for more information.
-->
<p>参阅 <a href="https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md">LimitRanger 设计文档</a>获取更多信息。</p>
<!--
For examples on using limits, see:

- See [how to configure minimum and maximum CPU constraints per namespace](/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/).
- See [how to configure minimum and maximum Memory constraints per namespace](/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/).
- See [how to configure default CPU Requests and Limits per namespace](/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/).
- See [how to configure default Memory Requests and Limits per namespace](/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/).
- Check [how to configure minimum and maximum Storage consumption per namespace](/docs/tasks/administer-cluster/limit-storage-consumption/#limitrange-to-limit-requests-for-storage).
- See a [detailed example on configuring quota per namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/).
-->
<p>关于使用限值的例子，可参看</p>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">如何配置每个命名空间最小和最大的 CPU 约束</a>。</li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">如何配置每个命名空间最小和最大的内存约束</a>。</li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">如何配置每个命名空间默认的 CPU 申请值和限制值</a>。</li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">如何配置每个命名空间默认的内存申请值和限制值</a>。</li>
<li><a href="/zh/docs/tasks/administer-cluster/limit-storage-consumption/#limitrange-to-limit-requests-for-storage">如何配置每个命名空间最小和最大存储使用量</a>。</li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">配置每个命名空间的配额的详细例子</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-94ddc6e901c30f256138db11d09f05a3">2 - 资源配额</h1>
    
	<!--
reviewers:
- derekwaynecarr
title: Resource Quotas
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
When several users or teams share a cluster with a fixed number of nodes,
there is a concern that one team could use more than its fair share of resources.

Resource quotas are a tool for administrators to address this concern.
-->
<p>当多个用户或团队共享具有固定节点数目的集群时，人们会担心有人使用超过其基于公平原则所分配到的资源量。</p>
<p>资源配额是帮助管理员解决这一问题的工具。</p>
<!-- body -->
<!--
A resource quota, defined by a `ResourceQuota` object, provides constraints that limit
aggregate resource consumption per namespace.  It can limit the quantity of objects that can
be created in a namespace by type, as well as the total amount of compute resources that may
be consumed by resources in that namespace.
-->
<p>资源配额，通过 <code>ResourceQuota</code> 对象来定义，对每个命名空间的资源消耗总量提供限制。
它可以限制命名空间中某种类型的对象的总数目上限，也可以限制命令空间中的 Pod 可以使用的计算资源的总上限。</p>
<!--
Resource quotas work like this:
-->
<p>资源配额的工作方式如下：</p>
<!--
- Different teams work in different namespaces.  Currently this is voluntary, but
  support for making this mandatory via ACLs is planned.
- The administrator creates one ResourceQuota for each namespace.
- Users create resources (pods, services, etc.) in the namespace, and the quota system
  tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.
- If creating or updating a resource violates a quota constraint, the request will fail with HTTP
  status code `403 FORBIDDEN` with a message explaining the constraint that would have been violated.
- If quota is enabled in a namespace for compute resources like `cpu` and `memory`, users must specify
  requests or limits for those values; otherwise, the quota system may reject pod creation.  Hint: Use
  the `LimitRanger` admission controller to force defaults for pods that make no compute resource requirements.
  See the [walkthrough](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/) for an example of how to avoid this problem.
-->
<ul>
<li>
<p>不同的团队可以在不同的命名空间下工作，目前这是非约束性的，在未来的版本中可能会通过
ACL (Access Control List 访问控制列表) 来实现强制性约束。</p>
</li>
<li>
<p>集群管理员可以为每个命名空间创建一个或多个 ResourceQuota 对象。</p>
</li>
<li>
<p>当用户在命名空间下创建资源（如 Pod、Service 等）时，Kubernetes 的配额系统会
跟踪集群的资源使用情况，以确保使用的资源用量不超过 ResourceQuota 中定义的硬性资源限额。</p>
</li>
<li>
<p>如果资源创建或者更新请求违反了配额约束，那么该请求会报错（HTTP 403 FORBIDDEN），
并在消息中给出有可能违反的约束。</p>
</li>
<li>
<p>如果命名空间下的计算资源 （如 <code>cpu</code> 和 <code>memory</code>）的配额被启用，则用户必须为
这些资源设定请求值（request）和约束值（limit），否则配额系统将拒绝 Pod 的创建。
提示: 可使用 <code>LimitRanger</code> 准入控制器来为没有设置计算资源需求的 Pod 设置默认值。</p>
<p>若想避免这类问题，请参考
<a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">演练</a>示例。</p>
</li>
</ul>
<!--
The name of a ResourceQuota object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>ResourceQuota 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
Examples of policies that could be created using namespaces and quotas are:
-->
<p>下面是使用命名空间和配额构建策略的示例：</p>
<!--
- In a cluster with a capacity of 32 GiB RAM, and 16 cores, let team A use 20 GiB and 10 cores,
  let B use 10GiB and 4 cores, and hold 2GiB and 2 cores in reserve for future allocation.
- Limit the "testing" namespace to using 1 core and 1GiB RAM.  Let the "production" namespace
  use any amount.
-->
<ul>
<li>在具有 32 GiB 内存和 16 核 CPU 资源的集群中，允许 A 团队使用 20 GiB 内存 和 10 核的 CPU 资源，
允许 B 团队使用 10 GiB 内存和 4 核的 CPU 资源，并且预留 2 GiB 内存和 2 核的 CPU 资源供将来分配。</li>
<li>限制 &quot;testing&quot; 命名空间使用 1 核 CPU 资源和 1GiB 内存。允许 &quot;production&quot; 命名空间使用任意数量。</li>
</ul>
<!--
In the case where the total capacity of the cluster is less than the sum of the quotas of the namespaces,
there may be contention for resources.  This is handled on a first-come-first-served basis.

Neither contention nor changes to quota will affect already created resources.
-->
<p>在集群容量小于各命名空间配额总和的情况下，可能存在资源竞争。资源竞争时，Kubernetes 系统会遵循先到先得的原则。</p>
<p>不管是资源竞争还是配额的修改，都不会影响已经创建的资源使用对象。</p>
<!--
## Enabling Resource Quota

Resource Quota support is enabled by default for many Kubernetes distributions. It is
enabled when the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a>
`--enable-admission-plugins=` flag has `ResourceQuota` as
one of its arguments.
-->
<h2 id="启用资源配额">启用资源配额</h2>
<p>资源配额的支持在很多 Kubernetes 版本中是默认启用的。
当 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>
的命令行标志 <code>--enable-admission-plugins=</code> 中包含 <code>ResourceQuota</code> 时，
资源配额会被启用。</p>
<!--
A resource quota is enforced in a particular namespace when there is a
ResourceQuota in that namespace.
-->
<p>当命名空间中存在一个 ResourceQuota 对象时，对于该命名空间而言，资源配额就是开启的。</p>
<!--
## Compute Resource Quota

You can limit the total sum of
[compute resources](/docs/concepts/configuration/manage-resources-containers/)
that can be requested in a given namespace.
-->
<h2 id="计算资源配额">计算资源配额</h2>
<p>用户可以对给定命名空间下的可被请求的
<a href="/zh/docs/concepts/configuration/manage-resources-containers/">计算资源</a>
总量进行限制。</p>
<!--
The following resource types are supported:
-->
<p>配额机制所支持的资源类型：</p>
<!--
| Resource Name | Description |
| --------------------- | --------------------------------------------------------- |
| `limits.cpu` | Across all pods in a non-terminal state, the sum of CPU limits cannot exceed this value. |
| `limits.memory` | Across all pods in a non-terminal state, the sum of memory limits cannot exceed this value. |
| `requests.cpu` | Across all pods in a non-terminal state, the sum of CPU requests cannot exceed this value. |
| `requests.memory` | Across all pods in a non-terminal state, the sum of memory requests cannot exceed this value. |
| `hugepages-<size>` | Across all pods in a non-terminal state, the number of huge page requests of the specified size cannot exceed this value. |
| `cpu` | Same as `requests.cpu` |
| `memory` | Same as `requests.memory` |
-->
<table>
<thead>
<tr>
<th>资源名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>limits.cpu</code></td>
<td>所有非终止状态的 Pod，其 CPU 限额总量不能超过该值。</td>
</tr>
<tr>
<td><code>limits.memory</code></td>
<td>所有非终止状态的 Pod，其内存限额总量不能超过该值。</td>
</tr>
<tr>
<td><code>requests.cpu</code></td>
<td>所有非终止状态的 Pod，其 CPU 需求总量不能超过该值。</td>
</tr>
<tr>
<td><code>requests.memory</code></td>
<td>所有非终止状态的 Pod，其内存需求总量不能超过该值。</td>
</tr>
<tr>
<td><code>hugepages-&lt;size&gt;</code></td>
<td>对于所有非终止状态的 Pod，针对指定尺寸的巨页请求总数不能超过此值。</td>
</tr>
<tr>
<td><code>cpu</code></td>
<td>与 <code>requests.cpu</code> 相同。</td>
</tr>
<tr>
<td><code>memory</code></td>
<td>与 <code>requests.memory</code> 相同。</td>
</tr>
</tbody>
</table>
<!--
### Resource Quota For Extended Resources

In addition to the resources mentioned above, in release 1.10, quota support for
[extended resources](/docs/concepts/configuration/manage-compute-resources-container/#extended-resources) is added.
-->
<h3 id="扩展资源的资源配额">扩展资源的资源配额</h3>
<p>除上述资源外，在 Kubernetes 1.10 版本中，还添加了对
<a href="/zh/docs/concepts/configuration/manage-resources-containers/#extended-resources">扩展资源</a>
的支持。</p>
<!--
As overcommit is not allowed for extended resources, it makes no sense to specify both `requests`
and `limits` for the same extended resource in a quota. So for extended resources, only quota items
with prefix `requests.` is allowed for now.
-->
<p>由于扩展资源不可超量分配，因此没有必要在配额中为同一扩展资源同时指定 <code>requests</code> 和 <code>limits</code>。
对于扩展资源而言，目前仅允许使用前缀为 <code>requests.</code> 的配额项。</p>
<!--
Take the GPU resource as an example, if the resource name is `nvidia.com/gpu`, and you want to
limit the total number of GPUs requested in a namespace to 4, you can define a quota as follows:
-->
<p>以 GPU 拓展资源为例，如果资源名称为 <code>nvidia.com/gpu</code>，并且要将命名空间中请求的 GPU
资源总数限制为 4，则可以如下定义配额：</p>
<ul>
<li><code>requests.nvidia.com/gpu: 4</code></li>
</ul>
<!--
See [Viewing and Setting Quotas](#viewing-and-setting-quotas) for more detail information.
-->
<p>有关更多详细信息，请参阅<a href="#viewing-and-setting-quotas">查看和设置配额</a>。</p>
<!--
## Storage Resource Quota

You can limit the total sum of [storage resources](/docs/concepts/storage/persistent-volumes/) that can be requested in a given namespace.

In addition, you can limit consumption of storage resources based on associated storage-class.
-->
<h2 id="存储资源配额">存储资源配额</h2>
<p>用户可以对给定命名空间下的<a href="/zh/docs/concepts/storage/persistent-volumes/">存储资源</a>
总量进行限制。</p>
<p>此外，还可以根据相关的存储类（Storage Class）来限制存储资源的消耗。</p>
<!--
| Resource Name | Description |
| --------------------- | --------------------------------------------------------- |
| `requests.storage` | Across all persistent volume claims, the sum of storage requests cannot exceed this value. |
| `persistentvolumeclaims` | The total number of [PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) that can exist in the namespace. |
| `<storage-class-name>.storageclass.storage.k8s.io/requests.storage` | Across all persistent volume claims associated with the `<storage-class-name>`, the sum of storage requests cannot exceed this value. |
| `<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims` | Across all persistent volume claims associated with the storage-class-name, the total number of [persistent volume claims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) that can exist in the namespace. |
-->
<table>
<thead>
<tr>
<th>资源名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>requests.storage</code></td>
<td>所有 PVC，存储资源的需求总量不能超过该值。</td>
</tr>
<tr>
<td><code>persistentvolumeclaims</code></td>
<td>在该命名空间中所允许的 <a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PVC</a> 总量。</td>
</tr>
<tr>
<td><code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/requests.storage</code></td>
<td>在所有与 <code>&lt;storage-class-name&gt;</code> 相关的持久卷申领中，存储请求的总和不能超过该值。</td>
</tr>
<tr>
<td><code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/persistentvolumeclaims</code></td>
<td>在与 storage-class-name 相关的所有持久卷申领中，命名空间中可以存在的<a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">持久卷申领</a>总数。</td>
</tr>
</tbody>
</table>
<!--
For example, if an operator wants to quota storage with `gold` storage class separate from `bronze` storage class, the operator can
define a quota as follows:
-->
<p>例如，如果一个操作人员针对 <code>gold</code> 存储类型与 <code>bronze</code> 存储类型设置配额，
操作人员可以定义如下配额：</p>
<ul>
<li><code>gold.storageclass.storage.k8s.io/requests.storage: 500Gi</code></li>
<li><code>bronze.storageclass.storage.k8s.io/requests.storage: 100Gi</code></li>
</ul>
<!--
In release 1.8, quota support for local ephemeral storage is added as an alpha feature:
-->
<p>在 Kubernetes 1.8 版本中，本地临时存储的配额支持已经是 Alpha 功能：</p>
<!--
| Resource Name | Description |
| ------------------------------- |----------------------------------------------------------- |
| `requests.ephemeral-storage` | Across all pods in the namespace, the sum of local ephemeral storage requests cannot exceed this value. |
| `limits.ephemeral-storage` | Across all pods in the namespace, the sum of local ephemeral storage limits cannot exceed this value. |
| `ephemeral-storage` | Same as `requests.ephemeral-storage`. |
-->
<table>
<thead>
<tr>
<th>资源名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>requests.ephemeral-storage</code></td>
<td>在命名空间的所有 Pod 中，本地临时存储请求的总和不能超过此值。</td>
</tr>
<tr>
<td><code>limits.ephemeral-storage</code></td>
<td>在命名空间的所有 Pod 中，本地临时存储限制值的总和不能超过此值。</td>
</tr>
<tr>
<td><code>ephemeral-storage</code></td>
<td>与 <code>requests.ephemeral-storage</code> 相同。</td>
</tr>
</tbody>
</table>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
When using a CRI container runtime, container logs will count against the ephemeral storage quota.
This can result in the unexpected eviction of pods that have exhausted their storage quotas.
Refer to [Logging Architecture](/docs/concepts/cluster-administration/logging/) for details.
-->
<p>如果所使用的是 CRI 容器运行时，容器日志会被计入临时存储配额。
这可能会导致存储配额耗尽的 Pods 被意外地驱逐出节点。
参考<a href="/zh/docs/concepts/cluster-administration/logging/">日志架构</a>
了解详细信息。
</div>
<!--
## Object Count Quota

You can set quota for the total number of certain resources of all standard,
namespaced resource types using the following syntax:

* `count/<resource>.<group>` for resources from non-core groups
* `count/<resource>` for resources from the core group
-->
<h2 id="对象数量配额">对象数量配额</h2>
<p>你可以使用以下语法对所有标准的、命名空间域的资源类型进行配额设置：</p>
<ul>
<li><code>count/&lt;resource&gt;.&lt;group&gt;</code>：用于非核心（core）组的资源</li>
<li><code>count/&lt;resource&gt;</code>：用于核心组的资源</li>
</ul>
<!--
Here is an example set of resources users may want to put under object count quota:
-->
<p>这是用户可能希望利用对象计数配额来管理的一组资源示例。</p>
<ul>
<li><code>count/persistentvolumeclaims</code></li>
<li><code>count/services</code></li>
<li><code>count/secrets</code></li>
<li><code>count/configmaps</code></li>
<li><code>count/replicationcontrollers</code></li>
<li><code>count/deployments.apps</code></li>
<li><code>count/replicasets.apps</code></li>
<li><code>count/statefulsets.apps</code></li>
<li><code>count/jobs.batch</code></li>
<li><code>count/cronjobs.batch</code></li>
</ul>
<!--
The same syntax can be used for custom resources.
For example, to create a quota on a `widgets` custom resource in the `example.com` API group, use `count/widgets.example.com`.
-->
<p>相同语法也可用于自定义资源。
例如，要对 <code>example.com</code> API 组中的自定义资源 <code>widgets</code> 设置配额，请使用
<code>count/widgets.example.com</code>。</p>
<!--
When using `count/*` resource quota, an object is charged against the quota if it exists in server storage.
These types of quotas are useful to protect against exhaustion of storage resources.  For example, you may
want to limit the number of Secrets in a server given their large size. Too many Secrets in a cluster can
actually prevent servers and controllers from starting. You can set a quota for Jobs to protect against
a poorly configured CronJob. CronJobs that create too many Jobs in a namespace can lead to a denial of service.
-->
<p>当使用 <code>count/*</code> 资源配额时，如果对象存在于服务器存储中，则会根据配额管理资源。
这些类型的配额有助于防止存储资源耗尽。例如，用户可能想根据服务器的存储能力来对服务器中
Secret 的数量进行配额限制。
集群中存在过多的 Secret 实际上会导致服务器和控制器无法启动。
用户可以选择对 Job 进行配额管理，以防止配置不当的 CronJob 在某命名空间中创建太多
Job 而导致集群拒绝服务。</p>
<!--
It is possible to do generic object count quota on a limited set of resources.
In addition, it is possible to further constrain quota for particular resources by their type.

The following types are supported:
-->
<p>对有限的一组资源上实施一般性的对象数量配额也是可能的。
此外，还可以进一步按资源的类型设置其配额。</p>
<p>支持以下类型：</p>
<!--
| Resource Name | Description |
| ----------------------------|--------------------------------------------- |
| `configmaps` | The total number of ConfigMaps that can exist in the namespace. |
| `persistentvolumeclaims` | The total number of [PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) that can exist in the namespace. |
| `pods` | The total number of Pods in a non-terminal state that can exist in the namespace.  A pod is in a terminal state if `.status.phase in (Failed, Succeeded)` is true.  |
| `replicationcontrollers` | The total number of ReplicationControllers that can exist in the namespace. |
| `resourcequotas` | The total number of ResourceQuotas that can exist in the namespace. |
| `services` | The total number of Services that can exist in the namespace. |
| `services.loadbalancers` | The total number of Services of type `LoadBalancer` that can exist in the namespace. |
| `services.nodeports` | The total number of Services of type `NodePort` that can exist in the namespace. |
| `secrets` | The total number of Secrets that can exist in the namespace. |
-->
<table>
<thead>
<tr>
<th>资源名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>configmaps</code></td>
<td>在该命名空间中允许存在的 ConfigMap 总数上限。</td>
</tr>
<tr>
<td><code>persistentvolumeclaims</code></td>
<td>在该命名空间中允许存在的 <a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PVC</a> 的总数上限。</td>
</tr>
<tr>
<td><code>pods</code></td>
<td>在该命名空间中允许存在的非终止状态的 Pod 总数上限。Pod 终止状态等价于 Pod 的 <code>.status.phase in (Failed, Succeeded)</code> 为真。</td>
</tr>
<tr>
<td><code>replicationcontrollers</code></td>
<td>在该命名空间中允许存在的 ReplicationController 总数上限。</td>
</tr>
<tr>
<td><code>resourcequotas</code></td>
<td>在该命名空间中允许存在的 ResourceQuota 总数上限。</td>
</tr>
<tr>
<td><code>services</code></td>
<td>在该命名空间中允许存在的 Service 总数上限。</td>
</tr>
<tr>
<td><code>services.loadbalancers</code></td>
<td>在该命名空间中允许存在的 LoadBalancer 类型的 Service 总数上限。</td>
</tr>
<tr>
<td><code>services.nodeports</code></td>
<td>在该命名空间中允许存在的 NodePort 类型的 Service 总数上限。</td>
</tr>
<tr>
<td><code>secrets</code></td>
<td>在该命名空间中允许存在的 Secret 总数上限。</td>
</tr>
</tbody>
</table>
<!--
For example, `pods` quota counts and enforces a maximum on the number of `pods`
created in a single namespace that are not terminal. You might want to set a `pods`
quota on a namespace to avoid the case where a user creates many small pods and
exhausts the cluster's supply of Pod IPs.
-->
<p>例如，<code>pods</code> 配额统计某个命名空间中所创建的、非终止状态的 <code>Pod</code> 个数并确保其不超过某上限值。
用户可能希望在某命名空间中设置 <code>pods</code> 配额，以避免有用户创建很多小的 Pod，
从而耗尽集群所能提供的 Pod IP 地址。</p>
<!--
## Quota Scopes

Each quota can have an associated set of `scopes`. A quota will only measure usage for a resource if it matches
the intersection of enumerated scopes.
-->
<h2 id="quota-scopes">配额作用域  </h2>
<p>每个配额都有一组相关的 <code>scope</code>（作用域），配额只会对作用域内的资源生效。
配额机制仅统计所列举的作用域的交集中的资源用量。</p>
<!--
When a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope.
Resources specified on the quota outside of the allowed set results in a validation error.
-->
<p>当一个作用域被添加到配额中后，它会对作用域相关的资源数量作限制。
如配额中指定了允许（作用域）集合之外的资源，会导致验证错误。</p>
<!--
| Scope | Description |
| ----- | ------------ |
| `Terminating` | Match pods where `.spec.activeDeadlineSeconds >= 0` |
| `NotTerminating` | Match pods where `.spec.activeDeadlineSeconds is nil` |
| `BestEffort` | Match pods that have best effort quality of service. |
| `NotBestEffort` | Match pods that do not have best effort quality of service. |
| `PriorityClass` | Match pods that references the specified [priority class](/docs/concepts/scheduling-eviction/pod-priority-preemption). |
| `CrossNamespacePodAffinity` | Match pods that have cross-namespace pod [(anti)affinity terms](/docs/concepts/scheduling-eviction/assign-pod-node). |
-->
<table>
<thead>
<tr>
<th>作用域</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Terminating</code></td>
<td>匹配所有 <code>spec.activeDeadlineSeconds</code> 不小于 0 的 Pod。</td>
</tr>
<tr>
<td><code>NotTerminating</code></td>
<td>匹配所有 <code>spec.activeDeadlineSeconds</code> 是 nil 的 Pod。</td>
</tr>
<tr>
<td><code>BestEffort</code></td>
<td>匹配所有 Qos 是 BestEffort 的 Pod。</td>
</tr>
<tr>
<td><code>NotBestEffort</code></td>
<td>匹配所有 Qos 不是 BestEffort 的 Pod。</td>
</tr>
<tr>
<td><code>PriorityClass</code></td>
<td>匹配所有引用了所指定的<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption">优先级类</a>的 Pods。</td>
</tr>
<tr>
<td><code>CrossNamespacePodAffinity</code></td>
<td>匹配那些设置了跨名字空间 <a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node">（反）亲和性条件</a>的 Pod。</td>
</tr>
</tbody>
</table>
<!--
The `BestEffort` scope restricts a quota to tracking the following resource:

* `pods`

The `Terminating`, `NotTerminating`, `NotBestEffort` and `PriorityClass`
scopes restrict a quota to tracking the following resources:
-->
<p><code>BestEffort</code> 作用域限制配额跟踪以下资源：</p>
<ul>
<li><code>pods</code></li>
</ul>
<p><code>Terminating</code>、<code>NotTerminating</code>、<code>NotBestEffort</code> 和 <code>PriorityClass</code> 这些作用域限制配额跟踪以下资源：</p>
<ul>
<li><code>pods</code></li>
<li><code>cpu</code></li>
<li><code>memory</code></li>
<li><code>requests.cpu</code></li>
<li><code>requests.memory</code></li>
<li><code>limits.cpu</code></li>
<li><code>limits.memory</code></li>
</ul>
<!--
Note that you cannot specify both the `Terminating` and the `NotTerminating`
scopes in the same quota, and you cannot specify both the `BestEffort` and
`NotBestEffort` scopes in the same quota either.

The `scopeSelector` supports the following values in the `operator` field:
-->
<p>需要注意的是，你不可以在同一个配额对象中同时设置 <code>Terminating</code> 和 <code>NotTerminating</code>
作用域，你也不可以在同一个配额中同时设置 <code>BestEffort</code> 和 <code>NotBestEffort</code>
作用域。</p>
<p><code>scopeSelector</code> 支持在 <code>operator</code> 字段中使用以下值：</p>
<ul>
<li><code>In</code></li>
<li><code>NotIn</code></li>
<li><code>Exists</code></li>
<li><code>DoesNotExist</code></li>
</ul>
<!--
When using one of the following values as the `scopeName` when defining the
`scopeSelector`, the `operator` must be `Exists`. 
-->
<p>定义 <code>scopeSelector</code> 时，如果使用以下值之一作为 <code>scopeName</code> 的值，则对应的
<code>operator</code> 只能是 <code>Exists</code>。</p>
<ul>
<li><code>Terminating</code></li>
<li><code>NotTerminating</code></li>
<li><code>BestEffort</code></li>
<li><code>NotBestEffort</code></li>
</ul>
<!--
If the `operator` is `In` or `NotIn`, the `values` field must have at least
one value. For example:
-->
<p>如果 <code>operator</code> 是 <code>In</code> 或 <code>NotIn</code> 之一，则 <code>values</code> 字段必须至少包含一个值。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- middle<span style="color:#bbb">
</span></code></pre></div><!--
If the `operator` is `Exists` or `DoesNotExist`, the `values field must *NOT* be
specified.
-->
<p>如果 <code>operator</code> 为 <code>Exists</code> 或 <code>DoesNotExist</code>，则<em>不</em>可以设置 <code>values</code> 字段。</p>
<!--
### Resource Quota Per PriorityClass
-->
<h3 id="基于优先级类-priorityclass-来设置资源配额">基于优先级类（PriorityClass）来设置资源配额</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code>
</div>


<!--
Pods can be created at a specific [priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority).
You can control a pod's consumption of system resources based on a pod's priority, by using the `scopeSelector`
field in the quota spec.
-->
<p>Pod 可以创建为特定的<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority">优先级</a>。
通过使用配额规约中的 <code>scopeSelector</code> 字段，用户可以根据 Pod 的优先级控制其系统资源消耗。</p>
<!--
A quota is matched and consumed only if `scopeSelector` in the quota spec selects the pod.
-->
<p>仅当配额规范中的 <code>scopeSelector</code> 字段选择到某 Pod 时，配额机制才会匹配和计量 Pod 的资源消耗。</p>
<!--
When quota is scoped for priority class using `scopeSelector` field, quota object
is restricted to track only following resources:
-->
<p>如果配额对象通过 <code>scopeSelector</code> 字段设置其作用域为优先级类，则配额对象只能
跟踪以下资源：</p>
<ul>
<li><code>pods</code></li>
<li><code>cpu</code></li>
<li><code>memory</code></li>
<li><code>ephemeral-storage</code></li>
<li><code>limits.cpu</code></li>
<li><code>limits.memory</code></li>
<li><code>limits.ephemeral-storage</code></li>
<li><code>requests.cpu</code></li>
<li><code>requests.memory</code></li>
<li><code>requests.ephemeral-storage</code></li>
</ul>
<!--
This example creates a quota object and matches it with pods at specific priorities. The example
works as follows:
-->
<p>本示例创建一个配额对象，并将其与具有特定优先级的 Pod 进行匹配。
该示例的工作方式如下：</p>
<!--
- Pods in the cluster have one of the three priority classes, "low", "medium", "high".
- One quota object is created for each priority.
-->
<ul>
<li>集群中的 Pod 可取三个优先级类之一，即 &quot;low&quot;、&quot;medium&quot;、&quot;high&quot;。</li>
<li>为每个优先级创建一个配额对象。</li>
</ul>
<!-- Save the following YAML to a file `quota.yml`.  -->
<p>将以下 YAML 保存到文件 <code>quota.yml</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>List<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pods-high<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1000&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Gi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">operator </span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;high&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pods-medium<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>20Gi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">operator </span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;medium&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pods-low<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">operator </span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;low&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
Apply the YAML using `kubectl create`.
-->
<p>使用 <code>kubectl create</code> 命令运行以下操作。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./quota.yml
</code></pre></div><pre><code>resourcequota/pods-high created
resourcequota/pods-medium created
resourcequota/pods-low created
</code></pre><!--
Verify that `Used` quota is `0` using `kubectl describe quota`.
-->
<p>使用 <code>kubectl describe quota</code> 操作验证配额的 <code>Used</code> 值为 <code>0</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe quota
</code></pre></div><pre><code>Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     1k
memory      0     200Gi
pods        0     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
</code></pre><!--
Create a pod with priority "high". Save the following YAML to a
file `high-priority-pod.yml`.
-->
<p>创建优先级为 &quot;high&quot; 的 Pod。
将以下 YAML 保存到文件 <code>high-priority-pod.yml</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>high-priority<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>high-priority<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>ubuntu<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/bin/sh&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;while true; do echo hello; sleep 10;done&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">priorityClassName</span>:<span style="color:#bbb"> </span>high<span style="color:#bbb">
</span></code></pre></div><!--
Apply it with `kubectl create`.
-->
<p>使用 <code>kubectl create</code> 运行以下操作。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./high-priority-pod.yml
</code></pre></div><!--
Verify that "Used" stats for "high" priority quota, `pods-high`, has changed and that
the other two quotas are unchanged.
-->
<p>确认 &quot;high&quot; 优先级配额 <code>pods-high</code> 的 &quot;Used&quot; 统计信息已更改，并且其他两个配额未更改。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe quota
</code></pre></div><pre><code>Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         500m  1k
memory      10Gi  200Gi
pods        1     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
</code></pre><!--
### Cross-namespace Pod Affinity Quota
-->
<h3 id="cross-namespace-pod-affinity-quota">跨名字空间的 Pod 亲和性配额  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
Operators can use `CrossNamespacePodAffinity` quota scope to limit which namespaces are allowed to
have pods with affinity terms that cross namespaces. Specifically, it controls which pods are allowed
to set `namespaces` or `namespaceSelector` fields in pod affinity terms.
-->
<p>集群运维人员可以使用 <code>CrossNamespacePodAffinity</code> 配额作用域来
限制哪个名字空间中可以存在包含跨名字空间亲和性规则的 Pod。
更为具体一点，此作用域用来配置哪些 Pod 可以在其 Pod 亲和性规则
中设置 <code>namespaces</code> 或 <code>namespaceSelector</code> 字段。</p>
<!--
Preventing users from using cross-namespace affinity terms might be desired since a pod
with anti-affinity constraints can block pods from all other namespaces 
from getting scheduled in a failure domain. 
-->
<p>禁止用户使用跨名字空间的亲和性规则可能是一种被需要的能力，因为带有
反亲和性约束的 Pod 可能会阻止所有其他名字空间的 Pod 被调度到某失效域中。</p>
<!--
Using this scope operators can prevent certain namespaces (`foo-ns` in the example below) 
from having pods that use cross-namespace pod affinity by creating a resource quota object in
that namespace with `CrossNamespaceAffinity` scope and hard limit of 0:
-->
<p>使用此作用域操作符可以避免某些名字空间（例如下面例子中的 <code>foo-ns</code>）运行
特别的 Pod，这类 Pod 使用跨名字空间的 Pod 亲和性约束，在该名字空间中创建
了作用域为 <code>CrossNamespaceAffinity</code> 的、硬性约束为 0 的资源配额对象。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>disable-cross-namespace-affinity<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>foo-ns<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>CrossNamespaceAffinity<span style="color:#bbb">
</span></code></pre></div><!--
If operators want to disallow using `namespaces` and `namespaceSelector` by default, and 
only allow it for specific namespaces, they could configure `CrossNamespaceAffinity` 
as a limited resource by setting the kube-apiserver flag -admission-control-config-file
to the path of the following configuration file:
-->
<p>如果集群运维人员希望默认禁止使用 <code>namespaces</code> 和 <code>namespaceSelector</code>，而
仅仅允许在特定名字空间中这样做，他们可以将 <code>CrossNamespaceAffinity</code> 作为一个
被约束的资源。方法是为 <code>kube-apiserver</code> 设置标志
<code>--admission-control-config-file</code>，使之指向如下的配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>AdmissionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">plugins</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;ResourceQuota&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">configuration</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuotaConfiguration<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">limitedResources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>pods<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchScopes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>CrossNamespaceAffinity<span style="color:#bbb">
</span></code></pre></div><!--
With the above configuration, pods can use `namespaces` and `namespaceSelector` in pod affinity only
if the namespace where they are created have a resource quota object with 
`CrossNamespaceAffinity` scope and a hard limit greater than or equal to the number of pods using those fields.
-->
<p>基于上面的配置，只有名字空间中包含作用域为 <code>CrossNamespaceAffinity</code> 且
硬性约束大于或等于使用 <code>namespaces</code> 和 <code>namespaceSelector</code> 字段的 Pods
个数时，才可以在该名字空间中继续创建在其 Pod 亲和性规则中设置 <code>namespaces</code>
或 <code>namespaceSelector</code> 的新 Pod。</p>
<!--
This feature is beta and enabled by default. You can disable it using the
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`PodAffinityNamespaceSelector` in both kube-apiserver and kube-scheduler.
-->
<p>此功能特性处于 Beta 阶段，默认被禁用。你可以通过为 kube-apiserver 和
kube-scheduler 设置
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>PodAffinityNamespaceSelector</code> 来启用此特性。</p>
<!--
## Requests compared to Limits {#requests-vs-limits}

When allocating compute resources, each container may specify a request and a limit value for either CPU or memory.
The quota can be configured to quota either value.
-->
<h2 id="requests-vs-limits">请求与限制的比较  </h2>
<p>分配计算资源时，每个容器可以为 CPU 或内存指定请求和约束。
配额可以针对二者之一进行设置。</p>
<!--
If the quota has a value specified for `requests.cpu` or `requests.memory`, then it requires that every incoming
container makes an explicit request for those resources.  If the quota has a value specified for `limits.cpu` or `limits.memory`,
then it requires that every incoming container specifies an explicit limit for those resources.
-->
<p>如果配额中指定了 <code>requests.cpu</code> 或 <code>requests.memory</code> 的值，则它要求每个容器都显式给出对这些资源的请求。
同理，如果配额中指定了 <code>limits.cpu</code> 或 <code>limits.memory</code> 的值，那么它要求每个容器都显式设定对应资源的限制。</p>
<!--
## Viewing and Setting Quotas

Kubectl supports creating, updating, and viewing quotas:
-->
<h2 id="viewing-and-setting-quotas">查看和设置配额</h2>
<p>Kubectl 支持创建、更新和查看配额：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace myspace
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt; compute-resources.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: ResourceQuota
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: compute-resources
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  hard:
</span><span style="color:#b44">    requests.cpu: &#34;1&#34;
</span><span style="color:#b44">    requests.memory: 1Gi
</span><span style="color:#b44">    limits.cpu: &#34;2&#34;
</span><span style="color:#b44">    limits.memory: 2Gi
</span><span style="color:#b44">    requests.nvidia.com/gpu: 4
</span><span style="color:#b44">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./compute-resources.yaml --namespace<span style="color:#666">=</span>myspace
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt; object-counts.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: ResourceQuota
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: object-counts
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  hard:
</span><span style="color:#b44">    configmaps: &#34;10&#34;
</span><span style="color:#b44">    persistentvolumeclaims: &#34;4&#34;
</span><span style="color:#b44">    pods: &#34;4&#34;
</span><span style="color:#b44">    replicationcontrollers: &#34;20&#34;
</span><span style="color:#b44">    secrets: &#34;10&#34;
</span><span style="color:#b44">    services: &#34;10&#34;
</span><span style="color:#b44">    services.loadbalancers: &#34;2&#34;
</span><span style="color:#b44">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./object-counts.yaml --namespace<span style="color:#666">=</span>myspace
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get quota --namespace<span style="color:#666">=</span>myspace
</code></pre></div><pre><code class="language-none" data-lang="none">NAME                    AGE
compute-resources       30s
object-counts           32s
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe quota compute-resources --namespace<span style="color:#666">=</span>myspace
</code></pre></div><pre><code class="language-none" data-lang="none">Name:                    compute-resources
Namespace:               myspace
Resource                 Used  Hard
--------                 ----  ----
limits.cpu               0     2
limits.memory            0     2Gi
requests.cpu             0     1
requests.memory          0     1Gi
requests.nvidia.com/gpu  0     4
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe quota object-counts --namespace<span style="color:#666">=</span>myspace
</code></pre></div><pre><code class="language-none" data-lang="none">Name:                   object-counts
Namespace:              myspace
Resource                Used    Hard
--------                ----    ----
configmaps              0       10
persistentvolumeclaims  0       4
pods                    0       4
replicationcontrollers  0       20
secrets                 1       10
services                0       10
services.loadbalancers  0       2
</code></pre><!--
Kubectl also supports object count quota for all standard namespaced resources
using the syntax `count/<resource>.<group>`:
-->
<p>kubectl 还使用语法 <code>count/&lt;resource&gt;.&lt;group&gt;</code> 支持所有标准的、命名空间域的资源的对象计数配额：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace myspace
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create quota <span style="color:#a2f">test</span> --hard<span style="color:#666">=</span>count/deployments.apps<span style="color:#666">=</span>2,count/replicasets.apps<span style="color:#666">=</span>4,count/pods<span style="color:#666">=</span>3,count/secrets<span style="color:#666">=</span><span style="color:#666">4</span> --namespace<span style="color:#666">=</span>myspace
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment nginx --image<span style="color:#666">=</span>nginx --namespace<span style="color:#666">=</span>myspace --replicas<span style="color:#666">=</span><span style="color:#666">2</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe quota --namespace<span style="color:#666">=</span>myspace
</code></pre></div><pre><code>Name:                         test
Namespace:                    myspace
Resource                      Used  Hard
--------                      ----  ----
count/deployments.apps        1     2
count/pods                    2     3
count/replicasets.apps        1     4
count/secrets                 1     4
</code></pre><!--
## Quota and Cluster Capacity

ResourceQuotas are independent of the cluster capacity. They are
expressed in absolute units.  So, if you add nodes to your cluster, this does *not*
automatically give each namespace the ability to consume more resources.
-->
<h2 id="quota-and-cluster-capacity">配额和集群容量  </h2>
<p>ResourceQuota 与集群资源总量是完全独立的。它们通过绝对的单位来配置。
所以，为集群添加节点时，资源配额<em>不会</em>自动赋予每个命名空间消耗更多资源的能力。</p>
<!--
Sometimes more complex policies may be desired, such as:

- Proportionally divide total cluster resources among several teams.
- Allow each tenant to grow resource usage as needed, but have a generous
  limit to prevent accidental resource exhaustion.
- Detect demand from one namespace, add nodes, and increase quota.
-->
<p>有时可能需要资源配额支持更复杂的策略，比如：</p>
<ul>
<li>在几个团队中按比例划分总的集群资源。</li>
<li>允许每个租户根据需要增加资源使用量，但要有足够的限制以防止资源意外耗尽。</li>
<li>探测某个命名空间的需求，添加物理节点并扩大资源配额值。</li>
</ul>
<!--
Such policies could be implemented using `ResourceQuotas` as building blocks, by
writing a "controller" that watches the quota usage and adjusts the quota
hard limits of each namespace according to other signals.
-->
<p>这些策略可以通过将资源配额作为一个组成模块、手动编写一个控制器来监控资源使用情况，
并结合其他信号调整命名空间上的硬性资源配额来实现。</p>
<!--
Note that resource quota divides up aggregate cluster resources, but it creates no
restrictions around nodes: pods from several namespaces may run on the same node.
-->
<p>注意：资源配额对集群资源总体进行划分，但它对节点没有限制：来自不同命名空间的 Pod 可能在同一节点上运行。</p>
<!--
## Limit Priority Class consumption by default

It may be desired that pods at a particular priority, eg. "cluster-services",
should be allowed in a namespace, if and only if, a matching quota object exists.
-->
<h2 id="默认情况下限制特定优先级的资源消耗">默认情况下限制特定优先级的资源消耗</h2>
<p>有时候可能希望当且仅当某名字空间中存在匹配的配额对象时，才可以创建特定优先级
（例如 &quot;cluster-services&quot;）的 Pod。</p>
<!--
With this mechanism, operators will be able to restrict usage of certain high
priority classes to a limited number of namespaces and not every namespace
will be able to consume these priority classes by default.
-->
<p>通过这种机制，操作人员能够将限制某些高优先级类仅出现在有限数量的命名空间中，
而并非每个命名空间默认情况下都能够使用这些优先级类。</p>
<!--
To enforce this, kube-apiserver flag `-admission-control-config-file` should be
used to pass path to the following configuration file:
-->
<p>要实现此目的，应设置 kube-apiserver 的标志 <code>--admission-control-config-file</code>
指向如下配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>AdmissionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">plugins</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;ResourceQuota&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">configuration</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuotaConfiguration<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">limitedResources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>pods<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchScopes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;cluster-services&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
Then, create a resource quota object in the `kube-system` namespace:
-->
<p>现在在 <code>kube-system</code> 名字空间中创建一个资源配额对象：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/policy/priority-class-resourcequota.yaml" download="policy/priority-class-resourcequota.yaml"><code>policy/priority-class-resourcequota.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('policy-priority-class-resourcequota-yaml')" title="Copy policy/priority-class-resourcequota.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="policy-priority-class-resourcequota-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pods-cluster-services<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">operator </span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;cluster-services&#34;</span>]</code></pre></div>
    </div>
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system
</code></pre></div><pre><code class="language-none" data-lang="none">resourcequota/pods-cluster-services created
</code></pre><!--
In this case, a pod creation will be allowed if:

1.  the Pod's `priorityClassName` is not specified.
1.  the Pod's `priorityClassName` is specified to a value other than `cluster-services`.
1.  the Pod's `priorityClassName` is set to `cluster-services`, it is to be created
   in the `kube-system` namespace, and it has passed the resource quota check.
-->
<p>在这里，当以下条件满足时可以创建 Pod：</p>
<ol>
<li>Pod 未设置 <code>priorityClassName</code></li>
<li>Pod 的 <code>priorityClassName</code> 设置值不是 <code>cluster-services</code></li>
<li>Pod 的 <code>priorityClassName</code> 设置值为 <code>cluster-services</code>，它将被创建于
<code>kube-system</code> 名字空间中，并且它已经通过了资源配额检查。</li>
</ol>
<!--
A Pod creation request is rejected if its `priorityClassName` is set to `cluster-services`
and it is to be created in a namespace other than `kube-system`.
-->
<p>如果 Pod 的 <code>priorityClassName</code> 设置为 <code>cluster-services</code>，但要被创建到
<code>kube-system</code> 之外的别的名字空间，则 Pod 创建请求也被拒绝。</p>
<h2 id="what-s-next">What's next</h2>
<!--
- See [ResourceQuota design doc](https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_resource_quota.md) for more information.
- See a [detailed example for how to use resource quota](/docs/tasks/administer-cluster/quota-api-object/).
- Read [Quota support for priority class design doc](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/pod-priority-resourcequota.md).
- See [LimitedResources](https://github.com/kubernetes/kubernetes/pull/36765)
-->
<ul>
<li>查看<a href="https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_resource_quota.md">资源配额设计文档</a></li>
<li>查看<a href="/zh/docs/tasks/administer-cluster/quota-api-object/">如何使用资源配额的详细示例</a>。</li>
<li>阅读<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/pod-priority-resourcequota.md">优先级类配额支持的设计文档</a>。
了解更多信息。</li>
<li>参阅 <a href="https://github.com/kubernetes/kubernetes/pull/36765">LimitedResources</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7352434db5f5954d2f7656b46fe5a324">3 - 进程 ID 约束与预留</h1>
    
	<!--
reviewers:
- derekwaynecarr
title: Process ID Limits And Reservations
content_type: concept
weight: 40
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>


<!--
Kubernetes allow you to limit the number of process IDs (PIDs) that a
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> can use.
You can also reserve a number of allocatable PIDs for each <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a>
for use by the operating system and daemons (rather than by Pods).
-->
<p>Kubernetes 允许你限制一个 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 中可以使用的
进程 ID（PID）数目。你也可以为每个 <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>
预留一定数量的可分配的 PID，供操作系统和守护进程（而非 Pod）使用。</p>
<!-- body -->
<!--
Process IDs (PIDs) are a fundamental resource on nodes. It is trivial to hit the
task limit without hitting any other resource limits, which can then cause
instability to a host machine.
-->
<p>进程 ID（PID）是节点上的一种基础资源。很容易就会在尚未超出其它资源约束的时候就
已经触及任务个数上限，进而导致宿主机器不稳定。</p>
<!--
Cluster administrators require mechanisms to ensure that Pods running in the
cluster cannot induce PID exhaustion that prevents host daemons (such as the
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> or
<a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a>,
and potentially also the container runtime) from running.
In addition, it is important to ensure that PIDs are limited among Pods in order
to ensure they have limited impact on other workloads on the same node.
-->
<p>集群管理员需要一定的机制来确保集群中运行的 Pod 不会导致 PID 资源枯竭，甚而
造成宿主机上的守护进程（例如
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 或者
<a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a>
乃至包括容器运行时本身）无法正常运行。
此外，确保 Pod 中 PID 的个数受限对于保证其不会影响到同一节点上其它负载也很重要。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
On certain Linux installations, the operating system sets the PIDs limit to a low default,
such as `32768`. Consider raising the value of `/proc/sys/kernel/pid_max`.
-->
<p>在某些 Linux 安装环境中，操作系统会将 PID 约束设置为一个较低的默认值，例如
<code>32768</code>。这时可以考虑提升 <code>/proc/sys/kernel/pid_max</code> 的设置值。
</div>
<!--
You can configure a kubelet to limit the number of PIDs a given Pod can consume.
For example, if your node's host OS is set to use a maximum of `262144` PIDs and
expect to host less than `250` Pods, one can give each Pod a budget of `1000`
PIDs to prevent using up that node's overall number of available PIDs. If the
admin wants to overcommit PIDs similar to CPU or memory, they may do so as well
with some additional risks. Either way, a single Pod will not be able to bring
the whole machine down. This kind of resource limiting helps to prevent simple
fork bombs from affecting operation of an entire cluster.
-->
<p>你可以配置 kubelet 限制给定 Pod 能够使用的 PID 个数。
例如，如果你的节点上的宿主操作系统被设置为最多可使用 <code>262144</code> 个 PID，同时预期
节点上会运行的 Pod 个数不会超过 <code>250</code>，那么你可以为每个 Pod 设置 <code>1000</code> 个 PID
的预算，避免耗尽该节点上可用 PID 的总量。
如果管理员系统像 CPU 或内存那样允许对 PID 进行过量分配（Overcommit），他们也可以
这样做，只是会有一些额外的风险。不管怎样，任何一个 Pod 都不可以将整个机器的运行
状态破坏。这类资源限制有助于避免简单的派生炸弹（Fork
Bomb）影响到整个集群的运行。</p>
<!--
Per-Pod PID limiting allows administrators to protect one Pod from another, but
does not ensure that all Pods scheduled onto that host are unable to impact the node overall.
Per-Pod limiting also does not protect the node agents themselves from PID exhaustion.

You can also reserve an amount of PIDs for node overhead, separate from the
allocation to Pods. This is similar to how you can reserve CPU, memory, or other
resources for use by the operating system and other facilities outside of Pods
and their containers.
-->
<p>在 Pod 级别设置 PID 限制使得管理员能够保护 Pod 之间不会互相伤害，不过无法
确保所有调度到该宿主机器上的所有 Pod 都不会影响到节点整体。
Pod 级别的限制也无法保护节点代理任务自身不会受到 PID 耗尽的影响。</p>
<p>你也可以预留一定量的 PID，作为节点的额外开销，与分配给 Pod 的 PID 集合独立。
这有点类似于在给操作系统和其它设施预留 CPU、内存或其它资源时所做的操作，
这些任务都在 Pod 及其所包含的容器之外运行。</p>
<!--
PID limiting is a an important sibling to [compute
resource](/docs/concepts/configuration/manage-resources-containers/) requests
and limits. However, you specify it in a different way: rather than defining a
Pod's resource limit in the `.spec` for a Pod, you configure the limit as a
setting on the kubelet. Pod-defined PID limits are not currently supported.
-->
<p>PID 限制是与<a href="/zh/docs/concepts/configuration/manage-resources-containers/">计算资源</a>
请求和限制相辅相成的一种机制。不过，你需要用一种不同的方式来设置这一限制：
你需要将其设置到 kubelet 上而不是在 Pod 的 <code>.spec</code> 中为 Pod 设置资源限制。
目前还不支持在 Pod 级别设置 PID 限制。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
This means that the limit that applies to a Pod may be different depending on
where the Pod is scheduled. To make things simple, it's easiest if all Nodes use
the same PID resource limits and reservations.
-->
<p>这意味着，施加在 Pod 之上的限制值可能因为 Pod 运行所在的节点不同而有差别。
为了简化系统，最简单的方法是为所有节点设置相同的 PID 资源限制和预留值。
</div>

<!--
## Node PID limits

Kubernetes allows you to reserve a number of process IDs for the system use. To
configure the reservation, use the parameter `pid=<number>` in the
`--system-reserved` and `--kube-reserved` command line options to the kubelet.
The value you specified declares that the specified number of process IDs will
be reserved for the system as a whole and for Kubernetes system daemons
respectively.
-->
<h2 id="node-pid-limits">节点级别 PID 限制  </h2>
<p>Kubernetes 允许你为系统预留一定量的进程 ID。为了配置预留数量，你可以使用
kubelet 的 <code>--system-reserved</code> 和 <code>--kube-reserved</code> 命令行选项中的参数
<code>pid=&lt;number&gt;</code>。你所设置的参数值分别用来声明为整个系统和 Kubernetes 系统
守护进程所保留的进程 ID 数目。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Before Kubernetes version 1.20, PID resource limiting with Node-level
reservations required enabling the [feature
gate](/docs/reference/command-line-tools-reference/feature-gates/)
`SupportNodePidsLimit` to work.
-->
<p>在 Kubernetes 1.20 版本之前，在节点级别通过 PID 资源限制预留 PID 的能力
需要启用<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>SupportNodePidsLimit</code> 才行。
</div>
<!--
## Pod PID limits

Kubernetes allows you to limit the number of processes running in a Pod. You
specify this limit at the node level, rather than configuring it as a resource
limit for a particular Pod. Each Node can have a different PID limit.  
To configure the limit, you can specify the command line parameter `--pod-max-pids`
to the kubelet, or set `PodPidsLimit` in the kubelet
[configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).
-->
<h2 id="pod-pid-limits">Pod 级别 PID 限制  </h2>
<p>Kubernetes 允许你限制 Pod 中运行的进程个数。你可以在节点级别设置这一限制，
而不是为特定的 Pod 来将其设置为资源限制。
每个节点都可以有不同的 PID 限制设置。
要设置限制值，你可以设置 kubelet 的命令行参数 <code>--pod-max-pids</code>，或者
在 kubelet 的<a href="/zh/docs/tasks/administer-cluster/kubelet-config-file/">配置文件</a>
中设置 <code>PodPidsLimit</code>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Before Kubernetes version 1.20, PID resource limiting for Pods required enabling
the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`SupportPodPidsLimit` to work.
-->
<p>在 Kubernetes 1.20 版本之前，为 Pod 设置 PID 资源限制的能力需要启用
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>SupportNodePidsLimit</code> 才行。
</div>
<!--
## PID based eviction

You can configure kubelet to start terminating a Pod when it is misbehaving and consuming abnormal amount of resources.
This feature is called eviction. You can
[Configure Out of Resource Handling](/docs/concepts/scheduling-eviction/node-pressure-eviction/)
for various eviction signals.
Use `pid.available` eviction signal to configure the threshold for number of PIDs used by Pod.
You can set soft and hard eviction policies.
However, even with the hard eviction policy, if the number of PIDs growing very fast,
node can still get into unstable state by hitting the node PIDs limit.
Eviction signal value is calculated periodically and does NOT enforce the limit.
-->
<h2 id="pid-based-eviction">基于 PID 的驱逐   </h2>
<p>你可以配置 kubelet 使之在 Pod 行为不正常或者消耗不正常数量资源的时候将其终止。
这一特性称作驱逐。你可以针对不同的驱逐信号
<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/">配置资源不足的处理</a>。
使用 <code>pid.available</code> 驱逐信号来配置 Pod 使用的 PID 个数的阈值。
你可以设置硬性的和软性的驱逐策略。不过，即使使用硬性的驱逐策略，
如果 PID 个数增长过快，节点仍然可能因为触及节点 PID 限制而进入一种不稳定状态。
驱逐信号的取值是周期性计算的，而不是一直能够强制实施约束。</p>
<!--
PID limiting - per Pod and per Node sets the hard limit.
Once the limit is hit, workload will start experiencing failures when trying to get a new PID.
It may or may not lead to rescheduling of a Pod,
depending on how workload reacts on these failures and how liveleness and readiness
probes are configured for the Pod. However, if limits were set correctly,
you can guarantee that other Pods workload and system processes will not run out of PIDs
when one Pod is misbehaving.
-->
<p>Pod 级别和节点级别的 PID 限制会设置硬性限制。
一旦触及限制值，工作负载会在尝试获得新的 PID 时开始遇到问题。
这可能会也可能不会导致 Pod 被重新调度，取决于工作负载如何应对这类失败
以及 Pod 的存活性和就绪态探测是如何配置的。
可是，如果限制值被正确设置，你可以确保其它 Pod 负载和系统进程不会因为某个
Pod 行为不正常而没有 PID 可用。</p>
<h2 id="what-s-next">What's next</h2>
<!--
- Refer to the [PID Limiting enhancement document](https://github.com/kubernetes/enhancements/blob/097b4d8276bc9564e56adf72505d43ce9bc5e9e8/keps/sig-node/20190129-pid-limiting.md) for more information.
- For historical context, read
  [Process ID Limiting for Stability Improvements in Kubernetes 1.14](/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/).
- Read [Managing Resources for Containers](/docs/concepts/configuration/manage-resources-containers/).
- Learn how to [Configure Out of Resource Handling](/docs/concepts/scheduling-eviction/node-pressure-eviction/).
-->
<ul>
<li>参阅 <a href="https://github.com/kubernetes/enhancements/blob/097b4d8276bc9564e56adf72505d43ce9bc5e9e8/keps/sig-node/20190129-pid-limiting.md">PID 约束改进文档</a>
以了解更多信息。</li>
<li>关于历史背景，请阅读
<a href="/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/">Kubernetes 1.14 中限制进程 ID 以提升稳定性</a>
的博文。</li>
<li>请阅读<a href="/zh/docs/concepts/configuration/manage-resources-containers/">为容器管理资源</a>。</li>
<li>学习如何<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/">配置资源不足情况的处理</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b528c4464c030f3f044124b38d778f04">4 - 节点资源管理器</h1>
    
	<!-- 
---
reviewers:
- derekwaynecarr
- klueska
title: Node Resource Managers 
content_type: concept
weight: 50
---
-->
<!-- overview -->
<!-- 
In order to support latency-critical and high-throughput workloads, Kubernetes offers a suite of Resource Managers. The managers aim to co-ordinate and optimise node's resources alignment for pods configured with a specific requirement for CPUs, devices, and memory (hugepages) resources. 
-->
<p>Kubernetes 提供了一组资源管理器，用于支持延迟敏感的、高吞吐量的工作负载。
资源管理器的目标是协调和优化节点资源，以支持对 CPU、设备和内存（巨页）等资源有特殊需求的 Pod。</p>
<!-- body -->
<!-- 
The main manager, the Topology Manager, is a Kubelet component that co-ordinates the overall resource management process through its [policy](/docs/tasks/administer-cluster/topology-manager/).

The configuration of individual managers is elaborated in dedicated documents:
-->
<p>主管理器，也叫拓扑管理器（Topology Manager），是一个 Kubelet 组件，
它通过<a href="/zh/docs/tasks/administer-cluster/topology-manager/">策略</a>，
协调全局的资源管理过程。</p>
<p>各个管理器的配置方式会在专项文档中详细阐述：</p>
<!-- 
- [CPU Manager Policies](/docs/tasks/administer-cluster/cpu-management-policies/)
- [Device Manager](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager)
- [Memory Manager Policies](/docs/tasks/administer-cluster/memory-manager/)
-->
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/cpu-management-policies/">CPU 管理器策略</a></li>
<li><a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager">设备管理器</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/memory-manager/">内存管理器策略</a></li>
</ul>

</div>



    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      
      <a class="text-white" href="/zh/docs/home/">主页</a>
      
      <a class="text-white" href="/zh/blog/">博客</a>
      
      <a class="text-white" href="/zh/training/">培训</a>
      
      <a class="text-white" href="/zh/partners/">合作伙伴</a>
      
      <a class="text-white" href="/zh/community/">社区</a>
      
      <a class="text-white" href="/zh/case-studies/">案例分析</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" href="https://discuss.kubernetes.io">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/kubernetesio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://youtube.com/kubernetescommunity">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/kubernetes/kubernetes">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://slack.k8s.io">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Contribute" aria-label="Contribute">
    <a class="text-white" target="_blank" href="https://git.k8s.io/community/contributors/guide">
      <i class="fas fa-edit"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" href="https://stackoverflow.com/questions/tagged/kubernetes">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2024 The Kubernetes Authors | Documentation Distributed under <a href="https://git.k8s.io/website/LICENSE" class="light-text">CC BY 4.0</a></small>
        <br/>
        <small class="text-white">Copyright &copy; 2024 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage" class="light-text">Trademark Usage page</a></small>
        <br/>
        <small class="text-white">ICP license: 京ICP备17074266号-3</small>
        
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>










<script src="/js/main.js"></script>






  </body>
</html>
