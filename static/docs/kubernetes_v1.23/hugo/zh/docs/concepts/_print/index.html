<!doctype html>
<html lang="zh" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>


<link rel="alternate" hreflang="en" href="http://localhost:1313/docs/concepts/">
<link rel="alternate" hreflang="ko" href="http://localhost:1313/ko/docs/concepts/">
<link rel="alternate" hreflang="ja" href="http://localhost:1313/ja/docs/concepts/">
<link rel="alternate" hreflang="fr" href="http://localhost:1313/fr/docs/concepts/">
<link rel="alternate" hreflang="it" href="http://localhost:1313/it/docs/concepts/">
<link rel="alternate" hreflang="de" href="http://localhost:1313/de/docs/concepts/">
<link rel="alternate" hreflang="pt-br" href="http://localhost:1313/pt-br/docs/concepts/">
<link rel="alternate" hreflang="es" href="http://localhost:1313/es/docs/concepts/">
<link rel="alternate" hreflang="id" href="http://localhost:1313/id/docs/concepts/">
<link rel="alternate" hreflang="ru" href="http://localhost:1313/ru/docs/concepts/">
<link rel="alternate" hreflang="pl" href="http://localhost:1313/pl/docs/concepts/">
<link rel="alternate" hreflang="uk" href="http://localhost:1313/uk/docs/concepts/">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.87.0" />
<link rel="canonical" type="text/html" href="http://localhost:1313/zh/docs/concepts/">
<link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="apple-touch-icon" href="/images/kubernetes-192x192.png">
<title>概念 | Kubernetes</title><meta property="og:title" content="概念" />
<meta property="og:description" content="生产级别的容器编排系统" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/zh/docs/concepts/" /><meta property="og:site_name" content="Kubernetes" />

<meta itemprop="name" content="概念">
<meta itemprop="description" content="生产级别的容器编排系统"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="概念"/>
<meta name="twitter:description" content="生产级别的容器编排系统"/>






<link href="/scss/main.css" rel="stylesheet">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "http://localhost:1313/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href="/css/feature-states.css">



<meta name="description" content="cluster, and helps you obtain a deeper understanding of how Kubernetes works. -- 概念部分可以帮助你了解 Kubernetes 的各个组成部分以及 Kubernetes 用来表示集群的一些抽象概念，并帮助你更加深入的理解 Kubernetes 是如何工作的。">
<meta property="og:description" content="cluster, and helps you obtain a deeper understanding of how Kubernetes works. -- 概念部分可以帮助你了解 Kubernetes 的各个组成部分以及 Kubernetes 用来表示集群的一些抽象概念，并帮助你更加深入的理解 Kubernetes 是如何工作的。">
<meta name="twitter:description" content="cluster, and helps you obtain a deeper understanding of how Kubernetes works. -- 概念部分可以帮助你了解 Kubernetes 的各个组成部分以及 Kubernetes 用来表示集群的一些抽象概念，并帮助你更加深入的理解 Kubernetes 是如何工作的。">
<meta property="og:url" content="http://localhost:1313/zh/docs/concepts/">
<meta property="og:title" content="概念">
<meta name="twitter:title" content="概念">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">

<script src="/js/script.js"></script>


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-column flex-md-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/zh/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/zh/docs/" >文档</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/blog/" >Kubernetes 博客</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/training/" >培训</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/partners/" >合作伙伴</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/community/" >社区</a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/case-studies/" >案例分析</a>
			</li>
			
			
			
			<li class="nav-item dropdown">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Versions
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/releases">Release Information</a>
	
	<a class="dropdown-item" href="https://kubernetes.io/zh/docs/concepts/">v1.23</a>
	
	<a class="dropdown-item" href="https://v1-22.docs.kubernetes.io/zh/docs/concepts/">v1.22</a>
	
	<a class="dropdown-item" href="https://v1-21.docs.kubernetes.io/zh/docs/concepts/">v1.21</a>
	
	<a class="dropdown-item" href="https://v1-20.docs.kubernetes.io/zh/docs/concepts/">v1.20</a>
	
	<a class="dropdown-item" href="https://v1-19.docs.kubernetes.io/zh/docs/concepts/">v1.19</a>
	
</div>
			</li>
			
			
			<li class="nav-item dropdown">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/concepts/">English</a>
	
	<a class="dropdown-item" href="/ko/docs/concepts/">한국어 Korean</a>
	
	<a class="dropdown-item" href="/ja/docs/concepts/">日本語 Japanese</a>
	
	<a class="dropdown-item" href="/fr/docs/concepts/">Français</a>
	
	<a class="dropdown-item" href="/it/docs/concepts/">Italiano</a>
	
	<a class="dropdown-item" href="/de/docs/concepts/">Deutsch</a>
	
	<a class="dropdown-item" href="/pt-br/docs/concepts/">Português</a>
	
	<a class="dropdown-item" href="/es/docs/concepts/">Español</a>
	
	<a class="dropdown-item" href="/id/docs/concepts/">Bahasa Indonesia</a>
	
	<a class="dropdown-item" href="/ru/docs/concepts/">Русский</a>
	
	<a class="dropdown-item" href="/pl/docs/concepts/">Polski</a>
	
	<a class="dropdown-item" href="/uk/docs/concepts/">Українська</a>
	
</div>

			</li>
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/zh/docs/concepts/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">概念</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-0554ac387412eaf4e6e89b2f847dacde">概述</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>1.1: <a href="#pg-45bdca6129cf540121623e903c18ba46">Kubernetes 是什么？</a></li>


    
  
    
    
	
<li>1.2: <a href="#pg-13b0f1dbe89228e3d76d2ac231e245f1">Kubernetes 组件</a></li>


    
  
    
    
	
<li>1.3: <a href="#pg-0c745f42e623d2b70a53bc0e6db73d95">Kubernetes API</a></li>


    
  
    
    
	
<li>1.4: <a href="#pg-110f33530cf761140cb1dab536baef04">使用 Kubernetes 对象</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>1.4.1: <a href="#pg-9f5adfa77f48c50d5cc81155a3cecb98">理解 Kubernetes 对象</a></li>


    
  
    
    
	
<li>1.4.2: <a href="#pg-6751db8ff5409476de8225d17d6c42dd">Kubernetes 对象管理</a></li>


    
  
    
    
	
<li>1.4.3: <a href="#pg-f37749a83c2916b63279ea60f3cfe53e">对象名称和 IDs</a></li>


    
  
    
    
	
<li>1.4.4: <a href="#pg-1127165f472b7181b9c1d5a0b187d620">名字空间</a></li>


    
  
    
    
	
<li>1.4.5: <a href="#pg-f1dec4557fb8ffbac9f11390aaaf9fa4">标签和选择算符</a></li>


    
  
    
    
	
<li>1.4.6: <a href="#pg-93cd7a1d4e1623e2bf01afc49a5af69c">注解</a></li>


    
  
    
    
	
<li>1.4.7: <a href="#pg-13ce5627ef1dc8cbb4530ed231cb7d38">Finalizers</a></li>


    
  
    
    
	
<li>1.4.8: <a href="#pg-046c03090d47bc4b89b818dc645c3865">字段选择器</a></li>


    
  
    
    
	
<li>1.4.9: <a href="#pg-efaa7a58910b58892dafd50e3b43c93c">属主与附属</a></li>


    
  
    
    
	
<li>1.4.10: <a href="#pg-5dd62c6a4a481b4cf1ac50f6799eb581">推荐使用的标签</a></li>


    
  

    </ul>
    
  

    </ul>
    
  
    
    
	
<li>2: <a href="#pg-2bf36ccd6b3dbeafecf87c39761b07c7">Kubernetes 架构</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1: <a href="#pg-9ef2890698e773b6c0d24fd2c20146f5">节点</a></li>


    
  
    
    
	
<li>2.2: <a href="#pg-c0251def6da29b30afebfb04549f1703">控制面到节点通信</a></li>


    
  
    
    
	
<li>2.3: <a href="#pg-ca8819042a505291540e831283da66df">控制器</a></li>


    
  
    
    
	
<li>2.4: <a href="#pg-bc804b02614d67025b4c788f1ca87fbc">云控制器管理器</a></li>


    
  
    
    
	
<li>2.5: <a href="#pg-44a2e2e592af0846101e970aff9243e5">垃圾收集</a></li>


    
  
    
    
	
<li>2.6: <a href="#pg-c0ea5310f52e22c5de34dc84d9ab5e0d">容器运行时接口（CRI）</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3: <a href="#pg-a5f7383c83ab9eb9cd0e3c4c020b3ae6">容器</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.1: <a href="#pg-16042b4652ad19e565c7263824029a43">镜像</a></li>


    
  
    
    
	
<li>3.2: <a href="#pg-643212488f778acf04bebed65ba34441">容器环境</a></li>


    
  
    
    
	
<li>3.3: <a href="#pg-a858027489648786a3b16264e451272b">容器运行时类（Runtime Class）</a></li>


    
  
    
    
	
<li>3.4: <a href="#pg-e6941d969d81540208a3e78bc56f43bc">容器生命周期回调</a></li>


    
  

    </ul>
    
  
    
    
	
<li>4: <a href="#pg-d52aadda80edd9f8c514cfe2321363c2">工作负载</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.1: <a href="#pg-4d68b0ccf9c683e6368ffdcc40c838d4">Pods</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.1.1: <a href="#pg-c3c2b9cf30915ec9d46c147201da3332">Pod 的生命周期</a></li>


    
  
    
    
	
<li>4.1.2: <a href="#pg-1ccbd4eeded6ab138d98b59175bd557e">Init 容器</a></li>


    
  
    
    
	
<li>4.1.3: <a href="#pg-c8d62295ca703fdcef1aaf89fb4c916a">Pod 拓扑分布约束</a></li>


    
  
    
    
	
<li>4.1.4: <a href="#pg-4aaf43c715cd764bc8ed4436f3537e68">干扰（Disruptions）</a></li>


    
  
    
    
	
<li>4.1.5: <a href="#pg-53a1005011e1bda2ce81819aad7c8b32">临时容器</a></li>


    
  

    </ul>
    
  
    
    
	
<li>4.2: <a href="#pg-89637410cacae45a36ab1cc278c482eb">工作负载资源</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.2.1: <a href="#pg-a2dc0393e0c4079e1c504b6429844e86">Deployments</a></li>


    
  
    
    
	
<li>4.2.2: <a href="#pg-d459b930218774655fa7fd1620625539">ReplicaSet</a></li>


    
  
    
    
	
<li>4.2.3: <a href="#pg-6d72299952c37ca8cc61b416e5bdbcd4">StatefulSets</a></li>


    
  
    
    
	
<li>4.2.4: <a href="#pg-41600eb8b6631c88848156f381e9d588">DaemonSet</a></li>


    
  
    
    
	
<li>4.2.5: <a href="#pg-cc7cc3c4907039d9f863162e20bfbbef">Jobs</a></li>


    
  
    
    
	
<li>4.2.6: <a href="#pg-4de50a37ebb6f2340484192126cb7a04">已完成 Job 的自动清理</a></li>


    
  
    
    
	
<li>4.2.7: <a href="#pg-2e4cec01c525b45eccd6010e21cc76d9">CronJob</a></li>


    
  
    
    
	
<li>4.2.8: <a href="#pg-27f1331d515d95f76aa1156088b4ad91">ReplicationController</a></li>


    
  

    </ul>
    
  

    </ul>
    
  
    
    
	
<li>5: <a href="#pg-0a0a7eca3e302a3c08f8c85e15d337fd">服务、负载均衡和联网</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.1: <a href="#pg-3a38878244d862dfdb8d7adb32f77584">使用拓扑键实现拓扑感知的流量路由</a></li>


    
  
    
    
	
<li>5.2: <a href="#pg-5701136fd2ce258047b6ddc389112352">服务</a></li>


    
  
    
    
	
<li>5.3: <a href="#pg-91cb8a4438b003df11bc1c426a81b756">Pod 与 Service 的 DNS</a></li>


    
  
    
    
	
<li>5.4: <a href="#pg-f804ac0532fcade3966ea2e3769ca031">使用 Service 连接到应用</a></li>


    
  
    
    
	
<li>5.5: <a href="#pg-199bcc92443dbc9bed44819467d7eb75">Ingress</a></li>


    
  
    
    
	
<li>5.6: <a href="#pg-5a8edeb1f2dc8e38cd6d561bb08b0d78">Ingress 控制器</a></li>


    
  
    
    
	
<li>5.7: <a href="#pg-374e5c954990aec58a0797adc70a5039">拓扑感知提示</a></li>


    
  
    
    
	
<li>5.8: <a href="#pg-cd7657b1056ad32451974db57a951ba5">服务内部流量策略</a></li>


    
  
    
    
	
<li>5.9: <a href="#pg-f51db1097575de8072afe1f5b156a70c">端点切片（Endpoint Slices）</a></li>


    
  
    
    
	
<li>5.10: <a href="#pg-ded1daafdcd293023ee333728007ca61">网络策略</a></li>


    
  
    
    
	
<li>5.11: <a href="#pg-21f8d19c60c33914baab66224c3d46a7">IPv4/IPv6 双协议栈</a></li>


    
  

    </ul>
    
  
    
    
	
<li>6: <a href="#pg-f018f568c6723865753f150c3c59bdda">存储</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>6.1: <a href="#pg-27795584640a03bd2024f1fe3b3ab754">卷</a></li>


    
  
    
    
	
<li>6.2: <a href="#pg-ffd12528a12882b282e1bd19e29f9e75">持久卷</a></li>


    
  
    
    
	
<li>6.3: <a href="#pg-2db414b26d4daec3ebed19dd837830c3">投射卷</a></li>


    
  
    
    
	
<li>6.4: <a href="#pg-df33eab51202c17bb0fe551d1d5cc5d2">临时卷</a></li>


    
  
    
    
	
<li>6.5: <a href="#pg-f0276d05eef111249272a1c932a91e2c">存储类</a></li>


    
  
    
    
	
<li>6.6: <a href="#pg-018f0a7fc6e2f6d16da37702fc39b4f3">动态卷供应</a></li>


    
  
    
    
	
<li>6.7: <a href="#pg-c262af210c6828dec445d2f55a1d877a">卷快照</a></li>


    
  
    
    
	
<li>6.8: <a href="#pg-4d00116c86dade62bdd5be7dc2afa1ca">卷快照类</a></li>


    
  
    
    
	
<li>6.9: <a href="#pg-707ca81a34eb1ca202f34692e9917d1e">CSI 卷克隆</a></li>


    
  
    
    
	
<li>6.10: <a href="#pg-00cd24f4570b7acaac75c2551c948bc7">存储容量</a></li>


    
  
    
    
	
<li>6.11: <a href="#pg-4f40cb95a671e51b4f0156a409d95c6d">卷健康监测</a></li>


    
  
    
    
	
<li>6.12: <a href="#pg-b2e4b16ac37988c678a3312a4a6639f8">特定于节点的卷数限制</a></li>


    
  

    </ul>
    
  
    
    
	
<li>7: <a href="#pg-275bea454e1cf4c5adeca4058b5af988">配置</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>7.1: <a href="#pg-ddef6fd0e47bb51c6f05e8e7fb11d2dd">配置最佳实践</a></li>


    
  
    
    
	
<li>7.2: <a href="#pg-6b5ccadd699df0904e8e9917c5450c4b">ConfigMap</a></li>


    
  
    
    
	
<li>7.3: <a href="#pg-e511ed821ada65d0053341dbd8ad2bb5">Secret</a></li>


    
  
    
    
	
<li>7.4: <a href="#pg-436057b96151ecb8a4a9a9f456b5d0fc">为 Pod 和容器管理资源</a></li>


    
  
    
    
	
<li>7.5: <a href="#pg-ab6d20f33ad930a67ee7ef57bff6c75e">使用 kubeconfig 文件组织集群访问</a></li>


    
  

    </ul>
    
  
    
    
	
<li>8: <a href="#pg-712cb3c03ff14a39e5a83a6d9b71d203">安全</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>8.1: <a href="#pg-04eeb110d75afc8acb2cf7a3db743985">云原生安全概述</a></li>


    
  
    
    
	
<li>8.2: <a href="#pg-1fb24c1dd155f43849da490a74c4b8c5">Pod 安全性标准</a></li>


    
  
    
    
	
<li>8.3: <a href="#pg-bc9934fccfeaf880eec6ea79025c0381">Pod 安全性准入</a></li>


    
  
    
    
	
<li>8.4: <a href="#pg-4d77d1ae4c06aa14f54b385191627881">Kubernetes API 访问控制</a></li>


    
  

    </ul>
    
  
    
    
	
<li>9: <a href="#pg-ac9161c6d952925b083ad9602b4e8e7f">策略</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>9.1: <a href="#pg-a935ff8c59eb116b43494255cc67f69a">限制范围</a></li>


    
  
    
    
	
<li>9.2: <a href="#pg-94ddc6e901c30f256138db11d09f05a3">资源配额</a></li>


    
  
    
    
	
<li>9.3: <a href="#pg-7352434db5f5954d2f7656b46fe5a324">进程 ID 约束与预留</a></li>


    
  
    
    
	
<li>9.4: <a href="#pg-b528c4464c030f3f044124b38d778f04">节点资源管理器</a></li>


    
  

    </ul>
    
  
    
    
	
<li>10: <a href="#pg-c21d05f31057c5bcd2ebdd01f4e62a0e">调度，抢占和驱逐</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>10.1: <a href="#pg-598f36d691ab197f9d995784574b0a12">Kubernetes 调度器</a></li>


    
  
    
    
	
<li>10.2: <a href="#pg-21169f516071aea5d16734a4c27789a5">将 Pod 指派给节点</a></li>


    
  
    
    
	
<li>10.3: <a href="#pg-da22fe2278df236f71efbe672f392677">Pod 开销</a></li>


    
  
    
    
	
<li>10.4: <a href="#pg-ede4960b56a3529ee0bfe7c8fe2d09a5">污点和容忍度</a></li>


    
  
    
    
	
<li>10.5: <a href="#pg-60e5a2861609e0848d58ce8bf99c4a31">Pod 优先级和抢占</a></li>


    
  
    
    
	
<li>10.6: <a href="#pg-78e0431b4b7516092662a7c289cbb304">节点压力驱逐</a></li>


    
  
    
    
	
<li>10.7: <a href="#pg-b87723bf81b079042860f0ebd37b0a64">API 发起的驱逐</a></li>


    
  
    
    
	
<li>10.8: <a href="#pg-961126cd43559012893979e568396a49">扩展资源的资源装箱</a></li>


    
  
    
    
	
<li>10.9: <a href="#pg-602208c95fe7b1f1170310ce993f5814">调度框架</a></li>


    
  
    
    
	
<li>10.10: <a href="#pg-d9574a30fcbc631b0d2a57850e161e89">调度器性能调优</a></li>


    
  

    </ul>
    
  
    
    
	
<li>11: <a href="#pg-285a3785fd3d20f437c28d87ca4dadca">集群管理</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>11.1: <a href="#pg-2bf9a93ab5ba014fb6ff70b22c29d432">证书</a></li>


    
  
    
    
	
<li>11.2: <a href="#pg-3aeeecf7cdb2a21eb4b31db7a71c81e2">管理资源</a></li>


    
  
    
    
	
<li>11.3: <a href="#pg-d649067a69d8d5c7e71564b42b96909e">集群网络系统</a></li>


    
  
    
    
	
<li>11.4: <a href="#pg-cbfd3654996eae9fcdef009f70fa83f0">Kubernetes 系统组件指标</a></li>


    
  
    
    
	
<li>11.5: <a href="#pg-c4b1e87a84441f8a90699a345ce48d68">日志架构</a></li>


    
  
    
    
	
<li>11.6: <a href="#pg-5cc31ecfba86467f8884856412cfb6b2">系统日志</a></li>


    
  
    
    
	
<li>11.7: <a href="#pg-3da54ad355f6fe6574d67bd9a9a42bcb">追踪 Kubernetes 系统组件</a></li>


    
  
    
    
	
<li>11.8: <a href="#pg-08e94e6a480e0d6b2de72d84a1b97617">Kubernetes 中的代理</a></li>


    
  
    
    
	
<li>11.9: <a href="#pg-31c9327d2332c585341b64ddafa19cdd">API 优先级和公平性</a></li>


    
  
    
    
	
<li>11.10: <a href="#pg-85d633ae590aa20ec024f1b7af1d74fc">安装扩展（Addons）</a></li>


    
  

    </ul>
    
  
    
    
	
<li>12: <a href="#pg-7e0d97616b15e2c383c6a0a96ec442cb">扩展 Kubernetes</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>12.1: <a href="#pg-0af41d3bd7c785621b58b7564793396a">扩展 Kubernetes API</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>12.1.1: <a href="#pg-342388440304e19ce30c0f8ada1c77ce">定制资源</a></li>


    
  
    
    
	
<li>12.1.2: <a href="#pg-1ea4977c0ebf97569bf54a477faa7fa5">通过聚合层扩展 Kubernetes API</a></li>


    
  

    </ul>
    
  
    
    
	
<li>12.2: <a href="#pg-3131452556176159fb269593c1a52012">Operator 模式</a></li>


    
  
    
    
	
<li>12.3: <a href="#pg-c8937cdc9df96f3328becf04f8211292">计算、存储和网络扩展</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>12.3.1: <a href="#pg-1ac2260db9ecccbf0303a899bc27ce6d">网络插件</a></li>


    
  
    
    
	
<li>12.3.2: <a href="#pg-53e1ea8892ceca307ba19e8d6a7b8d32">设备插件</a></li>


    
  

    </ul>
    
  
    
    
	
<li>12.4: <a href="#pg-b26fcf43d01abc16c8110766026dafed">服务目录</a></li>


    
  

    </ul>
    
  

    </ul>


<div class="content">
      <!-- ---
title: Concepts
main_menu: true
content_type: concept
weight: 40
--- -->
<!-- overview -->
<!--
The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your <a class='glossary-tooltip' title='集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cluster' target='_blank' aria-label='cluster'>cluster</a>, and helps you obtain a deeper understanding of how Kubernetes works.
-->
<p>概念部分可以帮助你了解 Kubernetes 的各个组成部分以及 Kubernetes 用来表示集群的一些抽象概念，并帮助你更加深入的理解 Kubernetes 是如何工作的。</p>

</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0554ac387412eaf4e6e89b2f847dacde">1 - 概述</h1>
    <div class="lead">了解 Kubernetes 及其构件的高层次概要。</div>
	<!--
title: "Overview"
weight: 20
description: Get a high-level outline of Kubernetes and the components it is built from.
sitemap:
  priority: 0.9
-->

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-45bdca6129cf540121623e903c18ba46">1.1 - Kubernetes 是什么？</h1>
    <div class="lead">Kubernetes 是一个可移植的，可扩展的开源平台，用于管理容器化的工作负载和服务，方便了声明式配置和自动化。它拥有一个庞大且快速增长的生态系统。Kubernetes 的服务，支持和工具广泛可用。</div>
	<!--
reviewers:
- bgrant0607
- mikedanese
title: What is Kubernetes
content_type: concept
weight: 10
card:
  name: concepts
  weight: 10
-->
<!-- overview -->
<!--
This page is an overview of Kubernetes.
-->
<p>此页面是 Kubernetes 的概述。</p>
<!-- body -->
<!--
Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.
-->
<p>Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。
Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。</p>
<!--
The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the "K" and the "s". Google open-sourced the Kubernetes project in 2014. Kubernetes combines [over 15 years of Google's experience](/blog/2015/04/borg-predecessor-to-kubernetes/) running production workloads at scale with best-of-breed ideas and practices from the community.
-->
<p><strong>Kubernetes</strong> 这个名字源于希腊语，意为“舵手”或“飞行员”。k8s 这个缩写是因为 k 和 s 之间有八个字符的关系。
Google 在 2014 年开源了 Kubernetes 项目。Kubernetes 建立在
<a href="https://research.google/pubs/pub43438">Google 在大规模运行生产工作负载方面拥有十几年的经验</a>
的基础上，结合了社区中最好的想法和实践。</p>
<!--
## Going back in time
Let's take a look at why Kubernetes is so useful by going back in time.
-->
<h2 id="时光回溯">时光回溯</h2>
<p>让我们回顾一下为什么 Kubernetes 如此有用。</p>
<!--
![Deployment evolution](/images/docs/Container_Evolution.svg)
-->
<p><img src="/images/docs/Container_Evolution.svg" alt="部署演进"></p>
<!--
**Traditional deployment era:**

Early on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a physical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can be instances where one application would take up most of the resources, and as a result, the other applications would underperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources were underutilized, and it was expensive for organizations to maintain many physical servers.
-->
<p><strong>传统部署时代：</strong></p>
<p>早期，各个组织机构在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。
例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况，
结果可能导致其他应用程序的性能下降。
一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展，
并且维护许多物理服务器的成本很高。</p>
<!--
**Virtualized deployment era:**
As a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization allows applications to be isolated between VMs and provides a level of security as the information of one application cannot be freely accessed by another application.
-->
<p><strong>虚拟化部署时代：</strong></p>
<p>作为解决方案，引入了虚拟化。虚拟化技术允许你在单个物理服务器的 CPU 上运行多个虚拟机（VM）。
虚拟化允许应用程序在 VM 之间隔离，并提供一定程度的安全，因为一个应用程序的信息
不能被另一应用程序随意访问。</p>
<!--
Virtualization allows better utilization of resources in a physical server and allows better scalability because an application can be added or updated easily, reduces hardware costs, and much more.

Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware.
-->
<p>虚拟化技术能够更好地利用物理服务器上的资源，并且因为可轻松地添加或更新应用程序
而可以实现更好的可伸缩性，降低硬件成本等等。</p>
<p>每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。</p>
<!--
**Container deployment era:**
Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications. Therefore, containers are considered lightweight. Similar to a VM, a container has its own filesystem, CPU, memory, process space, and more. As they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions.
-->
<p><strong>容器部署时代：</strong></p>
<p>容器类似于 VM，但是它们具有被放宽的隔离属性，可以在应用程序之间共享操作系统（OS）。
因此，容器被认为是轻量级的。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。
由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。</p>
<!--
Containers are becoming popular because they have many benefits. Some of the container benefits are listed below:
-->
<p>容器因具有许多优势而变得流行起来。下面列出的是容器的一些好处：</p>
<!--
* Agile application creation and deployment: increased ease and efficiency of container image creation compared to VM image use.
* Continuous development, integration, and deployment: provides for reliable and frequent container image build and deployment with quick and easy rollbacks (due to image immutability).
* Dev and Ops separation of concerns: create application container images at build/release time rather than deployment time, thereby decoupling applications from infrastructure.
* Observability: not only surfaces OS-level information and metrics, but also application health and other signals.
* Environmental consistency across development, testing, and production: Runs the same on a laptop as it does in the cloud.
* Cloud and OS distribution portability: Runs on Ubuntu, RHEL, CoreOS, on-prem, Google Kubernetes Engine, and anywhere else.
* Application-centric management: Raises the level of abstraction from running an OS on virtual hardware to running an application on an OS using logical resources.
* Loosely coupled, distributed, elastic, liberated micro-services: applications are broken into smaller, independent pieces and can be deployed and managed dynamically – not a monolithic stack running on one big single-purpose machine.
* Resource isolation: predictable application performance.
* Resource utilization: high efficiency and density.
-->
<ul>
<li>敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。</li>
<li>持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的
容器镜像构建和部署。</li>
<li>关注开发与运维的分离：在构建/发布时而不是在部署时创建应用程序容器镜像，
从而将应用程序与基础架构分离。</li>
<li>可观察性：不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。</li>
<li>跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。</li>
<li>跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、
Google Kubernetes Engine 和其他任何地方运行。</li>
<li>以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在
OS 上运行应用程序。</li>
<li>松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，
并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。</li>
<li>资源隔离：可预测的应用程序性能。</li>
<li>资源利用：高效率和高密度。</li>
</ul>
<!--
## Why you need Kubernetes and what can it do
-->
<h2 id="为什么需要-kubernetes-它能做什么">为什么需要 Kubernetes，它能做什么?</h2>
<!--
Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn't it be easier if this behavior was handled by a system?
-->
<p>容器是打包和运行应用程序的好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。
例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？</p>
<!--
That's how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of your scaling requirements, failover, deployment patterns, and more. For example, Kubernetes can easily manage a canary deployment for your system.
-->
<p>这就是 Kubernetes 来解决这些问题的方法！
Kubernetes 为你提供了一个可弹性运行分布式系统的框架。
Kubernetes 会满足你的扩展要求、故障转移、部署模式等。
例如，Kubernetes 可以轻松管理系统的 Canary 部署。</p>
<!--
Kubernetes provides you with:
-->
<p>Kubernetes 为你提供：</p>
<!--
* **Service discovery and load balancing**
Kubernetes can expose a container using the DNS name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable.
-->
<ul>
<li>
<p><strong>服务发现和负载均衡</strong></p>
<p>Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大，
Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。</p>
</li>
</ul>
<!--
* **Storage orchestration**
Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more.
-->
<ul>
<li>
<p><strong>存储编排</strong></p>
<p>Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。</p>
</li>
</ul>
<!--
* **Automated rollouts and rollbacks**
You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container.
-->
<ul>
<li>
<p><strong>自动部署和回滚</strong></p>
<p>你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态
更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器，
删除现有容器并将它们的所有资源用于新容器。</p>
</li>
</ul>
<!--
* **Automatic bin packing**
Kubernetes allows you to specify how much CPU and memory (RAM) each container needs. When containers have resource requests specified, Kubernetes can make better decisions to manage the resources for containers.
-->
<ul>
<li>
<p><strong>自动完成装箱计算</strong></p>
<p>Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。
当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。</p>
</li>
</ul>
<!--
* **Self-healing**
Kubernetes restarts containers that fail, replaces containers, kills containers that don’t respond to your user-defined health check, and doesn’t advertise them to clients until they are ready to serve.
-->
<ul>
<li>
<p><strong>自我修复</strong></p>
<p>Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的
运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。</p>
</li>
</ul>
<!--
* **Secret and configuration management**
Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration.
-->
<ul>
<li>
<p><strong>密钥与配置管理</strong></p>
<p>Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。
你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。</p>
</li>
</ul>
<!--
## What Kubernetes is not
-->
<h2 id="kubernetes-不是什么">Kubernetes 不是什么</h2>
<!--
Kubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system. Since Kubernetes operates at the container level rather than at the hardware level, it provides some generally applicable features common to PaaS offerings, such as deployment, scaling, load balancing, logging, and monitoring. However, Kubernetes is not monolithic, and these default solutions are optional and pluggable. Kubernetes provides the building blocks for building developer platforms, but preserves user choice and flexibility where it is important.
-->
<p>Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。
由于 Kubernetes 在容器级别而不是在硬件级别运行，它提供了 PaaS 产品共有的一些普遍适用的功能，
例如部署、扩展、负载均衡、日志记录和监视。
但是，Kubernetes 不是单体系统，默认解决方案都是可选和可插拔的。
Kubernetes 提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。</p>
<!--
Kubernetes:
-->
<p>Kubernetes：</p>
<!--
* Does not limit the types of applications supported. Kubernetes aims to support an extremely diverse variety of workloads, including stateless, stateful, and data-processing workloads. If an application can run in a container, it should run great on Kubernetes.
* Does not deploy source code and does not build your application. Continuous Integration, Delivery, and Deployment (CI/CD) workflows are determined by organization cultures and preferences as well as technical requirements.
* Does not provide application-level services, such as middleware (for example, message buses), data-processing frameworks (for example, Spark), databases (for example, mysql), caches, nor cluster storage systems (for example, Ceph) as built-in services. Such components can run on Kubernetes, and/or can be accessed by applications running on Kubernetes through portable mechanisms, such as the Open Service Broker.
-->
<ul>
<li>不限制支持的应用程序类型。
Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。
如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。</li>
<li>不部署源代码，也不构建你的应用程序。
持续集成(CI)、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。</li>
<li>不提供应用程序级别的服务作为内置服务，例如中间件（例如，消息中间件）、
数据处理框架（例如，Spark）、数据库（例如，mysql）、缓存、集群存储系统
（例如，Ceph）。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在
Kubernetes 上的应用程序通过可移植机制（例如，
<a href="https://openservicebrokerapi.org/">开放服务代理</a>）来访问。</li>
</ul>
<!--
* Does not dictate logging, monitoring, or alerting solutions. It provides some integrations as proof of concept, and mechanisms to collect and export metrics.
* Does not provide nor mandate a configuration language/system (for example, jsonnet). It provides a declarative API that may be targeted by arbitrary forms of declarative specifications.
* Does not provide nor adopt any comprehensive machine configuration, maintenance, management, or self-healing systems.
* Additionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need for orchestration. The technical definition of orchestration is execution of a defined workflow: first do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable control processes that continuously drive the current state towards the provided desired state. It shouldn’t matter how you get from A to C. Centralized control is also not required. This results in a system that is easier to use and more powerful, robust, resilient, and extensible.
-->
<ul>
<li>不要求日志记录、监视或警报解决方案。
它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。</li>
<li>不提供或不要求配置语言/系统（例如 jsonnet），它提供了声明性 API，
该声明性 API 可以由任意形式的声明性规范所构成。</li>
<li>不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。</li>
<li>此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。
编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。
相比之下，Kubernetes 包含一组独立的、可组合的控制过程，
这些过程连续地将当前状态驱动到所提供的所需状态。
如何从 A 到 C 的方式无关紧要，也不需要集中控制，这使得系统更易于使用
且功能更强大、系统更健壮、更为弹性和可扩展。</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
*   Take a look at the [Kubernetes Components](/docs/concepts/overview/components/)
*   Ready to [Get Started](/docs/setup/)?
-->
<ul>
<li>查阅 <a href="/zh/docs/concepts/overview/components/">Kubernetes 组件</a></li>
<li>开始 <a href="/zh/docs/setup/">Kubernetes 入门</a>?</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-13b0f1dbe89228e3d76d2ac231e245f1">1.2 - Kubernetes 组件</h1>
    <div class="lead">Kubernetes 集群由代表控制平面的组件和一组称为节点的机器组成。</div>
	<!--
reviewers:
- lavalamp
title: Kubernetes Components
content_type: concept
description: >
  A Kubernetes cluster consists of the components that represent the control plane
  and a set of machines called nodes
weight: 20
card:
  name: concepts
  weight: 20
-->
<!--
When you deploy Kubernetes, you get a cluster.
{{< glossary_definition term_id="cluster" length="all" prepend="A Kubernetes cluster consists of" >}}

This document outlines the various components you need to have for
a complete and working Kubernetes cluster.


<figure class="diagram-large">
    <img src="/images/docs/components-of-kubernetes.svg"
         alt="Components of Kubernetes"/> <figcaption>
            <p>The components of a Kubernetes cluster</p>
        </figcaption>
</figure>


-->
<!-- overview -->
<p>当你部署完 Kubernetes, 即拥有了一个完整的集群。
<!-- 
---
title: Cluster
id: cluster
date: 2019-06-15
full_link: 
short_description: >
   A set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node.

aka: 
tags:
- fundamental
- operation
--- 
-->
<!-- 
A set of worker machines, called <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='nodes'>nodes</a>,
that run containerized applications. Every cluster has at least one worker node.
-->
<p><p>一个 Kubernetes 集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。</p></p>
<!-- 
The worker node(s) host the <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> that are
the components of the application workload. The
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> manages the worker
nodes and the Pods in the cluster. In production environments, the control plane usually
runs across multiple computers and a cluster usually runs multiple nodes, providing
fault-tolerance and high availability.
-->
<p>工作节点托管作为应用负载的组件的 Pod 。控制平面管理集群中的工作节点和 Pod 。
为集群提供故障转移和高可用性，这些控制平面一般跨多主机运行，集群跨多个节点运行。</p></p>
<p>本文档概述了交付正常运行的 Kubernetes 集群所需的各种组件。</p>

<figure class="diagram-large">
    <img src="/images/docs/components-of-kubernetes.svg"
         alt="Kubernetes 的组件"/> <figcaption>
            <p>Kubernetes 集群的组件</p>
        </figcaption>
</figure>

<!-- body -->
<!--
## Control Plane Components

The control plane's components make global decisions about the cluster (for example, scheduling), as well as detecting and responding to cluster events (for example, starting up a new <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='pod'>pod</a> when a deployment's `replicas` field is unsatisfied).
 -->
<h2 id="control-plane-components">控制平面组件（Control Plane Components）   </h2>
<p>控制平面的组件对集群做出全局决策(比如调度)，以及检测和响应集群事件（例如，当不满足部署的
<code>replicas</code> 字段时，启动新的 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='pod'>pod</a>）。</p>
<!--
Control plane components can be run on any machine in the cluster. However,
for simplicity, set up scripts typically start all control plane components on
the same machine, and do not run user containers on this machine. See
[Creating Highly Available clusters with kubeadm](/docs/setup/production-environment/tools/kubeadm/high-availability/)
for an example control plane setup that runs across multiple machines.
 -->
<p>控制平面组件可以在集群中的任何节点上运行。
然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有控制平面组件，
并且不会在此计算机上运行用户容器。
请参阅<a href="/zh/docs/setup/production-environment/tools/kubeadm/high-availability/">使用 kubeadm 构建高可用性集群</a>
中关于跨多机器控制平面设置的示例。</p>
<h3 id="kube-apiserver">kube-apiserver</h3>
<!--
title: kube-apiserver
id: kube-apiserver
date: 2018-04-12
full_link: /zh/docs/reference/command-line-tools-reference/kube-apiserver/
short_description: >
  Control plane component that serves the Kubernetes API. 

aka: 
tags:
- architecture
- fundamental
-->
<!--
 The API server is a component of the Kubernetes
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> that exposes the Kubernetes API.
The API server is the front end for the Kubernetes control plane.
-->
<p>API 服务器是 Kubernetes <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a>的组件，
该组件公开了 Kubernetes API。
API 服务器是 Kubernetes 控制面的前端。</p>
<!--
The main implementation of a Kubernetes API server is [kube-apiserver](/docs/reference/generated/kube-apiserver/).
kube-apiserver is designed to scale horizontally&mdash;that is, it scales by deploying more instances.
You can run several instances of kube-apiserver and balance traffic between those instances.
-->
<p>Kubernetes API 服务器的主要实现是 <a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a>。
kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。
你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。</p>
<h3 id="etcd">etcd</h3>
<!--
---
title: etcd
id: etcd
date: 2018-04-12
full_link: /docs/tasks/administer-cluster/configure-upgrade-etcd/
short_description: >
  Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.

aka: 
tags:
- architecture
- storage
---
-->
<!--
 Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.
-->
<p>etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。</p>
<!--
If your Kubernetes cluster uses etcd as its backing store, make sure you have a
[back up](/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster) plan
for those data.
-->	
<p>您的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。</p>
<!--
You can find in-depth information about etcd in the official [documentation](https://etcd.io/docs/).
-->
<p>要了解 etcd 更深层次的信息，请参考 <a href="https://etcd.io/docs/">etcd 文档</a>。</p>
<h3 id="kube-scheduler">kube-scheduler</h3>
<!--
---
title: kube-scheduler
id: kube-scheduler
date: 2018-04-12
full_link: /docs/reference/command-line-tools-reference/kube-scheduler/
short_description: >
  Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.

aka: 
tags:
- architecture
---
-->
<!--
Control plane component that watches for newly created
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> with no assigned
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a>, and selects a node for them
to run on.-->
<p>控制平面组件，负责监视新创建的、未指定运行<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点（node）'>节点（node）</a>的 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>，选择节点让 Pod 在上面运行。</p>
<!--
Factors taken into account for scheduling decisions include individual and collective resource requirements,  hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines.
-->
<p>调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。</p>
<h3 id="kube-controller-manager">kube-controller-manager</h3>
<!--
---
title: kube-controller-manager
id: kube-controller-manager
date: 2018-04-12
full_link: /docs/reference/generated/kube-controller-manager/
short_description: >
  Component on the master that runs controllers.

aka: 
tags:
- architecture
- fundamental
---
-->
<!--
 Control plane component that runs <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> processes.
-->
<p>运行<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>进程的控制平面组件。</p>
<p>从逻辑上讲，每个<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>都是一个单独的进程，
但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。</p>
<!--
Some types of these controllers are:

  * Node controller: Responsible for noticing and responding when nodes go down.
  * Job controller: Watches for Job objects that represent one-off tasks, then creates
    Pods to run those tasks to completion.
  * Endpoints controller: Populates the Endpoints object (that is, joins Services & Pods).
  * Service Account & Token controllers: Create default accounts and API access tokens for new namespaces.
-->
<p>这些控制器包括:</p>
<ul>
<li>节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应</li>
<li>任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成</li>
<li>端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)</li>
<li>服务帐户和令牌控制器（Service Account &amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌</li>
</ul>
<!--
### cloud-controller-manager

The cloud-controller-manager only runs controllers that are specific to your cloud provider.
If you are running Kubernetes on your own premises, or in a learning environment inside your
own PC, the cluster does not have a cloud controller manager.

As with the kube-controller-manager, the cloud-controller-manager combines several logically
independent control loops into a single binary that you run as a single process. You can
scale horizontally (run more than one copy) to improve performance or to help tolerate failures.

The following controllers can have cloud provider dependencies:

  * Node controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding
  * Route controller: For setting up routes in the underlying cloud infrastructure
  * Service controller: For creating, updating and deleting cloud provider load balancers
-->
<h3 id="cloud-controller-manager">cloud-controller-manager</h3>
云控制器管理器是指嵌入特定云的控制逻辑的
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>组件。
云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上，
并将与该云平台交互的组件同与你的集群交互的组件分离开来。
<p><code>cloud-controller-manager</code> 仅运行特定于云平台的控制回路。
如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境，
所部署的环境中不需要云控制器管理器。</p>
<p>与 <code>kube-controller-manager</code> 类似，<code>cloud-controller-manager</code> 将若干逻辑上独立的
控制回路组合到同一个可执行文件中，供你以同一进程的方式运行。
你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。</p>
<p>下面的控制器都包含对云平台驱动的依赖：</p>
<ul>
<li>节点控制器（Node Controller）: 用于在节点终止响应后检查云提供商以确定节点是否已被删除</li>
<li>路由控制器（Route Controller）: 用于在底层云基础架构中设置路由</li>
<li>服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器</li>
</ul>
<!--
## Node Components

Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.
-->
<h2 id="node-components">Node 组件 </h2>
<p>节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。</p>
<h3 id="kubelet">kubelet</h3>
<!--
title: Kubelet
id: kubelet
date: 2018-04-12
full_link: /docs/reference/generated/kubelet
short_description: >
  An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.

aka: 
tags:
- fundamental
- core-object
-->
<!--
 An agent that runs on each <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a> in the cluster. It makes sure that <a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='containers'>containers</a> are running in a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>.
-->
<p>一个在集群中每个<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点（node）'>节点（node）</a>上运行的代理。
它保证<a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='容器（containers）'>容器（containers）</a>都
运行在 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 中。</p>
<!--
The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.
-->
<p>kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs
中描述的容器处于运行状态且健康。
kubelet 不会管理不是由 Kubernetes 创建的容器。</p>
<h3 id="kube-proxy">kube-proxy</h3>
<!-- ---
title: kube-proxy
id: kube-proxy
date: 2018-04-12
full_link: /zh/docs/reference/command-line-tools-reference/kube-proxy/
short_description: >
  `kube-proxy` is a network proxy that runs on each node in the cluster.

aka:
tags:
- fundamental
- networking
--- -->
 <!-- [kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/) is a
network proxy that runs on each node in your cluster, implementing part of
the Kubernetes <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a> concept. -->
<p><a href="/zh/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a> 是集群中每个节点上运行的网络代理，
实现 Kubernetes <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a> 概念的一部分。</p>
<!-- kube-proxy maintains network rules on nodes. These network rules allow
network communication to your Pods from network sessions inside or outside
of your cluster. -->
<p>kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。</p>
<!-- kube-proxy uses the operating system packet filtering layer if there is one
and it's available. Otherwise, kube-proxy forwards the traffic itself. -->
<p>如果操作系统提供了数据包过滤层并可用的话，kube-proxy 会通过它来实现网络规则。否则， kube-proxy 仅转发流量本身。</p>
<!--
### Container Runtime
-->
<h3 id="container-runtime">容器运行时（Container Runtime）   </h3>
<!--
---
title: Container Runtime
id: container-runtime
date: 2019-06-05
full_link: /docs/setup/production-environment/container-runtimes
short_description: >
 The container runtime is the software that is responsible for running containers.

aka:
tags:
- fundamental
- workload
---
-->
<!--
 The container runtime is the software that is responsible for running containers.
-->
<p>容器运行环境是负责运行容器的软件。</p>
<!--
Kubernetes supports container runtimes such sa
<a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a>,
<a class='glossary-tooltip' title='强调简单性、健壮性和可移植性的一种容器运行时' data-toggle='tooltip' data-placement='top' href='https://containerd.io/docs/' target='_blank' aria-label='containerd'>containerd</a>, <a class='glossary-tooltip' title='专用于 Kubernetes 的轻量级容器运行时软件' data-toggle='tooltip' data-placement='top' href='https://cri-o.io/#what-is-cri-o' target='_blank' aria-label='CRI-O'>CRI-O</a>,
and any other implementation of the [Kubernetes CRI (Container Runtime
Interface)](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md).
-->
<p>Kubernetes 支持容器运行时，例如
<a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a>、
<a class='glossary-tooltip' title='强调简单性、健壮性和可移植性的一种容器运行时' data-toggle='tooltip' data-placement='top' href='https://containerd.io/docs/' target='_blank' aria-label='containerd'>containerd</a>、<a class='glossary-tooltip' title='专用于 Kubernetes 的轻量级容器运行时软件' data-toggle='tooltip' data-placement='top' href='https://cri-o.io/#what-is-cri-o' target='_blank' aria-label='CRI-O'>CRI-O</a>
以及 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">Kubernetes CRI (容器运行环境接口)</a>
的其他任何实现。</p>
<!--
## Addons

Addons use Kubernetes resources (<a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>,
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>, etc)
to implement cluster features. Because these are providing cluster-level features, namespaced resources
for addons belong within the `kube-system` namespace.
-->
<h2 id="addons">插件（Addons）   </h2>
<p>插件使用 Kubernetes 资源（<a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>、
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>等）实现集群功能。
因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 <code>kube-system</code> 命名空间。</p>
<!--
Selected addons are described below; for an extended list of available addons, please
see [Addons](/docs/concepts/cluster-administration/addons/).
-->
<p>下面描述众多插件中的几种。有关可用插件的完整列表，请参见
<a href="/zh/docs/concepts/cluster-administration/addons/">插件（Addons）</a>。</p>
<!--
### DNS

While the other addons are not strictly required, all Kubernetes clusters should have [cluster DNS](/docs/concepts/services-networking/dns-pod-service/), as many examples rely on it.

Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes services.

Containers started by Kubernetes automatically include this DNS server in their DNS searches.
-->
<h3 id="dns">DNS  </h3>
<p>尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该
有<a href="/zh/docs/concepts/services-networking/dns-pod-service/">集群 DNS</a>，
因为很多示例都需要 DNS 服务。</p>
<p>集群 DNS 是一个 DNS 服务器，和环境中的其他 DNS 服务器一起工作，它为 Kubernetes 服务提供 DNS 记录。</p>
<p>Kubernetes 启动的容器自动将此 DNS 服务器包含在其 DNS 搜索列表中。</p>
<!--
### Web UI (Dashboard)

[Dashboard](/docs/tasks/access-application-cluster/web-ui-dashboard/) is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications running in the cluster, as well as the cluster itself.
-->
<h3 id="web-界面-仪表盘">Web 界面（仪表盘）</h3>
<p><a href="/zh/docs/tasks/access-application-cluster/web-ui-dashboard/">Dashboard</a>
是 Kubernetes 集群的通用的、基于 Web 的用户界面。
它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。</p>
<!--
### Container Resource Monitoring

[Container Resource Monitoring](/docs/tasks/debug/debug-cluster/resource-usage-monitoring/) records generic time-series metrics
about containers in a central database, and provides a UI for browsing that data.
-->
<h3 id="容器资源监控">容器资源监控</h3>
<p><a href="/zh/docs/tasks/debug/debug-cluster/resource-usage-monitoring/">容器资源监控</a>
将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面。</p>
<!--
### Cluster-level Logging

A [cluster-level logging](/docs/concepts/cluster-administration/logging/) mechanism is responsible for
saving container logs to a central log store with search/browsing interface.
-->
<h3 id="集群层面日志">集群层面日志</h3>
<p><a href="/zh/docs/concepts/cluster-administration/logging/">集群层面日志</a> 机制负责将容器的日志数据
保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Nodes](/docs/concepts/architecture/nodes/)
* Learn about [Controllers](/docs/concepts/architecture/controller/)
* Learn about [kube-scheduler](/docs/concepts/scheduling-eviction/kube-scheduler/)
* Read etcd's official [documentation](https://etcd.io/docs/)
-->
<ul>
<li>进一步了解<a href="/zh/docs/concepts/architecture/nodes/">节点</a></li>
<li>进一步了解<a href="/zh/docs/concepts/architecture/controller/">控制器</a></li>
<li>进一步了解 <a href="/zh/docs/concepts/scheduling-eviction/kube-scheduler/">kube-scheduler</a></li>
<li>阅读 etcd 官方<a href="https://etcd.io/docs/">文档</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0c745f42e623d2b70a53bc0e6db73d95">1.3 - Kubernetes API</h1>
    <div class="lead">Kubernetes API 使你可以查询和操纵 Kubernetes 中对象的状态。 Kubernetes 控制平面的核心是 API 服务器和它暴露的 HTTP API。 用户、集群的不同部分以及外部组件都通过 API 服务器相互通信。</div>
	<!-- overview -->
<!--
The core of Kubernetes' <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>
is the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a>. The API server
exposes an HTTP API that lets end users, different parts of your cluster, and
external components communicate with one another.

The Kubernetes API lets you query and manipulate the state of API objects in Kubernetes
(for example: Pods, Namespaces, ConfigMaps, and Events).

Most operations can be performed through the
[kubectl](/docs/reference/kubectl/) command-line interface or other
command-line tools, such as
[kubeadm](/docs/reference/setup-tools/kubeadm/), which in turn use the
API. However, you can also access the API directly using REST calls.
-->
<p>Kubernetes <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a>
的核心是 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>。
API 服务器负责提供 HTTP API，以供用户、集群中的不同部分和集群外部组件相互通信。</p>
<p>Kubernetes API 使你可以查询和操纵 Kubernetes API
中对象（例如：Pod、Namespace、ConfigMap 和 Event）的状态。</p>
<p>大部分操作都可以通过 <a href="/zh/docs/reference/kubectl/">kubectl</a> 命令行接口或
类似 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm</a> 这类命令行工具来执行，
这些工具在背后也是调用 API。不过，你也可以使用 REST 调用来访问这些 API。</p>
<!--
Consider using one of the [client libraries](/docs/reference/using-api/client-libraries/)
if you are writing an application using the Kubernetes API.
-->
<p>如果你正在编写程序来访问 Kubernetes API，可以考虑使用
<a href="/zh/docs/reference/using-api/client-libraries/">客户端库</a>之一。</p>
<!-- body -->
<!--
## OpenAPI specification {#api-specification}

Complete API details are documented using [OpenAPI](https://www.openapis.org/).

### OpenAPI V2

The Kubernetes API server serves an aggregated OpenAPI v2 spec via the
`/openapi/v2` endpoint. You can request the response format using
request headers as follows:
-->
<h2 id="api-specification">OpenAPI 规范    </h2>
<p>完整的 API 细节是用 <a href="https://www.openapis.org/">OpenAPI</a> 来表述的。</p>
<h3 id="openapi-v2">OpenAPI V2</h3>
<p>Kubernetes API 服务器通过 <code>/openapi/v2</code> 端点提供聚合的 OpenAPI v2 规范。
你可以按照下表所给的请求头部，指定响应的格式：</p>
<!--
<table>
  <thead>
     <tr>
        <th>Header</th>
        <th style="min-width: 50%;">Possible values</th>
        <th>Notes</th>
     </tr>
  </thead>
  <tbody>
     <tr>
        <td><code>Accept-Encoding</code></td>
        <td><code>gzip</code></td>
        <td><em>not supplying this header is also acceptable</em></td>
     </tr>
     <tr>
        <td rowspan="3"><code>Accept</code></td>
        <td><code>application/com.github.proto-openapi.spec.v2@v1.0+protobuf</code></td>
        <td><em>mainly for intra-cluster use</em></td>
     </tr>
     <tr>
        <td><code>application/json</code></td>
        <td><em>default</em></td>
     </tr>
     <tr>
        <td><code>*</code></td>
        <td><em>serves </em><code>application/json</code></td>
     </tr>
  </tbody>
  <caption>Valid request header values for OpenAPI v2 queries</caption>
</table>
-->
<table>
  <thead>
     <tr>
        <th>头部</th>
        <th style="min-width: 50%;">可选值</th>
        <th>说明</th>
     </tr>
  </thead>
  <tbody>
     <tr>
        <td><code>Accept-Encoding</code></td>
        <td><code>gzip</code></td>
        <td><em>不指定此头部也是可以的</em></td>
     </tr>
     <tr>
        <td rowspan="3"><code>Accept</code></td>
        <td><code>application/com.github.proto-openapi.spec.v2@v1.0+protobuf</code></td>
        <td><em>主要用于集群内部</em></td>
     </tr>
     <tr>
        <td><code>application/json</code></td>
        <td><em>默认值</em></td>
     </tr>
     <tr>
        <td><code>*</code></td>
        <td><em>提供</em><code>application/json</code></td>
     </tr>
  </tbody>
  <caption>OpenAPI v2 查询请求的合法头部值</caption>
</table>
<!--
Kubernetes implements an alternative Protobuf based serialization format that
is primarily intended for intra-cluster communication. For more information
about this format, see the [Kubernetes Protobuf serialization](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/protobuf.md) design proposal and the
Interface Definition Language (IDL) files for each schema located in the Go
packages that define the API objects.
-->
<p>Kubernetes 为 API 实现了一种基于 Protobuf 的序列化格式，主要用于集群内部通信。
关于此格式的详细信息，可参考
<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/protobuf.md">Kubernetes Protobuf 序列化</a>
设计提案。每种模式对应的接口描述语言（IDL）位于定义 API 对象的 Go 包中。</p>
<h3 id="openapi-v3">OpenAPI V3</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code>
</div>


<!--
Kubernetes v1.23 offers initial support for publishing its APIs as OpenAPI v3; this is an
alpha feature that is disabled by default.
You can enable the alpha feature by turning on the
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) named `OpenAPIV3`
for the kube-apiserver component.
-->
<p>Kubernetes v1.23 提供将其 API 以 OpenAPI v3 形式发布的初始支持；这一功能特性处于 Alpha
状态，默认被禁用。
你可以通过为 kube-apiserver 组件启用 <code>OpenAPIV3</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>来启用此
Alpha 特性。</p>
<!--
With the feature enabled, the Kubernetes API server serves an
aggregated OpenAPI v3 spec per Kubernetes group version at the
`/openapi/v3/apis/<group>/<version>` endpoint. Please refer to the
table below for accepted request headers.
-->
<p>特性被启用时，Kubernetes API 服务器会在端点 <code>/openapi/v3/apis/&lt;group&gt;/&lt;version&gt;</code>
提供按 Kubernetes 组版本聚合的 OpenAPI v3 规范。
请参阅下表了解可接受的请求头部。</p>
<table>
  <caption style="display:none"><!--Valid request header values for OpenAPI v3 queries-->OpenAPI v3 查询的合法请求头部值</caption>
  <thead>
     <tr>
        <th><!--Header-->头部</th>
        <th style="min-width: 50%;"><!--Possible values-->可选值</th>
        <th><!--Notes-->说明</th>
     </tr>
  </thead>
  <tbody>
     <tr>
        <td><code>Accept-Encoding</code></td>
        <td><code>gzip</code></td>
        <td><em><!--not supplying this header is also acceptable-->不提供此头部也是可接受的</em></td>
     </tr>
     <tr>
        <td rowspan="3"><code>Accept</code></td>
        <td><code>application/com.github.proto-openapi.spec.v3@v1.0+protobuf</code></td>
        <td><em><!--mainly for intra-cluster use-->主要用于集群内部使用</em></td>
     </tr>
     <tr>
        <td><code>application/json</code></td>
        <td><em><!--default-->默认</em></td>
     </tr>
     <tr>
        <td><code>*</code></td>
        <td><em><!--serves-->以</em> <code>application/json</code> 形式返回</td>
     </tr>
  </tbody>
</table>
<!--
A discovery endpoint `/openapi/v3` is provided to see a list of all
group/versions available. This endpoint only returns JSON.
-->
<p>发现端点 <code>/openapi/v3</code> 被提供用来查看可用的所有组、版本列表。
此列表仅返回 JSON。</p>
<!--
## API changes

Any system that is successful needs to grow and change as new use cases emerge or existing ones change.
Therefore, Kubernetes has designed its features to allow the Kubernetes API to continuously change and grow.
The Kubernetes project aims to _not_ break compatibility with existing clients, and to maintain that
compatibility for a length of time so that other projects have an opportunity to adapt.
-->
<h2 id="api-changes">API 变更    </h2>
<p>任何成功的系统都要随着新的使用案例的出现和现有案例的变化来成长和变化。
为此，Kubernetes 的功能特性设计考虑了让 Kubernetes API 能够持续变更和成长的因素。
Kubernetes 项目的目标是 <em>不要</em> 引发现有客户端的兼容性问题，并在一定的时期内
维持这种兼容性，以便其他项目有机会作出适应性变更。</p>
<!--
In general, new API resources and new resource fields can be added often and frequently.
Elimination of resources or fields requires following the
[API deprecation policy](/docs/reference/using-api/deprecation-policy/).
-->
<p>一般而言，新的 API 资源和新的资源字段可以被频繁地添加进来。
删除资源或者字段则要遵从
<a href="/zh/docs/reference/using-api/deprecation-policy/">API 废弃策略</a>。</p>
<!--
Kubernetes makes a strong commitment to maintain compatibility for official Kubernetes APIs
once they reach general availability (GA), typically at API version `v1`. Additionally,
Kubernetes keeps compatibility even for _beta_ API versions wherever feasible:
if you adopt a beta API you can continue to interact with your cluster using that API,
even after the feature goes stable.
-->
<p>Kubernetes 对维护达到正式发布（GA）阶段的官方 API 的兼容性有着很强的承诺，
通常这一 API 版本为 <code>v1</code>。此外，Kubernetes 在可能的时候还会保持 Beta API
版本的兼容性：如果你采用了 Beta API，你可以继续在集群上使用该 API，
即使该功能特性已进入稳定期也是如此。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Although Kubernetes also aims to maintain compatibility for _alpha_ APIs versions, in some
circumstances this is not possible. If you use any alpha API versions, check the release notes
for Kubernetes when upgrading your cluster, in case the API did change.
-->
<p>尽管 Kubernetes 也努力为 Alpha API 版本维护兼容性，在有些场合兼容性是无法做到的。
如果你使用了任何 Alpha API 版本，需要在升级集群时查看 Kubernetes 发布说明，
以防 API 的确发生变更。
</div>
<!--
Refer to [API versions reference](/docs/reference/using-api/#api-versioning)
for more details on the API version level definitions.
-->
<p>关于 API 版本分级的定义细节，请参阅
<a href="/zh/docs/reference/using-api/#api-versioning">API 版本参考</a>页面。</p>
<!--
## API Extension

The Kubernetes API can be extended in one of two ways:
-->
<h2 id="api-extension">API 扩展 </h2>
<p>有两种途径来扩展 Kubernetes API：</p>
<!--
1. [Custom resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
   let you declaratively define how the API server should provide your chosen resource API.
1. You can also extend the Kubernetes API by implementing an
   [aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/).
-->
<ol>
<li>你可以使用<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</a>
来以声明式方式定义 API 服务器如何提供你所选择的资源 API。</li>
<li>你也可以选择实现自己的
<a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">聚合层</a>
来扩展 Kubernetes API。</li>
</ol>
<h2 id="what-s-next">What's next</h2>
<!--
- Learn how to extend the Kubernetes API by adding your own
  [CustomResourceDefinition](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/).
- [Controlling Access To The Kubernetes API](/docs/concepts/security/controlling-access/) describes
  how the cluster manages authentication and authorization for API access.
- Learn about API endpoints, resource types and samples by reading
  [API Reference](/docs/reference/kubernetes-api/).
- Learn about what constitutes a compatible change, and how to change the API, from
  [API changes](https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#readme).
-->
<ul>
<li>了解如何通过添加你自己的
<a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">CustomResourceDefinition</a>
来扩展 Kubernetes API。</li>
<li><a href="/zh/docs/concepts/security/controlling-access/">控制 Kubernetes API 访问</a>页面描述了集群如何针对
API 访问管理身份认证和鉴权。</li>
<li>通过阅读 <a href="/zh/docs/reference/kubernetes-api/">API 参考</a>了解 API 端点、资源类型以及示例。</li>
<li>阅读 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#readme">API 变更（英文）</a>
以了解什么是兼容性的变更以及如何变更 API。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-110f33530cf761140cb1dab536baef04">1.4 - 使用 Kubernetes 对象</h1>
    <div class="lead">Kubernetes 对象是 Kubernetes 系统中的持久性实体。Kubernetes 使用这些实体表示你的集群状态。 了解 Kubernetes 对象模型以及如何使用这些对象。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-9f5adfa77f48c50d5cc81155a3cecb98">1.4.1 - 理解 Kubernetes 对象</h1>
    
	<!---
title: Understanding Kubernetes Objects
content_type: concept
weight: 10
card: 
  name: concepts
  weight: 40
-->
<!-- overview -->
<!--
This page explains how Kubernetes objects are represented in the Kubernetes API, and how you can express them in `.yaml` format.
-->
<p>本页说明了 Kubernetes 对象在 Kubernetes API 中是如何表示的，以及如何在 <code>.yaml</code> 格式的文件中表示。</p>
<!-- body -->
<!--
## Understanding Kubernetes Objects

*Kubernetes Objects* are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe:

* What containerized applications are running (and on which nodes)
* The resources available to those applications
* The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance
-->
<h2 id="理解-kubernetes-对象">理解 Kubernetes 对象</h2>
<p>在 Kubernetes 系统中，<em>Kubernetes 对象</em> 是持久化的实体。
Kubernetes 使用这些实体去表示整个集群的状态。特别地，它们描述了如下信息：</p>
<ul>
<li>哪些容器化应用在运行（以及在哪些节点上）</li>
<li>可以被应用使用的资源</li>
<li>关于应用运行时表现的策略，比如重启策略、升级策略，以及容错策略</li>
</ul>
<!--
A Kubernetes object is a "record of intent" - once you create the object, the Kubernetes system will constantly work to ensure that object exists. By creating an object, you're effectively telling the Kubernetes system what you want your cluster's workload to look like; this is your cluster's *desired state*.

To work with Kubernetes objects - whether to create, modify, or delete them - you'll need to use the [Kubernetes API](/docs/concepts/overview/kubernetes-api/). When you use the `kubectl` command-line interface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use the Kubernetes API directly in your own programs using one of the [Client Libraries](/docs/reference/using-api/client-libraries/).
-->
<p>Kubernetes 对象是 “目标性记录” —— 一旦创建对象，Kubernetes 系统将持续工作以确保对象存在。
通过创建对象，本质上是在告知 Kubernetes 系统，所需要的集群工作负载看起来是什么样子的，
这就是 Kubernetes 集群的 <strong>期望状态（Desired State）</strong>。</p>
<p>操作 Kubernetes 对象 —— 无论是创建、修改，或者删除 —— 需要使用
<a href="/zh/docs/concepts/overview/kubernetes-api">Kubernetes API</a>。
比如，当使用 <code>kubectl</code> 命令行接口时，CLI 会执行必要的 Kubernetes API 调用，
也可以在程序中使用
<a href="/zh/docs/reference/using-api/client-libraries/">客户端库</a>直接调用 Kubernetes API。</p>
<!--
### Object Spec and Status

Almost every Kubernetes object includes two nested object fields that govern
the object's configuration: the object *`spec`* and the object *`status`*.
For objects that have a `spec`, you have to set this when you create the object,
providing a description of the characteristics you want the resource to have:
its _desired state_.
-->
<h3 id="object-spec-and-status">对象规约（Spec）与状态（Status）   </h3>
<p>几乎每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置：
对象 <em><code>spec</code>（规约）</em> 和 对象 <em><code>status</code>（状态）</em> 。
对于具有 <code>spec</code> 的对象，你必须在创建对象时设置其内容，描述你希望对象所具有的特征：
<em>期望状态（Desired State）</em> 。</p>
<!--
The `status` describes the _current state_ of the object, supplied and updated
by the Kubernetes system and its components. The Kubernetes
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> continually
and actively manages every object's actual state to match the desired state you
supplied.
-->
<p><code>status</code> 描述了对象的 <em>当前状态（Current State）</em>，它是由 Kubernetes 系统和组件
设置并更新的。在任何时刻，Kubernetes
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>
都一直积极地管理着对象的实际状态，以使之与期望状态相匹配。</p>
<!--
For example: in Kubernetes, a Deployment is an object that can represent an
application running on your cluster. When you create the Deployment, you
might set the Deployment `spec` to specify that you want three replicas of
the application to be running. The Kubernetes system reads the Deployment
spec and starts three instances of your desired application-updating
the status to match your spec. If any of those instances should fail
(a status change), the Kubernetes system responds to the difference
between spec and status by making a correction-in this case, starting
a replacement instance.
-->
<p>例如，Kubernetes 中的 Deployment 对象能够表示运行在集群中的应用。
当创建 Deployment 时，可能需要设置 Deployment 的 <code>spec</code>，以指定该应用需要有 3 个副本运行。
Kubernetes 系统读取 Deployment 规约，并启动我们所期望的应用的 3 个实例
—— 更新状态以与规约相匹配。
如果这些实例中有的失败了（一种状态变更），Kubernetes 系统通过执行修正操作
来响应规约和状态间的不一致 —— 在这里意味着它会启动一个新的实例来替换。</p>
<!--
For more information on the object spec, status, and metadata, see the [Kubernetes API Conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md).
-->
<p>关于对象 spec、status 和 metadata 的更多信息，可参阅
<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md">Kubernetes API 约定</a>。</p>
<!--
### Describing a Kubernetes Object

When you create an object in Kubernetes, you must provide the object spec that describes its desired state, as well as some basic information about the object (such as a name). When you use the Kubernetes API to create the object (either directly or via `kubectl`), that API request must include that information as JSON in the request body. **Most often, you provide the information to `kubectl` in a .yaml file.** `kubectl` converts the information to JSON when making the API request.

Here's an example `.yaml` file that shows the required fields and object spec for a Kubernetes Deployment:
-->
<h3 id="描述-kubernetes-对象">描述 Kubernetes 对象</h3>
<p>创建 Kubernetes 对象时，必须提供对象的规约，用来描述该对象的期望状态，
以及关于对象的一些基本信息（例如名称）。
当使用 Kubernetes API 创建对象时（或者直接创建，或者基于<code>kubectl</code>），
API 请求必须在请求体中包含 JSON 格式的信息。
<strong>大多数情况下，需要在 .yaml 文件中为 <code>kubectl</code> 提供这些信息</strong>。
<code>kubectl</code> 在发起 API 请求时，将这些信息转换成 JSON 格式。</p>
<p>这里有一个 <code>.yaml</code> 示例文件，展示了 Kubernetes Deployment 的必需字段和对象规约：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/deployment.yaml" download="application/deployment.yaml"><code>application/deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-deployment-yaml')" title="Copy application/deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># tells deployment to run 2 pods matching the template</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
One way to create a Deployment using a `.yaml` file like the one above is to use the
[`kubectl apply`](/docs/reference/generated/kubectl/kubectl-commands#apply) command
in the `kubectl` command-line interface, passing the `.yaml` file as an argument. Here's an example:
-->
<p>使用类似于上面的 <code>.yaml</code> 文件来创建 Deployment的一种方式是使用 <code>kubectl</code> 命令行接口（CLI）中的
<a href="/docs/reference/generated/kubectl/kubectl-commands#apply"><code>kubectl apply</code></a> 命令，
将 <code>.yaml</code> 文件作为参数。下面是一个示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/deployment.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似如下这样：</p>
<pre><code>deployment.apps/nginx-deployment created
</code></pre><!--
### Required Fields

In the `.yaml` file for the Kubernetes object you want to create, you'll need to set values for the following fields:

* `apiVersion` - Which version of the Kubernetes API you're using to create this object
* `kind` - What kind of object you want to create
* `metadata` - Data that helps uniquely identify the object, including a `name` string, `UID`, and optional `namespace`
* `spec` - What state you desire for the object
-->
<h3 id="required-fields">必需字段 </h3>
<p>在想要创建的 Kubernetes 对象对应的 <code>.yaml</code> 文件中，需要配置如下的字段：</p>
<ul>
<li><code>apiVersion</code> - 创建该对象所使用的 Kubernetes API 的版本</li>
<li><code>kind</code> - 想要创建的对象的类别</li>
<li><code>metadata</code> - 帮助唯一性标识对象的一些数据，包括一个 <code>name</code> 字符串、UID 和可选的 <code>namespace</code></li>
<li><code>spec</code> - 你所期望的该对象的状态</li>
</ul>
<!--
The precise format of the object `spec` is different for every Kubernetes object, and contains nested fields specific to that object. The [Kubernetes API Reference](https://kubernetes.io/docs/reference/kubernetes-api/) can help you find the spec format for all of the objects you can create using Kubernetes.
-->
<p>对象 <code>spec</code> 的精确格式对每个 Kubernetes 对象来说是不同的，包含了特定于该对象的嵌套字段。
<a href="https://kubernetes.io/docs/reference/kubernetes-api/">Kubernetes API 参考</a>
能够帮助我们找到任何我们想创建的对象的规约格式。</p>
<!--
For example, see the [`spec` field](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)
for the Pod API reference.
For each Pod, the `.spec` field specifies the pod and its desired state (such as the container image name for
each container within that pod).
Another example of an object specification is the
[`spec` field](/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#StatefulSetSpec)
for the StatefulSet API. For StatefulSet, the `.spec` field specifies the StatefulSet and
its desired state.
Within the `.spec` of a StatefulSet is a [template](/docs/concepts/workloads/pods/#pod-templates)
for Pod objects. That template describes Pods that the StatefulSet controller will create in order to
satisfy the StatefulSet specification.
Different kinds of object can also have different `.status`; again, the API reference pages
detail the structure of that `.status` field, and its content for each different type of object.
-->
<p>例如，参阅 Pod API 参考文档中
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec"><code>spec</code> 字段</a>。
对于每个 Pod，其 <code>.spec</code> 字段设置了 Pod 及其期望状态（例如 Pod 中每个容器的容器镜像名称）。
另一个对象规约的例子是 StatefulSet API 中的
<a href="/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#StatefulSetSpec"><code>spec</code> 字段</a>。
对于 StatefulSet 而言，其 <code>.spec</code> 字段设置了 StatefulSet 及其期望状态。
在 StatefulSet 的 <code>.spec</code> 内，有一个为 Pod 对象提供的<a href="/zh/docs/concepts/workloads/pods/#pod-templates">模板</a>。该模板描述了 StatefulSet 控制器为了满足 StatefulSet 规约而要创建的 Pod。
不同类型的对象可以由不同的 <code>.status</code> 信息。API 参考页面给出了 <code>.status</code> 字段的详细结构，
以及针对不同类型 API 对象的具体内容。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about the most important basic Kubernetes objects, such as [Pod](/docs/concepts/workloads/pods/).
* Learn about [controllers](/docs/concepts/architecture/controller/) in Kubernetes.
* [Using the Kubernetes API](/docs/reference/using-api/) explains some more API concepts.
-->
<ul>
<li>了解最重要的 Kubernetes 基本对象，例如 <a href="/zh/docs/concepts/workloads/pods/">Pod</a>。</li>
<li>了解 Kubernetes 中的<a href="/zh/docs/concepts/architecture/controller/">控制器</a>。</li>
<li><a href="/zh/docs/reference/using-api/">使用 Kubernetes API</a> 一节解释了一些 API 概念。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-6751db8ff5409476de8225d17d6c42dd">1.4.2 - Kubernetes 对象管理</h1>
    
	<!-- overview -->
<!--
The `kubectl` command-line tool supports several different ways to create and manage
Kubernetes objects. This document provides an overview of the different
approaches. Read the [Kubectl book](https://kubectl.docs.kubernetes.io) for
details of managing objects by Kubectl.
-->
<p><code>kubectl</code> 命令行工具支持多种不同的方式来创建和管理 Kubernetes 对象。
本文档概述了不同的方法。
阅读 <a href="https://kubectl.docs.kubernetes.io">Kubectl book</a> 来了解 kubectl
管理对象的详细信息。</p>
<!-- body -->
<!--
## Management techniques
-->
<h2 id="管理技巧">管理技巧</h2>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
A Kubernetes object should be managed using only one technique. Mixing
and matching techniques for the same object results in undefined behavior.
-->
<p>应该只使用一种技术来管理 Kubernetes 对象。混合和匹配技术作用在同一对象上将导致未定义行为。
</div>


<!--
| Management technique             | Operates on          |Recommended environment | Supported writers  | Learning curve |
|----------------------------------|----------------------|------------------------|--------------------|----------------|
| Imperative commands              | Live objects         | Development projects   | 1+                 | Lowest         |
| Imperative object configuration  | Individual files     | Production projects    | 1                  | Moderate       |
| Declarative object configuration | Directories of files | Production projects    | 1+                 | Highest        |
-->
<table>
<thead>
<tr>
<th>管理技术</th>
<th>作用于</th>
<th>建议的环境</th>
<th>支持的写者</th>
<th>学习难度</th>
</tr>
</thead>
<tbody>
<tr>
<td>指令式命令</td>
<td>活跃对象</td>
<td>开发项目</td>
<td>1+</td>
<td>最低</td>
</tr>
<tr>
<td>指令式对象配置</td>
<td>单个文件</td>
<td>生产项目</td>
<td>1</td>
<td>中等</td>
</tr>
<tr>
<td>声明式对象配置</td>
<td>文件目录</td>
<td>生产项目</td>
<td>1+</td>
<td>最高</td>
</tr>
</tbody>
</table>
<!--
## Imperative commands
-->
<h2 id="指令式命令">指令式命令</h2>
<!--
When using imperative commands, a user operates directly on live objects
in a cluster. The user provides operations to
the `kubectl` command as arguments or flags.
-->
<p>使用指令式命令时，用户可以在集群中的活动对象上进行操作。用户将操作传给
<code>kubectl</code> 命令作为参数或标志。</p>
<!--
This is the recommended way to get started or to run a one-off task in
a cluster. Because this technique operates directly on live
objects, it provides no history of previous configurations.
-->
<p>这是开始或者在集群中运行一次性任务的推荐方法。因为这个技术直接在活跃对象
上操作，所以它不提供以前配置的历史记录。</p>
<!--
### Examples
-->
<h3 id="例子">例子</h3>
<!--
Run an instance of the nginx container by creating a Deployment object:
-->
<p>通过创建 Deployment 对象来运行 nginx 容器的实例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">kubectl create deployment nginx --image nginx
</code></pre></div><!--
### Trade-offs
-->
<h3 id="权衡">权衡</h3>
<!--
Advantages compared to object configuration:

- Commands are simple, easy to learn and easy to remember.
- Commands require only a single step to make changes to the cluster.
-->
<p>与对象配置相比的优点：</p>
<ul>
<li>命令简单，易学且易于记忆。</li>
<li>命令仅需一步即可对集群进行更改。</li>
</ul>
<!--
Disadvantages compared to object configuration:

- Commands do not integrate with change review processes.
- Commands do not provide an audit trail associated with changes.
- Commands do not provide a source of records except for what is live.
- Commands do not provide a template for creating new objects.
-->
<p>与对象配置相比的缺点：</p>
<ul>
<li>命令不与变更审查流程集成。</li>
<li>命令不提供与更改关联的审核跟踪。</li>
<li>除了实时内容外，命令不提供记录源。</li>
<li>命令不提供用于创建新对象的模板。</li>
</ul>
<!--
## Imperative object configuration
-->
<h2 id="指令式对象配置">指令式对象配置</h2>
<!--
In imperative object configuration, the kubectl command specifies the
operation (create, replace, etc.), optional flags and at least one file
name. The file specified must contain a full definition of the object
in YAML or JSON format.
-->
<p>在指令式对象配置中，kubectl 命令指定操作（创建，替换等），可选标志和
至少一个文件名。指定的文件必须包含 YAML 或 JSON 格式的对象的完整定义。</p>
<!--
See the [API reference](/docs/reference/generated/kubernetes-api/v1.23/)
for more details on object definitions.
-->
<p>有关对象定义的详细信息，请查看
<a href="/docs/reference/generated/kubernetes-api/v1.23/">API 参考</a>。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
The imperative `replace` command replaces the existing
spec with the newly provided one, dropping all changes to the object missing from
the configuration file.  This approach should not be used with resource
types whose specs are updated independently of the configuration file.
Services of type `LoadBalancer`, for example, have their `externalIPs` field updated
independently from the configuration by the cluster.
-->
<p><code>replace</code> 指令式命令将现有规范替换为新提供的规范，并放弃对配置文件中
缺少的对象的所有更改。此方法不应与对象规约被独立于配置文件进行更新的
资源类型一起使用。比如类型为 <code>LoadBalancer</code> 的服务，它的 <code>externalIPs</code>
字段就是独立于集群配置进行更新。
</div>


<!--
### Examples

Create the objects defined in a configuration file:
-->
<h3 id="例子-1">例子</h3>
<p>创建配置文件中定义的对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">kubectl create -f nginx.yaml
</code></pre></div><!--
Delete the objects defined in two configuration files:
-->
<p>删除两个配置文件中定义的对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">kubectl delete -f nginx.yaml -f redis.yaml
</code></pre></div><!--
Update the objects defined in a configuration file by overwriting
the live configuration:
-->
<p>通过覆盖活动配置来更新配置文件中定义的对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">kubectl replace -f nginx.yaml
</code></pre></div><!--
### Trade-offs
-->
<h3 id="权衡-1">权衡</h3>
<!--
Advantages compared to imperative commands:

- Object configuration can be stored in a source control system such as Git.
- Object configuration can integrate with processes such as reviewing changes before push and audit trails.
- Object configuration provides a template for creating new objects.
-->
<p>与指令式命令相比的优点：</p>
<ul>
<li>对象配置可以存储在源控制系统中，比如 Git。</li>
<li>对象配置可以与流程集成，例如在推送和审计之前检查更新。</li>
<li>对象配置提供了用于创建新对象的模板。</li>
</ul>
<!--
Disadvantages compared to imperative commands:

- Object configuration requires basic understanding of the object schema.
- Object configuration requires the additional step of writing a YAML file.
-->
<p>与指令式命令相比的缺点：</p>
<ul>
<li>对象配置需要对对象架构有基本的了解。</li>
<li>对象配置需要额外的步骤来编写 YAML 文件。</li>
</ul>
<!--
Advantages compared to declarative object configuration:

- Imperative object configuration behavior is simpler and easier to understand.
- As of Kubernetes version 1.5, imperative object configuration is more mature.
-->
<p>与声明式对象配置相比的优点：</p>
<ul>
<li>指令式对象配置行为更加简单易懂。</li>
<li>从 Kubernetes 1.5 版本开始，指令对象配置更加成熟。</li>
</ul>
<!--
Disadvantages compared to declarative object configuration:

- Imperative object configuration works best on files, not directories.
- Updates to live objects must be reflected in configuration files, or they will be lost during the next replacement.
-->
<p>与声明式对象配置相比的缺点：</p>
<ul>
<li>指令式对象配置更适合文件，而非目录。</li>
<li>对活动对象的更新必须反映在配置文件中，否则会在下一次替换时丢失。</li>
</ul>
<!--
## Declarative object configuration
-->
<h2 id="声明式对象配置">声明式对象配置</h2>
<!--
When using declarative object configuration, a user operates on object
configuration files stored locally, however the user does not define the
operations to be taken on the files. Create, update, and delete operations
are automatically detected per-object by `kubectl`. This enables working on
directories, where different operations might be needed for different objects.
-->
<p>使用声明式对象配置时，用户对本地存储的对象配置文件进行操作，但是用户
未定义要对该文件执行的操作。
<code>kubectl</code> 会自动检测每个文件的创建、更新和删除操作。
这使得配置可以在目录上工作，根据目录中配置文件对不同的对象执行不同的操作。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Declarative object configuration retains changes made by other
writers, even if the changes are not merged back to the object configuration file.
This is possible by using the `patch` API operation to write only
observed differences, instead of using the `replace`
API operation to replace the entire object configuration.
-->
<p>声明式对象配置保留其他编写者所做的修改，即使这些更改并未合并到对象配置文件中。
可以通过使用 <code>patch</code> API 操作仅写入观察到的差异，而不是使用 <code>replace</code> API
操作来替换整个对象配置来实现。
</div>
<!--
### Examples
-->
<h3 id="例子-2">例子</h3>
<!--
Process all object configuration files in the `configs` directory, and create or
patch the live objects. You can first `diff` to see what changes are going to be
made, and then apply:
-->
<p>处理 <code>configs</code> 目录中的所有对象配置文件，创建并更新活跃对象。
可以首先使用 <code>diff</code> 子命令查看将要进行的更改，然后在进行应用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">kubectl diff -f configs/
kubectl apply -f configs/
</code></pre></div><!--
Recursively process directories:
-->
<p>递归处理目录：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">kubectl diff -R -f configs/
kubectl apply -R -f configs/
</code></pre></div><!--
### Trade-offs

Advantages compared to imperative object configuration:

- Changes made directly to live objects are retained, even if they are not merged back into the configuration files.
- Declarative object configuration has better support for operating on directories and automatically detecting operation types (create, patch, delete) per-object.
-->
<h3 id="权衡-2">权衡</h3>
<p>与指令式对象配置相比的优点：</p>
<ul>
<li>对活动对象所做的更改即使未合并到配置文件中，也会被保留下来。</li>
<li>声明性对象配置更好地支持对目录进行操作并自动检测每个文件的操作类型（创建，修补，删除）。</li>
</ul>
<!--
Disadvantages compared to imperative object configuration:

- Declarative object configuration is harder to debug and understand results when they are unexpected.
- Partial updates using diffs create complex merge and patch operations.
-->
<p>与指令式对象配置相比的缺点：</p>
<ul>
<li>声明式对象配置难于调试并且出现异常时结果难以理解。</li>
<li>使用 diff 产生的部分更新会创建复杂的合并和补丁操作。</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
- [Managing Kubernetes Objects Using Imperative Commands](/docs/tasks/manage-kubernetes-objects/imperative-command/)
- [Managing Kubernetes Objects Using Object Configuration (Imperative)](/docs/tasks/manage-kubernetes-objects/imperative-config/)
- [Managing Kubernetes Objects Using Object Configuration (Declarative)](/docs/tasks/manage-kubernetes-objects/declarative-config/)
- [Managing Kubernetes Objects Using Kustomize (Declarative)](/docs/tasks/manage-kubernetes-objects/kustomization/)
- [Kubectl Command Reference](/docs/reference/generated/kubectl/kubectl-commands/)
- [Kubectl Book](https://kubectl.docs.kubernetes.io)
- [Kubernetes API Reference](/docs/reference/generated/kubernetes-api/v1.23/)
-->
<ul>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-command/">使用指令式命令管理 Kubernetes 对象</a></li>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-config/">使用对象配置管理 Kubernetes 对象（指令式）</a></li>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/declarative-config/">使用对象配置管理 Kubernetes 对象（声明式）</a></li>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/kustomization/">使用 Kustomize（声明式）管理 Kubernetes 对象</a></li>
<li><a href="/docs/reference/generated/kubectl/kubectl-commands/">Kubectl 命令参考</a></li>
<li><a href="https://kubectl.docs.kubernetes.io">Kubectl Book</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.23/">Kubernetes API 参考</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f37749a83c2916b63279ea60f3cfe53e">1.4.3 - 对象名称和 IDs</h1>
    
	<!-- overview -->
<!--
Each object in your cluster has a [_Name_](#names) that is unique for that type of resource.
Every Kubernetes object also has a [_UID_](#uids) that is unique across your whole cluster.

For example, you can only have one Pod named `myapp-1234` within the same [namespace](/docs/concepts/overview/working-with-objects/namespaces/), but you can have one Pod and one Deployment that are each named `myapp-1234`.
-->
<p>集群中的每一个对象都有一个<a href="#names"><em>名称</em></a> 来标识在同类资源中的唯一性。</p>
<p>每个 Kubernetes 对象也有一个<a href="#uids"><em>UID</em></a> 来标识在整个集群中的唯一性。</p>
<p>比如，在同一个<a href="/zh/docs/concepts/overview/working-with-objects/namespaces/">名字空间</a>
中有一个名为 <code>myapp-1234</code> 的 Pod, 但是可以命名一个 Pod 和一个 Deployment 同为 <code>myapp-1234</code>.</p>
<!--
For non-unique user-provided attributes, Kubernetes provides [labels](/docs/user-guide/labels) and [annotations](/docs/concepts/overview/working-with-objects/annotations/).
-->
<p>对于用户提供的非唯一性的属性，Kubernetes 提供了
<a href="/zh/docs/concepts/working-with-objects/labels">标签（Labels）</a>和
<a href="/zh/docs/concepts/overview/working-with-objects/annotations/">注解（Annotation）</a>机制。</p>
<!-- body -->
<!--
## Names
-->
<h2 id="names">名称 </h2>
<!--
---
title: Name
id: name
date: 2018-04-12
full_link: /docs/concepts/overview/working-with-objects/names/
short_description: >
  A client-provided string that refers to an object in a resource URL, such as `/api/v1/pods/some-name`.

aka: 
tags:
- fundamental
---
-->
<!--
 A client-provided string that refers to an object in a resource URL, such as `/api/v1/pods/some-name`.
-->
<p>客户端提供的字符串，引用资源 url 中的对象，如<code>/api/v1/pods/some name</code>。</p>
<!--
Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name.
-->
<p>某一时刻，只能有一个给定类型的对象具有给定的名称。但是，如果删除该对象，则可以创建同名的新对象。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In cases when objects represent a physical entity, like a Node representing a physical host, when the host is re-created under the same name without deleting and re-creating the Node, Kubernetes treats the new host as the old one, which may lead to inconsistencies.
-->
<p>当对象所代表的是一个物理实体（例如代表一台物理主机的 Node）时，
如果在 Node 对象未被删除并重建的条件下，重新创建了同名的物理主机，
则 Kubernetes 会将新的主机看作是老的主机，这可能会带来某种不一致性。
</div>
<!--
Below are four types of commonly used name constraints for resources.
-->
<p>以下是比较常见的四种资源命名约束。</p>
<!--
### DNS Subdomain Names

Most resource types require a name that can be used as a DNS subdomain name
as defined in [RFC 1123](https://tools.ietf.org/html/rfc1123).
This means the name must:

- contain no more than 253 characters
- contain only lowercase alphanumeric characters, '-' or '.'
- start with an alphanumeric character
- end with an alphanumeric character
-->
<h3 id="dns-subdomain-names">DNS 子域名 </h3>
<p>很多资源类型需要可以用作 DNS 子域名的名称。
DNS 子域名的定义可参见 <a href="https://tools.ietf.org/html/rfc1123">RFC 1123</a>。
这一要求意味着名称必须满足如下规则：</p>
<ul>
<li>不能超过253个字符</li>
<li>只能包含小写字母、数字，以及'-' 和 '.'</li>
<li>须以字母数字开头</li>
<li>须以字母数字结尾</li>
</ul>
<!--
### DNS Label Names

Some resource types require their names to follow the DNS
label standard as defined in [RFC 1123](https://tools.ietf.org/html/rfc1123).
This means the name must:

- contain at most 63 characters
- contain only lowercase alphanumeric characters or '-'
- start with an alphanumeric character
- end with an alphanumeric character
-->
<h3 id="dns-label-names">RFC 1123 标签名   </h3>
<p>某些资源类型需要其名称遵循 <a href="https://tools.ietf.org/html/rfc1123">RFC 1123</a>
所定义的 DNS 标签标准。也就是命名必须满足如下规则：</p>
<ul>
<li>最多 63 个字符</li>
<li>只能包含小写字母、数字，以及 '-'</li>
<li>须以字母数字开头</li>
<li>须以字母数字结尾</li>
</ul>
<!--
### RFC 1035 Label Names

Some resource types require their names to follow the DNS
label standard as defined in [RFC 1035](https://tools.ietf.org/html/rfc1035).
This means the name must:

- contain at most 63 characters
- contain only lowercase alphanumeric characters or '-'
- start with an alphabetic character
- end with an alphanumeric character
-->
<h3 id="rfc-1035-label-names">RFC 1035 标签名  </h3>
<p>某些资源类型需要其名称遵循 <a href="https://tools.ietf.org/html/rfc1035">RFC 1035</a>
所定义的 DNS 标签标准。也就是命名必须满足如下规则：</p>
<ul>
<li>最多 63 个字符</li>
<li>只能包含小写字母、数字，以及 '-'</li>
<li>须以字母开头</li>
<li>须以字母数字结尾</li>
</ul>
<!--
### Path Segment Names

Some resource types require their names to be able to be safely encoded as a
path segment. In other words, the name may not be "." or ".." and the name may
not contain "/" or "%".
-->
<h3 id="path-segment-names">路径分段名称   </h3>
<p>某些资源类型要求名称能被安全地用作路径中的片段。
换句话说，其名称不能是 <code>.</code>、<code>..</code>，也不可以包含 <code>/</code> 或 <code>%</code> 这些字符。</p>
<!--
Here’s an example manifest for a Pod named `nginx-demo`.
-->
<p>下面是一个名为<code>nginx-demo</code>的 Pod 的配置清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div><!--
Some resource types have additional restrictions on their names.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 某些资源类型可能具有额外的命名约束。
</div>
<h2 id="uids">UIDs</h2>
<!--
---
title: UID
id: uid
date: 2018-04-12
full_link: /docs/concepts/overview/working-with-objects/names
short_description: >
  A Kubernetes systems-generated string to uniquely identify objects.

aka: 
tags:
- fundamental
---
-->
<!--
 A Kubernetes systems-generated string to uniquely identify objects.
-->
<p>Kubernetes 系统生成的字符串，唯一标识对象。</p>
<!--
Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.
-->
<p>在 Kubernetes 集群的整个生命周期中创建的每个对象都有一个不同的 uid，它旨在区分类似实体的历史事件。</p>
<!--
Kubernetes UIDs are universally unique identifiers (also known as UUIDs).
UUIDs are standardized as ISO/IEC 9834-8 and as ITU-T X.667.
-->
<p>Kubernetes UIDs 是全局唯一标识符（也叫 UUIDs）。
UUIDs 是标准化的，见 ISO/IEC 9834-8 和 ITU-T X.667.</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about [labels](/docs/concepts/overview/working-with-objects/labels/) in Kubernetes.
* See the [Identifiers and Names in Kubernetes](https://git.k8s.io/community/contributors/design-proposals/architecture/identifiers.md) design document.
-->
<ul>
<li>进一步了解 Kubernetes <a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签</a></li>
<li>参阅 <a href="https://git.k8s.io/community/contributors/design-proposals/architecture/identifiers.md">Kubernetes 标识符和名称</a>的设计文档</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1127165f472b7181b9c1d5a0b187d620">1.4.4 - 名字空间</h1>
    
	<!--
reviewers:
- derekwaynecarr
- mikedanese
- thockin
title: Namespaces
content_type: concept
weight: 30
-->
<!-- overview -->
<!--
In Kubernetes, _namespaces_ provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects _(e.g. Deployments, Services, etc)_ and not for cluster-wide objects _(e.g. StorageClass, Nodes, PersistentVolumes, etc)_.
-->
<p>在 Kubernetes 中，“名字空间（Namespace）”提供一种机制，将同一集群中的资源划分为相互隔离的组。
同一名字空间内的资源名称要唯一，但跨名字空间时没有这个要求。
名字空间作用域仅针对带有名字空间的对象，例如 Deployment、Service 等，
这种作用域对集群访问的对象不适用，例如 StorageClass、Node、PersistentVolume 等。</p>
<!-- body -->
<!--
## When to Use Multiple Namespaces
-->
<h2 id="何时使用多个名字空间">何时使用多个名字空间</h2>
<!--
Namespaces are intended for use in environments with many users spread across multiple
teams, or projects.  For clusters with a few to tens of users, you should not
need to create or think about namespaces at all.  Start using namespaces when you
need the features they provide.
-->
<p>名字空间适用于存在很多跨多个团队或项目的用户的场景。对于只有几到几十个用户的集群，根本不需要创建或考虑名字空间。当需要名称空间提供的功能时，请开始使用它们。</p>
<!--
Namespaces provide a scope for names.  Names of resources need to be unique within a namespace,
but not across namespaces. Namespaces can not be nested inside one another and each Kubernetes
resource can only be in one namespace.
-->
<p>名字空间为名称提供了一个范围。资源的名称需要在名字空间内是唯一的，但不能跨名字空间。
名字空间不能相互嵌套，每个 Kubernetes 资源只能在一个名字空间中。</p>
<!--
Namespaces are a way to divide cluster resources between multiple users (via [resource quota](/docs/concepts/policy/resource-quotas/)).
-->
<p>名字空间是在多个用户之间划分集群资源的一种方法（通过<a href="/zh/docs/concepts/policy/resource-quotas/">资源配额</a>）。</p>
<!--
It is not necessary to use multiple namespaces to separate slightly different
resources, such as different versions of the same software: use
<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='labels'>labels</a> to distinguish
resources within the same namespace.
-->
<p>不必使用多个名字空间来分隔仅仅轻微不同的资源，例如同一软件的不同版本：
应该使用<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>
来区分同一名字空间中的不同资源。</p>
<!--
## Working with Namespaces

Creation and deletion of namespaces are described in the [Admin Guide documentation
for namespaces](/docs/tasks/administer-cluster/namespaces/).
-->
<h2 id="使用名字空间">使用名字空间</h2>
<p>名字空间的创建和删除在<a href="/zh/docs/tasks/administer-cluster/namespaces/">名字空间的管理指南文档</a>描述。</p>
<!--
Avoid creating namespaces with the prefix `kube-`, since it is reserved for Kubernetes system namespaces.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 避免使用前缀 <code>kube-</code> 创建名字空间，因为它是为 Kubernetes 系统名字空间保留的。
</div>
<!--
### Viewing namespaces

You can list the current namespaces in a cluster using:
-->
<h3 id="查看名字空间">查看名字空间</h3>
<p>你可以使用以下命令列出集群中现存的名字空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespace
</code></pre></div><pre><code>NAME          STATUS    AGE
default       Active    1d
kube-node-lease   Active   1d
kube-system   Active    1d
kube-public   Active    1d
</code></pre><!--
Kubernetes starts with four initial namespaces:

   * `default` The default namespace for objects with no other namespace
   * `kube-system` The namespace for objects created by the Kubernetes system
   * `kube-public` This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.
   * `kube-node-lease` This namespace holds [Lease](/docs/reference/kubernetes-api/cluster-resources/lease-v1/)
      objects associated with each node. Node leases allow the kubelet to send
      [heartbeats](/docs/concepts/architecture/nodes/#heartbeats) so that the control plane
      can detect node failure.
-->
<p>Kubernetes 会创建四个初始名字空间：</p>
<ul>
<li><code>default</code> 没有指明使用其它名字空间的对象所使用的默认名字空间</li>
<li><code>kube-system</code> Kubernetes 系统创建对象所使用的名字空间</li>
<li><code>kube-public</code> 这个名字空间是自动创建的，所有用户（包括未经过身份验证的用户）都可以读取它。
这个名字空间主要用于集群使用，以防某些资源在整个集群中应该是可见和可读的。
这个名字空间的公共方面只是一种约定，而不是要求。</li>
<li><code>kube-node-lease</code> 此名字空间用于与各个节点相关的
<a href="/docs/reference/kubernetes-api/cluster-resources/lease-v1/">租约（Lease）</a>对象。
节点租期允许 kubelet 发送<a href="/zh/docs/concepts/architecture/nodes/#heartbeats">心跳</a>，由此控制面能够检测到节点故障。</li>
</ul>
<!--
### Setting the namespace for a request

To set the namespace for a current request, use the `-namespace` flag.

For example:
-->
<h3 id="为请求设置名字空间">为请求设置名字空间</h3>
<p>要为当前请求设置名字空间，请使用 <code>--namespace</code> 参数。</p>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run nginx --image<span style="color:#666">=</span>nginx --namespace<span style="color:#666">=</span>&lt;名字空间名称&gt;
kubectl get pods --namespace<span style="color:#666">=</span>&lt;名字空间名称&gt;
</code></pre></div><!--
### Setting the namespace preference

You can permanently save the namespace for all subsequent kubectl commands in that
context.
-->
<h3 id="设置名字空间偏好">设置名字空间偏好</h3>
<p>你可以永久保存名字空间，以用于对应上下文中所有后续 kubectl 命令。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config set-context --current --namespace<span style="color:#666">=</span>&lt;名字空间名称&gt;
<span style="color:#080;font-style:italic"># 验证之</span>
kubectl config view | grep namespace:
</code></pre></div><!--
## Namespaces and DNS

When you create a [Service](/docs/user-guide/services), it creates a corresponding [DNS entry](/docs/concepts/services-networking/dns-pod-service/).
-->
<h2 id="名字空间和-dns">名字空间和 DNS</h2>
<p>当你创建一个<a href="/zh/docs/concepts/services-networking/service/">服务</a> 时，
Kubernetes 会创建一个相应的 <a href="/zh/docs/concepts/services-networking/dns-pod-service/">DNS 条目</a>。</p>
<!--
This entry is of the form `<service-name>.<namespace-name>.svc.cluster.local`, which means
that if a container only uses `<service-name>`, it will resolve to the service which
is local to a namespace.  This is useful for using the same configuration across
multiple namespaces such as Development, Staging and Production.  If you want to reach
across namespaces, you need to use the fully qualified domain name (FQDN).
-->
<p>该条目的形式是 <code>&lt;服务名称&gt;.&lt;名字空间名称&gt;.svc.cluster.local</code>，这意味着如果容器只使用
<code>&lt;服务名称&gt;</code>，它将被解析到本地名字空间的服务。这对于跨多个名字空间（如开发、分级和生产）
使用相同的配置非常有用。如果你希望跨名字空间访问，则需要使用完全限定域名（FQDN）。</p>
<!--
As a result, all namespace names must be valid
[RFC 1123 DNS labels](/docs/concepts/overview/working-with-objects/names/#dns-label-names).
-->
<p>因此，所有的名字空间名称都必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names/#dns-label-names">RFC 1123 DNS 标签</a>。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
By creating namespaces with the same name as [public top-level
domains](https://data.iana.org/TLD/tlds-alpha-by-domain.txt), Services in these
namespaces can have short DNS names that overlap with public DNS records.
Workloads from any namespace performing a DNS lookup without a [trailing dot](https://datatracker.ietf.org/doc/html/rfc1034#page-8) will
be redirected to those services, taking precedence over public DNS.
-->
<p>通过创建与<a href="https://data.iana.org/TLD/tlds-alpha-by-domain.txt">公共顶级域名</a>
同名的名字空间，这些名字空间中的服务可以拥有与公共 DNS 记录重叠的、较短的 DNS 名称。
所有名字空间中的负载在执行 DNS 查找时，如果查找的名称没有
<a href="https://datatracker.ietf.org/doc/html/rfc1034#page-8">尾部句点</a>，
就会被重定向到这些服务上，因此呈现出比公共 DNS 更高的优先序。</p>
<!--
To mitigate this, limit privileges for creating namespaces to trusted users. If
required, you could additionally configure third-party security controls, such
as [admission
webhooks](/docs/reference/access-authn-authz/extensible-admission-controllers/),
to block creating any namespace with the name of [public
TLDs](https://data.iana.org/TLD/tlds-alpha-by-domain.txt).
-->
<p>为了缓解这类问题，需要将创建名字空间的权限授予可信的用户。
如果需要，你可以额外部署第三方的安全控制机制，例如以
<a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/">准入 Webhook</a>
的形式，阻止用户创建与公共 <a href="https://data.iana.org/TLD/tlds-alpha-by-domain.txt">TLD</a>
同名的名字空间。</p>

</div>


<!--
## Not All Objects are in a Namespace
-->
<h2 id="并非所有对象都在名字空间中">并非所有对象都在名字空间中</h2>
<!--
Most Kubernetes resources (e.g. pods, services, replication controllers, and others) are
in some namespaces.  However namespace resources are not themselves in a namespace.
And low-level resources, such as [nodes](/docs/concepts/architecture/nodes/) and
persistentVolumes, are not in any namespace.
-->
<p>大多数 kubernetes 资源（例如 Pod、Service、副本控制器等）都位于某些名字空间中。
但是名字空间资源本身并不在名字空间中。而且底层资源，例如
<a href="/zh/docs/concepts/architecture/nodes/">节点</a> 和持久化卷不属于任何名字空间。</p>
<!--
To see which Kubernetes resources are and aren't in a namespace:
-->
<p>查看哪些 Kubernetes 资源在名字空间中，哪些不在名字空间中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 位于名字空间中的资源</span>
kubectl api-resources --namespaced<span style="color:#666">=</span><span style="color:#a2f">true</span>

<span style="color:#080;font-style:italic"># 不在名字空间中的资源</span>
kubectl api-resources --namespaced<span style="color:#666">=</span><span style="color:#a2f">false</span>
</code></pre></div><!--
## Automatic labelling
-->
<h2 id="automatic-labelling">自动打标签  </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.21 [beta]</code>
</div>


<!--
The Kubernetes control plane sets an immutable <a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='label'>label</a>
`kubernetes.io/metadata.name` on all namespaces, provided that the `NamespaceDefaultLabelName`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.
The value of the label is the namespace name.
-->
<p>Kubernetes 控制面会为所有名字空间设置一个不可变更的
<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>
<code>kubernetes.io/metadata.name</code>，只要 <code>NamespaceDefaultLabelName</code> 这一
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
被启用。标签的值是名字空间的名称。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [creating a new namespace](/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace).
* Learn more about [deleting a namespace](/docs/tasks/administer-cluster/namespaces/#deleting-a-namespace).
-->
<ul>
<li>进一步了解<a href="/zh/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace">建立新的名字空间</a>。</li>
<li>进一步了解<a href="/zh/docs/tasks/administer-cluster/namespaces/#deleting-a-namespace">删除名字空间</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f1dec4557fb8ffbac9f11390aaaf9fa4">1.4.5 - 标签和选择算符</h1>
    
	<!--
reviewers:
- mikedanese
title: Labels and Selectors
content_type: concept
weight: 40
-->
<!-- overview -->
<!--
_Labels_ are key/value pairs that are attached to objects, such as pods.
Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system.
Labels can be used to organize and to select subsets of objects.
Labels can be attached to objects at creation time and subsequently added and modified at any time.
Each object can have a set of key/value labels defined.  Each Key must be unique for a given object.
-->
<p><em>标签（Labels）</em> 是附加到 Kubernetes 对象（比如 Pods）上的键值对。
标签旨在用于指定对用户有意义且相关的对象的标识属性，但不直接对核心系统有语义含义。
标签可以用于组织和选择对象的子集。标签可以在创建时附加到对象，随后可以随时添加和修改。
每个对象都可以定义一组键/值标签。每个键对于给定对象必须是唯一的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#b44">&#34;metadata&#34;</span><span style="">:</span> {
  <span style="color:#008000;font-weight:bold">&#34;labels&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;key1&#34;</span> : <span style="color:#b44">&#34;value1&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;key2&#34;</span> : <span style="color:#b44">&#34;value2&#34;</span>
  }
}
</code></pre></div><!--
Labels allow for efficient queries and watches and are ideal for use in UIs
and CLIs. Non-identifying information should be recorded using
[annotations](/docs/concepts/overview/working-with-objects/annotations/).
-->
<p>标签能够支持高效的查询和监听操作，对于用户界面和命令行是很理想的。
应使用<a href="/zh/docs/concepts/overview/working-with-objects/annotations/">注解</a> 记录非识别信息。</p>
<!-- body -->
<!--
## Motivation

Labels enable users to map their own organizational structures onto system objects in a loosely coupled fashion, without requiring clients to store these mappings.
-->
<h2 id="动机">动机</h2>
<p>标签使用户能够以松散耦合的方式将他们自己的组织结构映射到系统对象，而无需客户端存储这些映射。</p>
<!--
Service deployments and batch processing pipelines are often multi-dimensional entities (e.g., multiple partitions or deployments, multiple release tracks, multiple tiers, multiple micro-services per tier). Management often requires cross-cutting operations, which breaks encapsulation of strictly hierarchical representations, especially rigid hierarchies determined by the infrastructure rather than by users.

Example labels:
-->
<p>服务部署和批处理流水线通常是多维实体（例如，多个分区或部署、多个发行序列、多个层，每层多个微服务）。
管理通常需要交叉操作，这打破了严格的层次表示的封装，特别是由基础设施而不是用户确定的严格的层次结构。</p>
<p>示例标签：</p>
<ul>
<li><code>&quot;release&quot; : &quot;stable&quot;</code>, <code>&quot;release&quot; : &quot;canary&quot;</code></li>
<li><code>&quot;environment&quot; : &quot;dev&quot;</code>, <code>&quot;environment&quot; : &quot;qa&quot;</code>, <code>&quot;environment&quot; : &quot;production&quot;</code></li>
<li><code>&quot;tier&quot; : &quot;frontend&quot;</code>, <code>&quot;tier&quot; : &quot;backend&quot;</code>, <code>&quot;tier&quot; : &quot;cache&quot;</code></li>
<li><code>&quot;partition&quot; : &quot;customerA&quot;</code>, <code>&quot;partition&quot; : &quot;customerB&quot;</code></li>
<li><code>&quot;track&quot; : &quot;daily&quot;</code>, <code>&quot;track&quot; : &quot;weekly&quot;</code></li>
</ul>
<!--
These are examples of [commonly used labels](/docs/concepts/overview/working-with-objects/common-labels/); you are free to develop your own conventions. Keep in mind that label Key must be unique for a given object.
-->
<p>有一些<a href="/zh/docs/concepts/overview/working-with-objects/common-labels/">常用标签</a>的例子; 你可以任意制定自己的约定。
请记住，标签的 Key 对于给定对象必须是唯一的。</p>
<!--
## Syntax and character set

_Labels_ are key/value pairs. Valid label keys have two segments: an optional prefix and name, separated by a slash (`/`).  The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character (`[a-z0-9A-Z]`) with dashes (`-`), underscores (`_`), dots (`.`), and alphanumerics between.  The prefix is optional.  If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (`.`), not longer than 253 characters in total, followed by a slash (`/`).

If the prefix is omitted, the label Key is presumed to be private to the user. Automated system components (e.g. `kube-scheduler`, `kube-controller-manager`, `kube-apiserver`, `kubectl`, or other third-party automation) which add labels to end-user objects must specify a prefix.

The `kubernetes.io/` and `k8s.io/` prefixes are [reserved](/docs/reference/labels-annotations-taints/) for Kubernetes core components.
-->
<h2 id="语法和字符集">语法和字符集</h2>
<p><em>标签</em> 是键值对。有效的标签键有两个段：可选的前缀和名称，用斜杠（<code>/</code>）分隔。
名称段是必需的，必须小于等于 63 个字符，以字母数字字符（<code>[a-z0-9A-Z]</code>）开头和结尾，
带有破折号（<code>-</code>），下划线（<code>_</code>），点（ <code>.</code>）和之间的字母数字。
前缀是可选的。如果指定，前缀必须是 DNS 子域：由点（<code>.</code>）分隔的一系列 DNS 标签，总共不超过 253 个字符，
后跟斜杠（<code>/</code>）。</p>
<p>如果省略前缀，则假定标签键对用户是私有的。
向最终用户对象添加标签的自动系统组件（例如 <code>kube-scheduler</code>、<code>kube-controller-manager</code>、
<code>kube-apiserver</code>、<code>kubectl</code> 或其他第三方自动化工具）必须指定前缀。</p>
<p><code>kubernetes.io/</code> 和 <code>k8s.io/</code> 前缀是为 Kubernetes 核心组件<a href="/zh/docs/reference/labels-annotations-taints/">保留的</a>。</p>
<!--
Valid label value:

* must be 63 characters or less (can be empty),
* unless empty, must begin and end with an alphanumeric character (`[a-z0-9A-Z]`),
* could contain dashes (`-`), underscores (`_`), dots (`.`), and alphanumerics between.
-->
<p>有效标签值：</p>
<ul>
<li>必须为 63 个字符或更少（可以为空）</li>
<li>除非标签值为空，必须以字母数字字符（<code>[a-z0-9A-Z]</code>）开头和结尾</li>
<li>包含破折号（<code>-</code>）、下划线（<code>_</code>）、点（<code>.</code>）和字母或数字。</li>
</ul>
<!--
## Label selectors

Unlike [names and UIDs](/docs/user-guide/identifiers), labels do not provide uniqueness. In general, we expect many objects to carry the same label(s).
-->
<h2 id="label-selectors">标签选择算符  </h2>
<p>与<a href="/zh/docs/concepts/overview/working-with-objects/names/">名称和 UID</a> 不同，
标签不支持唯一性。通常，我们希望许多对象携带相同的标签。</p>
<!--
Via a _label selector_, the client/user can identify a set of objects. The label selector is the core grouping primitive in Kubernetes.
-->
<p>通过 <em>标签选择算符</em>，客户端/用户可以识别一组对象。标签选择算符是 Kubernetes 中的核心分组原语。</p>
<!--
The API currently supports two types of selectors: _equality-based_ and _set-based_.
A label selector can be made of multiple _requirements_ which are comma-separated. In the case of multiple requirements, all must be satisfied so the comma separator acts as a logical _AND_ (`&&`) operator.
-->
<p>API 目前支持两种类型的选择算符：<em>基于等值的</em> 和 <em>基于集合的</em>。
标签选择算符可以由逗号分隔的多个 <em>需求</em> 组成。
在多个需求的情况下，必须满足所有要求，因此逗号分隔符充当逻辑 <em>与</em>（<code>&amp;&amp;</code>）运算符。</p>
<!--
The semantics of empty or non-specified selectors are dependent on the context,
and API types that use selectors should document the validity and meaning of
them.
-->
<p>空标签选择算符或者未指定的选择算符的语义取决于上下文，
支持使用选择算符的 API 类别应该将算符的合法性和含义用文档记录下来。</p>
<!--
For some API types, such as ReplicaSets, the label selectors of two instances must not overlap within a namespace, or the controller can see that as conflicting instructions and fail to determine how many replicas should be present.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 对于某些 API 类别（例如 ReplicaSet）而言，两个实例的标签选择算符不得在命名空间内重叠，
否则它们的控制器将互相冲突，无法确定应该存在的副本个数。
</div>
<!--
For both equality-based and set-based conditions there is no logical _OR_ (`||`) operator. Ensure your filter statements are structured accordingly.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 对于基于等值的和基于集合的条件而言，不存在逻辑或（<code>||</code>）操作符。
你要确保你的过滤语句按合适的方式组织。
</div>

<!--
### _Equality-based_ requirement

_Equality-_ or _inequality-based_ requirements allow filtering by label keys and values. Matching objects must satisfy all of the specified label constraints, though they may have additional labels as well.
Three kinds of operators are admitted `=`,`==`,`!=`. The first two represent _equality_ (and are simply synonyms), while the latter represents _inequality_. For example:
-->
<h3 id="基于等值的-需求"><em>基于等值的</em> 需求</h3>
<p><em>基于等值</em> 或 <em>基于不等值</em> 的需求允许按标签键和值进行过滤。
匹配对象必须满足所有指定的标签约束，尽管它们也可能具有其他标签。
可接受的运算符有<code>=</code>、<code>==</code> 和 <code>!=</code> 三种。
前两个表示 <em>相等</em>（并且只是同义词），而后者表示 <em>不相等</em>。例如：</p>
<pre><code>environment = production
tier != frontend
</code></pre><!--
The former selects all resources with key equal to `environment` and value equal to `production`.
The latter selects all resources with key equal to `tier` and value distinct from `frontend`, and all resources with no labels with the `tier` key.
One could filter for resources in `production` excluding `frontend` using the comma operator: `environment=production,tier!=frontend`
-->
<p>前者选择所有资源，其键名等于 <code>environment</code>，值等于 <code>production</code>。
后者选择所有资源，其键名等于 <code>tier</code>，值不同于 <code>frontend</code>，所有资源都没有带有 <code>tier</code> 键的标签。
可以使用逗号运算符来过滤 <code>production</code> 环境中的非 <code>frontend</code> 层资源：<code>environment=production,tier!=frontend</code>。</p>
<!--
One usage scenario for equality-based label requirement is for Pods to specify
node selection criteria. For example, the sample Pod below selects nodes with
the label "`accelerator=nvidia-tesla-p100`".
-->
<p>基于等值的标签要求的一种使用场景是 Pod 要指定节点选择标准。
例如，下面的示例 Pod 选择带有标签 &quot;<code>accelerator=nvidia-tesla-p100</code>&quot;。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cuda-test<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cuda-test<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;k8s.gcr.io/cuda-vector-add:v0.1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">nvidia.com/gpu</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">accelerator</span>:<span style="color:#bbb"> </span>nvidia-tesla-p100<span style="color:#bbb">
</span></code></pre></div><!--
### _Set-based_ requirement

_Set-based_ label requirements allow filtering keys according to a set of values. Three kinds of operators are supported: `in`,`notin` and `exists` (only the key identifier). For example:
-->
<h3 id="基于集合-的需求"><em>基于集合</em> 的需求</h3>
<p><em>基于集合</em> 的标签需求允许你通过一组值来过滤键。
支持三种操作符：<code>in</code>、<code>notin</code> 和 <code>exists</code> (只可以用在键标识符上)。例如：</p>
<pre><code>environment in (production, qa)
tier notin (frontend, backend)
partition
!partition
</code></pre><!--
* The first example selects all resources with key equal to `environment` and value equal to `production` or `qa`.
* The second example selects all resources with key equal to `tier` and values other than `frontend` and `backend`, and all resources with no labels with the `tier` key.
* The third example selects all resources including a label with key `partition`; no values are checked.
* The fourth example selects all resources without a label with key `partition`; no values are checked.

Similarly the comma separator acts as an _AND_ operator. So filtering resources with a `partition` key (no matter the value) and with `environment` different than  `qa` can be achieved using `partition,environment notin (qa)`.
-->
<ul>
<li>第一个示例选择了所有键等于 <code>environment</code> 并且值等于 <code>production</code> 或者 <code>qa</code> 的资源。</li>
<li>第二个示例选择了所有键等于 <code>tier</code> 并且值不等于 <code>frontend</code> 或者 <code>backend</code> 的资源，以及所有没有 <code>tier</code> 键标签的资源。</li>
<li>第三个示例选择了所有包含了有 <code>partition</code> 标签的资源；没有校验它的值。</li>
<li>第四个示例选择了所有没有 <code>partition</code> 标签的资源；没有校验它的值。</li>
</ul>
<p>类似地，逗号分隔符充当 <em>与</em> 运算符。因此，使用 <code>partition</code> 键（无论为何值）和
<code>environment</code> 不同于 <code>qa</code> 来过滤资源可以使用 <code>partition, environment notin（qa)</code> 来实现。</p>
<!--
The _set-based_ label selector is a general form of equality since `environment=production` is equivalent to `environment in (production)`; similarly for `!=` and `notin`.
-->
<p><em>基于集合</em> 的标签选择算符是相等标签选择算符的一般形式，因为 <code>environment=production</code>
等同于 <code>environment in（production）</code>；<code>!=</code> 和 <code>notin</code> 也是类似的。</p>
<!--
_Set-based_ requirements can be mixed with _equality-based_ requirements. For example: `partition in (customerA, customerB),environment!=qa`.
-->
<p><em>基于集合</em> 的要求可以与基于 <em>相等</em> 的要求混合使用。例如：<code>partition in (customerA, customerB),environment!=qa</code>。</p>
<h2 id="api">API</h2>
<!--
### LIST and WATCH filtering

LIST and WATCH operations may specify label selectors to filter the sets of objects returned using a query parameter. Both requirements are permitted (presented here as they would appear in a URL query string):
-->
<h3 id="list-和-watch-过滤">LIST 和 WATCH 过滤</h3>
<p>LIST 和 WATCH 操作可以使用查询参数指定标签选择算符过滤一组对象。
两种需求都是允许的。（这里显示的是它们出现在 URL 查询字符串中）</p>
<!--
* _equality-based_ requirements: `?labelSelector=environment%3Dproduction,tier%3Dfrontend`
* _set-based_ requirements: `?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29`
-->
<ul>
<li><em>基于等值</em> 的需求: <code>?labelSelector=environment%3Dproduction,tier%3Dfrontend</code></li>
<li><em>基于集合</em> 的需求: <code>?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29</code></li>
</ul>
<!--
Both label selector styles can be used to list or watch resources via a REST client. For example, targeting `apiserver` with `kubectl` and using _equality-based_ one may write:
-->
<p>两种标签选择算符都可以通过 REST 客户端用于 list 或者 watch 资源。
例如，使用 <code>kubectl</code> 定位 <code>apiserver</code>，可以使用 <em>基于等值</em> 的标签选择算符可以这么写：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">environment</span><span style="color:#666">=</span>production,tier<span style="color:#666">=</span>frontend
</code></pre></div><!-- or using _set-based_ requirements: -->
<p>或者使用 <em>基于集合的</em> 需求：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b44">&#39;environment in (production),tier in (frontend)&#39;</span>
</code></pre></div><!--
As already mentioned _set-based_ requirements are more expressive.  For instance, they can implement the _OR_ operator on values:
-->
<p>正如刚才提到的，<em>基于集合</em> 的需求更具有表达力。例如，它们可以实现值的 <em>或</em> 操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b44">&#39;environment in (production, qa)&#39;</span>
</code></pre></div><!-- or restricting negative matching via _exists_ operator: -->
<p>或者通过 <em>exists</em> 运算符限制不匹配：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b44">&#39;environment,environment notin (frontend)&#39;</span>
</code></pre></div><!--
### Set references in API objects

Some Kubernetes objects, such as [`services`](/docs/concepts/services-networking/service/)
and [`replicationcontrollers`](/docs/concepts/workloads/controllers/replicationcontroller/),
also use label selectors to specify sets of other resources, such as
[pods](/docs/concepts/workloads/pods/).
-->
<h3 id="在-api-对象中设置引用">在 API 对象中设置引用</h3>
<p>一些 Kubernetes 对象，例如 <a href="/zh/docs/concepts/services-networking/service/"><code>services</code></a>
和 <a href="/zh/docs/concepts/workloads/controllers/replicationcontroller/"><code>replicationcontrollers</code></a> ，
也使用了标签选择算符去指定了其他资源的集合，例如
<a href="/zh/docs/concepts/workloads/pods/">pods</a>。</p>
<!--
#### Service and ReplicationController

The set of pods that a `service` targets is defined with a label selector. Similarly, the population of pods that a `replicationcontroller` should manage is also defined with a label selector.

Labels selectors for both objects are defined in `json` or `yaml` files using maps, and only _equality-based_ requirement selectors are supported:
-->
<h4 id="service-和-replicationcontroller">Service 和 ReplicationController</h4>
<p>一个 <code>Service</code> 指向的一组 Pods 是由标签选择算符定义的。同样，一个 <code>ReplicationController</code>
应该管理的 pods 的数量也是由标签选择算符定义的。</p>
<p>两个对象的标签选择算符都是在 <code>json</code> 或者 <code>yaml</code> 文件中使用映射定义的，并且只支持
<em>基于等值</em> 需求的选择算符：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#b44">&#34;selector&#34;</span><span style="">:</span> {
    <span style="color:#008000;font-weight:bold">&#34;component&#34;</span> : <span style="color:#b44">&#34;redis&#34;</span>,
}
</code></pre></div><!-- or -->
<p>或者</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span></code></pre></div><!---
this selector (respectively in `json` or `yaml` format) is equivalent to `component=redis` or `component in (redis)`.
-->
<p>这个选择算符(分别在 <code>json</code> 或者 <code>yaml</code> 格式中) 等价于 <code>component=redis</code> 或 <code>component in (redis)</code> 。</p>
<!--
#### Resources that support set-based requirements

Newer resources, such as [`Job`](/docs/concepts/jobs/run-to-completion-finite-workloads/), [`Deployment`](/docs/concepts/workloads/controllers/deployment/), [`Replica Set`](/docs/concepts/workloads/controllers/replicaset/), and [`Daemon Set`](/docs/concepts/workloads/controllers/daemonset/), support _set-based_ requirements as well.
-->
<h4 id="支持基于集合需求的资源">支持基于集合需求的资源</h4>
<p>比较新的资源，例如 <a href="/zh/docs/concepts/workloads/controllers/job/"><code>Job</code></a>、
<a href="/zh/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a>、
<a href="/zh/docs/concepts/workloads/controllers/replicaset/"><code>Replica Set</code></a> 和
<a href="/zh/docs/concepts/workloads/controllers/daemonset/"><code>DaemonSet</code></a> ，
也支持 <em>基于集合的</em> 需求。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- {<span style="color:#008000;font-weight:bold">key: tier, operator: In, values</span>:<span style="color:#bbb"> </span>[cache]}<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- {<span style="color:#008000;font-weight:bold">key: environment, operator: NotIn, values</span>:<span style="color:#bbb"> </span>[dev]}<span style="color:#bbb">
</span></code></pre></div><!--
`matchLabels` is a map of `{key,value}` pairs. A single `{key,value}` in the `matchLabels` map is equivalent to an element of `matchExpressions`, whose `key` field is "key", the `operator` is "In", and the `values` array contains only "value". `matchExpressions` is a list of pod selector requirements. Valid operators include In, NotIn, Exists, and DoesNotExist. The values set must be non-empty in the case of In and NotIn. All of the requirements, from both `matchLabels` and `matchExpressions` are ANDed together - they must all be satisfied in order to match.
-->
<p><code>matchLabels</code> 是由 <code>{key,value}</code> 对组成的映射。
<code>matchLabels</code> 映射中的单个 <code>{key,value }</code> 等同于 <code>matchExpressions</code> 的元素，
其 <code>key</code> 字段为 &quot;key&quot;，<code>operator</code> 为 &quot;In&quot;，而 <code>values</code> 数组仅包含 &quot;value&quot;。
<code>matchExpressions</code> 是 Pod 选择算符需求的列表。
有效的运算符包括 <code>In</code>、<code>NotIn</code>、<code>Exists</code> 和 <code>DoesNotExist</code>。
在 <code>In</code> 和 <code>NotIn</code> 的情况下，设置的值必须是非空的。
来自 <code>matchLabels</code> 和 <code>matchExpressions</code> 的所有要求都按逻辑与的关系组合到一起
-- 它们必须都满足才能匹配。</p>
<!--
#### Selecting sets of nodes

One use case for selecting over labels is to constrain the set of nodes onto which a pod can schedule.
See the documentation on [node selection](/docs/concepts/configuration/assign-pod-node/) for more information.
-->
<h4 id="选择节点集">选择节点集</h4>
<p>通过标签进行选择的一个用例是确定节点集，方便 Pod 调度。
有关更多信息，请参阅<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">选择节点</a>文档。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-93cd7a1d4e1623e2bf01afc49a5af69c">1.4.6 - 注解</h1>
    
	<!--
title: Annotations
content_type: concept
weight: 50
-->
<!-- overview -->
<!--
You can use Kubernetes annotations to attach arbitrary non-identifying metadata
to objects. Clients such as tools and libraries can retrieve this metadata.
-->
<p>你可以使用 Kubernetes 注解为对象附加任意的非标识的元数据。客户端程序（例如工具和库）能够获取这些元数据信息。</p>
<!-- body -->
<!--
## Attaching metadata to objects

You can use either labels or annotations to attach metadata to Kubernetes
objects. Labels can be used to select objects and to find
collections of objects that satisfy certain conditions. In contrast, annotations
are not used to identify and select objects. The metadata
in an annotation can be small or large, structured or unstructured, and can
include characters not permitted by labels.

Annotations, like labels, are key/value maps:
-->
<h2 id="为对象附加元数据">为对象附加元数据</h2>
<p>你可以使用标签或注解将元数据附加到 Kubernetes 对象。
标签可以用来选择对象和查找满足某些条件的对象集合。 相反，注解不用于标识和选择对象。
注解中的元数据，可以很小，也可以很大，可以是结构化的，也可以是非结构化的，能够包含标签不允许的字符。</p>
<p>注解和标签一样，是键/值对:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#b44">&#34;metadata&#34;</span><span style="">:</span> {
  <span style="color:#008000;font-weight:bold">&#34;annotations&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;key1&#34;</span> : <span style="color:#b44">&#34;value1&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;key2&#34;</span> : <span style="color:#b44">&#34;value2&#34;</span>
  }
}
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The keys and the values in the map must be strings. In other words, you cannot use
numeric, boolean, list or other types for either the keys or the values.
-->
<p>Map 中的键和值必须是字符串。
换句话说，你不能使用数字、布尔值、列表或其他类型的键或值。
</div>
<!--
Here are some examples of information that could be recorded in annotations:
-->
<p>以下是一些例子，用来说明哪些信息可以使用注解来记录:</p>
<!--
* Fields managed by a declarative configuration layer. Attaching these fields
  as annotations distinguishes them from default values set by clients or
  servers, and from auto-generated fields and fields set by
  auto-sizing or auto-scaling systems.

* Build, release, or image information like timestamps, release IDs, git branch,
  PR numbers, image hashes, and registry address.

* Pointers to logging, monitoring, analytics, or audit repositories.
-->
<ul>
<li>由声明性配置所管理的字段。
将这些字段附加为注解，能够将它们与客户端或服务端设置的默认值、
自动生成的字段以及通过自动调整大小或自动伸缩系统设置的字段区分开来。</li>
<li>构建、发布或镜像信息（如时间戳、发布 ID、Git 分支、PR 数量、镜像哈希、仓库地址）。</li>
<li>指向日志记录、监控、分析或审计仓库的指针。</li>
</ul>
<!--
* Client library or tool information that can be used for debugging purposes:
  for example, name, version, and build information.

* User or tool/system provenance information, such as URLs of related objects
  from other ecosystem components.

* Lightweight rollout tool metadata: for example, config or checkpoints.

* Phone or pager numbers of persons responsible, or directory entries that
  specify where that information can be found, such as a team web site.

* Directives from the end-user to the implementations to modify behavior or
  engage non-standard features.
-->
<ul>
<li>
<p>可用于调试目的的客户端库或工具信息：例如，名称、版本和构建信息。</p>
</li>
<li>
<p>用户或者工具/系统的来源信息，例如来自其他生态系统组件的相关对象的 URL。</p>
</li>
<li>
<p>轻量级上线工具的元数据信息：例如，配置或检查点。</p>
</li>
<li>
<p>负责人员的电话或呼机号码，或指定在何处可以找到该信息的目录条目，如团队网站。</p>
</li>
<li>
<p>从用户到最终运行的指令，以修改行为或使用非标准功能。</p>
</li>
</ul>
<!--
Instead of using annotations, you could store this type of information in an
external database or directory, but that would make it much harder to produce
shared client libraries and tools for deployment, management, introspection,
and the like.
-->
<p>你可以将这类信息存储在外部数据库或目录中而不使用注解，
但这样做就使得开发人员很难生成用于部署、管理、自检的客户端共享库和工具。</p>
<!--
## Syntax and character set

_Annotations_ are key/value pairs. Valid annotation keys have two segments: an optional prefix and name, separated by a slash (`/`). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character (`[a-z0-9A-Z]`) with dashes (`-`), underscores (`_`), dots (`.`), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (`.`), not longer than 253 characters in total, followed by a slash (`/`).

If the prefix is omitted, the annotation Key is presumed to be private to the user. Automated system components (e.g. `kube-scheduler`, `kube-controller-manager`, `kube-apiserver`, `kubectl`, or other third-party automation) which add annotations to end-user objects must specify a prefix.
-->
<h2 id="语法和字符集">语法和字符集</h2>
<p><em>注解（Annotations）</em> 存储的形式是键/值对。有效的注解键分为两部分：
可选的前缀和名称，以斜杠（<code>/</code>）分隔。
名称段是必需项，并且必须在63个字符以内，以字母数字字符（<code>[a-z0-9A-Z]</code>）开头和结尾，
并允许使用破折号（<code>-</code>），下划线（<code>_</code>），点（<code>.</code>）和字母数字。
前缀是可选的。如果指定，则前缀必须是DNS子域：一系列由点（<code>.</code>）分隔的DNS标签，
总计不超过253个字符，后跟斜杠（<code>/</code>）。
如果省略前缀，则假定注解键对用户是私有的。 由系统组件添加的注解
（例如，<code>kube-scheduler</code>，<code>kube-controller-manager</code>，<code>kube-apiserver</code>，<code>kubectl</code>
或其他第三方组件），必须为终端用户添加注解前缀。</p>
<!--
The `kubernetes.io/` and `k8s.io/` prefixes are reserved for Kubernetes core components.

For example, here's the configuration file for a Pod that has the annotation `imageregistry: https://hub.docker.com/` :
-->
<p><code>kubernetes.io/</code> 和 <code>k8s.io/</code> 前缀是为Kubernetes核心组件保留的。</p>
<p>例如，下面是一个 Pod 的配置文件，其注解中包含 <code>imageregistry: https://hub.docker.com/</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>annotations-demo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imageregistry</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;https://hub.docker.com/&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.7.9<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [Labels and Selectors](/docs/concepts/overview/working-with-objects/labels/).
-->
<ul>
<li>进一步了解<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签和选择算符</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-13ce5627ef1dc8cbb4530ed231cb7d38">1.4.7 - Finalizers</h1>
    
	<!-- overview -->
<!--
---
title: Finalizer
id: finalizer
date: 2021-07-07
full_link: /zh/docs/concepts/overview/working-with-objects/finalizers/
short_description: >
  A namespaced key that tells Kubernetes to wait until specific conditions are met
  before it fully deletes an object marked for deletion.
aka: 
tags:
- fundamental
- operation
-->
<!--
Finalizers are namespaced keys that tell Kubernetes to wait until specific
conditions are met before it fully deletes resources marked for deletion.
Finalizers alert <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controllers'>controllers</a>
to clean up resources the deleted object owned.
-->
<p>Finalizer 是带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后，
再完全删除被标记为删除的资源。
Finalizer 提醒<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>清理被删除的对象拥有的资源。</p>
<!--
When you tell Kubernetes to delete an object that has finalizers specified for
it, the Kubernetes API marks the object for deletion by populating `.metadata.deletionTimestamp`,
and returns a `202` status code (HTTP "Accepted"). The target object remains in a terminating state while the
control plane, or other components, take the actions defined by the finalizers.
After these actions are complete, the controller removes the relevant finalizers
from the target object. When the `metadata.finalizers` field is empty,
Kubernetes considers the deletion complete.
-->
<p>当你告诉 Kubernetes 删除一个指定了 Finalizer 的对象时，
Kubernetes API 通过填充 <code>.metadata.deletionTimestamp</code> 来标记要删除的对象，
并返回<code>202</code>状态码 (HTTP &quot;已接受&quot;) 使其进入只读状态。
此时控制平面或其他组件会采取 Finalizer 所定义的行动，
而目标对象仍然处于终止中（Terminating）的状态。
这些行动完成后，控制器会删除目标对象相关的 Finalizer。
当 <code>metadata.finalizers</code> 字段为空时，Kubernetes 认为删除已完成。</p>
<!--
You can use finalizers to control <a class='glossary-tooltip' title='Kubernetes 用于清理集群资源的各种机制的统称。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/garbage-collection/' target='_blank' aria-label='garbage collection'>garbage collection</a>
of resources. For example, you can define a finalizer to clean up related resources or
infrastructure before the controller deletes the target resource.
-->
<p>你可以使用 Finalizer 控制资源的<a class='glossary-tooltip' title='Kubernetes 用于清理集群资源的各种机制的统称。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/garbage-collection/' target='_blank' aria-label='垃圾收集'>垃圾收集</a>。
例如，你可以定义一个 Finalizer，在删除目标资源前清理相关资源或基础设施。</p>
<!--
You can use finalizers to control <a class='glossary-tooltip' title='Kubernetes 用于清理集群资源的各种机制的统称。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/garbage-collection/' target='_blank' aria-label='garbage collection'>garbage collection</a>
of resources by alerting <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controllers'>controllers</a> to perform specific cleanup tasks before
deleting the target resource. 
-->
<p>你可以通过使用 Finalizers 提醒<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>
在删除目标资源前执行特定的清理任务，
来控制资源的<a class='glossary-tooltip' title='Kubernetes 用于清理集群资源的各种机制的统称。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/garbage-collection/' target='_blank' aria-label='垃圾收集'>垃圾收集</a>。</p>
<!--
Finalizers don't usually specify the code to execute. Instead, they are
typically lists of keys on a specific resource similar to annotations.
Kubernetes specifies some finalizers automatically, but you can also specify
your own.
-->
<p>Finalizers 通常不指定要执行的代码。
相反，它们通常是特定资源上的键的列表，类似于注解。
Kubernetes 自动指定了一些 Finalizers，但你也可以指定你自己的。</p>
<!--
## How finalizers work

When you create a resource using a manifest file, you can specify finalizers in
the `metadata.finalizers` field. When you attempt to delete the resource, the
API server handling the delete request notices the values in the `finalizers` field
and does the following: 

  * Modifies the object to add a `metadata.deletionTimestamp` field with the
    time you started the deletion.
  * Prevents the object from being removed until its `metadata.finalizers` field is empty.
  * Returns a `202` status code (HTTP "Accepted")
-->
<h2 id="how-finalizers-work">Finalizers 如何工作  </h2>
<p>当你使用清单文件创建资源时，你可以在 <code>metadata.finalizers</code> 字段指定 Finalizers。
当你试图删除该资源时，处理删除请求的 API 服务器会注意到 <code>finalizers</code> 字段中的值，
并进行以下操作：</p>
<ul>
<li>修改对象，将你开始执行删除的时间添加到 <code>metadata.deletionTimestamp</code> 字段。</li>
<li>禁止对象被删除，直到其 <code>metadata.finalizers</code> 字段为空。</li>
<li>返回 <code>202</code> 状态码（HTTP &quot;Accepted&quot;）。</li>
</ul>
<!--
The controller managing that finalizer notices the update to the object setting the
`metadata.deletionTimestamp`, indicating deletion of the object has been requested.
The controller then attempts to satisfy the requirements of the finalizers
specified for that resource. Each time a finalizer condition is satisfied, the
controller removes that key from the resource's `finalizers` field. When the
`finalizers` field is emptied, an object with a `deletionTimestamp` field set
is automatically deleted. You can also use finalizers to prevent deletion of unmanaged resources.
-->
<p>管理 finalizer 的控制器注意到对象上发生的更新操作，对象的 <code>metadata.deletionTimestamp</code>
被设置，意味着已经请求删除该对象。然后，控制器会试图满足资源的 Finalizers 的条件。
每当一个 Finalizer 的条件被满足时，控制器就会从资源的 <code>finalizers</code> 字段中删除该键。
当 <code>finalizers</code> 字段为空时，<code>deletionTimestamp</code> 字段被设置的对象会被自动删除。
你也可以使用 Finalizers 来阻止删除未被管理的资源。</p>
<!--
A common example of a finalizer is `kubernetes.io/pv-protection`, which prevents
accidental deletion of `PersistentVolume` objects. When a `PersistentVolume`
object is in use by a Pod, Kubernetes adds the `pv-protection` finalizer. If you
try to delete the `PersistentVolume`, it enters a `Terminating` status, but the
controller can't delete it because the finalizer exists. When the Pod stops
using the `PersistentVolume`, Kubernetes clears the `pv-protection` finalizer,
and the controller deletes the volume.
-->
<p>一个常见的 Finalizer 的例子是 <code>kubernetes.io/pv-protection</code>，
它用来防止意外删除 <code>PersistentVolume</code> 对象。
当一个 <code>PersistentVolume</code> 对象被 Pod 使用时，
Kubernetes 会添加 <code>pv-protection</code> Finalizer。
如果你试图删除 <code>PersistentVolume</code>，它将进入 <code>Terminating</code> 状态，
但是控制器因为该 Finalizer 存在而无法删除该资源。
当 Pod 停止使用 <code>PersistentVolume</code> 时，
Kubernetes 清除 <code>pv-protection</code> Finalizer，控制器就会删除该卷。</p>
<!--
## Owner references, labels, and finalizers {#owners-labels-finalizers}

Like <a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='labels'>labels</a>,
[owner references](/concepts/overview/working-with-objects/owners-dependents/)
describe the relationships between objects in Kubernetes, but are used for a
different purpose. When a
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> manages objects
like Pods, it uses labels to track changes to groups of related objects. For
example, when a <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a> creates one or
more Pods, the Job controller applies labels to those pods and tracks changes to
any Pods in the cluster with the same label.
-->
<h2 id="owners-labels-finalizers">属主引用、标签和 Finalizers</h2>
<p>与<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>类似，
<a href="/zh/concepts/overview/working-with-objects/owners-dependents/">属主引用</a>
描述了 Kubernetes 中对象之间的关系，但它们作用不同。
当一个<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>
管理类似于 Pod 的对象时，它使用标签来跟踪相关对象组的变化。
例如，当 <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a> 创建一个或多个 Pod 时，
Job 控制器会给这些 Pod 应用上标签，并跟踪集群中的具有相同标签的 Pod 的变化。</p>
<!--
The Job controller also adds *owner references* to those Pods, pointing at the
Job that created the Pods. If you delete the Job while these Pods are running,
Kubernetes uses the owner references (not labels) to determine which Pods in the
cluster need cleanup.

Kubernetes also processes finalizers when it identifies owner references on a
resource targeted for deletion. 

In some situations, finalizers can block the deletion of dependent objects,
which can cause the targeted owner object to remain for
longer than expected without being fully deleted. In these situations, you
should check finalizers and owner references on the target owner and dependent
objects to troubleshoot the cause. 
-->
<p>Job 控制器还为这些 Pod 添加了<em>属主引用</em>，指向创建 Pod 的 Job。
如果你在这些 Pod 运行的时候删除了 Job，
Kubernetes 会使用属主引用（而不是标签）来确定集群中哪些 Pod 需要清理。</p>
<p>当 Kubernetes 识别到要删除的资源上的属主引用时，它也会处理 Finalizers。</p>
<p>在某些情况下，Finalizers 会阻止依赖对象的删除，
这可能导致目标属主对象被保留的时间比预期的长，而没有被完全删除。
在这些情况下，你应该检查目标属主和附属对象上的 Finalizers 和属主引用，来排查原因。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In cases where objects are stuck in a deleting state, avoid manually
removing finalizers to allow deletion to continue. Finalizers are usually added
to resources for a reason, so forcefully removing them can lead to issues in
your cluster. This should only be done when the purpose of the finalizer is
understood and is accomplished in another way (for example, manually cleaning
up some dependent object).

-->
<p>在对象卡在删除状态的情况下，要避免手动移除 Finalizers，以允许继续删除操作。
Finalizers 通常因为特殊原因被添加到资源上，所以强行删除它们会导致集群出现问题。
只有了解 finalizer 的用途时才能这样做，并且应该通过一些其他方式来完成
（例如，手动清除其余的依赖对象）。
</div>
<h2 id="what-s-next">What's next</h2>
<!--
* Read [Using Finalizers to Control Deletion](/blog/2021/05/14/using-finalizers-to-control-deletion/)
  on the Kubernetes blog.
-->
<ul>
<li>在 Kubernetes 博客上阅读<a href="/blog/2021/05/14/using-finalizers-to-control-deletion/">使用 Finalizers 控制删除</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-046c03090d47bc4b89b818dc645c3865">1.4.8 - 字段选择器</h1>
    
	<!--
title: Field Selectors
weight: 60
-->
<!--
_Field selectors_ let you [select Kubernetes resources](/docs/concepts/overview/working-with-objects/kubernetes-objects) based on the value of one or more resource fields. Here are some example field selector queries:
-->
<p>“字段选择器（Field selectors）”允许你根据一个或多个资源字段的值
<a href="/zh/docs/concepts/overview/working-with-objects/kubernetes-objects">筛选 Kubernetes 资源</a>。
下面是一些使用字段选择器查询的例子：</p>
<ul>
<li><code>metadata.name=my-service</code></li>
<li><code>metadata.namespace!=default</code></li>
<li><code>status.phase=Pending</code></li>
</ul>
<!--
This `kubectl` command selects all Pods for which the value of the [`status.phase`](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase) field is `Running`:
-->
<p>下面这个 <code>kubectl</code> 命令将筛选出 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase"><code>status.phase</code></a>
字段值为 <code>Running</code> 的所有 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --field-selector status.phase<span style="color:#666">=</span>Running
</code></pre></div><!--
Field selectors are essentially resource *filters*. By default, no selectors/filters are applied, meaning that all resources of the specified type are selected. This makes the following `kubectl` queries equivalent:
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>字段选择器本质上是资源<em>过滤器（Filters）</em>。默认情况下，字段选择器/过滤器是未被应用的，
这意味着指定类型的所有资源都会被筛选出来。
这使得以下的两个 <code>kubectl</code> 查询是等价的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
kubectl get pods --field-selector <span style="color:#b44">&#34;&#34;</span>
</code></pre></div>
</div>
<!--
## Supported fields

Supported field selectors vary by Kubernetes resource type. All resource types support the `metadata.name` and `metadata.namespace` fields. Using unsupported field selectors produces an error. For example:
-->
<h2 id="supported-fields">支持的字段 </h2>
<p>不同的 Kubernetes 资源类型支持不同的字段选择器。
所有资源类型都支持 <code>metadata.name</code> 和 <code>metadata.namespace</code> 字段。
使用不被支持的字段选择器会产生错误。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get ingress --field-selector foo.bar<span style="color:#666">=</span>baz
</code></pre></div><pre><code>Error from server (BadRequest): Unable to find &quot;ingresses&quot; that match label selector &quot;&quot;, field selector &quot;foo.bar=baz&quot;: &quot;foo.bar&quot; is not a known field selector: only &quot;metadata.name&quot;, &quot;metadata.namespace&quot;
</code></pre><!--
## Supported operators

You can use the `=`, `==`, and `!=` operators with field selectors (`=` and `==` mean the same thing). This `kubectl` command, for example, selects all Kubernetes Services that aren't in the `default` namespace:
-->
<h2 id="supported-operators">支持的操作符  </h2>
<p>你可在字段选择器中使用 <code>=</code>、<code>==</code>和 <code>!=</code> （<code>=</code> 和 <code>==</code> 的意义是相同的）操作符。
例如，下面这个 <code>kubectl</code> 命令将筛选所有不属于 <code>default</code> 命名空间的 Kubernetes 服务：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get services  --all-namespaces --field-selector metadata.namespace!<span style="color:#666">=</span>default
</code></pre></div><!--
## Chained selectors

As with [label](/docs/concepts/overview/working-with-objects/labels) and other selectors, field selectors can be chained together as a comma-separated list. This `kubectl` command selects all Pods for which the `status.phase` does not equal `Running` and the `spec.restartPolicy` field equals `Always`:
-->
<h2 id="chained-selectors">链式选择器  </h2>
<p>同<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签</a>和其他选择器一样，
字段选择器可以通过使用逗号分隔的列表组成一个选择链。
下面这个 <code>kubectl</code> 命令将筛选 <code>status.phase</code> 字段不等于 <code>Running</code> 同时
<code>spec.restartPolicy</code> 字段等于 <code>Always</code> 的所有 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --field-selector<span style="color:#666">=</span>status.phase!<span style="color:#666">=</span>Running,spec.restartPolicy<span style="color:#666">=</span>Always
</code></pre></div><!--
## Multiple resource types

You can use field selectors across multiple resource types. This `kubectl` command selects all Statefulsets and Services that are not in the `default` namespace:
-->
<h2 id="multiple-resource-types">多种资源类型  </h2>
<p>你能够跨多种资源类型来使用字段选择器。
下面这个 <code>kubectl</code> 命令将筛选出所有不在 <code>default</code> 命名空间中的 StatefulSet 和 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!<span style="color:#666">=</span>default
</code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-efaa7a58910b58892dafd50e3b43c93c">1.4.9 - 属主与附属</h1>
    
	<!-- 
title: Owners and Dependents
content_type: concept
weight: 60
-->
<!-- overview -->
<!-- 
In Kubernetes, some objects are *owners* of other objects. For example, a
<a class='glossary-tooltip' title='ReplicaSet 是下一代副本控制器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/replicaset/' target='_blank' aria-label='ReplicaSet'>ReplicaSet</a> is the owner of a set of Pods. These owned objects are *dependents*
of their owner. 
-->
<p>在 Kubernetes 中，一些对象是其他对象的<em>属主（Owner）</em>。
例如，<a class='glossary-tooltip' title='ReplicaSet 是下一代副本控制器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/replicaset/' target='_blank' aria-label='ReplicaSet'>ReplicaSet</a> 是一组 Pod 的属主。
具有属主的对象是属主的<em>附属（Dependent）</em> 。</p>
<!--
Ownership is different from the [labels and selectors](/docs/concepts/overview/working-with-objects/labels/)
mechanism that some resources also use. For example, consider a Service that 
creates `EndpointSlice` objects. The Service uses labels to allow the control plane to
determine which `EndpointSlice` objects are used for that Service. In addition
to the labels, each `EndpointSlice` that is managed on behalf of a Service has
an owner reference. Owner references help different parts of Kubernetes avoid
interfering with objects they don’t control. 
-->
<p>属主关系不同于一些资源使用的<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签和选择算符</a>机制。
例如，有一个创建 <code>EndpointSlice</code> 对象的 Service，
该 Service 使用标签来让控制平面确定，哪些 <code>EndpointSlice</code> 对象属于该 Service。
除开标签，每个代表 Service 所管理的 <code>EndpointSlice</code> 都有一个属主引用。
属主引用避免 Kubernetes 的不同部分干扰到不受它们控制的对象。</p>
<!--
## Owner references in object specifications

Dependent objects have a `metadata.ownerReferences` field that references their
owner object. A valid owner reference consists of the object name and a UID
within the same namespace as the dependent object. Kubernetes sets the value of
this field automatically for objects that are dependents of other objects like
ReplicaSets, DaemonSets, Deployments, Jobs and CronJobs, and ReplicationControllers.
You can also configure these relationships manually by changing the value of
this field. However, you usually don't need to and can allow Kubernetes to
automatically manage the relationships.
-->
<h2 id="owner-references-in-object-specifications">对象规约中的属主引用  </h2>
<p>附属对象有一个 <code>metadata.ownerReferences</code> 字段，用于引用其属主对象。
一个有效的属主引用，包含与附属对象同在一个命名空间下的对象名称和一个 UID。
Kubernetes 自动为一些对象的附属资源设置属主引用的值，
这些对象包含 ReplicaSet、DaemonSet、Deployment、Job、CronJob、ReplicationController 等。
你也可以通过改变这个字段的值，来手动配置这些关系。
然而，你通常不需要这么做，你可以让 Kubernetes 自动管理附属关系。</p>
<!--
Dependent objects also have an `ownerReferences.blockOwnerDeletion` field that
takes a boolean value and controls whether specific dependents can block garbage
collection from deleting their owner object. Kubernetes automatically sets this
field to `true` if a <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> 
(for example, the Deployment controller) sets the value of the
`metadata.ownerReferences` field. You can also set the value of the
`blockOwnerDeletion` field manually to control which dependents block garbage
collection.

A Kubernetes admission controller controls user access to change this field for
dependent resources, based on the delete permissions of the owner. This control
prevents unauthorized users from delaying owner object deletion.
-->
<p>附属对象还有一个 <code>ownerReferences.blockOwnerDeletion</code> 字段，该字段使用布尔值，
用于控制特定的附属对象是否可以阻止垃圾收集删除其属主对象。
如果<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>（例如 Deployment 控制器）
设置了 <code>metadata.ownerReferences</code> 字段的值，Kubernetes 会自动设置
<code>blockOwnerDeletion</code> 的值为 <code>true</code>。
你也可以手动设置 <code>blockOwnerDeletion</code> 字段的值，以控制哪些附属对象会阻止垃圾收集。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Cross-namespace owner references are disallowed by design.
Namespaced dependents can specify cluster-scoped or namespaced owners.
A namespaced owner **must** exist in the same namespace as the dependent.
If it does not, the owner reference is treated as absent, and the dependent
is subject to deletion once all owners are verified absent.

Cluster-scoped dependents can only specify cluster-scoped owners.
In v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner,
it is treated as having an unresolvable owner reference, and is not able to be garbage collected.

In v1.20+, if the garbage collector detects an invalid cross-namespace `ownerReference`,
or a cluster-scoped dependent with an `ownerReference` referencing a namespaced kind, a warning Event 
with a reason of `OwnerRefInvalidNamespace` and an `involvedObject` of the invalid dependent is reported.
You can check for that kind of Event by running
`kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace`.
-->
<p>根据设计，kubernetes 不允许跨名字空间指定属主。
名字空间范围的附属可以指定集群范围的或者名字空间范围的属主。
名字空间范围的属主<strong>必须</strong>和该附属处于相同的名字空间。
如果名字空间范围的属主和附属不在相同的名字空间，那么该属主引用就会被认为是缺失的，
并且当附属的所有属主引用都被确认不再存在之后，该附属就会被删除。</p>
<p>集群范围的附属只能指定集群范围的属主。
在 v1.20+ 版本，如果一个集群范围的附属指定了一个名字空间范围类型的属主，
那么该附属就会被认为是拥有一个不可解析的属主引用，并且它不能够被垃圾回收。</p>
<p>在 v1.20+ 版本，如果垃圾收集器检测到无效的跨名字空间的属主引用，
或者一个集群范围的附属指定了一个名字空间范围类型的属主，
那么它就会报告一个警告事件。该事件的原因是 <code>OwnerRefInvalidNamespace</code>，
<code>involvedObject</code> 属性中包含无效的附属。
你可以运行 <code>kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace</code>
来获取该类型的事件。</p>

</div>
<!--
## Ownership and finalizers

When you tell Kubernetes to delete a resource, the API server allows the
managing controller to process any [finalizer rules](/docs/concepts/overview/working-with-objects/finalizers/)
for the resource. <a class='glossary-tooltip' title='一个带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后， 再完全删除被标记为删除的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/finalizers/' target='_blank' aria-label='Finalizers'>Finalizers</a>
prevent accidental deletion of resources your cluster may still need to function
correctly. For example, if you try to delete a `PersistentVolume` that is still
in use by a Pod, the deletion does not happen immediately because the
`PersistentVolume` has the `kubernetes.io/pv-protection` finalizer on it.
Instead, the volume remains in the `Terminating` status until Kubernetes clears
the finalizer, which only happens after the `PersistentVolume` is no longer
bound to a Pod. 
-->
<h2 id="ownership-and-finalizers">属主关系与 Finalizer  </h2>
<p>当你告诉 Kubernetes 删除一个资源，API 服务器允许管理控制器处理该资源的任何
<a href="/zh/docs/concepts/overview/working-with-objects/finalizers/">Finalizer 规则</a>。
<a class='glossary-tooltip' title='一个带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后， 再完全删除被标记为删除的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/finalizers/' target='_blank' aria-label='Finalizer'>Finalizer</a>
防止意外删除你的集群所依赖的、用于正常运作的资源。
例如，如果你试图删除一个仍被 Pod 使用的 <code>PersistentVolume</code>，该资源不会被立即删除，
因为 <code>PersistentVolume</code> 有 <code>kubernetes.io/pv-protection</code> Finalizer。
相反，它将进入 <code>Terminating</code> 状态，直到 Kubernetes 清除这个 Finalizer，
而这种情况只会发生在 <code>PersistentVolume</code> 不再被挂载到 Pod 上时。</p>
<!--
Kubernetes also adds finalizers to an owner resource when you use either
[foreground or orphan cascading deletion](/docs/concepts/architecture/garbage-collection/#cascading-deletion).
In foreground deletion, it adds the `foreground` finalizer so that the
controller must delete dependent resources that also have
`ownerReferences.blockOwnerDeletion=true` before it deletes the owner. If you
specify an orphan deletion policy, Kubernetes adds the `orphan` finalizer so
that the controller ignores dependent resources after it deletes the owner
object. 
-->
<p>当你使用<a href="/zh/docs/concepts/architecture/garbage-collection/#cascading-deletion">前台或孤立级联删除</a>时，
Kubernetes 也会向属主资源添加 Finalizer。
在前台删除中，会添加 <code>foreground</code> Finalizer，这样控制器必须在删除了拥有
<code>ownerReferences.blockOwnerDeletion=true</code> 的附属资源后，才能删除属主对象。
如果你指定了孤立删除策略，Kubernetes 会添加 <code>orphan</code> Finalizer，
这样控制器在删除属主对象后，会忽略附属资源。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [Kubernetes finalizers](/docs/concepts/overview/working-with-objects/finalizers/).
* Learn about [garbage collection](/docs/concepts/architecture/garbage-collection).
* Read the API reference for [object metadata](/docs/reference/kubernetes-api/common-definitions/object-meta/#System).
-->
<ul>
<li>了解更多关于 <a href="/zh/docs/concepts/overview/working-with-objects/finalizers/">Kubernetes Finalizer</a>。</li>
<li>了解关于<a href="/zh/docs/concepts/architecture/garbage-collection">垃圾收集</a>。</li>
<li>阅读<a href="/docs/reference/kubernetes-api/common-definitions/object-meta/#System">对象元数据</a>的 API 参考文档。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5dd62c6a4a481b4cf1ac50f6799eb581">1.4.10 - 推荐使用的标签</h1>
    
	<!--
---
title: Recommended Labels
content_type: concept
---
-->
<!-- overview -->
<!--
You can visualize and manage Kubernetes objects with more tools than kubectl and
the dashboard. A common set of labels allows tools to work interoperably, describing
objects in a common manner that all tools can understand.
-->
<p>除了 kubectl 和 dashboard 之外，您可以使用其他工具来可视化和管理 Kubernetes 对象。一组通用的标签可以让多个工具之间相互操作，用所有工具都能理解的通用方式描述对象。</p>
<!--
In addition to supporting tooling, the recommended labels describe applications
in a way that can be queried.
-->
<p>除了支持工具外，推荐的标签还以一种可以查询的方式描述了应用程序。</p>
<!-- body -->
<!--
The metadata is organized around the concept of an _application_. Kubernetes is not
a platform as a service (PaaS) and doesn't have or enforce a formal notion of an application.
Instead, applications are informal and described with metadata. The definition of
what an application contains is loose.
-->
<p>元数据围绕 <em>应用（application）</em> 的概念进行组织。Kubernetes 不是
平台即服务（PaaS），没有或强制执行正式的应用程序概念。
相反，应用程序是非正式的，并使用元数据进行描述。应用程序包含的定义是松散的。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
These are recommended labels. They make it easier to manage applications
but aren't required for any core tooling.
-->
<p>这些是推荐的标签。它们使管理应用程序变得更容易但不是任何核心工具所必需的。
</div>
<!--
Shared labels and annotations share a common prefix: `app.kubernetes.io`. Labels
without a prefix are private to users. The shared prefix ensures that shared labels
do not interfere with custom user labels.
-->
<p>共享标签和注解都使用同一个前缀：<code>app.kubernetes.io</code>。没有前缀的标签是用户私有的。共享前缀可以确保共享标签不会干扰用户自定义的标签。</p>
<!--
## Labels

In order to take full advantage of using these labels, they should be applied
on every resource object.
-->
<h2 id="标签">标签</h2>
<p>为了充分利用这些标签，应该在每个资源对象上都使用它们。</p>
<!--
| Key                                 | Description           | Example  | Type |
| ----------------------------------- | --------------------- | -------- | ---- |
| `app.kubernetes.io/name`            | The name of the application | `mysql` | string |
| `app.kubernetes.io/instance`        | A unique name identifying the instance of an application | `mysql-abcxzy` | string |
| `app.kubernetes.io/version`         | The current version of the application (e.g., a semantic version, revision hash, etc.) | `5.7.21` | string |
| `app.kubernetes.io/component`       | The component within the architecture | `database` | string |
| `app.kubernetes.io/part-of`         | The name of a higher level application this one is part of | `wordpress` | string |
| `app.kubernetes.io/managed-by`      | The tool being used to manage the operation of an application | `helm` | string |
| `app.kubernetes.io/created-by`      | The controller/user who created this resource | `controller-manager` | string |
-->
<table>
<thead>
<tr>
<th>键</th>
<th>描述</th>
<th>示例</th>
<th>类型</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>app.kubernetes.io/name</code></td>
<td>应用程序的名称</td>
<td><code>mysql</code></td>
<td>字符串</td>
</tr>
<tr>
<td><code>app.kubernetes.io/instance</code></td>
<td>用于唯一确定应用实例的名称</td>
<td><code>mysql-abcxzy</code></td>
<td>字符串</td>
</tr>
<tr>
<td><code>app.kubernetes.io/version</code></td>
<td>应用程序的当前版本（例如，语义版本，修订版哈希等）</td>
<td><code>5.7.21</code></td>
<td>字符串</td>
</tr>
<tr>
<td><code>app.kubernetes.io/component</code></td>
<td>架构中的组件</td>
<td><code>database</code></td>
<td>字符串</td>
</tr>
<tr>
<td><code>app.kubernetes.io/part-of</code></td>
<td>此级别的更高级别应用程序的名称</td>
<td><code>wordpress</code></td>
<td>字符串</td>
</tr>
<tr>
<td><code>app.kubernetes.io/managed-by</code></td>
<td>用于管理应用程序的工具</td>
<td><code>helm</code></td>
<td>字符串</td>
</tr>
<tr>
<td><code>app.kubernetes.io/created-by</code></td>
<td>创建该资源的控制器或者用户</td>
<td><code>controller-manager</code></td>
<td>字符串</td>
</tr>
</tbody>
</table>
<!--
To illustrate these labels in action, consider the following <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a> object:
-->
<p>为说明这些标签的实际使用情况，请看下面的 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a> 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># 这是一段节选</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/instance</span>:<span style="color:#bbb"> </span>mysql-abcxzy<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/version</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5.7.21&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/component</span>:<span style="color:#bbb"> </span>database<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/part-of</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/managed-by</span>:<span style="color:#bbb"> </span>helm<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/created-by</span>:<span style="color:#bbb"> </span>controller-manager<span style="color:#bbb">
</span></code></pre></div><!--
## Applications And Instances Of Applications

An application can be installed one or more times into a Kubernetes cluster and,
in some cases, the same namespace. For example, WordPress can be installed more
than once where different websites are different installations of WordPress.

The name of an application and the instance name are recorded separately. For
example, WordPress has a `app.kubernetes.io/name` of `wordpress` while it has
an instance name, represented as `app.kubernetes.io/instance` with a value of
`wordpress-abcxzy`. This enables the application and instance of the application
to be identifiable. Every instance of an application must have a unique name.
-->
<h2 id="应用和应用实例">应用和应用实例</h2>
<p>应用可以在 Kubernetes 集群中安装一次或多次。在某些情况下，可以安装在同一命名空间中。例如，可以不止一次地为不同的站点安装不同的 WordPress。</p>
<p>应用的名称和实例的名称是分别记录的。例如，WordPress 应用的
<code>app.kubernetes.io/name</code> 为 <code>wordpress</code>，而其实例名称
<code>app.kubernetes.io/instance</code> 为 <code>wordpress-abcxzy</code>。
这使得应用和应用的实例均可被识别，应用的每个实例都必须具有唯一的名称。</p>
<!--
## Examples
-->
<h2 id="示例">示例</h2>
<!--
To illustrate different ways to use these labels the following examples have varying complexity.
-->
<p>为了说明使用这些标签的不同方式，以下示例具有不同的复杂性。</p>
<!--
### A Simple Stateless Service
-->
<h3 id="一个简单的无状态服务">一个简单的无状态服务</h3>
<!--
Consider the case for a simple stateless service deployed using `Deployment` and `Service` objects. The following two snippets represent how the labels could be used in their simplest form.
-->
<p>考虑使用 <code>Deployment</code> 和 <code>Service</code> 对象部署的简单无状态服务的情况。以下两个代码段表示如何以最简单的形式使用标签。</p>
<!--
The `Deployment` is used to oversee the pods running the application itself.
-->
<p>下面的 <code>Deployment</code> 用于监督运行应用本身的 pods。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>myservice<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/instance</span>:<span style="color:#bbb"> </span>myservice-abcxzy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
The `Service` is used to expose the application.
-->
<p>下面的 <code>Service</code> 用于暴露应用。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>myservice<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/instance</span>:<span style="color:#bbb"> </span>myservice-abcxzy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
### Web Application With A Database
-->
<h3 id="带有一个数据库的-web-应用程序">带有一个数据库的 Web 应用程序</h3>
<!--
Consider a slightly more complicated application: a web application (WordPress)
using a database (MySQL), installed using Helm. The following snippets illustrate
the start of objects used to deploy this application.

The start to the following `Deployment` is used for WordPress:
-->
<p>考虑一个稍微复杂的应用：一个使用 Helm 安装的 Web 应用（WordPress），其中
使用了数据库（MySQL）。以下代码片段说明用于部署此应用程序的对象的开始。</p>
<p>以下 <code>Deployment</code> 的开头用于 WordPress：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/instance</span>:<span style="color:#bbb"> </span>wordpress-abcxzy<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/version</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;4.9.4&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/managed-by</span>:<span style="color:#bbb"> </span>helm<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/component</span>:<span style="color:#bbb"> </span>server<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/part-of</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
The `Service` is used to expose WordPress:
-->
<p>这个 <code>Service</code> 用于暴露 WordPress：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/instance</span>:<span style="color:#bbb"> </span>wordpress-abcxzy<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/version</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;4.9.4&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/managed-by</span>:<span style="color:#bbb"> </span>helm<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/component</span>:<span style="color:#bbb"> </span>server<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/part-of</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
MySQL is exposed as a `StatefulSet` with metadata for both it and the larger application it belongs to:
-->
<p>MySQL 作为一个 <code>StatefulSet</code> 暴露，包含它和它所属的较大应用程序的元数据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/instance</span>:<span style="color:#bbb"> </span>mysql-abcxzy<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/version</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5.7.21&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/managed-by</span>:<span style="color:#bbb"> </span>helm<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/component</span>:<span style="color:#bbb"> </span>database<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/part-of</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
The `Service` is used to expose MySQL as part of WordPress:
-->
<p><code>Service</code> 用于将 MySQL 作为 WordPress 的一部分暴露：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/instance</span>:<span style="color:#bbb"> </span>mysql-abcxzy<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/version</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5.7.21&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/managed-by</span>:<span style="color:#bbb"> </span>helm<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/component</span>:<span style="color:#bbb"> </span>database<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/part-of</span>:<span style="color:#bbb"> </span>wordpress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
With the MySQL `StatefulSet` and `Service` you'll notice information about both MySQL and Wordpress, the broader application, are included.
-->
<p>使用 MySQL <code>StatefulSet</code> 和 <code>Service</code>，您会注意到有关 MySQL 和 Wordpress 的信息，包括更广泛的应用程序。</p>

</div>



    
	
  

    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2bf36ccd6b3dbeafecf87c39761b07c7">2 - Kubernetes 架构</h1>
    <div class="lead">Kubernetes 背后的架构概念。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-9ef2890698e773b6c0d24fd2c20146f5">2.1 - 节点</h1>
    
	<!--
reviewers:
- caesarxuchao
- dchen1107
title: Nodes
content_type: concept
weight: 10
-->
<!-- overview -->
<!--
Kubernetes runs your workload by placing containers into Pods to run on _Nodes_.
A node may be a virtual or physical machine, depending on the cluster. Each node
is managed by the 
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>
and contains the services necessary to run
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>.

Typically you have several nodes in a cluster; in a learning or resource-limited
environment, you might have just one.

The [components](/docs/concepts/overview/components/#node-components) on a node include the
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>, a
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>, and the
<a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a>.
-->
<p>Kubernetes 通过将容器放入在节点（Node）上运行的 Pod 中来执行你的工作负载。
节点可以是一个虚拟机或者物理机器，取决于所在的集群配置。
每个节点包含运行 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> 所需的服务；
这些节点由 <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a> 负责管理。</p>
<p>通常集群中会有若干个节点；而在一个学习用或者资源受限的环境中，你的集群中也可能
只有一个节点。</p>
<p>节点上的<a href="/zh/docs/concepts/overview/components/#node-components">组件</a>包括
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>、
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>以及
<a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a>。</p>
<!-- body -->
<!--
## Management

There are two main ways to have Nodes added to the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a>:

1. The kubelet on a node self-registers to the control plane
2. You, or another human user, manually add a Node object

After you create a Node <a class='glossary-tooltip' title='Kubernetes 系统中的实体, 代表了集群的部分状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/#kubernetes-objects' target='_blank' aria-label='object'>object</a>,
or the kubelet on a node self-registers, the control plane checks whether the new Node object is
valid. For example, if you try to create a Node from the following JSON manifest:
-->
<h2 id="management">管理 </h2>
<p>向 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>添加节点的方式主要有两种：</p>
<ol>
<li>节点上的 <code>kubelet</code> 向控制面执行自注册；</li>
<li>你，或者别的什么人，手动添加一个 Node 对象。</li>
</ol>
<p>在你创建了 Node <a class='glossary-tooltip' title='Kubernetes 系统中的实体, 代表了集群的部分状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/#kubernetes-objects' target='_blank' aria-label='对象'>对象</a>或者节点上的
<code>kubelet</code> 执行了自注册操作之后，控制面会检查新的 Node 对象是否合法。
例如，如果你尝试使用下面的 JSON 对象来创建 Node 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Node&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;v1&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;metadata&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;10.240.79.157&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;labels&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;my-first-k8s-node&#34;</span>
    }
  }
}
</code></pre></div><!--
Kubernetes creates a Node object internally (the representation). Kubernetes checks
that a kubelet has registered to the API server that matches the `metadata.name`
field of the Node. If the node is healthy (if all necessary services are running),
it is eligible to run a Pod. Otherwise, that node is ignored for any cluster activity
until it becomes healthy.
-->
<p>Kubernetes 会在内部创建一个 Node 对象作为节点的表示。Kubernetes 检查 <code>kubelet</code>
向 API 服务器注册节点时使用的 <code>metadata.name</code> 字段是否匹配。
如果节点是健康的（即所有必要的服务都在运行中），则该节点可以用来运行 Pod。
否则，直到该节点变为健康之前，所有的集群活动都会忽略该节点。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Kubernetes keeps the object for the invalid Node and continues checking to see whether
it becomes healthy.

You, or a <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a>, must explicitly
delete the Node object to stop that health checking.
-->
<p>Kubernetes 会一直保存着非法节点对应的对象，并持续检查该节点是否已经变得健康。
你，或者某个<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>必须显式地删除该
Node 对象以停止健康检查操作。
</div>
<!--
The name of a Node object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>Node 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
### Node name uniqueness

The [name](/docs/concepts/overview/working-with-objects/names#names) identifies a Node. Two Nodes
cannot have the same name at the same time. Kubernetes also assumes that a resource with the same
name is the same object. In case of a Node, it is implicitly assumed that an instance using the
same name will have the same state (e.g. network settings, root disk contents)
and attributes like node labels. This may lead to
inconsistencies if an instance was modified without changing its name. If the Node needs to be
replaced or updated significantly, the existing Node object needs to be removed from API server
first and re-added after the update.
-->
<h3 id="node-name-uniqueness">节点名称唯一性    </h3>
<p>节点的<a href="/zh/docs/concepts/overview/working-with-objects/names#names">名称</a>用来标识 Node 对象。
没有两个 Node 可以同时使用相同的名称。 Kubernetes 还假定名字相同的资源是同一个对象。
就 Node 而言，隐式假定使用相同名称的实例会具有相同的状态（例如网络配置、根磁盘内容）
和类似节点标签这类属性。这可能在节点被更改但其名称未变时导致系统状态不一致。
如果某个 Node 需要被替换或者大量变更，需要从 API 服务器移除现有的 Node 对象，
之后再在更新之后重新将其加入。</p>
<!--
### Self-registration of Nodes

When the kubelet flag `-register-node` is true (the default), the kubelet will attempt to
register itself with the API server.  This is the preferred pattern, used by most distros.

For self-registration, the kubelet is started with the following options:
-->
<h3 id="self-registration-of-nodes">节点自注册</h3>
<p>当 kubelet 标志 <code>--register-node</code> 为 true（默认）时，它会尝试向 API 服务注册自己。
这是首选模式，被绝大多数发行版选用。</p>
<p>对于自注册模式，kubelet 使用下列参数启动：</p>
<!--
- `--kubeconfig` - Path to credentials to authenticate itself to the API server.
- `--cloud-provider` - How to talk to a <a class='glossary-tooltip' title='一个提供云计算平台的组织。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cloud-provider' target='_blank' aria-label='cloud provider'>cloud provider</a> to read metadata about itself.
- `--register-node` - Automatically register with the API server.
- `--register-with-taints` - Register the node with the given list of <a class='glossary-tooltip' title='污点是一种一个核心对象，包含三个必需的属性：key、value 和 effect。 污点会阻止在节点或节点组上调度 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='taints'>taints</a> (comma separated `<key>=<value>:<effect>`).

    No-op if `register-node` is false.
- `--node-ip` - IP address of the node.
- `--node-labels` - <a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='Labels'>Labels</a> to add when registering the node in the cluster (see label restrictions enforced by the [NodeRestriction admission plugin](/docs/reference/access-authn-authz/admission-controllers/#noderestriction)).
- `--node-status-update-frequency` - Specifies how often kubelet posts node status to master.
-->
<ul>
<li><code>--kubeconfig</code> - 用于向 API 服务器执行身份认证所用的凭据的路径。</li>
<li><code>--cloud-provider</code> - 与某<a class='glossary-tooltip' title='一个提供云计算平台的组织。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cloud-provider' target='_blank' aria-label='云驱动'>云驱动</a>
进行通信以读取与自身相关的元数据的方式。</li>
<li><code>--register-node</code> - 自动向 API 服务注册。</li>
<li><code>--register-with-taints</code> - 使用所给的<a class='glossary-tooltip' title='污点是一种一个核心对象，包含三个必需的属性：key、value 和 effect。 污点会阻止在节点或节点组上调度 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='污点'>污点</a>列表
（逗号分隔的 <code>&lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</code>）注册节点。当 <code>register-node</code> 为 false 时无效。</li>
<li><code>--node-ip</code> - 节点 IP 地址。</li>
<li><code>--node-labels</code> - 在集群中注册节点时要添加的<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>。
（参见 <a href="/zh/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction 准入控制插件</a>所实施的标签限制）。</li>
<li><code>--node-status-update-frequency</code> - 指定 kubelet 向控制面发送状态的频率。</li>
</ul>
<!--
When the [Node authorization mode](/docs/reference/access-authn-authz/node/) and
[NodeRestriction admission plugin](/docs/reference/access-authn-authz/admission-controllers/#noderestriction) are enabled,
kubelets are only authorized to create/modify their own Node resource.
-->
<p>启用<a href="/zh/docs/reference/access-authn-authz/node/">Node 鉴权模式</a>和
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction 准入插件</a>时，
仅授权 <code>kubelet</code> 创建或修改其自己的节点资源。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
As mentioned in the [Node name uniqueness](#node-name-uniqueness) section,
when Node configuration needs to be updated, it is a good practice to re-register
the node with the API server. For example, if the kubelet being restarted with
the new set of `--node-labels`, but the same Node name is used, the change will
not take an effect, as labels are being set on the Node registration.
-->
<p>正如<a href="#node-name-uniqueness">节点名称唯一性</a>一节所述，当 Node 的配置需要被更新时，
一种好的做法是重新向 API 服务器注册该节点。例如，如果 kubelet 重启时其 <code>--node-labels</code>
是新的值集，但同一个 Node 名称已经被使用，则所作变更不会起作用，
因为节点标签是在 Node 注册时完成的。</p>
<!--
Pods already scheduled on the Node may misbehave or cause issues if the Node
configuration will be changed on kubelet restart. For example, already running
Pod may be tainted against the new labels assigned to the Node, while other
Pods, that are incompatible with that Pod will be scheduled based on this new
label.  Node re-registration ensures all Pods will be drained and properly
re-scheduled.
-->
<p>如果在 kubelet 重启期间 Node 配置发生了变化，已经被调度到某 Node 上的 Pod
可能会出现行为不正常或者出现其他问题，例如，已经运行的 Pod
可能通过污点机制设置了与 Node 上新设置的标签相排斥的规则，也有一些其他 Pod，
本来与此 Pod 之间存在不兼容的问题，也会因为新的标签设置而被调到到同一节点。
节点重新注册操作可以确保节点上所有 Pod 都被排空并被正确地重新调度。</p>

</div>
<!--
### Manual Node administration

You can create and modify Node objects using
<a class='glossary-tooltip' title='kubectl 是用来和 Kubernetes 集群进行通信的命令行工具。' data-toggle='tooltip' data-placement='top' href='/docs/user-guide/kubectl-overview/' target='_blank' aria-label='kubectl'>kubectl</a>.

When you want to create Node objects manually, set the kubelet flag `--register-node=false`.

You can modify Node objects regardless of the setting of `--register-node`.
For example, you can set labels on an existing Node, or mark it unschedulable.
-->
<h3 id="manual-node-administration">手动节点管理</h3>
<p>你可以使用 <a class='glossary-tooltip' title='kubectl 是用来和 Kubernetes 集群进行通信的命令行工具。' data-toggle='tooltip' data-placement='top' href='/docs/user-guide/kubectl-overview/' target='_blank' aria-label='kubectl'>kubectl</a>
来创建和修改 Node 对象。</p>
<p>如果你希望手动创建节点对象时，请设置 kubelet 标志 <code>--register-node=false</code>。</p>
<p>你可以修改 Node 对象（忽略 <code>--register-node</code> 设置）。
例如，修改节点上的标签或标记其为不可调度。</p>
<!--
You can use labels on Nodes in conjunction with node selectors on Pods to control
scheduling. For example, you can to constrain a Pod to only be eligible to run on
a subset of the available nodes.

Marking a node as unschedulable prevents the scheduler from placing new pods onto
that Node, but does not affect existing Pods on the Node. This is useful as a
preparatory step before a node reboot or other maintenance.

To mark a Node unschedulable, run:
-->
<p>你可以结合使用 Node 上的标签和 Pod 上的选择算符来控制调度。
例如，你可以限制某 Pod 只能在符合要求的节点子集上运行。</p>
<p>如果标记节点为不可调度（unschedulable），将阻止新 Pod 调度到该 Node 之上，
但不会影响任何已经在其上的 Pod。
这是重启节点或者执行其他维护操作之前的一个有用的准备步骤。</p>
<p>要标记一个 Node 为不可调度，执行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cordon <span style="color:#b8860b">$NODENAME</span>
</code></pre></div><!--
See [Safely Drain a Node](/docs/tasks/administer-cluster/safely-drain-node/)
for more details.
-->
<p>更多细节参考<a href="/zh/docs/tasks/administer-cluster/safely-drain-node/">安全地腾空节点</a>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Pods that are part of a <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a> tolerate
being run on an unschedulable Node. DaemonSets typically provide node-local services
that should run on the Node even if it is being drained of workload applications.
-->
<p>被 <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a> 控制器创建的 Pod
能够容忍节点的不可调度属性。
DaemonSet 通常提供节点本地的服务，即使节点上的负载应用已经被腾空，
这些服务也仍需运行在节点之上。
</div>
<!--
## Node Status

A node's status contains the following information:

* [Addresses](#addresses)
* [Conditions](#condition)
* [Capacity and Allocatable](#capacity)
* [Info](#info)
-->
<h2 id="node-status">节点状态  </h2>
<p>一个节点的状态包含以下信息:</p>
<ul>
<li><a href="#addresses">地址（Addresses）</a></li>
<li><a href="#condition">状况（Condition）</a></li>
<li><a href="#capacity">容量与可分配（Capacity）</a></li>
<li><a href="#info">信息（Info）</a></li>
</ul>
<!--
You can use `kubectl` to view a Node's status and other details:
-->
<p>你可以使用 <code>kubectl</code> 来查看节点状态和其他细节信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe node &lt;节点名称&gt;
</code></pre></div><!-- Each section is described in detail below. -->
<p>下面对每个部分进行详细描述。</p>
<!--
### Addresses

The usage of these fields varies depending on your cloud provider or bare metal configuration.
-->
<h3 id="addresses">地址  </h3>
<p>这些字段的用法取决于你的云服务商或者物理机配置。</p>
<!--
* HostName: The hostname as reported by the node's kernel. Can be overridden via the kubelet `-hostname-override` parameter.
* ExternalIP: Typically the IP address of the node that is externally routable (available from outside the cluster).
* InternalIP: Typichostnameally the IP address of the node that is routable only within the cluster.
-->
<ul>
<li>HostName：由节点的内核报告。可以通过 kubelet 的 <code>--hostname-override</code> 参数覆盖。</li>
<li>ExternalIP：通常是节点的可外部路由（从集群外可访问）的 IP 地址。</li>
<li>InternalIP：通常是节点的仅可在集群内部路由的 IP 地址。</li>
</ul>
<!--
### Conditions {#condition}

The `conditions` field describes the status of all `Running` nodes. Examples of conditions include:
-->
<h3 id="condition">状况</h3>
<p><code>conditions</code> 字段描述了所有 <code>Running</code> 节点的状况。状况的示例包括：</p>
<!--





<table><caption style="display: none;">Node conditions, and a description of when each condition applies.</caption>
<thead>
<tr>
<th>Node Condition</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Ready</code></td>
<td><code>True</code> if the node is healthy and ready to accept pods, <code>False</code> if the node is not healthy and is not accepting pods, and <code>Unknown</code> if the node controller has not heard from the node in the last <code>node-monitor-grace-period</code> (default is 40 seconds)</td>
</tr>
<tr>
<td><code>DiskPressure</code></td>
<td><code>True</code> if pressure exists on the disk size—that is, if the disk capacity is low; otherwise <code>False</code></td>
</tr>
<tr>
<td><code>MemoryPressure</code></td>
<td><code>True</code> if pressure exists on the node memory—that is, if the node memory is low; otherwise <code>False</code></td>
</tr>
<tr>
<td><code>PIDPressure</code></td>
<td><code>True</code> if pressure exists on the processes - that is, if there are too many processes on the node; otherwise <code>False</code></td>
</tr>
<tr>
<td><code>NetworkUnavailable</code></td>
<td><code>True</code> if the network for the node is not correctly configured, otherwise <code>False</code></td>
</tr>
</tbody>
</table>

-->





<table><caption style="display: none;">节点状况及每种状况适用场景的描述</caption>
<thead>
<tr>
<th>节点状况</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Ready</code></td>
<td>如节点是健康的并已经准备好接收 Pod 则为 <code>True</code>；<code>False</code> 表示节点不健康而且不能接收 Pod；<code>Unknown</code> 表示节点控制器在最近 <code>node-monitor-grace-period</code> 期间（默认 40 秒）没有收到节点的消息</td>
</tr>
<tr>
<td><code>DiskPressure</code></td>
<td><code>True</code> 表示节点存在磁盘空间压力，即磁盘可用量低, 否则为 <code>False</code></td>
</tr>
<tr>
<td><code>MemoryPressure</code></td>
<td><code>True</code> 表示节点存在内存压力，即节点内存可用量低，否则为 <code>False</code></td>
</tr>
<tr>
<td><code>PIDPressure</code></td>
<td><code>True</code> 表示节点存在进程压力，即节点上进程过多；否则为 <code>False</code></td>
</tr>
<tr>
<td><code>NetworkUnavailable</code></td>
<td><code>True</code> 表示节点网络配置不正确；否则为 <code>False</code></td>
</tr>
</tbody>
</table>

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you use command-line tools to print details of a cordoned Node, the Condition includes
`SchedulingDisabled`. `SchedulingDisabled` is not a Condition in the Kubernetes API; instead,
cordoned nodes are marked Unschedulable in their spec.
-->
<p>如果使用命令行工具来打印已保护（Cordoned）节点的细节，其中的 Condition 字段可能包括
<code>SchedulingDisabled</code>。<code>SchedulingDisabled</code> 不是 Kubernetes API 中定义的
Condition，被保护起来的节点在其规约中被标记为不可调度（Unschedulable）。
</div>
<!--
In the Kubernetes API, a node's condition is represented as part of the `.status`
of the Node resource. For example, the following JSON structure describes a healthy node:
-->
<p>在 Kubernetes API 中，节点的状况表示节点资源中<code>.status</code> 的一部分。
例如，以下 JSON 结构描述了一个健康节点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#b44">&#34;conditions&#34;</span><span style="">:</span> [
  {
    <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;Ready&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;status&#34;</span>: <span style="color:#b44">&#34;True&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;reason&#34;</span>: <span style="color:#b44">&#34;KubeletReady&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;message&#34;</span>: <span style="color:#b44">&#34;kubelet is posting ready status&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;lastHeartbeatTime&#34;</span>: <span style="color:#b44">&#34;2019-06-05T18:38:35Z&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;lastTransitionTime&#34;</span>: <span style="color:#b44">&#34;2019-06-05T11:41:27Z&#34;</span>
  }
]
</code></pre></div><!--
If the `status` of the Ready condition remains `Unknown` or `False` for longer
than the `pod-eviction-timeout` (an argument passed to the
<a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>), then the [node controller](#node-controller) triggers
<a class='glossary-tooltip' title='API 发起的驱逐是一个先调用 Eviction API 创建驱逐对象，再由该对象体面地中止 Pod 的过程。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/pod-eviction/#api-eviction' target='_blank' aria-label='API-initiated eviction'>API-initiated eviction</a>
for all Pods assigned to that node. The default eviction timeout duration is
**five minutes**.
-->
<p>如果 Ready 状况的 <code>status</code> 处于 <code>Unknown</code> 或者 <code>False</code> 状态的时间超过了
<code>pod-eviction-timeout</code> 值（一个传递给
<a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>
的参数），<a href="#node-controller">节点控制器</a>会对节点上的所有 Pod 触发
<a class='glossary-tooltip' title='API 发起的驱逐是一个先调用 Eviction API 创建驱逐对象，再由该对象体面地中止 Pod 的过程。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/pod-eviction/#api-eviction' target='_blank' aria-label='API-发起的驱逐'>API-发起的驱逐</a>。
默认的逐出超时时长为 <strong>5 分钟</strong>。</p>
<!--
In some cases when the node is unreachable, the API server is unable to communicate
with the kubelet on the node. The decision to delete the pods cannot be communicated to
the kubelet until communication with the API server is re-established. In the meantime,
the pods that are scheduled for deletion may continue to run on the partitioned node.
-->
<p>某些情况下，当节点不可达时，API 服务器不能和其上的 kubelet 通信。
删除 Pod 的决定不能传达给 kubelet，直到它重新建立和 API 服务器的连接为止。
与此同时，被计划删除的 Pod 可能会继续在游离的节点上运行。</p>
<!--
The node controller does not force delete pods until it is confirmed that they have stopped
running in the cluster. You can see the pods that might be running on an unreachable node as
being in the `Terminating` or `Unknown` state. In cases where Kubernetes cannot deduce from the
underlying infrastructure if a node has permanently left a cluster, the cluster administrator
may need to delete the node object by hand.  Deleting the node object from Kubernetes causes
all the Pod objects running on the node to be deleted from the API server, and frees up their
names.
-->
<p>节点控制器在确认 Pod 在集群中已经停止运行前，不会强制删除它们。
你可以看到可能在这些无法访问的节点上运行的 Pod 处于 <code>Terminating</code> 或者 <code>Unknown</code> 状态。
如果 kubernetes 不能基于下层基础设施推断出某节点是否已经永久离开了集群，
集群管理员可能需要手动删除该节点对象。
从 Kubernetes 删除节点对象将导致 API 服务器删除节点上所有运行的 Pod 对象并释放它们的名字。</p>
<!--
When problems occur on nodes, the Kubernetes control plane automatically creates
[taints](/docs/concepts/scheduling-eviction/taint-and-toleration/) that match the conditions
affecting the node.
The scheduler takes the Node's taints into consideration when assigning a Pod to a Node.
Pods can also have <a class='glossary-tooltip' title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='tolerations'>tolerations</a> that let
them run on a Node even though it has a specific taint.
-->
<p>当节点上出现问题时，Kubernetes 控制面会自动创建与影响节点的状况对应的
<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">污点</a>。
调度器在将 Pod 指派到某 Node 时会考虑 Node 上的污点设置。
Pod 也可以设置<a class='glossary-tooltip' title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='容忍度'>容忍度</a>，
以便能够在设置了特定污点的 Node 上运行。</p>
<!--
See [Taint Nodes by Condition](/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-nodes-by-condition)
for more details.
-->
<p>进一步的细节可参阅<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-nodes-by-condition">根据状况为节点设置污点</a>。</p>
<!--
### Capacity and Allocatable {#capacity}

Describes the resources available on the node: CPU, memory and the maximum
number of pods that can be scheduled onto the node.
-->
<h3 id="capacity">容量（Capacity）与可分配（Allocatable）    </h3>
<p>这两个值描述节点上的可用资源：CPU、内存和可以调度到节点上的 Pod 的个数上限。</p>
<!--
The fields in the capacity block indicate the total amount of resources that a
Node has. The allocatable block indicates the amount of resources on a
Node that is available to be consumed by normal Pods.
-->
<p><code>capacity</code> 块中的字段标示节点拥有的资源总量。
<code>allocatable</code> 块指示节点上可供普通 Pod 消耗的资源量。</p>
<!--
You may read more about capacity and allocatable resources while learning how
to [reserve compute resources](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable) on a Node.
-->
<p>可以在学习如何在节点上<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">预留计算资源</a>
的时候了解有关容量和可分配资源的更多信息。</p>
<!--
### Info

Describes general information about the node, such as kernel version, Kubernetes
version (kubelet and kube-proxy version), container runtime details, and which
operating system the node uses.
The kubelet gathers this information from the node and publishes it into
the Kubernetes API.
-->
<h3 id="info">信息（Info）</h3>
<p>Info 指的是节点的一般信息，如内核版本、Kubernetes 版本（<code>kubelet</code> 和 <code>kube-proxy</code> 版本）、
容器运行时详细信息，以及节点使用的操作系统。
<code>kubelet</code> 从节点收集这些信息并将其发布到 Kubernetes API。</p>
<!--
## Heartbeats

Heartbeats, sent by Kubernetes nodes, help your cluster determine the
availability of each node, and to take action when failures are detected.

For nodes there are two forms of heartbeats:
-->
<h2 id="heartbeats">心跳 </h2>
<p>Kubernetes 节点发送的心跳帮助你的集群确定每个节点的可用性，并在检测到故障时采取行动。</p>
<p>对于节点，有两种形式的心跳:</p>
<!--
* updates to the `.status` of a Node
* [Lease](/docs/reference/kubernetes-api/cluster-resources/lease-v1/) objects
  within the `kube-node-lease`
  <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespace'>namespace</a>.
  Each Node has an associated Lease object.
-->
<ul>
<li>更新节点的 <code>.status</code></li>
<li><code>kube-node-lease</code> <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>中的
<a href="/docs/reference/kubernetes-api/cluster-resources/lease-v1/">Lease（租约）</a>对象。
每个节点都有一个关联的 Lease 对象。</li>
</ul>
<!--
Compared to updates to `.status` of a Node, a Lease is a lightweight resource.
Using Leases for heartbeats reduces the performance impact of these updates
for large clusters.

The kubelet is responsible for creating and updating the `.status` of Nodes,
and for updating their related Leases.
-->
<p>与 Node 的 <code>.status</code> 更新相比，Lease 是一种轻量级资源。
使用 Lease 来表达心跳在大型集群中可以减少这些更新对性能的影响。</p>
<p>kubelet 负责创建和更新节点的 <code>.status</code>，以及更新它们对应的 Lease。</p>
<!--
- The kubelet updates the node's `.status` either when there is change in status
  or if there has been no update for a configured interval. The default interval
  for `.status` updates to Nodes is 5 minutes, which is much longer than the 40
  second default timeout for unreachable nodes.
- The kubelet creates and then updates its Lease object every 10 seconds
  (the default update interval). Lease updates occur independently from
  updates to the Node's `.status`. If the Lease update fails, the kubelet retries,
  using exponential backoff that starts at 200 milliseconds and capped at 7 seconds.
-->
<ul>
<li>当节点状态发生变化时，或者在配置的时间间隔内没有更新事件时，kubelet 会更新 <code>.status</code>。
<code>.status</code> 更新的默认间隔为 5 分钟（比节点不可达事件的 40 秒默认超时时间长很多）。</li>
<li><code>kubelet</code> 会创建并每 10 秒（默认更新间隔时间）更新 Lease 对象。
Lease 的更新独立于 Node 的 <code>.status</code> 更新而发生。
如果 Lease 的更新操作失败，kubelet 会采用指数回退机制，从 200 毫秒开始重试，
最长重试间隔为 7 秒钟。</li>
</ul>
<!--
## Node Controller

The node <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> is a
Kubernetes control plane component that manages various aspects of nodes.

The node controller has multiple roles in a node's life. The first is assigning a
CIDR block to the node when it is registered (if CIDR assignment is turned on).
-->
<h2 id="node-controller">节点控制器 </h2>
<p>节点<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>是 Kubernetes 控制面组件，
管理节点的方方面面。</p>
<p>节点控制器在节点的生命周期中扮演多个角色。
第一个是当节点注册时为它分配一个 CIDR 区段（如果启用了 CIDR 分配）。</p>
<!--
The second is keeping the node controller's internal list of nodes up to date with
the cloud provider's list of available machines. When running in a cloud
environment, whenever a node is unhealthy, the node controller asks the cloud
provider if the VM for that node is still available. If not, the node
controller deletes the node from its list of nodes.
-->
<p>第二个是保持节点控制器内的节点列表与云服务商所提供的可用机器列表同步。
如果在云环境下运行，只要某节点不健康，节点控制器就会询问云服务是否节点的虚拟机仍可用。
如果不可用，节点控制器会将该节点从它的节点列表删除。</p>
<!--
The third is monitoring the nodes' health. The node controller is
responsible for:

- In the case that a node becomes unreachable, updating the `Ready` condition
  in the Node's `.status` field. In this case the node controller sets the
  `Ready` condition to `Unknown`.
- If a node remains unreachable: triggering
  [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/)
  for all of the Pods on the unreachable node. By default, the node controller
  waits 5 minutes between marking the node as `Unknown` and submitting
  the first eviction request.

By default, the node controller checks the state of each node every 5 seconds.
This period can be configured using the `--node-monitor-period` flag on the
`kube-controller-manager` component.
-->
<p>第三个是监控节点的健康状况。节点控制器负责：</p>
<ul>
<li>在节点不可达的情况下，在 Node 的 <code>.status</code> 中更新 <code>Ready</code> 状况。
在这种情况下，节点控制器将 NodeReady 状况更新为 <code>Unknown</code> 。</li>
<li>如果节点仍然无法访问：对于不可达节点上的所有 Pod 触发
<a href="/zh/docs/concepts/scheduling-eviction/api-eviction/">API 发起的逐出</a>操作。
默认情况下，节点控制器在将节点标记为 <code>Unknown</code> 后等待 5 分钟提交第一个驱逐请求。</li>
</ul>
<p>默认情况下，节点控制器每 5 秒检查一次节点状态，可以使用 <code>kube-controller-manager</code>
组件上的 <code>--node-monitor-period</code> 参数来配置周期。</p>
<!--
### Rate limits on eviction

In most cases, the node controller limits the eviction rate to
`-node-eviction-rate` (default 0.1) per second, meaning it won't evict pods
from more than 1 node per 10 seconds.
-->
<h3 id="rate-limits-on-eviction">逐出速率限制 </h3>
<p>大部分情况下，节点控制器把逐出速率限制在每秒 <code>--node-eviction-rate</code> 个（默认为 0.1）。
这表示它每 10 秒钟内至多从一个节点驱逐 Pod。</p>
<!--
The node eviction behavior changes when a node in a given availability zone
becomes unhealthy. The node controller checks what percentage of nodes in the zone
are unhealthy (the `Ready` condition is `Unknown` or `False`) at
the same time:
-->
<p>当一个可用区域（Availability Zone）中的节点变为不健康时，节点的驱逐行为将发生改变。
节点控制器会同时检查可用区域中不健康（<code>Ready</code> 状况为 <code>Unknown</code> 或 <code>False</code>）
的节点的百分比：</p>
<!--
- If the fraction of unhealthy nodes is at least `--unhealthy-zone-threshold`
  (default 0.55), then the eviction rate is reduced.
- If the cluster is small (i.e. has less than or equal to
  `--large-cluster-size-threshold` nodes - default 50), then evictions are stopped.
- Otherwise, the eviction rate is reduced to `--secondary-node-eviction-rate`
  (default 0.01) per second.
-->
<ul>
<li>如果不健康节点的比例超过 <code>--unhealthy-zone-threshold</code> （默认为 0.55），
驱逐速率将会降低。</li>
<li>如果集群较小（意即小于等于 <code>--large-cluster-size-threshold</code> 个节点 - 默认为 50），
驱逐操作将会停止。</li>
<li>否则驱逐速率将降为每秒 <code>--secondary-node-eviction-rate</code> 个（默认为 0.01）。</li>
</ul>
<!--
The reason these policies are implemented per availability zone is because one
availability zone might become partitioned from the master while the others remain
connected. If your cluster does not span multiple cloud provider availability zones,
then the eviction mechanism does not take per-zone unavailability into account.
-->
<p>在逐个可用区域中实施这些策略的原因是，
当一个可用区域可能从控制面脱离时其它可用区域可能仍然保持连接。
如果你的集群没有跨越云服务商的多个可用区域，那（整个集群）就只有一个可用区域。</p>
<!--
A key reason for spreading your nodes across availability zones is so that the
workload can be shifted to healthy zones when one entire zone goes down.
Therefore, if all nodes in a zone are unhealthy then node controller evicts at
the normal rate `-node-eviction-rate`.  The corner case is when all zones are
completely unhealthy (none of the nodes in the cluster are healthy). In such a
case, the node controller assumes that there is some problem with connectivity
between the control plane and the nodes, and doesn't perform any evictions.
(If there has been an outage and some nodes reappear, the node controller does
evict pods from the remaining nodes that are unhealthy or unreachable).
-->
<p>跨多个可用区域部署你的节点的一个关键原因是当某个可用区域整体出现故障时，
工作负载可以转移到健康的可用区域。
因此，如果一个可用区域中的所有节点都不健康时，节点控制器会以正常的速率
<code>--node-eviction-rate</code> 进行驱逐操作。
在所有的可用区域都不健康（也即集群中没有健康节点）的极端情况下，
节点控制器将假设控制面与节点间的连接出了某些问题，它将停止所有驱逐动作
（如果故障后部分节点重新连接，节点控制器会从剩下不健康或者不可达节点中驱逐 Pod）。</p>
<!--
The Node Controller is also responsible for evicting pods running on nodes with
`NoExecute` taints, unless the pods do not tolerate the taints.
The Node Controller also adds <a class='glossary-tooltip' title='污点是一种一个核心对象，包含三个必需的属性：key、value 和 effect。 污点会阻止在节点或节点组上调度 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='taints'>taints</a>
corresponding to node problems like node unreachable or not ready. This means
that the scheduler won't place Pods onto unhealthy nodes.
-->
<p>节点控制器还负责驱逐运行在拥有 <code>NoExecute</code> 污点的节点上的 Pod，
除非这些 Pod 能够容忍此污点。
节点控制器还负责根据节点故障（例如节点不可访问或没有就绪）
为其添加<a class='glossary-tooltip' title='污点是一种一个核心对象，包含三个必需的属性：key、value 和 effect。 污点会阻止在节点或节点组上调度 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='污点'>污点</a>。
这意味着调度器不会将 Pod 调度到不健康的节点上。</p>
<!--
## Resource capacity tracking {#node-capacity}

Node objects track information about the Node's resource capacity (for example: the amount
of memory available, and the number of CPUs).
Nodes that [self register](#self-registration-of-nodes) report their capacity during
registration. If you [manually](#manual-node-administration) add a Node, then
you need to set the node's capacity information when you add it.
-->
<h3 id="node-capacity">资源容量跟踪  </h3>
<p>Node 对象会跟踪节点上资源的容量（例如可用内存和 CPU 数量）。
通过<a href="#self-registration-of-nodes">自注册</a>机制生成的 Node 对象会在注册期间报告自身容量。
如果你<a href="#manual-node-administration">手动</a>添加了 Node，
你就需要在添加节点时手动设置节点容量。</p>
<!--
The Kubernetes <a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='scheduler'>scheduler</a> ensures that
there are enough resources for all the pods on a node.  The scheduler checks that the sum
of the requests of containers on the node is no greater than the node capacity.
The sum of requests includes all containers started by the kubelet, but excludes any
containers started directly by the container runtime, and also excludes any
process running outside of the kubelet's control.
-->
<p>Kubernetes <a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='调度器'>调度器</a>
保证节点上有足够的资源供其上的所有 Pod 使用。
它会检查节点上所有容器的请求的总和不会超过节点的容量。
总的请求包括由 kubelet 启动的所有容器，但不包括由容器运行时直接启动的容器，
也不包括不受 <code>kubelet</code> 控制的其他进程。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you want to explicitly reserve resources for non-Pod processes, follow this tutorial to
[reserve resources for system daemons](/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved).
-->
<p>如果要为非 Pod 进程显式保留资源。
请参考<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved">为系统守护进程预留资源</a>。
</div>
<!--
## Node topology
-->
<h2 id="node-topology">节点拓扑 </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<!--
If you have enabled the `TopologyManager`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/), then
the kubelet can use topology hints when making resource assignment decisions.
See [Control Topology Management Policies on a Node](/docs/tasks/administer-cluster/topology-manager/)
for more information.
-->
<p>如果启用了 <code>TopologyManager</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>，
<code>kubelet</code> 可以在作出资源分配决策时使用拓扑提示。
参考<a href="/zh/docs/tasks/administer-cluster/topology-manager/">控制节点上拓扑管理策略</a>了解详细信息。</p>
<!-- 
## Graceful node shutdown {#graceful-node-shutdown}
-->
<h2 id="graceful-node-shutdown">节点体面关闭</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code>
</div>


<!-- 
The kubelet attempts to detect node system shutdown and terminates pods running on the node.

Kubelet ensures that pods follow the normal
[pod termination process](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)
during the node shutdown.
-->
<p>kubelet 会尝试检测节点系统关闭事件并终止在节点上运行的 Pods。</p>
<p>在节点终止期间，kubelet 保证 Pod 遵从常规的
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">Pod 终止流程</a>。</p>
<!-- 
The graceful node shutdown feature depends on systemd since it takes advantage of
[systemd inhibitor locks](https://www.freedesktop.org/wiki/Software/systemd/inhibit/) to
delay the node shutdown with a given duration.
-->
<p>体面节点关闭特性依赖于 systemd，因为它要利用
<a href="https://www.freedesktop.org/wiki/Software/systemd/inhibit/">systemd 抑制器锁</a>机制，
在给定的期限内延迟节点关闭。</p>
<!--
Graceful node shutdown is controlled with the `GracefulNodeShutdown`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) which is
enabled by default in 1.21.
-->
<p>体面节点关闭特性受 <code>GracefulNodeShutdown</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>控制，
在 1.21 版本中是默认启用的。</p>
<!--
Note that by default, both configuration options described below,
`ShutdownGracePeriod` and `ShutdownGracePeriodCriticalPods` are set to zero,
thus not activating the graceful node shutdown functionality.
To activate the feature, the two kubelet config settings should be configured appropriately and set to non-zero values.
-->
<p>注意，默认情况下，下面描述的两个配置选项，<code>shutdownGracePeriod</code> 和
<code>shutdownGracePeriodCriticalPods</code> 都是被设置为 0 的，因此不会激活体面节点关闭功能。
要激活此功能特性，这两个 kubelet 配置选项要适当配置，并设置为非零值。</p>
<!-- 
During a graceful shutdown, kubelet terminates pods in two phases:

1. Terminate regular pods running on the node.
2. Terminate [critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical) running on the node.
-->
<p>在体面关闭节点过程中，kubelet 分两个阶段来终止 Pod：</p>
<ol>
<li>终止在节点上运行的常规 Pod。</li>
<li>终止在节点上运行的<a href="/zh/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">关键 Pod</a>。</li>
</ol>
<!-- 
Graceful Node Shutdown feature is configured with two [`KubeletConfiguration`](/docs/tasks/administer-cluster/kubelet-config-file/) options:
* `ShutdownGracePeriod`:
  * Specifies the total duration that the node should delay the shutdown by. This is the total grace period for pod termination for both regular and [critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical).
* `ShutdownGracePeriodCriticalPods`:
  * Specifies the duration used to terminate [critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical) during a node shutdown. This value should be less than `ShutdownGracePeriod`.
-->
<p>节点体面关闭的特性对应两个
<a href="/zh/docs/tasks/administer-cluster/kubelet-config-file/"><code>KubeletConfiguration</code></a> 选项：</p>
<ul>
<li><code>shutdownGracePeriod</code>：
<ul>
<li>指定节点应延迟关闭的总持续时间。此时间是 Pod 体面终止的时间总和，不区分常规 Pod
还是<a href="/zh/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">关键 Pod</a>。</li>
</ul>
</li>
<li><code>shutdownGracePeriodCriticalPods</code>：
<ul>
<li>在节点关闭期间指定用于终止<a href="/zh/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">关键 Pod</a>
的持续时间。该值应小于 <code>shutdownGracePeriod</code>。</li>
</ul>
</li>
</ul>
<!--  
For example, if `ShutdownGracePeriod=30s`, and
`ShutdownGracePeriodCriticalPods=10s`, kubelet will delay the node shutdown by
30 seconds. During the shutdown, the first 20 (30-10) seconds would be reserved
for gracefully terminating normal pods, and the last 10 seconds would be
reserved for terminating [critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical).
-->
<p>例如，如果设置了 <code>shutdownGracePeriod=30s</code> 和 <code>shutdownGracePeriodCriticalPods=10s</code>，
则 kubelet 将延迟 30 秒关闭节点。
在关闭期间，将保留前 20（30 - 10）秒用于体面终止常规 Pod，
而保留最后 10 秒用于终止<a href="/zh/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">关键 Pod</a>。</p>
<!--
When pods were evicted during the graceful node shutdown, they are marked as failed.
Running `kubectl get pods` shows the status of the the evicted pods as `Shutdown`.
And `kubectl describe pod` indicates that the pod was evicted because of node shutdown:
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>当 Pod 在正常节点关闭期间被驱逐时，它们会被标记为已经失败（Failed）。
运行 <code>kubectl get pods</code> 时，被驱逐的 Pod 的状态显示为 <code>Shutdown</code>。
并且 <code>kubectl describe pod</code> 表示 Pod 因节点关闭而被驱逐：</p>
<pre><code>Reason:         Terminated
Message:        Pod was terminated in response to imminent node shutdown.
</code></pre>
</div>
<!--
### Pod Priority based graceful node shutdown {#pod-priority-graceful-node-shutdown}
-->
<h3 id="pod-priority-graceful-node-shutdown">基于 Pod 优先级的体面节点关闭   </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code>
</div>


<!--
To provide more flexibility during graceful node shutdown around the ordering
of pods during shutdown, graceful node shutdown honors the PriorityClass for
Pods, provided that you enabled this feature in your cluster. The feature
allows cluster administers to explicitly define the ordering of pods
during graceful node shutdown based on
[priority classes](/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass).
-->
<p>为了在体面节点关闭期间提供更多的灵活性，尤其是处理关闭期间的 Pod 排序问题，
体面节点关闭机制能够关注 Pod 的 PriorityClass 设置，前提是你已经在集群中启用了此功能特性。
此功能特性允许集群管理员基于 Pod
的<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">优先级类（Priority Class）</a>
显式地定义体面节点关闭期间 Pod 的处理顺序。</p>
<!--
The [Graceful Node Shutdown](#graceful-node-shutdown) feature, as described
above, shuts down pods in two phases, non-critical pods, followed by critical
pods. If additional flexibility is needed to explicitly define the ordering of
pods during shutdown in a more granular way, pod priority based graceful
shutdown can be used.
-->
<p>前文所述的<a href="#graceful-node-shutdown">体面节点关闭</a>特性能够分两个阶段关闭 Pod，
首先关闭的是非关键的 Pod，之后再处理关键 Pod。
如果需要显式地以更细粒度定义关闭期间 Pod 的处理顺序，需要一定的灵活度，
这时可以使用基于 Pod 优先级的体面关闭机制。</p>
<!--
When graceful node shutdown honors pod priorities, this makes it possible to do
graceful node shutdown in multiple phases, each phase shutting down a
particular priority class of pods. The kubelet can be configured with the exact
phases and shutdown time per phase.
-->
<p>当体面节点关闭能够处理 Pod 优先级时，体面节点关闭的处理可以分为多个阶段，
每个阶段关闭特定优先级类的 Pod。kubelet 可以被配置为按确切的阶段处理 Pod，
且每个阶段可以独立设置关闭时间。</p>
<!--
Assuming the following custom pod
[priority classes](/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)
in a cluster,
-->
<p>假设集群中存在以下自定义的 Pod
<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">优先级类</a>。</p>
<table>
<thead>
<tr>
<th>Pod 优先级类名称</th>
<th>Pod 优先级类数值</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>custom-class-a</code></td>
<td>100000</td>
</tr>
<tr>
<td><code>custom-class-b</code></td>
<td>10000</td>
</tr>
<tr>
<td><code>custom-class-c</code></td>
<td>1000</td>
</tr>
<tr>
<td><code>regular/unset</code></td>
<td>0</td>
</tr>
</tbody>
</table>
<!--
Within the [kubelet configuration](/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration)
the settings for `shutdownGracePeriodByPodPriority` could look like:
-->
<p>在 <a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration">kubelet 配置</a>中，
<code>shutdownGracePeriodByPodPriority</code> 可能看起来是这样：</p>
<table>
<thead>
<tr>
<th>Pod 优先级类数值</th>
<th>关闭期限</th>
</tr>
</thead>
<tbody>
<tr>
<td>100000</td>
<td>10 秒</td>
</tr>
<tr>
<td>10000</td>
<td>180 秒</td>
</tr>
<tr>
<td>1000</td>
<td>120 秒</td>
</tr>
<tr>
<td>0</td>
<td>60 秒</td>
</tr>
</tbody>
</table>
<!--
The corresponding kubelet config YAML configuration would be:
-->
<p>对应的 kubelet 配置 YAML 将会是：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">shutdownGracePeriodByPodPriority</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">priority</span>:<span style="color:#bbb"> </span><span style="color:#666">100000</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shutdownGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">priority</span>:<span style="color:#bbb"> </span><span style="color:#666">10000</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shutdownGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">180</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">priority</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shutdownGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">120</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">priority</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shutdownGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></code></pre></div><!--
The above table implies that any pod with `priority` value >= 100000 will get
just 10 seconds to stop, any pod with value >= 10000 and < 100000 will get 180
seconds to stop, any pod with value >= 1000 and < 10000 will get 120 seconds to stop.
Finally, all other pods will get 60 seconds to stop.

One doesn't have to specify values corresponding to all of the classes. For
example, you could instead use these settings:
-->
<p>上面的表格表明，所有 <code>priority</code> 值大于等于 100000 的 Pod 会得到 10 秒钟期限停止，
所有 <code>priority</code> 值介于 10000 和 100000 之间的 Pod 会得到 180 秒钟期限停止，
所有 <code>priority</code> 值介于 1000 和 10000 之间的 Pod 会得到 120 秒钟期限停止，
所有其他 Pod 将获得 60 秒的时间停止。</p>
<p>用户不需要为所有的优先级类都设置数值。例如，你也可以使用下面这种配置：</p>
<table>
<thead>
<tr>
<th>Pod 优先级类数值</th>
<th>关闭期限</th>
</tr>
</thead>
<tbody>
<tr>
<td>100000</td>
<td>300 秒</td>
</tr>
<tr>
<td>1000</td>
<td>120 秒</td>
</tr>
<tr>
<td>0</td>
<td>60 秒</td>
</tr>
</tbody>
</table>
<!--
In the above case, the pods with `custom-class-b` will go into the same bucket
as `custom-class-c` for shutdown.

If there are no pods in a particular range, then the kubelet does not wait
for pods in that priority range. Instead, the kubelet immediately skips to the
next priority class value range.
-->
<p>在上面这个场景中，优先级类为 <code>custom-class-b</code> 的 Pod 会与优先级类为 <code>custom-class-c</code>
的 Pod 在关闭时按相同期限处理。</p>
<p>如果在特定的范围内不存在 Pod，则 kubelet 不会等待对应优先级范围的 Pod。
kubelet 会直接跳到下一个优先级数值范围进行处理。</p>
<!--
If this feature is enabled and no configuration is provided, then no ordering
action will be taken.

Using this feature, requires enabling the
`GracefulNodeShutdownBasedOnPodPriority` feature gate, and setting the kubelet
config's `ShutdownGracePeriodByPodPriority` to the desired configuration
containing the pod priority class values and their respective shutdown periods.
-->
<p>如果此功能特性被启用，但没有提供配置数据，则不会出现排序操作。</p>
<p>使用此功能特性需要启用 <code>GracefulNodeShutdownBasedOnPodPriority</code> 特性门控，
并将 kubelet 配置中的 <code>shutdownGracePeriodByPodPriority</code> 设置为期望的配置，
其中包含 Pod 的优先级类数值以及对应的关闭期限。</p>
<!--
Metrics `graceful_shutdown_start_time_seconds` and `graceful_shutdown_end_time_seconds`
are emitted under the kubelet subsystem to monitor node shutdowns.
-->
<p>kubelet 子系统中会生成 <code>graceful_shutdown_start_time_seconds</code> 和
<code>graceful_shutdown_end_time_seconds</code> 度量指标以便监视节点关闭行为。</p>
<!--
## Swap memory management {#swap-memory}
-->
<h2 id="swap-memory">交换内存管理</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>


<!--
Prior to Kubernetes 1.22, nodes did not support the use of swap memory, and a
kubelet would by default fail to start if swap was detected on a node. In 1.22
onwards, swap memory support can be enabled on a per-node basis.
-->
<p>在 Kubernetes 1.22 之前，节点不支持使用交换内存，并且默认情况下，
如果在节点上检测到交换内存配置，kubelet 将无法启动。
在 1.22 以后，可以逐个节点地启用交换内存支持。</p>
<!--
To enable swap on a node, the `NodeSwap` feature gate must be enabled on
the kubelet, and the `--fail-swap-on` command line flag or `failSwapOn`
[configuration setting](/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration)
must be set to false.
-->
<p>要在节点上启用交换内存，必须启用kubelet 的 <code>NodeSwap</code> 特性门控，
同时使用 <code>--fail-swap-on</code> 命令行参数或者将 <code>failSwapOn</code>
<a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration">配置</a>设置为 false。</p>
<!--
A user can also optionally configure `memorySwap.swapBehavior` in order to
specify how a node will use swap memory. For example,
-->
<p>用户还可以选择配置 <code>memorySwap.swapBehavior</code> 以指定节点使用交换内存的方式。例如:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">memorySwap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">swapBehavior</span>:<span style="color:#bbb"> </span>LimitedSwap<span style="color:#bbb">
</span></code></pre></div><!--
The available configuration options for `swapBehavior` are:

- `LimitedSwap`: Kubernetes workloads are limited in how much swap they can
  use. Workloads on the node not managed by Kubernetes can still swap.
- `UnlimitedSwap`: Kubernetes workloads can use as much swap memory as they
  request, up to the system limit.
-->
<p>可用的 <code>swapBehavior</code> 的配置选项有：</p>
<ul>
<li><code>LimitedSwap</code>：Kubernetes 工作负载的交换内存会受限制。
不受 Kubernetes 管理的节点上的工作负载仍然可以交换。</li>
<li><code>UnlimitedSwap</code>：Kubernetes 工作负载可以使用尽可能多的交换内存请求，
一直到达到系统限制为止。</li>
</ul>
<!--
If configuration for `memorySwap` is not specified and the feature gate is
enabled, by default the kubelet will apply the same behaviour as the
`LimitedSwap` setting.

The behaviour of the `LimitedSwap` setting depends if the node is running with
v1 or v2 of control groups (also known as "cgroups"):
-->
<p>如果启用了特性门控但是未指定 <code>memorySwap</code> 的配置，默认情况下 kubelet 将使用
<code>LimitedSwap</code> 设置。</p>
<p><code>LimitedSwap</code> 这种设置的行为取决于节点运行的是 v1 还是 v2 的控制组（也就是 <code>cgroups</code>）：</p>
<!--
- **cgroupsv1:** Kubernetes workloads can use any combination of memory and
  swap, up to the pod's memory limit, if set.
- **cgroupsv2:** Kubernetes workloads cannot use swap memory.
-->
<ul>
<li><strong>cgroupsv1:</strong> Kubernetes 工作负载可以使用内存和交换，上限为 Pod 的内存限制值（如果设置了的话）。</li>
<li><strong>cgroupsv2:</strong> Kubernetes 工作负载不能使用交换内存。</li>
</ul>
<!--
For more information, and to assist with testing and provide feedback, please
see [KEP-2400](https://github.com/kubernetes/enhancements/issues/2400) and its
[design proposal](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md).
-->
<p>如需更多信息以及协助测试和提供反馈，请参见
<a href="https://github.com/kubernetes/enhancements/issues/2400">KEP-2400</a>
及其<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md">设计提案</a>。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about the [components](/docs/concepts/overview/components/#node-components) that make up a node.
* Read the [API definition for Node](/docs/reference/generated/kubernetes-api/v1.23/#node-v1-core).
* Read the [Node](https://git.k8s.io/community/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node)
  section of the architecture design document.
* Read about [taints and tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration/).
-->
<ul>
<li>进一步了解节点<a href="/zh/docs/concepts/overview/components/#node-components">组件</a>。</li>
<li>阅读 <a href="/docs/reference/generated/kubernetes-api/v1.23/#node-v1-core">Node 的 API 定义</a>。</li>
<li>阅读架构设计文档中有关
<a href="https://git.k8s.io/community/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node">Node</a>
的章节。</li>
<li>了解<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">污点和容忍度</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c0251def6da29b30afebfb04549f1703">2.2 - 控制面到节点通信</h1>
    
	<!--
title: Control Plane-Node Communication
content_type: concept
weight: 20
aliases:
- master-node-communication
-->
<!-- overview -->
<!--
This document catalogs the communication paths between the control plane (apiserver) and the Kubernetes cluster. The intent is to allow users to customize their installation to harden the network configuration such that the cluster can be run on an untrusted network (or on fully public IPs on a cloud provider).
-->
<p>本文列举控制面节点（确切说是 API 服务器）和 Kubernetes 集群之间的通信路径。
目的是为了让用户能够自定义他们的安装，以实现对网络配置的加固，使得集群能够在不可信的网络上
（或者在一个云服务商完全公开的 IP 上）运行。</p>
<!-- body -->
<!--
## Node to Control Plane
Kubernetes has a "hub-and-spoke" API pattern. All API usage from nodes (or the pods they run) terminate at the apiserver. None of the other control plane components are designed to expose remote services. The apiserver is configured to listen for remote connections on a secure HTTPS port (typically 443) with one or more forms of client [authentication](/docs/reference/access-authn-authz/authentication/) enabled.
One or more forms of [authorization](/docs/reference/access-authn-authz/authorization/) should be enabled, especially if [anonymous requests](/docs/reference/access-authn-authz/authentication/#anonymous-requests) or [service account tokens](/docs/reference/access-authn-authz/authentication/#service-account-tokens) are allowed.
-->
<h2 id="节点到控制面">节点到控制面</h2>
<p>Kubernetes 采用的是中心辐射型（Hub-and-Spoke）API 模式。
所有从集群（或所运行的 Pods）发出的 API 调用都终止于 API 服务器。
其它控制面组件都没有被设计为可暴露远程服务。
API 服务器被配置为在一个安全的 HTTPS 端口（通常为 443）上监听远程连接请求，
并启用一种或多种形式的客户端<a href="/zh/docs/reference/access-authn-authz/authentication/">身份认证</a>机制。
一种或多种客户端<a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权机制</a>应该被启用，
特别是在允许使用<a href="/zh/docs/reference/access-authn-authz/authentication/#anonymous-requests">匿名请求</a>
或<a href="/zh/docs/reference/access-authn-authz/authentication/#service-account-tokens">服务账号令牌</a>的时候。</p>
<!--
Nodes should be provisioned with the public root certificate for the cluster such that they can connect securely to the apiserver along with valid client credentials. A good approach is that the client credentials provided to the kubelet are in the form of a client certificate. See [kubelet TLS bootstrapping](/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/) for automated provisioning of kubelet client certificates.
-->
<p>应该使用集群的公共根证书开通节点，这样它们就能够基于有效的客户端凭据安全地连接 API 服务器。
一种好的方法是以客户端证书的形式将客户端凭据提供给 kubelet。
请查看 <a href="/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">kubelet TLS 启动引导</a>
以了解如何自动提供 kubelet 客户端证书。</p>
<!--
Pods that wish to connect to the apiserver can do so securely by leveraging a service account so that Kubernetes will automatically inject the public root certificate and a valid bearer token into the pod when it is instantiated.
The `kubernetes` service (in `default` namespace) is configured with a virtual IP address that is redirected (via kube-proxy) to the HTTPS endpoint on the apiserver.

The control plane components also communicate with the cluster apiserver over the secure port.
-->
<p>想要连接到 API 服务器的 Pod 可以使用服务账号安全地进行连接。
当 Pod 被实例化时，Kubernetes 自动把公共根证书和一个有效的持有者令牌注入到 Pod 里。
<code>kubernetes</code> 服务（位于 <code>default</code> 名字空间中）配置了一个虚拟 IP 地址，用于（通过 kube-proxy）转发
请求到 API 服务器的 HTTPS 末端。</p>
<p>控制面组件也通过安全端口与集群的 API 服务器通信。</p>
<!--
As a result, the default operating mode for connections from the nodes and pods running on the nodes to the control plane is secured by default and can run over untrusted and/or public networks.
-->
<p>这样，从集群节点和节点上运行的 Pod 到控制面的连接的缺省操作模式即是安全的，
能够在不可信的网络或公网上运行。</p>
<!--
## Control Plane to node

There are two primary communication paths from the control plane (apiserver) to the nodes. The first is from the apiserver to the kubelet process which runs on each node in the cluster. The second is from the apiserver to any node, pod, or service through the apiserver's proxy functionality.
-->
<h2 id="控制面到节点">控制面到节点</h2>
<p>从控制面（API 服务器）到节点有两种主要的通信路径。
第一种是从 API 服务器到集群中每个节点上运行的 kubelet 进程。
第二种是从 API 服务器通过它的代理功能连接到任何节点、Pod 或者服务。</p>
<!--
### apiserver to kubelet

The connections from the apiserver to the kubelet are used for:

* Fetching logs for pods.
* Attaching (through kubectl) to running pods.
* Providing the kubelet's port-forwarding functionality.

These connections terminate at the kubelet's HTTPS endpoint. By default, the apiserver does not verify the kubelet's serving certificate, which makes the connection subject to man-in-the-middle attacks and **unsafe** to run over untrusted and/or public networks.
-->
<h3 id="api-服务器到-kubelet">API 服务器到 kubelet</h3>
<p>从 API 服务器到 kubelet 的连接用于：</p>
<ul>
<li>获取 Pod 日志</li>
<li>挂接（通过 kubectl）到运行中的 Pod</li>
<li>提供 kubelet 的端口转发功能。</li>
</ul>
<p>这些连接终止于 kubelet 的 HTTPS 末端。
默认情况下，API 服务器不检查 kubelet 的服务证书。这使得此类连接容易受到中间人攻击，
在非受信网络或公开网络上运行也是 <strong>不安全的</strong>。</p>
<!--
To verify this connection, use the `--kubelet-certificate-authority` flag to provide the apiserver with a root certificate bundle to use to verify the kubelet's serving certificate.

If that is not possible, use [SSH tunneling](/docs/concepts/architecture/master-node-communication/#ssh-tunnels) between the apiserver and kubelet if required to avoid connecting over an
untrusted or public network.

Finally, [Kubelet authentication and/or authorization](/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/) should be enabled to secure the kubelet API.
-->
<p>为了对这个连接进行认证，使用 <code>--kubelet-certificate-authority</code> 标志给 API
服务器提供一个根证书包，用于 kubelet 的服务证书。</p>
<p>如果无法实现这点，又要求避免在非受信网络或公共网络上进行连接，可在 API 服务器和
kubelet 之间使用 <a href="#ssh-tunnels">SSH 隧道</a>。</p>
<p>最后，应该启用
<a href="/zh/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/">kubelet 用户认证和/或鉴权</a>
来保护 kubelet API。</p>
<!--
### apiserver to nodes, pods, and services

The connections from the apiserver to a node, pod, or service default to plain HTTP connections and are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS connection by prefixing `https:` to the node, pod, or service name in the API URL, but they will not validate the certificate provided by the HTTPS endpoint nor provide client credentials so while the connection will be encrypted, it will not provide any guarantees of integrity. These connections **are not currently safe** to run over untrusted and/or public networks.
-->
<h3 id="api-服务器到节点-pod-和服务">API 服务器到节点、Pod 和服务</h3>
<p>从 API 服务器到节点、Pod 或服务的连接默认为纯 HTTP 方式，因此既没有认证，也没有加密。
这些连接可通过给 API URL 中的节点、Pod 或服务名称添加前缀 <code>https:</code> 来运行在安全的 HTTPS 连接上。
不过这些连接既不会验证 HTTPS 末端提供的证书，也不会提供客户端证书。
因此，虽然连接是加密的，仍无法提供任何完整性保证。
这些连接 <strong>目前还不能安全地</strong> 在非受信网络或公共网络上运行。</p>
<!--
### SSH tunnels

Kubernetes supports SSH tunnels to protect the control plane to nodes communication paths. In this configuration, the apiserver initiates an SSH tunnel to each node in the cluster (connecting to the ssh server listening on port 22) and passes all traffic destined for a kubelet, node, pod, or service through the tunnel.
This tunnel ensures that the traffic is not exposed outside of the network in which the nodes are running.

SSH tunnels are currently deprecated so you shouldn't opt to use them unless you know what you are doing. The Konnectivity service is a replacement for this communication channel.
-->
<h3 id="ssh-tunnels">SSH 隧道</h3>
<p>Kubernetes 支持使用 SSH 隧道来保护从控制面到节点的通信路径。在这种配置下，API
服务器建立一个到集群中各节点的 SSH 隧道（连接到在 22 端口监听的 SSH 服务）
并通过这个隧道传输所有到 kubelet、节点、Pod 或服务的请求。
这一隧道保证通信不会被暴露到集群节点所运行的网络之外。</p>
<p>SSH 隧道目前已被废弃。除非你了解个中细节，否则不应使用。
Konnectivity 服务是对此通信通道的替代品。</p>
<!--
### Konnectivity service






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>



As a replacement to the SSH tunnels, the Konnectivity service provides TCP level proxy for the control plane to cluster communication. The Konnectivity service consists of two parts: the Konnectivity server in the control plane network and the Konnectivity agents in the nodes network. The Konnectivity agents initiate connections to the Konnectivity server and maintain the network connections.
After enabling the Konnectivity service, all control plane to nodes traffic goes through these connections.

Follow the [Konnectivity service task](/docs/tasks/extend-kubernetes/setup-konnectivity/) to set up the Konnectivity service in your cluster.
-->
<h3 id="konnectivity-服务">Konnectivity 服务</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<p>作为 SSH 隧道的替代方案，Konnectivity 服务提供 TCP 层的代理，以便支持从控制面到集群的通信。
Konnectivity 服务包含两个部分：Konnectivity 服务器和 Konnectivity 代理，分别运行在
控制面网络和节点网络中。Konnectivity 代理建立并维持到 Konnectivity 服务器的网络连接。
启用 Konnectivity 服务之后，所有控制面到节点的通信都通过这些连接传输。</p>
<p>请浏览 <a href="/zh/docs/tasks/extend-kubernetes/setup-konnectivity/">Konnectivity 服务任务</a>
在你的集群中配置 Konnectivity 服务。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ca8819042a505291540e831283da66df">2.3 - 控制器</h1>
    
	<!-- overview -->
<!--
In robotics and automation, a _control loop_ is
a non-terminating loop that regulates the state of a system.

Here is one example of a control loop: a thermostat in a room.

When you set the temperature, that's telling the thermostat
about your *desired state*. The actual room temperature is the
*current state*. The thermostat acts to bring the current state
closer to the desired state, by turning equipment on or off.
-->
<p>在机器人技术和自动化领域，控制回路（Control Loop）是一个非终止回路，用于调节系统状态。</p>
<p>这是一个控制环的例子：房间里的温度自动调节器。</p>
<p>当你设置了温度，告诉了温度自动调节器你的<em>期望状态（Desired State）</em>。
房间的实际温度是<em>当前状态（Current State）</em>。
通过对设备的开关控制，温度自动调节器让其当前状态接近期望状态。</p>
在 Kubernetes 中，控制器通过监控<a class='glossary-tooltip' title='集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cluster' target='_blank' aria-label='集群'>集群</a>
的公共状态，并致力于将当前状态转变为期望的状态。
<!-- body -->
<!--
## Controller pattern

A controller tracks at least one Kubernetes resource type.
These [objects](/docs/concepts/overview/working-with-objects/kubernetes-objects/)
have a spec field that represents the desired state. The
controller(s) for that resource are responsible for making the current
state come closer to that desired state.

The controller might carry the action out itself; more commonly, in Kubernetes,
a controller will send messages to the
<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a> that have
useful side effects. You'll see examples of this below.


-->
<h2 id="controller-pattern">控制器模式</h2>
<p>一个控制器至少追踪一种类型的 Kubernetes 资源。这些
<a href="/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/">对象</a>
有一个代表期望状态的 <code>spec</code> 字段。
该资源的控制器负责确保其当前状态接近期望状态。</p>
<p>控制器可能会自行执行操作；在 Kubernetes 中更常见的是一个控制器会发送信息给
<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>，这会有副作用。
具体可参看后文的例子。</p>

<!--
### Control via API server

The <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a> controller is an example of a
Kubernetes built-in controller. Built-in controllers manage state by
interacting with the cluster API server.

Job is a Kubernetes resource that runs a
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>, or perhaps several Pods, to carry out
a task and then stop.

(Once [scheduled](/docs/concepts/scheduling/), Pod objects become part of the
desired state for a kubelet).

When the Job controller sees a new task it makes sure that, somewhere
in your cluster, the kubelets on a set of Nodes are running the right
number of Pods to get the work done.
The Job controller does not run any Pods or containers
itself. Instead, the Job controller tells the API server to create or remove
Pods.
Other components in the
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>
act on the new information (there are new Pods to schedule and run),
and eventually the work is done.
-->
<h3 id="control-via-API-server">通过 API 服务器来控制</h3>
<p><a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a> 控制器是一个 Kubernetes 内置控制器的例子。
内置控制器通过和集群 API 服务器交互来管理状态。</p>
<p>Job 是一种 Kubernetes 资源，它运行一个或者多个 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>，
来执行一个任务然后停止。
（一旦<a href="/zh/docs/concepts/scheduling-eviction/">被调度了</a>，对 <code>kubelet</code> 来说 Pod
对象就会变成了期望状态的一部分）。</p>
<p>在集群中，当 Job 控制器拿到新任务时，它会保证一组 Node 节点上的 <code>kubelet</code>
可以运行正确数量的 Pod 来完成工作。
Job 控制器不会自己运行任何的 Pod 或者容器。Job 控制器是通知 API 服务器来创建或者移除 Pod。
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a>中的其它组件
根据新的消息作出反应（调度并运行新 Pod）并且最终完成工作。</p>
<!--
After you create a new Job, the desired state is for that Job to be completed.
The Job controller makes the current state for that Job be nearer to your
desired state: creating Pods that do the work you wanted for that Job, so that
the Job is closer to completion.

Controllers also update the objects that configure them.
For example: once the work is done for a Job, the Job controller
updates that Job object to mark it `Finished`.

(This is a bit like how some thermostats turn a light off to
indicate that your room is now at the temperature you set).
-->
<p>创建新 Job 后，所期望的状态就是完成这个 Job。Job 控制器会让 Job 的当前状态不断接近期望状态：创建为 Job 要完成工作所需要的 Pod，使 Job 的状态接近完成。</p>
<p>控制器也会更新配置对象。例如：一旦 Job 的工作完成了，Job 控制器会更新 Job 对象的状态为 <code>Finished</code>。</p>
<p>（这有点像温度自动调节器关闭了一个灯，以此来告诉你房间的温度现在到你设定的值了）。</p>
<!--
### Direct control

By contrast with Job, some controllers need to make changes to
things outside of your cluster.

For example, if you use a control loop to make sure there
are enough <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='Nodes'>Nodes</a>
in your cluster, then that controller needs something outside the
current cluster to set up new Nodes when needed.

Controllers that interact with external state find their desired state from
the API server, then communicate directly with an external system to bring
the current state closer in line.

(There actually is a [controller](https://github.com/kubernetes/autoscaler/)
that horizontally scales the nodes in your cluster.)
-->
<h3 id="direct-control">直接控制</h3>
<p>相比 Job 控制器，有些控制器需要对集群外的一些东西进行修改。</p>
<p>例如，如果你使用一个控制回路来保证集群中有足够的
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>，那么控制器就需要当前集群外的
一些服务在需要时创建新节点。</p>
<p>和外部状态交互的控制器从 API 服务器获取到它想要的状态，然后直接和外部系统进行通信
并使当前状态更接近期望状态。</p>
<p>（实际上有一个<a href="https://github.com/kubernetes/autoscaler/">控制器</a>
可以水平地扩展集群中的节点。）</p>
<!--
The important point here is that the controller makes some change to bring about
your desired state, and then reports current state back to your cluster's API server.
Other control loops can observe that reported data and take their own actions.
-->
<p>这里，很重要的一点是，控制器做出了一些变更以使得事物更接近你的期望状态，
之后将当前状态报告给集群的 API 服务器。
其他控制回路可以观测到所汇报的数据的这种变化并采取其各自的行动。</p>
<!--
In the thermostat example, if the room is very cold then a different controller
might also turn on a frost protection heater. With Kubernetes clusters, the control
plane indirectly works with IP address management tools, storage services,
cloud provider APIs, and other services by
[extending Kubernetes](/docs/concepts/extend-kubernetes/) to implement that.
-->
<p>在温度计的例子中，如果房间很冷，那么某个控制器可能还会启动一个防冻加热器。
就 Kubernetes 集群而言，控制面间接地与 IP 地址管理工具、存储服务、云驱动
APIs 以及其他服务协作，通过<a href="/zh/docs/concepts/extend-kubernetes/">扩展 Kubernetes</a>
来实现这点。</p>
<!--
## Desired versus current state {#desired-vs-current}

Kubernetes takes a cloud-native view of systems, and is able to handle
constant change.

Your cluster could be changing at any point as work happens and
control loops automatically fix failures. This means that,
potentially, your cluster never reaches a stable state.

As long as the controllers for your cluster are running and able to make
useful changes, it doesn't matter if the overall state is stable or not.
-->
<h2 id="desired-vs-current">期望状态与当前状态</h2>
<p>Kubernetes 采用了系统的云原生视图，并且可以处理持续的变化。</p>
<p>在任务执行时，集群随时都可能被修改，并且控制回路会自动修复故障。
这意味着很可能集群永远不会达到稳定状态。</p>
<p>只要集群中的控制器在运行并且进行有效的修改，整体状态的稳定与否是无关紧要的。</p>
<!--
## Design

As a tenet of its design, Kubernetes uses lots of controllers that each manage
a particular aspect of cluster state. Most commonly, a particular control loop
(controller) uses one kind of resource as its desired state, and has a different
kind of resource that it manages to make that desired state happen.

It's useful to have simple controllers rather than one, monolithic set of control
loops that are interlinked. Controllers can fail, so Kubernetes is designed to
allow for that.

-->
<h2 id="design">设计</h2>
<p>作为设计原则之一，Kubernetes 使用了很多控制器，每个控制器管理集群状态的一个特定方面。
最常见的一个特定的控制器使用一种类型的资源作为它的期望状态，
控制器管理控制另外一种类型的资源向它的期望状态演化。</p>
<p>使用简单的控制器而不是一组相互连接的单体控制回路是很有用的。
控制器会失败，所以 Kubernetes 的设计正是考虑到了这一点。</p>
<!--
There can be several controllers that create or update the same kind of object.
Behind the scenes, Kubernetes controllers make sure that they only pay attention
to the resources linked to their controlling resource.

For example, you can have Deployments and Jobs; these both create Pods.
The Job controller does not delete the Pods that your Deployment created,
because there is information (<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='labels'>labels</a>)
the controllers can use to tell those Pods apart.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>可以有多个控制器来创建或者更新相同类型的对象。
在后台，Kubernetes 控制器确保它们只关心与其控制资源相关联的资源。</p>
<p>例如，你可以创建 Deployment 和 Job；它们都可以创建 Pod。
Job 控制器不会删除 Deployment 所创建的 Pod，因为有信息
（<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>）让控制器可以区分这些 Pod。</p>

</div>
<!--
## Ways of running controllers {#running-controllers}

Kubernetes comes with a set of built-in controllers that run inside
the <a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>. These
built-in controllers provide important core behaviors.

The Deployment controller and Job controller are examples of controllers that
come as part of Kubernetes itself (“built-in” controllers).
Kubernetes lets you run a resilient control plane, so that if any of the built-in
controllers were to fail, another part of the control plane will take over the work.

You can find controllers that run outside the control plane, to extend Kubernetes.
Or, if you want, you can write a new controller yourself.
You can run your own controller as a set of Pods,
or externally to Kubernetes. What fits best will depend on what that particular
controller does.
-->
<h2 id="running-controllers">运行控制器的方式</h2>
<p>Kubernetes 内置一组控制器，运行在 <a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a> 内。
这些内置的控制器提供了重要的核心功能。</p>
<p>Deployment 控制器和 Job 控制器是 Kubernetes 内置控制器的典型例子。
Kubernetes 允许你运行一个稳定的控制平面，这样即使某些内置控制器失败了，
控制平面的其他部分会接替它们的工作。</p>
<p>你会遇到某些控制器运行在控制面之外，用以扩展 Kubernetes。
或者，如果你愿意，你也可以自己编写新控制器。
你可以以一组 Pod 来运行你的控制器，或者运行在 Kubernetes 之外。
最合适的方案取决于控制器所要执行的功能是什么。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about the [Kubernetes control plane](/docs/concepts/overview/components/#control-plane-components)
* Discover some of the basic [Kubernetes objects](/docs/concepts/overview/working-with-objects/kubernetes-objects/)
* Learn more about the [Kubernetes API](/docs/concepts/overview/kubernetes-api/)
* If you want to write your own controller, see [Extension Patterns](/docs/concepts/extend-kubernetes/extend-cluster/#extension-patterns) in Extending Kubernetes.
-->
<ul>
<li>阅读 <a href="/zh/docs/concepts/overview/components/#control-plane-components">Kubernetes 控制平面组件</a></li>
<li>了解 <a href="/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/">Kubernetes 对象</a>
的一些基本知识</li>
<li>进一步学习 <a href="/zh/docs/concepts/overview/kubernetes-api/">Kubernetes API</a></li>
<li>如果你想编写自己的控制器，请看 Kubernetes 的
<a href="/zh/docs/concepts/extend-kubernetes/#extension-patterns">扩展模式</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bc804b02614d67025b4c788f1ca87fbc">2.4 - 云控制器管理器</h1>
    
	<!--
title: Cloud Controller Manager
content_type: concept
weight: 40
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code>
</div>


<!--
Cloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds.
Kubernetes believes in automated, API-driven infrastructure without tight coupling between
components.
-->
<p>使用云基础设施技术，你可以在公有云、私有云或者混合云环境中运行 Kubernetes。
Kubernetes 的信条是基于自动化的、API 驱动的基础设施，同时避免组件间紧密耦合。</p>
<!--
title: Cloud Controller Manager
id: cloud-controller-manager
date: 2018-04-12
full_link: /docs/concepts/architecture/cloud-controller/
short_description: >
  Control plane component that integrates Kubernetes with third-party cloud providers.

aka: 
tags:
- core-object
- architecture
- operation
-->
<!--
 A Kubernetes <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.
-->
<p><p>组件 cloud-controller-manager 是指云控制器管理器， 云控制器管理器是指嵌入特定云的控制逻辑的
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>组件。
云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上，
并将与该云平台交互的组件同与你的集群交互的组件分离开来。</p></p>
<!--
By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.
-->
<p>通过分离 Kubernetes 和底层云基础设置之间的互操作性逻辑，
云控制器管理器组件使云提供商能够以不同于 Kubernetes 主项目的
步调发布新特征。</p>
<!--
The cloud-controller-manager is structured using a plugin
mechanism that allows different cloud providers to integrate their platforms with Kubernetes.
-->
<p><code>cloud-controller-manager</code> 组件是基于一种插件机制来构造的，
这种机制使得不同的云厂商都能将其平台与 Kubernetes 集成。</p>
<!-- body -->
<!--
## Design

![Kubernetes components](/images/docs/components-of-kubernetes.svg)

The cloud controller manager runs in the control plane as a replicated set of processes
(usually, these are containers in Pods). Each cloud-controller-manager implements
multiple <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controllers'>controllers</a> in a single
process.
-->
<h2 id="design">设计 </h2>
<p><img src="/images/docs/components-of-kubernetes.svg" alt="Kubernetes 组件"></p>
<p>云控制器管理器以一组多副本的进程集合的形式运行在控制面中，通常表现为 Pod
中的容器。每个 <code>cloud-controller-manager</code> 在同一进程中实现多个
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>。</p>
<!--
You can also run the cloud controller manager as a Kubernetes
<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='addon'>addon</a> rather than as part
of the control plane.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你也可以用 Kubernetes <a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='插件'>插件</a>
的形式而不是控制面中的一部分来运行云控制器管理器。
</div>
<!--
## Cloud controller manager functions {#functions-of-the-ccm}

The controllers inside the cloud controller manager include:
-->
<h2 id="functions-of-the-ccm">云控制器管理器的功能</h2>
<p>云控制器管理器中的控制器包括：</p>
<!--
### Node controller

The node controller is responsible for updating <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='Node'>Node</a> objects
when new servers are created in your cloud infrastructure. The node controller obtains information about the
hosts running inside your tenancy with the cloud provider. The node controller performs the following functions:
-->
<h3 id="node-controller">节点控制器  </h3>
<p>节点控制器负责在云基础设施中创建了新服务器时为之 更新
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点（Node）'>节点（Node）</a>对象。
节点控制器从云提供商获取当前租户中主机的信息。节点控制器执行以下功能：</p>
<!--
1. Update a Node object with the corresponding server's unique identifier obtained from the cloud provider API.
2. Annotating and labelling the Node object with cloud-specific information, such as the region the node
   is deployed into and the resources (CPU, memory, etc) that it has available.
3. Obtain the node's hostname and network addresses.
4. Verifying the node's health. In case a node becomes unresponsive, this controller checks with
   your cloud provider's API to see if the server has been deactivated / deleted / terminated.
   If the node has been deleted from the cloud, the controller deletes the Node object from your Kubernetes
   cluster.
-->
<ol>
<li>使用从云平台 API 获取的对应服务器的唯一标识符更新 Node 对象；</li>
<li>利用特定云平台的信息为 Node 对象添加注解和标签，例如节点所在的
区域（Region）和所具有的资源（CPU、内存等等）；</li>
<li>获取节点的网络地址和主机名；</li>
<li>检查节点的健康状况。如果节点无响应，控制器通过云平台 API 查看该节点是否
已从云中禁用、删除或终止。如果节点已从云中删除，则控制器从 Kubernetes 集群
中删除 Node 对象。</li>
</ol>
<!--
Some cloud provider implementations split this into a node controller and a separate node
lifecycle controller.
-->
<p>某些云驱动实现中，这些任务被划分到一个节点控制器和一个节点生命周期控制器中。</p>
<!--
### Route controller

The route controller is responsible for configuring routes in the cloud
appropriately so that containers on different nodes in your Kubernetes
cluster can communicate with each other.

Depending on the cloud provider, the route controller might also allocate blocks
of IP addresses for the Pod network.
-->
<h3 id="route-controller">路由控制器  </h3>
<p>Route 控制器负责适当地配置云平台中的路由，以便 Kubernetes 集群中不同节点上的
容器之间可以相互通信。</p>
<p>取决于云驱动本身，路由控制器可能也会为 Pod 网络分配 IP 地址块。</p>
<!--
### Service controller

<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a> integrate with cloud
infrastructure components such as managed load balancers, IP addresses, network
packet filtering, and target health checking. The service controller interacts with your
cloud provider's APIs to set up load balancers and other infrastructure components
when you declare a Service resource that requires them.
-->
<h3 id="service-controller">服务控制器  </h3>
<p><a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a>与受控的负载均衡器、
IP 地址、网络包过滤、目标健康检查等云基础设施组件集成。
服务控制器与云驱动的 API 交互，以配置负载均衡器和其他基础设施组件。
你所创建的 Service 资源会需要这些组件服务。</p>
<!--
## Authorization

This section breaks down the access that the cloud controller manager requires
on various API objects, in order to perform its operations.
-->
<h2 id="authorization">鉴权  </h2>
<p>本节分别讲述云控制器管理器为了完成自身工作而产生的对各类 API 对象的访问需求。</p>
<!--
### Node controller {#authorization-node-controller}

The Node controller only works with Node objects. It requires full access
to read and modify Node objects.
-->
<h3 id="authorization-node-controller">节点控制器 </h3>
<p>节点控制器只操作 Node 对象。它需要读取和修改 Node 对象的完全访问权限。</p>
<p><code>v1/Node</code>:</p>
<ul>
<li>Get</li>
<li>List</li>
<li>Create</li>
<li>Update</li>
<li>Patch</li>
<li>Watch</li>
<li>Delete</li>
</ul>
<!--
### Route controller {#authorization-route-controller}

The route controller listens to Node object creation and configures
routes appropriately. It requires Get access to Node objects.
-->
<h3 id="authorization-route-controller">路由控制器</h3>
<p>路由控制器会监听 Node 对象的创建事件，并据此配置路由设施。
它需要读取 Node 对象的 Get 权限。</p>
<p><code>v1/Node</code>:</p>
<ul>
<li>Get</li>
</ul>
<!--
### Service controller {#authorization-service-controller}

The service controller listens to Service object Create, Update and Delete events and then configures Endpoints for those Services appropriately.

To access Services, it requires List, and Watch access. To update Services, it requires Patch and Update access.

To set up Endpoints resources for the Services, it requires access to Create, List, Get, Watch, and Update.
-->
<h3 id="authorization-service-controller">服务控制器</h3>
<p>服务控制器监测 Service 对象的 Create、Update 和 Delete 事件，并配置
对应服务的 Endpoints 对象。
为了访问 Service 对象，它需要 List、Watch 访问权限；为了更新 Service 对象
它需要 Patch 和 Update 访问权限。
为了能够配置 Service 对应的 Endpoints 资源，它需要 Create、List、Get、Watch
和 Update 等访问权限。</p>
<p><code>v1/Service</code>:</p>
<ul>
<li>List</li>
<li>Get</li>
<li>Watch</li>
<li>Patch</li>
<li>Update</li>
</ul>
<!--
### Others {#authorization-miscellaneous}

The implementation of the core of the cloud controller manager requires access to create Event objects, and to ensure secure operation, it requires access to create ServiceAccounts.

`v1/Event`:

- Create
- Patch
- Update

`v1/ServiceAccount`:

- Create

The <a class='glossary-tooltip' title='管理授权决策，允许管理员通过 Kubernetes API 动态配置访问策略。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/access-authn-authz/rbac/' target='_blank' aria-label='RBAC'>RBAC</a> ClusterRole for the cloud
controller manager looks like:
-->
<h3 id="authorization-miscellaneous">其他 </h3>
<p>云控制器管理器的实现中，其核心部分需要创建 Event 对象的访问权限以及
创建 ServiceAccount 资源以保证操作安全性的权限。</p>
<p><code>v1/Event</code>:</p>
<ul>
<li>Create</li>
<li>Patch</li>
<li>Update</li>
</ul>
<p><code>v1/ServiceAccount</code>:</p>
<ul>
<li>Create</li>
</ul>
<p>用于云控制器管理器 <a class='glossary-tooltip' title='管理授权决策，允许管理员通过 Kubernetes API 动态配置访问策略。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/access-authn-authz/rbac/' target='_blank' aria-label='RBAC'>RBAC</a>
的 ClusterRole 如下例所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- events<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- create<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- patch<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- update<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- nodes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#b44">&#39;*&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- nodes/status<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- patch<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- services<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- list<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- patch<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- update<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- watch<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- serviceaccounts<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- create<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- persistentvolumes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- get<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- list<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- update<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- watch<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- endpoints<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- create<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- get<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- list<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- watch<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- update<span style="color:#bbb">
</span></code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
[Cloud Controller Manager Administration](/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager)
has instructions on running and managing the cloud controller manager.

To upgrade a HA control plane to use the cloud controller manager, see [Migrate Replicated Control Plane To Use Cloud Controller Manager](/docs/tasks/administer-cluster/controller-manager-leader-migration/).

Want to know how to implement your own cloud controller manager, or extend an existing project?
-->
<p><a href="/zh/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager">云控制器管理器的管理</a>
给出了运行和管理云控制器管理器的指南。</p>
<p>要升级 HA 控制平面以使用云控制器管理器，请参见 <a href="/zh/docs/tasks/administer-cluster/controller-manager-leader-migration/">将复制的控制平面迁移以使用云控制器管理器</a></p>
<p>想要了解如何实现自己的云控制器管理器，或者对现有项目进行扩展么？</p>
<!--
The cloud controller manager uses Go interfaces to allow implementations from any cloud to be plugged in. Specifically, it uses the `CloudProvider` interface defined in [`cloud.go`](https://github.com/kubernetes/cloud-provider/blob/release-1.21/cloud.go#L42-L69) from [kubernetes/cloud-provider](https://github.com/kubernetes/cloud-provider).
-->
<p>云控制器管理器使用 Go 语言的接口，从而使得针对各种云平台的具体实现都可以接入。
其中使用了在 <a href="https://github.com/kubernetes/cloud-provider">kubernetes/cloud-provider</a>
项目中 <a href="https://github.com/kubernetes/cloud-provider/blob/release-1.21/cloud.go#L42-L69"><code>cloud.go</code></a>
文件所定义的 <code>CloudProvider</code> 接口。</p>
<!--
The implementation of the shared controllers highlighted in this document (Node, Route, and Service), and some scaffolding along with the shared cloudprovider interface, is part of the Kubernetes core. Implementations specific to cloud providers are outside the core of Kubernetes and implement the `CloudProvider` interface.

For more information about developing plugins, see [Developing Cloud Controller Manager](/docs/tasks/administer-cluster/developing-cloud-controller-manager/).
-->
<p>本文中列举的共享控制器（节点控制器、路由控制器和服务控制器等）的实现以及
其他一些生成具有 CloudProvider 接口的框架的代码，都是 Kubernetes 的核心代码。
特定于云驱动的实现虽不是 Kubernetes 核心成分，仍要实现 <code>CloudProvider</code> 接口。</p>
<p>关于如何开发插件的详细信息，可参考
<a href="/zh/docs/tasks/administer-cluster/developing-cloud-controller-manager/">开发云控制器管理器</a>
文档。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-44a2e2e592af0846101e970aff9243e5">2.5 - 垃圾收集</h1>
    
	<!--
title: Garbage Collection
content_type: concept
weight: 50
-->
<!-- overview -->
<!--
垃圾收集是 Kubernetes 用于清理集群资源的各种机制的统称。 This
allows the clean up of resources like the following:
-->
<p>垃圾收集是 Kubernetes 用于清理集群资源的各种机制的统称。
垃圾收集允许系统清理如下资源：</p>
<!--
* [Failed pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)
* [Completed Jobs](/docs/concepts/workloads/controllers/ttlafterfinished/)
* [Objects without owner references](#owners-dependents)
* [Unused containers and container images](#containers-images)
* [Dynamically provisioned PersistentVolumes with a StorageClass reclaim policy of Delete](/docs/concepts/storage/persistent-volumes/#delete)
* [Stale or expired CertificateSigningRequests (CSRs)](/docs/reference/access-authn-authz/certificate-signing-requests/#request-signing-process)
* <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='Nodes'>Nodes</a> deleted in the following scenarios:
  * On a cloud when the cluster uses a [cloud controller manager](/docs/concepts/architecture/cloud-controller/)
  * On-premises when the cluster uses an addon similar to a cloud controller
    manager
* [Node Lease objects](/docs/concepts/architecture/nodes/#heartbeats)
-->
<ul>
<li><a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection">失败的 Pod</a></li>
<li><a href="/zh/docs/concepts/workloads/controllers/ttlafterfinished/">已完成的 Job</a></li>
<li><a href="#owners-dependents">不再存在属主引用的对象</a></li>
<li><a href="#containers-images">未使用的容器和容器镜像</a></li>
<li><a href="/zh/docs/concepts/storage/persistent-volumes/#delete">动态制备的、StorageClass 回收策略为 Delete 的 PV 卷</a></li>
<li><a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/#request-signing-process">阻滞或者过期的 CertificateSigningRequest (CSRs)</a></li>
<li>在以下情形中删除了的<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>对象：
<ul>
<li>当集群使用<a href="/zh/docs/concepts/architecture/cloud-controller/">云控制器管理器</a>运行于云端时；</li>
<li>当集群使用类似于云控制器管理器的插件运行在本地环境中时。</li>
</ul>
</li>
<li><a href="/zh/docs/concepts/architecture/nodes/#heartbeats">节点租约对象</a></li>
</ul>
<!--
## Owners and dependents {#owners-dependents}

Many objects in Kubernetes link to each other through [*owner references*](/docs/concepts/overview/working-with-objects/owners-dependents/). 
Owner references tell the control plane which objects are dependent on others.
Kubernetes uses owner references to give the control plane, and other API
clients, the opportunity to clean up related resources before deleting an
object. In most cases, Kubernetes manages owner references automatically.
-->
<h2 id="owners-dependents">属主与依赖  </h2>
<p>Kubernetes 中很多对象通过<a href="/zh/docs/concepts/overview/working-with-objects/owners-dependents/"><em>属主引用</em></a>
链接到彼此。属主引用（Owner Reference）可以告诉控制面哪些对象依赖于其他对象。
Kubernetes 使用属主引用来为控制面以及其他 API 客户端在删除某对象时提供一个
清理关联资源的机会。在大多数场合，Kubernetes 都是自动管理属主引用的。</p>
<!--
Ownership is different from the [labels and selectors](/docs/concepts/overview/working-with-objects/labels/)
mechanism that some resources also use. For example, consider a
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> that creates
`EndpointSlice` objects. The Service uses *labels* to allow the control plane to
determine which `EndpointSlice` objects are used for that Service. In addition
to the labels, each `EndpointSlice` that is managed on behalf of a Service has
an owner reference. Owner references help different parts of Kubernetes avoid
interfering with objects they don’t control.
-->
<p>属主关系与某些资源所使用的的<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签和选择算符</a>
不同。例如，考虑一个创建 <code>EndpointSlice</code> 对象的 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>
对象。Service 对象使用<em>标签</em>来允许控制面确定哪些 <code>EndpointSlice</code> 对象被该
Service 使用。除了标签，每个被 Service 托管的 <code>EndpointSlice</code> 对象还有一个属主引用属性。
属主引用可以帮助 Kubernetes 中的不同组件避免干预并非由它们控制的对象。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Cross-namespace owner references are disallowed by design.
Namespaced dependents can specify cluster-scoped or namespaced owners.
A namespaced owner **must** exist in the same namespace as the dependent.
If it does not, the owner reference is treated as absent, and the dependent
is subject to deletion once all owners are verified absent.
-->
<p>根据设计，系统不允许出现跨名字空间的属主引用。名字空间作用域的依赖对象可以指定集群作用域或者名字空间作用域的属主。
名字空间作用域的属主<strong>必须</strong>存在于依赖对象所在的同一名字空间。
如果属主位于不同名字空间，则属主引用被视为不存在，而当检查发现所有属主都已不存在时，
依赖对象会被删除。</p>
<!--
Cluster-scoped dependents can only specify cluster-scoped owners.
In v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner,
it is treated as having an unresolvable owner reference, and is not able to be garbage collected.
-->
<p>集群作用域的依赖对象只能指定集群作用域的属主。
在 1.20 及更高版本中，如果一个集群作用域的依赖对象指定了某个名字空间作用域的类别作为其属主，
则该对象被视为拥有一个无法解析的属主引用，因而无法被垃圾收集处理。</p>
<!--
In v1.20+, if the garbage collector detects an invalid cross-namespace `ownerReference`,
or a cluster-scoped dependent with an `ownerReference` referencing a namespaced kind, a warning Event 
with a reason of `OwnerRefInvalidNamespace` and an `involvedObject` of the invalid dependent is reported.
You can check for that kind of Event by running
`kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace`.
-->
<p>在 1.20 及更高版本中，如果垃圾收集器检测到非法的跨名字空间 <code>ownerReference</code>，
或者某集群作用域的依赖对象的 <code>ownerReference</code> 引用某名字空间作用域的类别，
系统会生成一个警告事件，其原因为 <code>OwnerRefInvalidNamespace</code>，<code>involvedObject</code>
设置为非法的依赖对象。你可以通过运行
<code>kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace</code>
来检查是否存在这类事件。</p>

</div>
<!--
## Cascading deletion {#cascading-deletion}

Kubernetes checks for and deletes objects that no longer have owner
references, like the pods left behind when you delete a ReplicaSet. When you
delete an object, you can control whether Kubernetes deletes the object's
dependents automatically, in a process called *cascading deletion*. There are
two types of cascading deletion, as follows: 

* Foreground cascading deletion
* Background cascading deletion
-->
<h2 id="cascading-deletion">级联删除   </h2>
<p>Kubernetes 会检查并删除那些不再拥有属主引用的对象，例如在你删除了 ReplicaSet
之后留下来的 Pod。当你删除某个对象时，你可以控制 Kubernetes 是否要通过一个称作
级联删除（Cascading Deletion）的过程自动删除该对象的依赖对象。
级联删除有两种类型，分别如下：</p>
<ul>
<li>前台级联删除</li>
<li>后台级联删除</li>
</ul>
<!--
You can also control how and when garbage collection deletes resources that have
owner references using Kubernetes <a class='glossary-tooltip' title='一个带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后， 再完全删除被标记为删除的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/finalizers/' target='_blank' aria-label='finalizers'>finalizers</a>. 
-->
<p>你也可以使用 Kubernetes <a class='glossary-tooltip' title='一个带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后， 再完全删除被标记为删除的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/finalizers/' target='_blank' aria-label='Finalizers'>Finalizers</a>
来控制垃圾收集机制如何以及何时删除包含属主引用的资源。</p>
<!--
### Foreground cascading deletion {#foreground-deletion}

In foreground cascading deletion, the owner object you're deleting first enters
a *deletion in progress* state. In this state, the following happens to the
owner object: 
-->
<h3 id="foreground-deletion">前台级联删除</h3>
<p>在前台级联删除中，正在被你删除的对象首先进入 <em>deletion in progress</em> 状态。
在这种状态下，针对属主对象会发生以下事情：</p>
<!--
* The Kubernetes API server sets the object's `metadata.deletionTimestamp`
  field to the time the object was marked for deletion.
* The Kubernetes API server also sets the `metadata.finalizers` field to
  `foregroundDeletion`. 
* The object remains visible through the Kubernetes API until the deletion
  process is complete.
-->
<ul>
<li>Kubernetes API 服务器将对象的 <code>metadata.deletionTimestamp</code>
字段设置为对象被标记为要删除的时间点。</li>
<li>Kubernetes API 服务器也会将 <code>metadata.finalizers</code> 字段设置为 <code>foregroundDeletion</code>。</li>
<li>在删除过程完成之前，通过 Kubernetes API 仍然可以看到该对象。</li>
</ul>
<!--
After the owner object enters the deletion in progress state, the controller
deletes the dependents. After deleting all the dependent objects, the controller
deletes the owner object. At this point, the object is no longer visible in the
Kubernetes API. 

During foreground cascading deletion, the only dependents that block owner
deletion are those that have the `ownerReference.blockOwnerDeletion=true` field.
See [Use foreground cascading deletion](/docs/tasks/administer-cluster/use-cascading-deletion/#use-foreground-cascading-deletion)
to learn more.
-->
<p>当属主对象进入删除过程中状态后，控制器删除其依赖对象。控制器在删除完所有依赖对象之后，
删除属主对象。这时，通过 Kubernetes API 就无法再看到该对象。</p>
<p>在前台级联删除过程中，唯一的可能阻止属主对象被删除的依赖对象是那些带有
<code>ownerReference.blockOwnerDeletion=true</code> 字段的对象。
参阅<a href="/zh/docs/tasks/administer-cluster/use-cascading-deletion/#use-foreground-cascading-deletion">使用前台级联删除</a>
以了解进一步的细节。</p>
<!--
### Background cascading deletion {#background-deletion}

In background cascading deletion, the Kubernetes API server deletes the owner
object immediately and the controller cleans up the dependent objects in
the background. By default, Kubernetes uses background cascading deletion unless
you manually use foreground deletion or choose to orphan the dependent objects.

See [Use background cascading deletion](/docs/tasks/administer-cluster/use-cascading-deletion/#use-background-cascading-deletion)
to learn more.
-->
<h3 id="background-deletion">后台级联删除</h3>
<p>在后台级联删除过程中，Kubernetes 服务器立即删除属主对象，控制器在后台清理所有依赖对象。
默认情况下，Kubernetes 使用后台级联删除方案，除非你手动设置了要使用前台删除，
或者选择遗弃依赖对象。</p>
<p>参阅<a href="/zh/docs/tasks/administer-cluster/use-cascading-deletion/#use-background-cascading-deletion">使用后台级联删除</a>
以了解进一步的细节。</p>
<!--
### Orphaned dependents

When Kubernetes deletes an owner object, the dependents left behind are called
*orphan* objects. By default, Kubernetes deletes dependent objects. To learn how
to override this behaviour, see [Delete owner objects and orphan dependents](/docs/tasks/administer-cluster/use-cascading-deletion/#set-orphan-deletion-policy).
-->
<h3 id="orphaned-dependents">被遗弃的依赖对象   </h3>
<p>当 Kubernetes 删除某个属主对象时，被留下来的依赖对象被称作被遗弃的（Orphaned）对象。
默认情况下，Kubernetes 会删除依赖对象。要了解如何重载这种默认行为，可参阅
<a href="/zh/docs/tasks/administer-cluster/use-cascading-deletion/#set-orphan-deletion-policy">删除属主对象和遗弃依赖对象</a>。</p>
<!--
## Garbage collection of unused containers and images {#containers-images}

The <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> performs garbage
collection on unused images every five minutes and on unused containers every
minute. You should avoid using external garbage collection tools, as these can
break the kubelet behavior and remove containers that should exist. 
-->
<h2 id="containers-images">未使用容器和镜像的垃圾收集    </h2>
<p><a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 会每五分钟对未使用的镜像执行一次垃圾收集，
每分钟对未使用的容器执行一次垃圾收集。
你应该避免使用外部的垃圾收集工具，因为外部工具可能会破坏 kubelet
的行为，移除应该保留的容器。</p>
<!--
To configure options for unused container and image garbage collection, tune the
kubelet using a [configuration file](/docs/tasks/administer-cluster/kubelet-config-file/)
and change the parameters related to garbage collection using the
[`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration)
resource type.
-->
<p>要配置对未使用容器和镜像的垃圾收集选项，可以使用一个
<a href="/zh/docs/tasks/administer-cluster/kubelet-config-file/">配置文件</a>，基于
<a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration"><code>KubeletConfiguration</code></a>
资源类型来调整与垃圾搜集相关的 kubelet 行为。</p>
<!--
### Container image lifecycle

Kubernetes manages the lifecycle of all images through its *image manager*,
which is part of the kubelet, with the cooperation of 
<a class='glossary-tooltip' title='帮助理解容器的资源用量与性能特征的工具' data-toggle='tooltip' data-placement='top' href='https://github.com/google/cadvisor/' target='_blank' aria-label='cadvisor'>cadvisor</a>. The kubelet
considers the following disk usage limits when making garbage collection
decisions:
-->
<h3 id="container-image-lifecycle">容器镜像生命期    </h3>
<p>Kubernetes 通过其镜像管理器（Image Manager）来管理所有镜像的生命周期，
该管理器是 kubelet 的一部分，工作时与
<a class='glossary-tooltip' title='帮助理解容器的资源用量与性能特征的工具' data-toggle='tooltip' data-placement='top' href='https://github.com/google/cadvisor/' target='_blank' aria-label='cadvisor'>cadvisor</a> 协同。
kubelet 在作出垃圾收集决定时会考虑如下磁盘用量约束：</p>
<ul>
<li><code>HighThresholdPercent</code></li>
<li><code>LowThresholdPercent</code></li>
</ul>
<!--
Disk usage above the configured `HighThresholdPercent` value triggers garbage
collection, which deletes images in order based on the last time they were used,
starting with the oldest first. The kubelet deletes images
until disk usage reaches the `LowThresholdPercent` value.
-->
<p>磁盘用量超出所配置的 <code>HighThresholdPercent</code> 值时会触发垃圾收集，
垃圾收集器会基于镜像上次被使用的时间来按顺序删除它们，首先删除的是最老的镜像。
kubelet 会持续删除镜像，直到磁盘用量到达 <code>LowThresholdPercent</code> 值为止。</p>
<!--
### Container garbage collection {#container-image-garbage-collection}

The kubelet garbage collects unused containers based on the following variables,
which you can define: 
-->
<h3 id="container-image-garbage-collection">容器垃圾收集   </h3>
<p>kubelet 会基于如下变量对所有未使用的容器执行垃圾收集操作，这些变量都是你可以定义的：</p>
<!--
* `MinAge`: the minimum age at which the kubelet can garbage collect a
  container. Disable by setting to `0`.
* `MaxPerPodContainer`: the maximum number of dead containers each Pod pair
  can have. Disable by setting to less than `0`.
* `MaxContainers`: the maximum number of dead containers the cluster can have.
  Disable by setting to less than `0`. 
-->
<ul>
<li><code>MinAge</code>：kubelet 可以垃圾回收某个容器时该容器的最小年龄。设置为 <code>0</code>
表示禁止使用此规则。</li>
<li><code>MaxPerPodContainer</code>：每个 Pod 可以包含的已死亡的容器个数上限。设置为小于 <code>0</code>
的值表示禁止使用此规则。</li>
<li><code>MaxContainers</code>：集群中可以存在的已死亡的容器个数上限。设置为小于 <code>0</code>
的值意味着禁止应用此规则。</li>
</ul>
<!--
In addition to these variables, the kubelet garbage collects unidentified and
deleted containers, typically starting with the oldest first. 

`MaxPerPodContainer` and `MaxContainer` may potentially conflict with each other
in situations where retaining the maximum number of containers per Pod
(`MaxPerPodContainer`) would go outside the allowable total of global dead
containers (`MaxContainers`). In this situation, the kubelet adjusts
`MaxPerPodContainer` to address the conflict. A worst-case scenario would be to
downgrade `MaxPerPodContainer` to `1` and evict the oldest containers.
Additionally, containers owned by pods that have been deleted are removed once
they are older than `MinAge`.
-->
<p>除以上变量之外，kubelet 还会垃圾收集除无标识的以及已删除的容器，通常从最老的容器开始。</p>
<p>当保持每个 Pod 的最大数量的容器（<code>MaxPerPodContainer</code>）会使得全局的已死亡容器个数超出上限
（<code>MaxContainers</code>）时，<code>MaxPerPodContainer</code> 和 <code>MaxContainers</code> 之间可能会出现冲突。
在这种情况下，kubelet 会调整 <code>MaxPerPodContainer</code> 来解决这一冲突。
最坏的情形是将 <code>MaxPerPodContainer</code> 降格为 <code>1</code>，并驱逐最老的容器。
此外，当隶属于某已被删除的 Pod 的容器的年龄超过 <code>MinAge</code> 时，它们也会被删除。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The kubelet only garbage collects the containers it manages.
-->
<p>kubelet 仅会回收由它所管理的容器。
</div>
<!--
## Configuring garbage collection {#configuring-gc}

You can tune garbage collection of resources by configuring options specific to
the controllers managing those resources. The following pages show you how to
configure garbage collection:

  * [Configuring cascading deletion of Kubernetes objects](/docs/tasks/administer-cluster/use-cascading-deletion/)
  * [Configuring cleanup of finished Jobs](/docs/concepts/workloads/controllers/ttlafterfinished/)
--> 
<h2 id="configuring-gc">配置垃圾收集    </h2>
<p>你可以通过配置特定于管理资源的控制器来调整资源的垃圾收集行为。
下面的页面为你展示如何配置垃圾收集：</p>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/use-cascading-deletion/">配置 Kubernetes 对象的级联删除</a></li>
<li><a href="/zh/docs/concepts/workloads/controllers/ttlafterfinished/">配置已完成 Job 的清理</a></li>
</ul>
<!-- * [Configuring unused container and image garbage collection](/docs/tasks/administer-cluster/reconfigure-kubelet/) -->
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [ownership of Kubernetes objects](/docs/concepts/overview/working-with-objects/owners-dependents/).
* Learn more about Kubernetes [finalizers](/docs/concepts/overview/working-with-objects/finalizers/).
* Learn about the [TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) (beta) that cleans up finished Jobs.
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/overview/working-with-objects/owners-dependents/">Kubernetes 对象的属主关系</a>。</li>
<li>进一步了解 Kubernetes <a href="/zh/docs/concepts/overview/working-with-objects/finalizers/">finalizers</a>。</li>
<li>进一步了解 <a href="/zh/docs/concepts/workloads/controllers/ttlafterfinished/">TTL 控制器</a> (beta)，
该控制器负责清理已完成的 Job。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c0ea5310f52e22c5de34dc84d9ab5e0d">2.6 - 容器运行时接口（CRI）</h1>
    
	<!-- 
title: Container Runtime Interface (CRI)
content_type: concept
weight: 50
-->
<!-- overview -->
<!-- 
The CRI is a plugin interface which enables the kubelet to use a wide variety of
container runtimes, without having a need to recompile the cluster components.

You need a working
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a> on
each Node in your cluster, so that the
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> can launch
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> and their containers.
-->
<p>CRI 是一个插件接口，它使 kubelet 能够使用各种容器运行时，无需重新编译集群组件。</p>
<p>你需要在集群中的每个节点上都有一个可以正常工作的
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>，
这样
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 能启动
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 及其容器。</p>
<!--
title: Container Runtime Interface
id: container-runtime-interface
date: 2021-11-24
full_link: /docs/concepts/architecture/cri
short_description: >
  The main protocol for the communication between the kubelet and Container Runtime.

aka:
tags:
  - cri
-->
<!-- The main protocol for the communication between the kubelet and Container Runtime. -->
<p><p>容器运行时接口（CRI）是 kubelet 和容器运行时之间通信的主要协议。</p></p>
<!-- 
The Kubernetes Container Runtime Interface (CRI) defines the main
[gRPC](https://grpc.io) protocol for the communication between the
[cluster components](/docs/concepts/overview/components/#node-components)
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> and
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>.
-->
<p>Kubernetes 容器运行时接口（CRI）定义了主要 <a href="https://grpc.io">gRPC</a> 协议，
用于<a href="/zh/docs/concepts/overview/components/#node-components">集群组件</a>
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 和
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>。</p>
<!-- body -->
<!-- ## The API {#api} -->
<h2 id="api">API</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code>
</div>


<!--
The kubelet acts as a client when connecting to the container runtime via gRPC.
The runtime and image service endpoints have to be available in the container
runtime, which can be configured separately within the kubelet by using the
`--image-service-endpoint` and `--container-runtime-endpoint` [command line
flags](/docs/reference/command-line-tools-reference/kubelet)
-->
<p>当通过 gRPC 连接到容器运行时时，kubelet 充当客户端。
运行时和镜像服务端点必须在容器运行时中可用，可以使用
<a href="/zh/docs/reference/command-line-tools-reference/kubelet">命令行标志</a>的
<code>--image-service-endpoint</code> 和 <code>--container-runtime-endpoint</code>
在 kubelet 中单独配置。</p>
<!-- 
For Kubernetes v1.23, the kubelet prefers to use CRI `v1`.
If a container runtime does not support `v1` of the CRI, then the kubelet tries to
negotiate any older supported version.
The v1.23 kubelet can also negotiate CRI `v1alpha2`, but
this version is considered as deprecated.
If the kubelet cannot negotiate a supported CRI version, the kubelet gives up
and doesn't register as a node.
-->
<p>对 Kubernetes v1.23，kubelet 偏向于使用 CRI <code>v1</code> 版本。
如果容器运行时不支持 CRI 的 <code>v1</code> 版本，那么 kubelet 会尝试协商任何旧的其他支持版本。
如果 kubelet 无法协商支持的 CRI 版本，则 kubelet 放弃并且不会注册为节点。</p>
<!-- 
## Upgrading

When upgrading Kubernetes, then the kubelet tries to automatically select the
latest CRI version on restart of the component. If that fails, then the fallback
will take place as mentioned above. If a gRPC re-dial was required because the
container runtime has been upgraded, then the container runtime must also
support the initially selected version or the redial is expected to fail. This
requires a restart of the kubelet.
-->
<h2 id="upgrading">升级 </h2>
<p>升级 Kubernetes 时，kubelet 会尝试在组件重启时自动选择最新的 CRI 版本。
如果失败，则将如上所述进行回退。如果由于容器运行时已升级而需要 gRPC 重拨，
则容器运行时还必须支持最初选择的版本，否则重拨预计会失败。
这需要重新启动 kubelet。</p>
<h2 id="what-s-next">What's next</h2>
<!-- 
- Learn more about the CRI [protocol definition](https://github.com/kubernetes/cri-api/blob/c75ef5b/pkg/apis/runtime/v1/api.proto)
-->
<ul>
<li>了解更多有关 CRI <a href="https://github.com/kubernetes/cri-api/blob/c75ef5b/pkg/apis/runtime/v1/api.proto">协议定义</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a5f7383c83ab9eb9cd0e3c4c020b3ae6">3 - 容器</h1>
    <div class="lead">打包应用及其运行依赖环境的技术。</div>
	<!-- overview -->
<!--
Each container that you run is repeatable; the standardization from having
dependencies included means that you get the same behavior wherever you
run it.

Containers decouple applications from underlying host infrastructure.
This makes deployment easier in different cloud or OS environments.
-->
<p>每个运行的容器都是可重复的；
包含依赖环境在内的标准，意味着无论您在哪里运行它，您都会得到相同的行为。</p>
<p>容器将应用程序从底层的主机设施中解耦。
这使得在不同的云或 OS 环境中部署更加容易。</p>
<!-- body -->
<!--
## Container images
A [container image](/docs/concepts/containers/images/) is a ready-to-run
software package, containing everything needed to run an application:
the code and any runtime it requires, application and system libraries,
and default values for any essential settings.

By design, a container is immutable: you cannot change the code of a
container that is already running. If you have a containerized application
and want to make changes, you need to build a new image that includes
the change, then recreate the container to start from the updated image.
-->
<h2 id="container-images">容器镜像</h2>
<p><a href="/zh/docs/concepts/containers/images/">容器镜像</a>是一个随时可以运行的软件包，
包含运行应用程序所需的一切：代码和它需要的所有运行时、应用程序和系统库，以及一些基本设置的默认值。</p>
<p>根据设计，容器是不可变的：你不能更改已经运行的容器的代码。
如果有一个容器化的应用程序需要修改，则需要构建包含更改的新镜像，然后再基于新构建的镜像重新运行容器。</p>
<!-- ## Container runtimes -->
<h2 id="container-runtimes">容器运行时 </h2>
<!--
---
title: Container Runtime
id: container-runtime
date: 2019-06-05
full_link: /docs/setup/production-environment/container-runtimes
short_description: >
 The container runtime is the software that is responsible for running containers.

aka:
tags:
- fundamental
- workload
---
-->
<!--
 The container runtime is the software that is responsible for running containers.
-->
<p>容器运行环境是负责运行容器的软件。</p>
<!--
Kubernetes supports container runtimes such sa
<a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a>,
<a class='glossary-tooltip' title='强调简单性、健壮性和可移植性的一种容器运行时' data-toggle='tooltip' data-placement='top' href='https://containerd.io/docs/' target='_blank' aria-label='containerd'>containerd</a>, <a class='glossary-tooltip' title='专用于 Kubernetes 的轻量级容器运行时软件' data-toggle='tooltip' data-placement='top' href='https://cri-o.io/#what-is-cri-o' target='_blank' aria-label='CRI-O'>CRI-O</a>,
and any other implementation of the [Kubernetes CRI (Container Runtime
Interface)](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md).
-->
<p>Kubernetes 支持容器运行时，例如
<a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a>、
<a class='glossary-tooltip' title='强调简单性、健壮性和可移植性的一种容器运行时' data-toggle='tooltip' data-placement='top' href='https://containerd.io/docs/' target='_blank' aria-label='containerd'>containerd</a>、<a class='glossary-tooltip' title='专用于 Kubernetes 的轻量级容器运行时软件' data-toggle='tooltip' data-placement='top' href='https://cri-o.io/#what-is-cri-o' target='_blank' aria-label='CRI-O'>CRI-O</a>
以及 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">Kubernetes CRI (容器运行环境接口)</a>
的其他任何实现。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about [container images](/docs/concepts/containers/images/)
* Read about [Pods](/docs/concepts/workloads/pods/)
-->
<ul>
<li>进一步阅读<a href="/zh/docs/concepts/containers/images/">容器镜像</a></li>
<li>进一步阅读 <a href="/zh/docs/concepts/workloads/pods/">Pods</a></li>
</ul>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-16042b4652ad19e565c7263824029a43">3.1 - 镜像</h1>
    
	<!--
reviewers:
- erictune
- thockin
title: Images
content_type: concept
weight: 10
-->
<!-- overview -->
<!--
A container image represents binary data that encapsulates an application and all its
software dependencies. Container images are executable software bundles that can run
standalone and that make very well defined assumptions about their runtime environment.

You typically create a container image of your application and push it to a registry
before referring to it in a
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>

This page provides an outline of the container image concept.
-->
<p>容器镜像（Image）所承载的是封装了应用程序及其所有软件依赖的二进制数据。
容器镜像是可执行的软件包，可以单独运行；该软件包对所处的运行时环境具有
良定（Well Defined）的假定。</p>
<p>你通常会创建应用的容器镜像并将其推送到某仓库（Registry），然后在
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 中引用它。</p>
<p>本页概要介绍容器镜像的概念。</p>
<!-- body -->
<!--
## Image names

Container images are usually given a name such as `pause`, `example/mycontainer`, or `kube-apiserver`.
Images can also include a registry hostname; for example: `fictional.registry.example/imagename`,
and possibly a port number as well; for example: `fictional.registry.example:10443/imagename`.

If you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public registry.

After the image name part you can add a _tag_ (in the same way you would when using with commands like `docker` or `podman`).
Tags let you identify different versions of the same series of images.
-->
<h2 id="image-names">镜像名称   </h2>
<p>容器镜像通常会被赋予 <code>pause</code>、<code>example/mycontainer</code> 或者 <code>kube-apiserver</code> 这类的名称。
镜像名称也可以包含所在仓库的主机名。例如：<code>fictional.registry.example/imagename</code>。
还可以包含仓库的端口号，例如：<code>fictional.registry.example:10443/imagename</code>。</p>
<p>如果你不指定仓库的主机名，Kubernetes 认为你在使用 Docker 公共仓库。</p>
<p>在镜像名称之后，你可以添加一个标签（Tag）（与使用 <code>docker</code> 或 <code>podman</code> 等命令时的方式相同）。
使用标签能让你辨识同一镜像序列中的不同版本。</p>
<!--
Image tags consist of lowercase and uppercase letters, digits, underscores (`_`),
periods (`.`), and dashes (`-`).  
There are additional rules about where you can place the separator
characters (`_`, `-`, and `.`) inside an image tag.  
If you don't specify a tag, Kubernetes assumes you mean the tag `latest`.
-->
<p>镜像标签可以包含小写字母、大写字母、数字、下划线（<code>_</code>）、句点（<code>.</code>）和连字符（<code>-</code>）。
关于在镜像标签中何处可以使用分隔字符（<code>_</code>、<code>-</code> 和 <code>.</code>）还有一些额外的规则。
如果你不指定标签，Kubernetes 认为你想使用标签 <code>latest</code>。</p>
<!--
## Updating images

When you first create a <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>,
<a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>, Pod, or other
object that includes a Pod template, then by default the pull policy of all
containers in that pod will be set to `IfNotPresent` if it is not explicitly
specified. This policy causes the
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> to skip pulling an
image if it already exists.
-->
<h2 id="updating-images">更新镜像 </h2>
<p>当你最初创建一个 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>、
<a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>、Pod
或者其他包含 Pod 模板的对象时，如果没有显式设定的话，Pod 中所有容器的默认镜像
拉取策略是 <code>IfNotPresent</code>。这一策略会使得
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>
在镜像已经存在的情况下直接略过拉取镜像的操作。</p>
<!--
### Image pull policy

The `imagePullPolicy` for a container and the tag of the image affect when the
[kubelet](/docs/reference/command-line-tools-reference/kubelet/) attempts to pull (download) the specified image.

Here's a list of the values you can set for `imagePullPolicy` and the effects
these values have:
-->
<h3 id="image-pull-policy">镜像拉取策略  </h3>
<p>容器的 <code>imagePullPolicy</code> 和镜像的标签会影响 <a href="/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> 尝试拉取（下载）指定的镜像。</p>
<p>以下列表包含了 <code>imagePullPolicy</code> 可以设置的值，以及这些值的效果：</p>
<!--
`IfNotPresent`
: the image is pulled only if it is not already present locally.

`Always`
: every time the kubelet launches a container, the kubelet queries the container
  image registry to resolve the name to an image
  [digest](https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier). If the kubelet has a
  container image with that exact digest cached locally, the kubelet uses its cached
  image; otherwise, the kubelet pulls the image with the resolved digest,
  and uses that image to launch the container.

`Never`
: the kubelet does not try fetching the image. If the image is somehow already present
  locally, the kubelet attempts to start the container; otherwise, startup fails.
  See [pre-pulled images](#pre-pulled-images) for more details.
-->
<dl>
<dt><code>IfNotPresent</code></dt>
<dd>只有当镜像在本地不存在时才会拉取。</dd>
<dt><code>Always</code></dt>
<dd>每当 kubelet 启动一个容器时，kubelet 会查询容器的镜像仓库，
将名称解析为一个镜像<a href="https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier">摘要</a>。
如果 kubelet 有一个容器镜像，并且对应的摘要已在本地缓存，kubelet 就会使用其缓存的镜像；
否则，kubelet 就会使用解析后的摘要拉取镜像，并使用该镜像来启动容器。</dd>
<dt><code>Never</code></dt>
<dd>Kubelet 不会尝试获取镜像。如果镜像已经以某种方式存在本地，
kubelet 会尝试启动容器；否则，会启动失败。
更多细节见<a href="#pre-pulled-images">提前拉取镜像</a>。</dd>
</dl>
<!--
The caching semantics of the underlying image provider make even
`imagePullPolicy: Always` efficient, as long as the registry is reliably accessible.
Your container runtime can notice that the image layers already exist on the node
so that they don't need to be downloaded again.
-->
<p>只要能够可靠地访问镜像仓库，底层镜像提供者的缓存语义甚至可以使 <code>imagePullPolicy: Always</code> 高效。
你的容器运行时可以注意到节点上已经存在的镜像层，这样就不需要再次下载。</p>
<!--
You should avoid using the `:latest` tag when deploying containers in production as
it is harder to track which version of the image is running and more difficult to
roll back properly.

Instead, specify a meaningful tag such as `v1.42.0`.

To make sure the Pod always uses the same version of a container image, you can specify
the image's digest;
replace `<image-name>:<tag>` with `<image-name>@<digest>`
(for example, `image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2`).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>在生产环境中部署容器时，你应该避免使用 <code>:latest</code> 标签，因为这使得正在运行的镜像的版本难以追踪，并且难以正确地回滚。</p>
<p>相反，应指定一个有意义的标签，如 <code>v1.42.0</code>。</p>

</div>
<p>为了确保 Pod 总是使用相同版本的容器镜像，你可以指定镜像的摘要；
将 <code>&lt;image-name&gt;:&lt;tag&gt;</code> 替换为 <code>&lt;image-name&gt;@&lt;digest&gt;</code>，例如 <code>image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2</code>。</p>
<!--
When using image tags, if the image registry were to change the code that the tag on that image represents, you might end up with a mix of Pods running the old and new code. An image digest uniquely identifies a specific version of the image, so Kubernetes runs the same code every time it starts a container with that image name and digest specified. Specifying an image by digest fixes the code that you run so that a change at the registry cannot lead to that mix of versions.

There are third-party [admission controllers](/docs/reference/access-authn-authz/admission-controllers/)
that mutate Pods (and pod templates) when they are created, so that the
running workload is defined based on an image digest rather than a tag.
That might be useful if you want to make sure that all your workload is
running the same code no matter what tag changes happen at the registry.
-->
<p>当使用镜像标签时，如果镜像仓库修改了代码所对应的镜像标签，可能会出现新旧代码混杂在 Pod 中运行的情况。
镜像摘要唯一标识了镜像的特定版本，因此 Kubernetes 每次启动具有指定镜像名称和摘要的容器时，都会运行相同的代码。
通过摘要指定镜像可固定你运行的代码，这样镜像仓库的变化就不会导致版本的混杂。</p>
<p>有一些第三方的<a href="/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制器</a>
在创建 Pod（和 Pod 模板）时产生变更，这样运行的工作负载就是根据镜像摘要，而不是标签来定义的。
无论镜像仓库上的标签发生什么变化，你都想确保你所有的工作负载都运行相同的代码，那么指定镜像摘要会很有用。</p>
<!-- 
#### Default image pull policy {#imagepullpolicy-defaulting}

When you (or a controller) submit a new Pod to the API server, your cluster sets the
`imagePullPolicy` field when specific conditions are met:
-->
<h4 id="imagepullpolicy-defaulting">默认镜像拉取策略   </h4>
<p>当你（或控制器）向 API 服务器提交一个新的 Pod 时，你的集群会在满足特定条件时设置 <code>imagePullPolicy </code>字段：</p>
<!--
- if you omit the `imagePullPolicy` field, and the tag for the container image is
  `:latest`, `imagePullPolicy` is automatically set to `Always`;
- if you omit the `imagePullPolicy` field, and you don't specify the tag for the
  container image, `imagePullPolicy` is automatically set to `Always`;
- if you omit the `imagePullPolicy` field, and you specify the tag for the
  container image that isn't `:latest`, the `imagePullPolicy` is automatically set to
  `IfNotPresent`.
-->
<ul>
<li>如果你省略了 <code>imagePullPolicy</code> 字段，并且容器镜像的标签是 <code>:latest</code>，
<code>imagePullPolicy</code> 会自动设置为 <code>Always</code>。</li>
<li>如果你省略了 <code>imagePullPolicy</code> 字段，并且没有指定容器镜像的标签，
<code>imagePullPolicy</code> 会自动设置为 <code>Always</code>。</li>
<li>如果你省略了 <code>imagePullPolicy</code> 字段，并且为容器镜像指定了非 <code>:latest</code> 的标签，
<code>imagePullPolicy</code> 就会自动设置为 <code>IfNotPresent</code>。</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The value of `imagePullPolicy` of the container is always set when the object is
first _created_, and is not updated if the image's tag later changes.

For example, if you create a Deployment with an image whose tag is _not_
`:latest`, and later update that Deployment's image to a `:latest` tag, the
`imagePullPolicy` field will _not_ change to `Always`. You must manually change
the pull policy of any object after its initial creation.
-->
<p>容器的 <code>imagePullPolicy</code> 的值总是在对象初次 <em>创建</em> 时设置的，如果后来镜像的标签发生变化，则不会更新。</p>
<p>例如，如果你用一个 <em>非</em> <code>:latest</code> 的镜像标签创建一个 Deployment，
并在随后更新该 Deployment 的镜像标签为 <code>:latest</code>，则 <code>imagePullPolicy</code> 字段 <em>不会</em> 变成 <code>Always</code>。
你必须手动更改已经创建的资源的拉取策略。</p>

</div>
<!--
#### Required image pull

If you would like to always force a pull, you can do one of the following:

- Set the `imagePullPolicy` of the container to `Always`.
- Omit the `imagePullPolicy` and use `:latest` as the tag for the image to use;
  Kubernetes will set the policy to `Always` when you submit the Pod.
- Omit the `imagePullPolicy` and the tag for the image to use;
  Kubernetes will set the policy to `Always` when you submit the Pod.
- Enable the [AlwaysPullImages](/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages) admission controller.
-->
<h4 id="required-image-pull">必要的镜像拉取  </h4>
<p>如果你想总是强制执行拉取，你可以使用下述的一中方式：</p>
<ul>
<li>设置容器的 <code>imagePullPolicy</code> 为 <code>Always</code>。</li>
<li>省略 <code>imagePullPolicy</code>，并使用 <code>:latest</code> 作为镜像标签；
当你提交 Pod 时，Kubernetes 会将策略设置为 <code>Always</code>。</li>
<li>省略 <code>imagePullPolicy</code> 和镜像的标签；
当你提交 Pod 时，Kubernetes 会将策略设置为 <code>Always</code>。</li>
<li>启用准入控制器 <a href="/zh/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages">AlwaysPullImages</a>。</li>
</ul>
<!--
### ImagePullBackOff

When a kubelet starts creating containers for a Pod using a container runtime,
it might be possible the container is in [Waiting](/docs/concepts/workloads/pods/pod-lifecycle/#container-state-waiting)
state because of `ImagePullBackOff`.
-->
<h3 id="imagepullbackoff">ImagePullBackOff</h3>
<p>当 kubelet 使用容器运行时创建 Pod 时，容器可能因为 <code>ImagePullBackOff</code> 导致状态为
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#container-state-waiting">Waiting</a>。</p>
<!--
The status `ImagePullBackOff` means that a container could not start because Kubernetes
could not pull a container image (for reasons such as invalid image name, or pulling
from a private registry without `imagePullSecret`). The `BackOff` part indicates
that Kubernetes will keep trying to pull the image, with an increasing back-off delay.

Kubernetes raises the delay between each attempt until it reaches a compiled-in limit,
which is 300 seconds (5 minutes).
-->
<p><code>ImagePullBackOff</code> 状态意味着容器无法启动，
因为 Kubernetes 无法拉取容器镜像（原因包括无效的镜像名称，或从私有仓库拉取而没有 <code>imagePullSecret</code>）。
<code>BackOff</code> 部分表示 Kubernetes 将继续尝试拉取镜像，并增加回退延迟。</p>
<p>Kubernetes 会增加每次尝试之间的延迟，直到达到编译限制，即 300 秒（5 分钟）。</p>
<!--
## Multi-architecture images with image indexes

As well as providing binary images, a container registry can also serve a [container image index](https://github.com/opencontainers/image-spec/blob/master/image-index.md). An image index can point to multiple [image manifests](https://github.com/opencontainers/image-spec/blob/master/manifest.md) for architecture-specific versions of a container. The idea is that you can have a name for an image (for example: `pause`, `example/mycontainer`, `kube-apiserver`) and allow different systems to fetch the right binary image for the machine architecture they are using.

Kubernetes itself typically names container images with a suffix `-$(ARCH)`. For backward compatibility, please generate the older images with suffixes. The idea is to generate say `pause` image which has the manifest for all the arch(es) and say `pause-amd64` which is backwards compatible for older configurations or YAML files which may have hard coded the images with suffixes.
-->
<h2 id="multi-architecture-images-with-image-indexes">带镜像索引的多架构镜像 </h2>
<p>除了提供二进制的镜像之外，容器仓库也可以提供
<a href="https://github.com/opencontainers/image-spec/blob/master/image-index.md">容器镜像索引</a>。
镜像索引可以根据特定于体系结构版本的容器指向镜像的多个
<a href="https://github.com/opencontainers/image-spec/blob/master/manifest.md">镜像清单</a>。
这背后的理念是让你可以为镜像命名（例如：<code>pause</code>、<code>example/mycontainer</code>、<code>kube-apiserver</code>）
的同时，允许不同的系统基于它们所使用的机器体系结构取回正确的二进制镜像。</p>
<p>Kubernetes 自身通常在命名容器镜像时添加后缀 <code>-$(ARCH)</code>。
为了向前兼容，请在生成较老的镜像时也提供后缀。
这里的理念是为某镜像（如 <code>pause</code>）生成针对所有平台都适用的清单时，
生成 <code>pause-amd64</code> 这类镜像，以便较老的配置文件或者将镜像后缀影编码到其中的
YAML 文件也能兼容。</p>
<!--
## Using a private registry

Private registries may require keys to read images from them.  
Credentials can be provided in several ways:
-->
<h2 id="using-a-private-registry">使用私有仓库  </h2>
<p>从私有仓库读取镜像时可能需要密钥。
凭证可以用以下方式提供:</p>
<!--
  - Configuring Nodes to Authenticate to a Private Registry
    - all pods can read any configured private registries
    - requires node configuration by cluster administrator
  - Pre-pulled Images
    - all pods can use any images cached on a node
    - requires root access to all nodes to setup
  - Specifying ImagePullSecrets on a Pod
    - only pods which provide own keys can access the private registry
  - Vendor-specific or local extensions
    - if you're using a custom node configuration, you (or your cloud
      provider) can implement your mechanism for authenticating the node
      to the container registry.
-->
<ul>
<li>配置节点向私有仓库进行身份验证
<ul>
<li>所有 Pod 均可读取任何已配置的私有仓库</li>
<li>需要集群管理员配置节点</li>
</ul>
</li>
<li>预拉镜像
<ul>
<li>所有 Pod 都可以使用节点上缓存的所有镜像</li>
<li>需要所有节点的 root 访问权限才能进行设置</li>
</ul>
</li>
<li>在 Pod 中设置 ImagePullSecrets
<ul>
<li>只有提供自己密钥的 Pod 才能访问私有仓库</li>
</ul>
</li>
<li>特定于厂商的扩展或者本地扩展
<ul>
<li>如果你在使用定制的节点配置，你（或者云平台提供商）可以实现让节点
向容器仓库认证的机制</li>
</ul>
</li>
</ul>
<!--
These options are explained in more detail below.
-->
<p>下面将详细描述每一项。</p>
<!--
### Configuring nodes to authenticate to a private registry

Specific instructions for setting credentials depends on the container runtime and registry you chose to use. You should refer to your solution's documentation for the most accurate information.
-->
<h3 id="配置-node-对私有仓库认证">配置 Node 对私有仓库认证</h3>
<p>设置凭据的具体说明取决于你选择使用的容器运行时和仓库。
你应该参考解决方案的文档来获取最准确的信息。</p>
<!--
Default Kubernetes only supports the `auths` and `HttpHeaders` section in Docker configuration.
Docker credential helpers (`credHelpers` or `credsStore`) are not supported.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Kubernetes 默认仅支持 Docker 配置中的 <code>auths</code> 和 <code>HttpHeaders</code> 部分，
不支持 Docker 凭据辅助程序（<code>credHelpers</code> 或 <code>credsStore</code>）。
</div>
<!--
For an example of configuring a private container image registry, see the
[Pull an Image from a Private Registry](/docs/tasks/configure-pod-container/pull-image-private-registry)
task. That example uses a private registry in Docker Hub.
-->
<p>有关配置私有容器镜像仓库的示例，请参阅任务
<a href="/zh/docs/tasks/configure-pod-container/pull-image-private-registry">从私有镜像库中提取图像</a>。
该示例使用 Docker Hub 中的私有注册表。</p>
<!--
### Interpretation of config.json {#config-json}
-->
<h3 id="config-json">config.json 说明</h3>
<!--
The interpretation of `config.json` varies between the original Docker
implementation and the Kubernetes interpretation. In Docker, the `auths` keys
can only specify root URLs, whereas Kubernetes allows glob URLs as well as
prefix-matched paths. This means that a `config.json` like this is valid:
-->
<p>对于 <code>config.json</code> 的解释在原始 Docker 实现和 Kubernetes 的解释之间有所不同。
在 Docker 中，<code>auths</code> 键只能指定根 URL ，而 Kubernetes 允许 glob URLs 以及
前缀匹配的路径。这意味着，像这样的 <code>config.json</code> 是有效的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#008000;font-weight:bold">&#34;auths&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;*my-registry.io/images&#34;</span>: {
            <span style="color:#008000;font-weight:bold">&#34;auth&#34;</span>: <span style="color:#b44">&#34;…&#34;</span>
        }
    }
}
</code></pre></div><!--
The root URL (`*my-registry.io`) is matched by using the following syntax:

```
pattern:
    { term }

term:
    '*'         matches any sequence of non-Separator characters
    '?'         matches any single non-Separator character
    '[' [ '^' ] { character-range } ']'
                character class (must be non-empty)
    c           matches character c (c != '*', '?', '\\', '[')
    '\\' c      matches character c

character-range:
    c           matches character c (c != '\\', '-', ']')
    '\\' c      matches character c
    lo '-' hi   matches character c for lo <= c <= hi
```
-->
<p>使用以下语法匹配根 URL （<code>*my-registry.io</code>）：</p>
<pre><code>pattern:
    { term }

term:
    '*'         匹配任何无分隔符字符序列
    '?'         匹配任意单个非分隔符
    '[' [ '^' ] 字符范围
                  字符集（必须非空）
    c           匹配字符 c （c 不为 '*','?','\\','['）
    '\\' c      匹配字符 c

字符范围: 
    c           匹配字符 c （c 不为 '\\','?','-',']'）
    '\\' c      匹配字符 c
    lo '-' hi   匹配字符范围在 lo 到 hi 之间字符
</code></pre><!--
Image pull operations would now pass the credentials to the CRI container
runtime for every valid pattern. For example the following container image names
would match successfully:

- `my-registry.io/images`
- `my-registry.io/images/my-image`
- `my-registry.io/images/another-image`
- `sub.my-registry.io/images/my-image`
- `a.sub.my-registry.io/images/my-image`
-->
<p>现在镜像拉取操作会将每种有效模式的凭据都传递给 CRI 容器运行时。例如下面的容器镜像名称会匹配成功：</p>
<ul>
<li><code>my-registry.io/images</code></li>
<li><code>my-registry.io/images/my-image</code></li>
<li><code>my-registry.io/images/another-image</code></li>
<li><code>sub.my-registry.io/images/my-image</code></li>
<li><code>a.sub.my-registry.io/images/my-image</code></li>
</ul>
<!--
The kubelet performs image pulls sequentially for every found credential. This
means, that multiple entries in `config.json` are possible, too:
-->
<p>kubelet 为每个找到的凭证的镜像按顺序拉取。 这意味着在 <code>config.json</code> 中可能有多项：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#008000;font-weight:bold">&#34;auths&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;my-registry.io/images&#34;</span>: {
            <span style="color:#008000;font-weight:bold">&#34;auth&#34;</span>: <span style="color:#b44">&#34;…&#34;</span>
        },
        <span style="color:#008000;font-weight:bold">&#34;my-registry.io/images/subpath&#34;</span>: {
            <span style="color:#008000;font-weight:bold">&#34;auth&#34;</span>: <span style="color:#b44">&#34;…&#34;</span>
        }
    }
}
</code></pre></div><!--
If now a container specifies an image `my-registry.io/images/subpath/my-image`
to be pulled, then the kubelet will try to download them from both
authentication sources if one of them fails.
-->
<p>如果一个容器指定了要拉取的镜像 <code>my-registry.io/images/subpath/my-image</code>，
并且其中一个失败，kubelet 将尝试从另一个身份验证源下载镜像。</p>
<!--
### Pre-pulled images
-->
<h3 id="pre-pulled-images">提前拉取镜像  </h3>
<!--
This approach is suitable if you can control node configuration.  It
will not work reliably if your cloud provider manages nodes and replaces
them automatically.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 该方法适用于你能够控制节点配置的场合。
如果你的云供应商负责管理节点并自动置换节点，这一方案无法可靠地工作。
</div>
<!--
By default, the kubelet tries to pull each image from the specified registry.
However, if the `imagePullPolicy` property of the container is set to `IfNotPresent` or `Never`,
then a local image is used (preferentially or exclusively, respectively).
-->
<p>默认情况下，<code>kubelet</code> 会尝试从指定的仓库拉取每个镜像。
但是，如果容器属性 <code>imagePullPolicy</code> 设置为 <code>IfNotPresent</code> 或者 <code>Never</code>，
则会优先使用（对应 <code>IfNotPresent</code>）或者一定使用（对应 <code>Never</code>）本地镜像。</p>
<!--
If you want to rely on pre-pulled images as a substitute for registry authentication,
you must ensure all nodes in the cluster have the same pre-pulled images.

This can be used to preload certain images for speed or as an alternative to authenticating to a private registry.

All pods will have read access to any pre-pulled images.
-->
<p>如果你希望使用提前拉取镜像的方法代替仓库认证，就必须保证集群中所有节点提前拉取的镜像是相同的。</p>
<p>这一方案可以用来提前载入指定的镜像以提高速度，或者作为向私有仓库执行身份认证的一种替代方案。</p>
<p>所有的 Pod 都可以使用节点上提前拉取的镜像。</p>
<!--
### Specifying imagePullSecrets on a Pod
-->
<h3 id="specifying-imagepullsecrets-on-a-pod">在 Pod 上指定 ImagePullSecrets  </h3>
<!--
This is the recommended approach to run containers based on images
in private registries.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 运行使用私有仓库中镜像的容器时，建议使用这种方法。
</div>
<!--
Kubernetes supports specifying container image registry keys on a Pod.
-->
<p>Kubernetes 支持在 Pod 中设置容器镜像仓库的密钥。</p>
<!--
#### Creating a Secret with a Docker config

You need to know the username, registry password and client email address for authenticating
to the registry, as well as its hostname.
Run the following command, substituting the appropriate uppercase values:
-->
<h4 id="creating-a-secret-with-docker-config">使用 Docker Config 创建 Secret  </h4>
<p>你需要知道用于向仓库进行身份验证的用户名、密码和客户端电子邮件地址，以及它的主机名。
运行以下命令，注意替换适当的大写值：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret docker-registry &lt;name&gt; --docker-server<span style="color:#666">=</span>DOCKER_REGISTRY_SERVER --docker-username<span style="color:#666">=</span>DOCKER_USER --docker-password<span style="color:#666">=</span>DOCKER_PASSWORD --docker-email<span style="color:#666">=</span>DOCKER_EMAIL
</code></pre></div><!--
If you already have a Docker credentials file then, rather than using the above
command, you can import the credentials file as a Kubernetes
<a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secrets'>Secrets</a>.  
[Create a Secret based on existing Docker credentials](/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials) explains how to set this up.
-->
<p>如果你已经有 Docker 凭据文件，则可以将凭据文件导入为 Kubernetes
<a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>，
而不是执行上面的命令。
<a href="/zh/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials">基于已有的 Docker 凭据创建 Secret</a>
解释了如何完成这一操作。</p>
<!--
This is particularly useful if you are using multiple private container
registries, as `kubectl create secret docker-registry` creates a Secret that
only works with a single private registry.
-->
<p>如果你在使用多个私有容器仓库，这种技术将特别有用。
原因是 <code>kubectl create secret docker-registry</code> 创建的是仅适用于某个私有仓库的 Secret。</p>
<!--
Pods can only reference image pull secrets in their own namespace,
so this process needs to be done one time per namespace.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Pod 只能引用位于自身所在名字空间中的 Secret，因此需要针对每个名字空间
重复执行上述过程。
</div>
<!--
#### Referring to an imagePullSecrets on a Pod

Now, you can create pods which reference that secret by adding an `imagePullSecrets`
section to a Pod definition.

For example:
-->
<h4 id="在-pod-中引用-imagepullsecrets">在 Pod 中引用 ImagePullSecrets</h4>
<p>现在，在创建 Pod 时，可以在 Pod 定义中增加 <code>imagePullSecrets</code> 部分来引用该 Secret。</p>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt; pod.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: Pod
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: foo
</span><span style="color:#b44">  namespace: awesomeapps
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  containers:
</span><span style="color:#b44">    - name: foo
</span><span style="color:#b44">      image: janedoe/awesomeapp:v1
</span><span style="color:#b44">  imagePullSecrets:
</span><span style="color:#b44">    - name: myregistrykey
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF &gt;&gt; ./kustomization.yaml
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- pod.yaml
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
This needs to be done for each pod that is using a private registry.  

However, setting of this field can be automated by setting the imagePullSecrets
in a [ServiceAccount](/docs/tasks/configure-pod-container/configure-service-account/) resource.

Check [Add ImagePullSecrets to a Service Account](/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account) for detailed instructions.

You can use this in conjunction with a per-node `.docker/config.json`.  The credentials
will be merged.
-->
<p>你需要对使用私有仓库的每个 Pod 执行以上操作。
不过，设置该字段的过程也可以通过为
<a href="/zh/docs/tasks/configure-pod-container/configure-service-account/">服务账号</a>
资源设置 <code>imagePullSecrets</code> 来自动完成。
有关详细指令可参见
<a href="/zh/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">将 ImagePullSecrets 添加到服务账号</a>。</p>
<p>你也可以将此方法与节点级别的 <code>.docker/config.json</code> 配置结合使用。
来自不同来源的凭据会被合并。</p>
<!--
## Use cases

There are a number of solutions for configuring private registries.  Here are some
common use cases and suggested solutions.
-->
<h2 id="use-cases">使用案例 </h2>
<p>配置私有仓库有多种方案，以下是一些常用场景和建议的解决方案。</p>
<!--
1. Cluster running only non-proprietary (e.g. open-source) images.  No need to hide images.
   - Use public images on the Docker hub.
     - No configuration required.
     - Some cloud providers automatically cache or mirror public images, which improves availability and reduces the time to pull images.
-->
<ol>
<li>集群运行非专有镜像（例如，开源镜像）。镜像不需要隐藏。
<ul>
<li>使用 Docker hub 上的公开镜像
<ul>
<li>无需配置</li>
<li>某些云厂商会自动为公开镜像提供高速缓存，以便提升可用性并缩短拉取镜像所需时间</li>
</ul>
</li>
</ul>
</li>
</ol>
<!--
1. Cluster running some proprietary images which should be hidden to those outside the company, but
   visible to all cluster users.
   - Use a hosted private [Docker registry](https://docs.docker.com/registry/).
     - It may be hosted on the [Docker Hub](https://hub.docker.com/signup), or elsewhere.
     - Manually configure .docker/config.json on each node as described above.
   - Or, run an internal private registry behind your firewall with open read access.
     - No Kubernetes configuration is required.
   - Use a hosted container image registry service that controls image access
     - It will work better with cluster autoscaling than manual node configuration.
   - Or, on a cluster where changing the node configuration is inconvenient, use `imagePullSecrets`.
-->
<ol start="2">
<li>集群运行一些专有镜像，这些镜像需要对公司外部隐藏，对所有集群用户可见
<ul>
<li>使用托管的私有 <a href="https://docs.docker.com/registry/">Docker 仓库</a>。
<ul>
<li>可以托管在 <a href="https://hub.docker.com/account/signup/">Docker Hub</a> 或者其他地方</li>
<li>按照上面的描述，在每个节点上手动配置 <code>.docker/config.json</code> 文件</li>
</ul>
</li>
<li>或者，在防火墙内运行一个组织内部的私有仓库，并开放读取权限
<ul>
<li>不需要配置 Kubenretes</li>
</ul>
</li>
<li>使用控制镜像访问的托管容器镜像仓库服务
<ul>
<li>与手动配置节点相比，这种方案能更好地处理集群自动扩缩容</li>
</ul>
</li>
<li>或者，在不方便更改节点配置的集群中，使用 <code>imagePullSecrets</code></li>
</ul>
</li>
</ol>
<!--
1. Cluster with proprietary images, a few of which require stricter access control.
   - Ensure [AlwaysPullImages admission controller](/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages) is active. Otherwise, all Pods potentially have access to all images.
   - Move sensitive data into a "Secret" resource, instead of packaging it in an image.
-->
<ol start="3">
<li>集群使用专有镜像，且有些镜像需要更严格的访问控制
<ul>
<li>确保 <a href="/zh/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages">AlwaysPullImages 准入控制器</a>被启用。否则，所有 Pod 都可以使用所有镜像。</li>
<li>确保将敏感数据存储在 Secret 资源中，而不是将其打包在镜像里</li>
</ul>
</li>
</ol>
<!--
1. A multi-tenant cluster where each tenant needs own private registry.
   - Ensure [AlwaysPullImages admission controller](/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages) is active. Otherwise, all Pods of all tenants potentially have access to all images.
   - Run a private registry with authorization required.
   - Generate registry credential for each tenant, put into secret, and populate secret to each tenant namespace.
   - The tenant adds that secret to imagePullSecrets of each namespace.
-->
<ol start="4">
<li>集群是多租户的并且每个租户需要自己的私有仓库
<ul>
<li>确保 <a href="/zh/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages">AlwaysPullImages 准入控制器</a>。否则，所有租户的所有的 Pod 都可以使用所有镜像。</li>
<li>为私有仓库启用鉴权</li>
<li>为每个租户生成访问仓库的凭据，放置在 Secret 中，并将 Secrert 发布到各租户的命名空间下。</li>
<li>租户将 Secret 添加到每个名字空间中的 imagePullSecrets</li>
</ul>
</li>
</ol>
<!--
If you need access to multiple registries, you can create one secret for each registry.
Kubelet will merge any `imagePullSecrets` into a single virtual `.docker/config.json`
-->
<p>如果你需要访问多个仓库，可以为每个仓库创建一个 Secret。
<code>kubelet</code> 将所有 <code>imagePullSecrets</code> 合并为一个虚拟的 <code>.docker/config.json</code> 文件。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read the [OCI Image Manifest Specification](https://github.com/opencontainers/image-spec/blob/master/manifest.md).
* Learn about [container image garbage collection](/docs/concepts/architecture/garbage-collection/#container-image-garbage-collection).
-->
<ul>
<li>阅读 <a href="https://github.com/opencontainers/image-spec/blob/master/manifest.md">OCI Image Manifest 规范</a>。</li>
<li>了解<a href="/zh/docs/concepts/architecture/garbage-collection/#container-image-garbage-collection">容器镜像垃圾收集</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-643212488f778acf04bebed65ba34441">3.2 - 容器环境</h1>
    
	<!--
reviewers:
- mikedanese
- thockin
title: Container Environment
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
This page describes the resources available to Containers in the Container environment. 
-->
<p>本页描述了在容器环境里容器可用的资源。</p>
<!-- body -->
<!--
## Container environment

The Kubernetes Container environment provides several important resources to Containers:

* A filesystem, which is a combination of an [image](/docs/concepts/containers/images/) and one or more [volumes](/docs/concepts/storage/volumes/).
* Information about the Container itself.
* Information about other objects in the cluster.
-->
<h2 id="container-environment">容器环境 </h2>
<p>Kubernetes 的容器环境给容器提供了几个重要的资源：</p>
<ul>
<li>文件系统，其中包含一个<a href="/zh/docs/concepts/containers/images/">镜像</a>
和一个或多个的<a href="/zh/docs/concepts/storage/volumes/">卷</a></li>
<li>容器自身的信息</li>
<li>集群中其他对象的信息</li>
</ul>
<!--
### Container information

The *hostname* of a Container is the name of the Pod in which the Container is running.
It is available through the `hostname` command or the
[`gethostname`](http://man7.org/linux/man-pages/man2/gethostname.2.html)
function call in libc.

The Pod name and namespace are available as environment variables through the
[downward API](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/).

User defined environment variables from the Pod definition are also available to the Container,
as are any environment variables specified statically in the container image.
-->
<h3 id="容器信息">容器信息</h3>
<p>容器的 <em>hostname</em> 是它所运行在的 pod 的名称。它可以通过 <code>hostname</code> 命令或者调用 libc 中的
<a href="https://man7.org/linux/man-pages/man2/gethostname.2.html"><code>gethostname</code></a> 函数来获取。</p>
<p>Pod 名称和命名空间可以通过
<a href="/zh/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">下行 API</a>
转换为环境变量。</p>
<p>Pod 定义中的用户所定义的环境变量也可在容器中使用，就像在 container 镜像中静态指定的任何环境变量一样。</p>
<!--
### Cluster information

A list of all services that were running when a Container was created is available to that Container as environment variables.
This list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.

For a service named *foo* that maps to a Container named *bar*,
the following variables are defined:
-->
<h3 id="集群信息">集群信息</h3>
<p>创建容器时正在运行的所有服务都可用作该容器的环境变量。
这里的服务仅限于新容器的 Pod 所在的名字空间中的服务，以及 Kubernetes 控制面的服务。</p>
<p>对于名为 <em>foo</em> 的服务，当映射到名为 <em>bar</em> 的容器时，以下变量是被定义了的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">FOO_SERVICE_HOST</span><span style="color:#666">=</span>&lt;the host the service is running on&gt;
<span style="color:#b8860b">FOO_SERVICE_PORT</span><span style="color:#666">=</span>&lt;the port the service is running on&gt;
</code></pre></div><!--
Services have dedicated IP addresses and are available to the Container via DNS,
if [DNS addon](https://releases.k8s.io/v1.23.0/cluster/addons/dns/) is enabled. 
-->
<p>服务具有专用的 IP 地址。如果启用了
<a href="https://releases.k8s.io/v1.23.0/cluster/addons/dns/">DNS 插件</a>，
可以在容器中通过 DNS 来访问服务。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [Container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).
* Get hands-on experience
  [attaching handlers to Container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).
-->
<ul>
<li>学习更多有关<a href="/zh/docs/concepts/containers/container-lifecycle-hooks/">容器生命周期回调</a>的知识</li>
<li>动手<a href="/zh/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">为容器生命周期事件添加处理程序</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a858027489648786a3b16264e451272b">3.3 - 容器运行时类（Runtime Class）</h1>
    
	<!--
reviewers:
- tallclair
- dchen1107
title: Runtime Class
content_type: concept
weight: 20
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>


<!-- 
This page describes the RuntimeClass resource and runtime selection mechanism.

RuntimeClass is a feature for selecting the container runtime configuration. The container runtime
configuration is used to run a Pod's containers.
-->
<p>本页面描述了 RuntimeClass 资源和运行时的选择机制。</p>
<p>RuntimeClass 是一个用于选择容器运行时配置的特性，容器运行时配置用于运行 Pod 中的容器。</p>
<!-- body -->
<!-- 
## Motivation

You can set a different RuntimeClass between different Pods to provide a balance of
performance versus security. For example, if part of your workload deserves a high
level of information security assurance, you might choose to schedule those Pods so
that they run in a container runtime that uses hardware virtualization. You'd then
benefit from the extra isolation of the alternative runtime, at the expense of some
additional overhead.
-->
<h2 id="motivation">动机  </h2>
<p>你可以在不同的 Pod 设置不同的 RuntimeClass，以提供性能与安全性之间的平衡。
例如，如果你的部分工作负载需要高级别的信息安全保证，你可以决定在调度这些 Pod
时尽量使它们在使用硬件虚拟化的容器运行时中运行。
这样，你将从这些不同运行时所提供的额外隔离中获益，代价是一些额外的开销。</p>
<!--
You can also use RuntimeClass to run different Pods with the same container runtime
but with different settings.
-->
<p>你还可以使用 RuntimeClass 运行具有相同容器运行时但具有不同设置的 Pod。</p>
<!-- 
## Setup
-->
<h2 id="setup">设置 </h2>
<!--
1. Configure the CRI implementation on nodes (runtime dependent)
2. Create the corresponding RuntimeClass resources
-->
<ol>
<li>在节点上配置 CRI 的实现（取决于所选用的运行时）</li>
<li>创建相应的 RuntimeClass 资源</li>
</ol>
<!--
### 1. Configure the CRI implementation on nodes
-->
<h3 id="1-在节点上配置-cri-实现">1. 在节点上配置 CRI 实现</h3>
<!--
The configurations available through RuntimeClass are Container Runtime Interface (CRI)
implementation dependent. See the corresponding documentation ([below](#cri-configuration)) for your
CRI implementation for how to configure.
-->
<p>RuntimeClass 的配置依赖于 运行时接口（CRI）的实现。
根据你使用的 CRI 实现，查阅相关的文档（<a href="#cri-configuration">下方</a>）来了解如何配置。</p>
<!--
RuntimeClass assumes a homogeneous node configuration across the cluster by default (which means
that all nodes are configured the same way with respect to container runtimes). To support
heterogenous node configurations, see [Scheduling](#scheduling) below.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> RuntimeClass 假设集群中的节点配置是同构的（换言之，所有的节点在容器运行时方面的配置是相同的）。
如果需要支持异构节点，配置方法请参阅下面的 <a href="#scheduling">调度</a>。
</div>
<!--
The configurations have a corresponding `handler` name, referenced by the RuntimeClass. The
handler must be a valid [DNS label name](/docs/concepts/overview/working-with-objects/names/#dns-label-names).
-->
<p>所有这些配置都具有相应的 <code>handler</code> 名，并被 RuntimeClass 引用。
handler 必须是有效的 <a href="/zh/docs/concepts/overview/working-with-objects/names/#dns-label-names">DNS 标签名</a>。</p>
<!--
### 2. Create the corresponding RuntimeClass resources

The configurations setup in step 1 should each have an associated `handler` name, which identifies
the configuration. For each handler, create a corresponding RuntimeClass object.
-->
<h3 id="2-创建相应的-runtimeclass-资源">2. 创建相应的 RuntimeClass 资源</h3>
<p>在上面步骤 1 中，每个配置都需要有一个用于标识配置的 <code>handler</code>。
针对每个 handler 需要创建一个 RuntimeClass 对象。</p>
<!--
The RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name
(`metadata.name`) and the handler (`handler`). The object definition looks like this:
-->
<p>RuntimeClass 资源当前只有两个重要的字段：RuntimeClass 名 (<code>metadata.name</code>) 和 handler (<code>handler</code>)。
对象定义如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>node.k8s.io/v1 <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># RuntimeClass 定义于 node.k8s.io API 组</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>RuntimeClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myclass <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 用来引用 RuntimeClass 的名字</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># RuntimeClass 是一个集群层面的资源</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">handler</span>:<span style="color:#bbb"> </span>myconfiguration <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 对应的 CRI 配置的名称</span><span style="color:#bbb">
</span></code></pre></div><!--
It is recommended that RuntimeClass write operations (create/update/patch/delete) be
restricted to the cluster administrator. This is typically the default. See [Authorization
Overview](/docs/reference/access-authn-authz/authorization/) for more details.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 建议将 RuntimeClass 写操作（create、update、patch 和 delete）限定于集群管理员使用。
通常这是默认配置。参阅<a href="/zh/docs/reference/access-authn-authz/authorization/">授权概述</a>了解更多信息。
</div>
<!--
## Usage

Once RuntimeClasses are configured for the cluster, using them is very simple. Specify a
`runtimeClassName` in the Pod spec. For example:
-->
<h2 id="usage">使用说明 </h2>
<p>一旦完成集群中 RuntimeClasses 的配置，使用起来非常方便。
在 Pod spec 中指定 <code>runtimeClassName</code> 即可。例如:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">runtimeClassName</span>:<span style="color:#bbb"> </span>myclass<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span></code></pre></div><!--
This will instruct the kubelet to use the named RuntimeClass to run this pod. If the named
RuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will enter the
`Failed` terminal [phase](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase). Look for a
corresponding [event](/docs/tasks/debug-application-cluster/debug-application-introspection/) for an
error message.
-->
<p>这一设置会告诉 kubelet 使用所指的 RuntimeClass 来运行该 pod。
如果所指的 RuntimeClass 不存在或者 CRI 无法运行相应的 handler，
那么 pod 将会进入 <code>Failed</code> 终止<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase">阶段</a>。
你可以查看相应的<a href="/zh/docs/tasks/debug-application-cluster/debug-application-introspection/">事件</a>，
获取执行过程中的错误信息。</p>
<!--
If no `runtimeClassName` is specified, the default RuntimeHandler will be used, which is equivalent
to the behavior when the RuntimeClass feature is disabled.
-->
<p>如果未指定 <code>runtimeClassName</code> ，则将使用默认的 RuntimeHandler，相当于禁用 RuntimeClass 功能特性。</p>
<!-- 
### CRI Configuration

For more details on setting up CRI runtimes, see [CRI installation](/docs/setup/production-environment/container-runtimes/).
-->
<h3 id="cri-configuration">CRI 配置  </h3>
<p>关于如何安装 CRI 运行时，请查阅
<a href="/zh/docs/setup/production-environment/container-runtimes/">CRI 安装</a>。</p>
<h4 id="dockershim">dockershim</h4>
<!--





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [deprecated]</code>
</div>



Dockershim is deprecated as of Kubernetes v1.20, and will be removed in v1.24. For more information on the deprecation,
see [dockershim deprecation](/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation)
-->
<p>Dockershim 自 Kubernetes v1.20 起已弃用，并将在 v1.24 中删除。
有关弃用的更多信息查看 <a href="/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation">dockershim 弃用</a>。</p>
<!--
RuntimeClasses with dockershim must set the runtime handler to `docker`. Dockershim does not support
custom configurable runtime handlers.
-->
<p>为 dockershim 设置 RuntimeClass 时，必须将运行时处理程序设置为 <code>docker</code>。
Dockershim 不支持自定义的可配置的运行时处理程序。</p>
<h4 id="containerd-https-containerd-io"><a href="https://containerd.io/">containerd</a></h4>
<!--
Runtime handlers are configured through containerd's configuration at
`/etc/containerd/config.toml`. Valid handlers are configured under the runtimes section:
-->
<p>通过 containerd 的 <code>/etc/containerd/config.toml</code> 配置文件来配置运行时 handler。
handler 需要配置在 runtimes 块中：</p>
<pre><code>[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.${HANDLER_NAME}]
</code></pre><!--
See the containerd [CRI Plugin Config Guide](https://github.com/containerd/containerd/blob/main/docs/cri/config.md) for more details.
-->
<p>更详细信息，请查阅 containerd
<a href="https://github.com/containerd/cri/blob/master/docs/config.md">CRI 插件配置指南</a></p>
<h4 id="cri-o-https-cri-o-io"><a href="https://cri-o.io/">cri-o</a></h4>
<!--
Runtime handlers are configured through cri-o's configuration at `/etc/crio/crio.conf`. Valid
handlers are configured under the [crio.runtime
table](https://github.com/kubernetes-sigs/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table):
-->
<p>通过 cri-o 的 <code>/etc/crio/crio.conf</code> 配置文件来配置运行时 handler。
handler 需要配置在
<a href="https://github.com/kubernetes-sigs/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table">crio.runtime 表</a>
下面：</p>
<pre><code>[crio.runtime.runtimes.${HANDLER_NAME}]
  runtime_path = &quot;${PATH_TO_BINARY}&quot;
</code></pre><!--
See CRI-O's [config documentation](https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md) for more details.
-->
<p>更详细信息，请查阅 CRI-O <a href="https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md">配置文档</a>。</p>
<!-- 
## Scheduling
 -->
<h2 id="scheduling">调度 </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.16 [beta]</code>
</div>


<!--
By specifying the `scheduling` field for a RuntimeClass, you can set constraints to
ensure that Pods running with this RuntimeClass are scheduled to nodes that support it.
If `scheduling` is not set, this RuntimeClass is assumed to be supported by all nodes.
-->
<p>通过为 RuntimeClass 指定 <code>scheduling</code> 字段，
你可以通过设置约束，确保运行该 RuntimeClass 的 Pod 被调度到支持该 RuntimeClass 的节点上。
如果未设置 <code>scheduling</code>，则假定所有节点均支持此 RuntimeClass 。</p>
<!--
To ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have a
common label which is then selected by the `runtimeclass.scheduling.nodeSelector` field. The
RuntimeClass's nodeSelector is merged with the pod's nodeSelector in admission, effectively taking
the intersection of the set of nodes selected by each. If there is a conflict, the pod will be
rejected.
-->
<p>为了确保 pod 会被调度到支持指定运行时的 node 上，每个 node 需要设置一个通用的 label 用于被
<code>runtimeclass.scheduling.nodeSelector</code> 挑选。在 admission 阶段，RuntimeClass 的 nodeSelector 将会与
pod 的 nodeSelector 合并，取二者的交集。如果有冲突，pod 将会被拒绝。</p>
<!--
If the supported nodes are tainted to prevent other RuntimeClass pods from running on the node, you
can add `tolerations` to the RuntimeClass. As with the `nodeSelector`, the tolerations are merged
with the pod's tolerations in admission, effectively taking the union of the set of nodes tolerated
by each.
-->
<p>如果 node 需要阻止某些需要特定 RuntimeClass 的 pod，可以在 <code>tolerations</code> 中指定。
与 <code>nodeSelector</code> 一样，tolerations 也在 admission 阶段与 pod 的 tolerations 合并，取二者的并集。</p>
<!--
To learn more about configuring the node selector and tolerations, see [Assigning Pods to
Nodes](/docs/concepts/configuration/assign-pod-node/).
-->
<p>更多有关 node selector 和 tolerations 的配置信息，请查阅
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">将 Pod 分派到节点</a>。</p>
<!-- 
### Pod Overhead
 -->
<h3 id="pod-overhead">Pod 开销  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<!--
You can specify _overhead_ resources that are associated with running a Pod. Declaring overhead allows
the cluster (including the scheduler) to account for it when making decisions about Pods and resources.  
To use Pod overhead, you must have the PodOverhead [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
enabled (it is on by default).
-->
<p>你可以指定与运行 Pod 相关的 <em>开销</em> 资源。声明开销即允许集群（包括调度器）在决策 Pod 和资源时将其考虑在内。
若要使用 Pod 开销特性，你必须确保 PodOverhead
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
处于启用状态（默认为启用状态）。</p>
<!--
Pod overhead is defined in RuntimeClass through the `Overhead` fields. Through the use of these fields,
you can specify the overhead of running pods utilizing this RuntimeClass and ensure these overheads
are accounted for in Kubernetes.
-->
<p>Pod 开销通过 RuntimeClass 的 <code>overhead</code> 字段定义。
通过使用这些字段，你可以指定使用该 RuntimeClass 运行 Pod 时的开销并确保 Kubernetes 将这些开销计算在内。</p>
<h2 id="what-s-next">What's next</h2>
<!--
- [RuntimeClass Design](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md)
- [RuntimeClass Scheduling Design](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md#runtimeclass-scheduling)
- Read about the [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/) concept
- [PodOverhead Feature Design](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead)
-->
<ul>
<li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md">RuntimeClass 设计</a></li>
<li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md#runtimeclass-scheduling">RuntimeClass 调度设计</a></li>
<li>阅读关于 <a href="/zh/docs/concepts/scheduling-eviction/pod-overhead/">Pod 开销</a> 的概念</li>
<li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead">PodOverhead 特性设计</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e6941d969d81540208a3e78bc56f43bc">3.4 - 容器生命周期回调</h1>
    
	<!--
reviewers:
- mikedanese
- thockin
title: Container Lifecycle Hooks
content_type: concept
weight: 30
-->
<!-- overview -->
<!--
This page describes how kubelet managed Containers can use the Container lifecycle hook framework
to run code triggered by events during their management lifecycle.
-->
<p>这个页面描述了 kubelet 管理的容器如何使用容器生命周期回调框架，
藉由其管理生命周期中的事件触发，运行指定代码。</p>
<!-- body -->
<!--
## Overview

Analogous to many programming language frameworks that have component lifecycle hooks, such as Angular,
Kubernetes provides Containers with lifecycle hooks.
The hooks enable Containers to be aware of events in their management lifecycle
and run code implemented in a handler when the corresponding lifecycle hook is executed.
-->
<h2 id="概述">概述</h2>
<p>类似于许多具有生命周期回调组件的编程语言框架，例如 Angular、Kubernetes 为容器提供了生命周期回调。
回调使容器能够了解其管理生命周期中的事件，并在执行相应的生命周期回调时运行在处理程序中实现的代码。</p>
<!--
## Container hooks

There are two hooks that are exposed to Containers:
-->
<h2 id="容器回调">容器回调</h2>
<p>有两个回调暴露给容器：</p>
<p><code>PostStart</code></p>
<!--
This hook is executed immediately after a container is created.
However, there is no guarantee that the hook will execute before the container ENTRYPOINT.
No parameters are passed to the handler.
-->
<p>这个回调在容器被创建之后立即被执行。
但是，不能保证回调会在容器入口点（ENTRYPOINT）之前执行。
没有参数传递给处理程序。</p>
<p><code>PreStop</code></p>
<!--
This hook is called immediately before a container is terminated due to an API request or management
event such as a liveness/startup probe failure, preemption, resource contention and others. A call
to the `PreStop` hook fails if the container is already in a terminated or completed state and the
hook must complete before the TERM signal to stop the container can be sent. The Pod's termination
grace period countdown begins before the `PreStop` hook is executed, so regardless of the outcome of
the handler, the container will eventually terminate within the Pod's termination grace period. No
parameters are passed to the handler.
-->
<p>在容器因 API 请求或者管理事件（诸如存活态探针、启动探针失败、资源抢占、资源竞争等）
而被终止之前，此回调会被调用。
如果容器已经处于已终止或者已完成状态，则对 preStop 回调的调用将失败。
在用来停止容器的 TERM 信号被发出之前，回调必须执行结束。
Pod 的终止宽限周期在 <code>PreStop</code> 回调被执行之前即开始计数，所以无论
回调函数的执行结果如何，容器最终都会在 Pod 的终止宽限期内被终止。
没有参数会被传递给处理程序。</p>
<!--
A more detailed description of the termination behavior can be found in
[Termination of Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination).
-->
<p>有关终止行为的更详细描述，请参见
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#termination-of-pods">终止 Pod</a>。</p>
<!--
### Hook handler implementations

Containers can access a hook by implementing and registering a handler for that hook.
There are two types of hook handlers that can be implemented for Containers:
-->
<h3 id="回调处理程序的实现">回调处理程序的实现</h3>
<p>容器可以通过实现和注册该回调的处理程序来访问该回调。
针对容器，有两种类型的回调处理程序可供实现：</p>
<!--
* Exec - Executes a specific command, such as `pre-stop.sh`, inside the cgroups and namespaces of the Container.
Resources consumed by the command are counted against the Container.
* HTTP - Executes an HTTP request against a specific endpoint on the Container.
-->
<ul>
<li>Exec - 在容器的 cgroups 和名称空间中执行特定的命令（例如 <code>pre-stop.sh</code>）。
命令所消耗的资源计入容器的资源消耗。</li>
<li>HTTP - 对容器上的特定端点执行 HTTP 请求。</li>
</ul>
<!--
### Hook handler execution

When a Container lifecycle management hook is called,
the Kubernetes management system executes the handler according to the hook action,
`httpGet` and `tcpSocket` are executed by the kubelet process, and `exec` is executed in the container.
-->
<h3 id="回调处理程序执行">回调处理程序执行</h3>
<p>当调用容器生命周期管理回调时，Kubernetes 管理系统根据回调动作执行其处理程序，
<code>httpGet</code> 和 <code>tcpSocket</code> 在kubelet 进程执行，而 <code>exec</code> 则由容器内执行 。</p>
<!--
Hook handler calls are synchronous within the context of the Pod containing the Container.
This means that for a `PostStart` hook,
the Container ENTRYPOINT and hook fire asynchronously.
However, if the hook takes too long to run or hangs,
the Container cannot reach a `running` state.
-->
<p>回调处理程序调用在包含容器的 Pod 上下文中是同步的。
这意味着对于 <code>PostStart</code> 回调，容器入口点和回调异步触发。
但是，如果回调运行或挂起的时间太长，则容器无法达到 <code>running</code> 状态。</p>
<!--
`PreStop` hooks are not executed asynchronously from the signal
to stop the Container; the hook must complete its execution before
the TERM signal can be sent.
If a `PreStop` hook hangs during execution,
the Pod's phase will be `Terminating` and remain there until the Pod is
killed after its `terminationGracePeriodSeconds` expires.
This grace period applies to the total time it takes for both
the `PreStop` hook to execute and for the Container to stop normally.
If, for example, `terminationGracePeriodSeconds` is 60, and the hook
takes 55 seconds to complete, and the Container takes 10 seconds to stop
normally after receiving the signal, then the Container will be killed
before it can stop normally, since `terminationGracePeriodSeconds` is
less than the total time (55+10) it takes for these two things to happen.

`PreStop` hooks are not executed asynchronously from the signal
to stop the Container; the hook must complete its execution before
the signal can be sent.
If a `PreStop` hook hangs during execution,
the Pod's phase will be `Terminating` and remain there until the Pod is
killed after its `terminationGracePeriodSeconds` expires.
This grace period applies to the total time it takes for both
the `PreStop` hook to execute and for the Container to stop normally.
If, for example, `terminationGracePeriodSeconds` is 60, and the hook
takes 55 seconds to complete, and the Container takes 10 seconds to stop
normally after receiving the signal, then the Container will be killed
before it can stop normally, since `terminationGracePeriodSeconds` is
less than the total time (55+10) it takes for these two things to happen.
-->
<p><code>PreStop</code> 回调并不会与停止容器的信号处理程序异步执行；回调必须在
可以发送信号之前完成执行。
如果 <code>PreStop</code> 回调在执行期间停滞不前，Pod 的阶段会变成 <code>Terminating</code>
并且一直处于该状态，直到其 <code>terminationGracePeriodSeconds</code> 耗尽为止，
这时 Pod 会被杀死。
这一宽限期是针对 <code>PreStop</code> 回调的执行时间及容器正常停止时间的总和而言的。
例如，如果 <code>terminationGracePeriodSeconds</code> 是 60，回调函数花了 55 秒钟
完成执行，而容器在收到信号之后花了 10 秒钟来正常结束，那么容器会在其
能够正常结束之前即被杀死，因为 <code>terminationGracePeriodSeconds</code> 的值
小于后面两件事情所花费的总时间（55+10）。</p>
<!--
If either a `PostStart` or `PreStop` hook fails,
it kills the Container.
-->
<p>如果 <code>PostStart</code> 或 <code>PreStop</code> 回调失败，它会杀死容器。</p>
<!--
Users should make their hook handlers as lightweight as possible.
There are cases, however, when long running commands make sense,
such as when saving state prior to stopping a Container.
-->
<p>用户应该使他们的回调处理程序尽可能的轻量级。
但也需要考虑长时间运行的命令也很有用的情况，比如在停止容器之前保存状态。</p>
<!--
### Hook delivery guarantees

Hook delivery is intended to be *at least once*,
which means that a hook may be called multiple times for any given event,
such as for `PostStart` or `PreStop`.
It is up to the hook implementation to handle this correctly.
-->
<h3 id="回调递送保证">回调递送保证</h3>
<p>回调的递送应该是 <em>至少一次</em>，这意味着对于任何给定的事件，
例如 <code>PostStart</code> 或 <code>PreStop</code>，回调可以被调用多次。
如何正确处理被多次调用的情况，是回调实现所要考虑的问题。</p>
<!--
Generally, only single deliveries are made.
If, for example, an HTTP hook receiver is down and is unable to take traffic,
there is no attempt to resend.
In some rare cases, however, double delivery may occur.
For instance, if a kubelet restarts in the middle of sending a hook,
the hook might be resent after the kubelet comes back up.
-->
<p>通常情况下，只会进行单次递送。
例如，如果 HTTP 回调接收器宕机，无法接收流量，则不会尝试重新发送。
然而，偶尔也会发生重复递送的可能。
例如，如果 kubelet 在发送回调的过程中重新启动，回调可能会在 kubelet 恢复后重新发送。</p>
<!--
### Debugging Hook handlers

The logs for a Hook handler are not exposed in Pod events.
If a handler fails for some reason, it broadcasts an event.
For `PostStart`, this is the `FailedPostStartHook` event,
and for `PreStop`, this is the `FailedPreStopHook` event.
To generate a failed `FailedPreStopHook` event yourself, modify the [lifecycle-events.yaml](https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/lifecycle-events.yaml) file to change the postStart command to "badcommand" and apply it.
Here is some example output of the resulting events you see from running `kubectl describe pod lifecycle-demo`:
-->
<h3 id="调试回调处理程序">调试回调处理程序</h3>
<p>回调处理程序的日志不会在 Pod 事件中公开。
如果处理程序由于某种原因失败，它将播放一个事件。
对于 <code>PostStart</code>，这是 <code>FailedPostStartHook</code> 事件，对于 <code>PreStop</code>，这是 <code>FailedPreStopHook</code> 事件。
要自己生成失败的 <code>FailedPreStopHook</code> 事件，请修改
<a href="https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/lifecycle-events.yaml">lifecycle-events.yaml</a>
文件将 postStart 命令更改为 ”badcommand“ 并应用它。
以下是通过运行 <code>kubectl describe pod lifecycle-demo</code> 后你看到的一些结果事件的示例输出：</p>
<pre><code>Events:
  Type     Reason               Age              From               Message
  ----     ------               ----             ----               -------
  Normal   Scheduled            7s               default-scheduler  Successfully assigned default/lifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2...
  Normal   Pulled               6s               kubelet            Successfully pulled image &quot;nginx&quot; in 229.604315ms
  Normal   Pulling              4s (x2 over 6s)  kubelet            Pulling image &quot;nginx&quot;
  Normal   Created              4s (x2 over 5s)  kubelet            Created container lifecycle-demo-container
  Normal   Started              4s (x2 over 5s)  kubelet            Started container lifecycle-demo-container
  Warning  FailedPostStartHook  4s (x2 over 5s)  kubelet            Exec lifecycle hook ([badcommand]) for Container &quot;lifecycle-demo-container&quot; in Pod &quot;lifecycle-demo_default(30229739-9651-4e5a-9a32-a8f1688862db)&quot; failed - error: command 'badcommand' exited with 126: , message: &quot;OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \&quot;badcommand\&quot;: executable file not found in $PATH: unknown\r\n&quot;
  Normal   Killing              4s (x2 over 5s)  kubelet            FailedPostStartHook
  Normal   Pulled               4s               kubelet            Successfully pulled image &quot;nginx&quot; in 215.66395ms
  Warning  BackOff              2s (x2 over 3s)  kubelet            Back-off restarting failed container
</code></pre><h2 id="what-s-next">What's next</h2>
<!--
* Learn more about the [Container environment](/docs/concepts/containers/container-environment/).
* Get hands-on experience
  [attaching handlers to Container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).
-->
<ul>
<li>进一步了解<a href="/zh/docs/concepts/containers/container-environment/">容器环境</a></li>
<li>动手实践，<a href="/zh/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">为容器生命周期事件添加处理程序</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d52aadda80edd9f8c514cfe2321363c2">4 - 工作负载</h1>
    <div class="lead">理解 Pods，Kubernetes 中可部署的最小计算对象，以及辅助它运行它们的高层抽象对象。</div>
	<!--
title: "Workloads"
weight: 50
description: >
  Understand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.
no_list: true
-->
工作负载是在 Kubernetes 上运行的应用程序。
<!--
Whether your workload is a single component or several that work together, on Kubernetes you run
it inside a set of [Pods](/docs/concepts/workloads/pods).
In Kubernetes, a Pod represents a set of running
<a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='containers'>containers</a> on your cluster.

A Pod has a defined lifecycle. For example, once a Pod is running in your cluster then
a critical failure on the <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a> where that
Pod is running means that all the Pods on that node fail. Kubernetes treats that level
of failure as final: you would need to create a new Pod even if the node later recovers.
-->
<p>无论你的负载是单一组件还是由多个一同工作的组件构成，在 Kubernetes 中你
可以在一组 <a href="/zh/docs/concepts/workloads/pods">Pods</a> 中运行它。
在 Kubernetes 中，Pod 代表的是集群上处于运行状态的一组
<a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='容器'>容器</a>。</p>
<!--
Kubernetes pods have a [defined lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/).
For example, once a pod is running in your cluster then a critical fault on the
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a> where that pod is running means that
all the pods on that node fail. Kubernetes treats that level of failure as final: you
would need to create a new `Pod` to recover, even if the node later becomes healthy.
-->
<p>Kubernetes Pods 有<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">确定的生命周期</a>。
例如，当某 Pod 在你的集群中运行时，Pod 运行所在的
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a> 出现致命错误时，
所有该节点上的 Pods 都会失败。Kubernetes 将这类失败视为最终状态：
即使该节点后来恢复正常运行，你也需要创建新的 Pod 来恢复应用。</p>
<!--
However, to make life considerably easier, you don't need to manage each Pod directly.
Instead, you can use _workload resources_ that manage a set of Pods on your behalf.
These resources configure <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controllers'>controllers</a>
that make sure the right number of the right kind of Pod are running, to match the state
you specified.

Kubernetes provides several built-in workload resources:
-->
<p>不过，为了让用户的日子略微好过一些，你并不需要直接管理每个 Pod。
相反，你可以使用 <em>负载资源</em> 来替你管理一组 Pods。
这些资源配置 <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>
来确保合适类型的、处于运行状态的 Pod 个数是正确的，与你所指定的状态相一致。</p>
<p>Kubernetes 提供若干种内置的工作负载资源：</p>
<!--
* [Deployment](/docs/concepts/workloads/controllers/deployment/) and [ReplicaSet](/docs/concepts/workloads/controllers/replicaset/)
  (replacing the legacy resource
  <a class='glossary-tooltip' title='一种管理多副本应用的（已启用）的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-replication-controller' target='_blank' aria-label='ReplicationController'>ReplicationController</a>).
  `Deployment` is a good fit for managing a stateless application workload on your cluster,
  where any `Pod` in the `Deployment` is interchangeable and can be replaced if needed.
* [`StatefulSet`](/docs/concepts/workloads/controllers/statefulset/) lets you
  run one or more related Pods that do track state somehow. For example, if your workload
  records data persistently, you can run a `StatefulSet` that matches each `Pod` with a
  [`PersistentVolume`](/docs/concepts/storage/persistent-volumes/). Your code, running in the
  `Pods` for that `StatefulSet`, can replicate data to other `Pods` in the same `StatefulSet`
  to improve overall resilience.
-->
<ul>
<li><a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a> 和
<a href="/zh/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>
（替换原来的资源 <a class='glossary-tooltip' title='一种管理多副本应用的（已启用）的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-replication-controller' target='_blank' aria-label='ReplicationController'>ReplicationController</a>）。
<code>Deployment</code> 很适合用来管理你的集群上的无状态应用，<code>Deployment</code> 中的所有
<code>Pod</code> 都是相互等价的，并且在需要的时候被换掉。</li>
<li><a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>
让你能够运行一个或者多个以某种方式跟踪应用状态的 Pods。
例如，如果你的负载会将数据作持久存储，你可以运行一个 <code>StatefulSet</code>，将每个
<code>Pod</code> 与某个 <a href="/zh/docs/concepts/storage/persistent-volumes/"><code>PersistentVolume</code></a>
对应起来。你在 <code>StatefulSet</code> 中各个 <code>Pod</code> 内运行的代码可以将数据复制到同一
<code>StatefulSet</code> 中的其它 <code>Pod</code> 中以提高整体的服务可靠性。</li>
</ul>
<!--
* [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) defines `Pods` that provide
  node-local facilities. These might be fundamental to the operation of your cluster, such
  as a networking helper tool, or be part of an
  <a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='add-on'>add-on</a>.
  Every time you add a node to your cluster that matches the specification in a `DaemonSet`,
  the control plane schedules a `Pod` for that `DaemonSet` onto the new node.
* [`Job`](/docs/concepts/workloads/controllers/job/) and
  [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/)
  define tasks that run to completion and then stop. Jobs represent one-off tasks, whereas
  `CronJobs` recur according to a schedule.
-->
<ul>
<li><a href="/zh/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>
定义提供节点本地支撑设施的 <code>Pods</code>。这些 Pods 可能对于你的集群的运维是
非常重要的，例如作为网络链接的辅助工具或者作为网络
<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='插件'>插件</a>
的一部分等等。每次你向集群中添加一个新节点时，如果该节点与某 <code>DaemonSet</code>
的规约匹配，则控制面会为该 <code>DaemonSet</code> 调度一个 <code>Pod</code> 到该新节点上运行。</li>
<li><a href="/zh/docs/concepts/workloads/controllers/job/">Job</a> 和
<a href="/zh/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a>。
定义一些一直运行到结束并停止的任务。<code>Job</code> 用来表达的是一次性的任务，而
<code>CronJob</code> 会根据其时间规划反复运行。</li>
</ul>
<!--
In the wider Kubernetes ecosystem, you can find third-party workload resources that provide
additional behaviors. Using a
[custom resource definition](/docs/concepts/extend-kubernetes/api-extension/custom-resources/),
you can add in a third-party workload resource if you want a specific behavior that's not part
of Kubernetes' core. For example, if you wanted to run a group of `Pods` for your application but
stop work unless _all_ the Pods are available (perhaps for some high-throughput distributed task),
then you can implement or install an extension that does provide that feature.
-->
<p>在庞大的 Kubernetes 生态系统中，你还可以找到一些提供额外操作的第三方
工作负载资源。通过使用
<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">定制资源定义（CRD）</a>，
你可以添加第三方工作负载资源，以完成原本不是 Kubernetes 核心功能的工作。
例如，如果你希望运行一组 <code>Pods</code>，但要求所有 Pods 都可用时才执行操作
（比如针对某种高吞吐量的分布式任务），你可以实现一个能够满足这一需求
的扩展，并将其安装到集群中运行。</p>
<h2 id="what-s-next">What's next</h2>
<!--
As well as reading about each resource, you can learn about specific tasks that relate to them:

* [Run a stateless application using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/)
* Run a stateful application either as a [single instance](/docs/tasks/run-application/run-single-instance-stateful-application/)
  or as a [replicated set](/docs/tasks/run-application/run-replicated-stateful-application/)
* [Run Automated Tasks with a `CronJob`](/docs/tasks/job/automated-tasks-with-cron-jobs/)
-->
<p>除了阅读了解每类资源外，你还可以了解与这些资源相关的任务：</p>
<ul>
<li><a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">使用 Deployment 运行一个无状态的应用</a></li>
<li>以<a href="/zh/docs/tasks/run-application/run-single-instance-stateful-application/">单实例</a>
或者<a href="/zh/docs/tasks/run-application/run-replicated-stateful-application/">多副本集合</a>
的形式运行有状态的应用；</li>
<li><a href="/zh/docs/tasks/job/automated-tasks-with-cron-jobs/">使用 <code>CronJob</code> 运行自动化的任务</a></li>
</ul>
<!--
To learn about Kubernetes' mechanisms for separating code from configuration,
visit [Configuration](/docs/concepts/configuration/).
-->
<p>要了解 Kubernetes 将代码与配置分离的实现机制，可参阅
<a href="/zh/docs/concepts/configuration/">配置部分</a>。</p>
<!--
There are two supporting concepts that provide backgrounds about how Kubernetes manages pods
for applications:
* [Garbage collection](/docs/concepts/workloads/controllers/garbage-collection/) tidies up objects
  from your cluster after their _owning resource_ has been removed.
* The [_time-to-live after finished_ controller](/docs/concepts/workloads/controllers/ttlafterfinished/)
  removes Jobs once a defined time has passed since they completed.
-->
<p>关于 Kubernetes 如何为应用管理 Pods，还有两个支撑概念能够提供相关背景信息：</p>
<ul>
<li><a href="/zh/docs/concepts/workloads/controllers/garbage-collection/">垃圾收集</a>机制负责在
对象的 <em>属主资源</em> 被删除时在集群中清理这些对象。</li>
<li><a href="/zh/docs/concepts/workloads/controllers/ttlafterfinished/"><em>Time-to-Live</em> 控制器</a>
会在 Job 结束之后的指定时间间隔之后删除它们。</li>
</ul>
<!--
Once your application is running, you might want to make it available on the internet as
a [`Service`](/docs/concepts/services-networking/service/) or, for web application only,
using an [`Ingress`](/docs/concepts/services-networking/ingress).
-->
<p>一旦你的应用处于运行状态，你就可能想要以
<a href="/zh/docs/concepts/services-networking/service/"><code>Service</code></a>
的形式使之可在互联网上访问；或者对于 Web 应用而言，使用
<a href="/zh/docs/concepts/services-networking/ingress"><code>Ingress</code></a> 资源将其暴露到互联网上。</p>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4d68b0ccf9c683e6368ffdcc40c838d4">4.1 - Pods</h1>
    
	<!--
reviewers:
- erictune
title: Pods
content_type: concept
weight: 10
no_list: true
card:
  name: concepts
  weight: 60
-->
<!-- overview -->
<!--
_Pods_ are the smallest deployable units of computing that you can create and manage in Kubernetes.

A _Pod_ (as in a pod of whales or pea pod) is a group of one or more
<a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='containers'>containers</a>
with shared storage and network resources, and a specification
for how to run the containers. A Pod's contents are always co-located and
co-scheduled, and run in a shared context. A Pod models an
application-specific "logical host": it contains one or more application
containers which are relatively tightly coupled. 
In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.
-->
<p><em>Pod</em> 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。</p>
<p><em>Pod</em> （就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个）
<a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='容器'>容器</a>；
这些容器共享存储、网络、以及怎样运行这些容器的声明。
Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。
Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器，
这些容器是相对紧密的耦合在一起的。
在非云环境中，在相同的物理机或虚拟机上运行的应用类似于
在同一逻辑主机上运行的云应用。</p>
<!--
As well as application containers, a Pod can contain
[init containers](/docs/concepts/workloads/pods/init-containers/) that run
during Pod startup. You can also inject
[ephemeral containers](/docs/concepts/workloads/pods/ephemeral-containers/)
for debugging if your cluster offers this.
-->
<p>除了应用容器，Pod 还可以包含在 Pod 启动期间运行的
<a href="/zh/docs/concepts/workloads/pods/init-containers/">Init 容器</a>。
你也可以在集群中支持<a href="/zh/docs/concepts/workloads/pods/ephemeral-containers/">临时性容器</a>
的情况下，为调试的目的注入临时性容器。</p>
<!-- body -->
<h2 id="what-is-a-pod">什么是 Pod？  </h2>
<!--
While Kubernetes supports more
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtimes'>container runtimes</a>
than just Docker, [Docker](https://www.docker.com/) is the most commonly known
runtime, and it helps to describe Pods using some terminology from Docker.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 除了 Docker 之外，Kubernetes 支持
很多其他<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>，
<a href="https://www.docker.com/">Docker</a> 是最有名的运行时，
使用 Docker 的术语来描述 Pod 会很有帮助。
</div>
<!--
The shared context of a Pod is a set of Linux namespaces, cgroups, and
potentially other facets of isolation - the same things that isolate a Docker
container.  Within a Pod's context, the individual applications may have
further sub-isolations applied.

In terms of Docker concepts, a Pod is similar to a group of Docker containers
with shared namespaces and shared filesystem volumes.
-->
<p>Pod 的共享上下文包括一组 Linux 名字空间、控制组（cgroup）和可能一些其他的隔离
方面，即用来隔离 Docker 容器的技术。
在 Pod 的上下文中，每个独立的应用可能会进一步实施隔离。</p>
<p>就 Docker 概念的术语而言，Pod 类似于共享名字空间和文件系统卷的一组 Docker
容器。</p>
<!--
## Using Pods

The following is an example of a Pod which consists of a container running the image `nginx:1.14.2`.



 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/simple-pod.yaml" download="pods/simple-pod.yaml"><code>pods/simple-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-simple-pod-yaml')" title="Copy pods/simple-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-simple-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>



To create the Pod shown above, run the following command:
-->
<h2 id="using-pods">使用 Pod  </h2>
<p>下面是一个 Pod 示例，它由一个运行镜像 <code>nginx:1.14.2</code> 的容器组成。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/simple-pod.yaml" download="pods/simple-pod.yaml"><code>pods/simple-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-simple-pod-yaml')" title="Copy pods/simple-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-simple-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<p>要创建上面显示的 Pod，请运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml
</code></pre></div><!--
Pods are generally not created directly and are created using workload resources.
See [Working with Pods](#working-with-pods) for more information on how Pods are used
with workload resources.

### Workload resources for managing pods
-->
<p>Pod 通常不是直接创建的，而是使用工作负载资源创建的。
有关如何将 Pod 用于工作负载资源的更多信息，请参阅 <a href="#working-with-pods">使用 Pod</a>。</p>
<h3 id="用于管理-pod-的工作负载资源">用于管理 pod 的工作负载资源</h3>
<!--
Usually you don't need to create Pods directly, even singleton Pods. 
Instead, create them using workload resources such as <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a> or <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a>.
If your Pods need to track state, consider the 
<a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a> resource.

Pods in a Kubernetes cluster are used in two main ways:
-->
<p>通常你不需要直接创建 Pod，甚至单实例 Pod。
相反，你会使用诸如
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a> 或
<a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a> 这类工作负载资源
来创建 Pod。如果 Pod 需要跟踪状态，
可以考虑 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>
资源。</p>
<p>Kubernetes 集群中的 Pod 主要有两种用法：</p>
<!--
* **Pods that run a single container**. The "one-container-per-Pod" model is the
  most common Kubernetes use case; in this case, you can think of a Pod as a
  wrapper around a single container; Kubernetes manages Pods rather than managing
  the containers directly.
* **Pods that run multiple containers that need to work together**. A Pod can
  encapsulate an application composed of multiple co-located containers that are
  tightly coupled and need to share resources. These co-located containers
  form a single cohesive unit of service—for example, one container serving data
  stored in a shared volume to the public, while a separate _sidecar_ container
  refreshes or updates those files.  
  The Pod wraps these containers, storage resources, and an ephemeral network
  identity together as a single unit.

  Grouping multiple co-located and co-managed containers in a single Pod is a
  relatively advanced use case. You should use this pattern only in specific
  instances in which your containers are tightly coupled.
-->
<ul>
<li>
<p><strong>运行单个容器的 Pod</strong>。&quot;每个 Pod 一个容器&quot;模型是最常见的 Kubernetes 用例；
在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。</p>
</li>
<li>
<p><strong>运行多个协同工作的容器的 Pod</strong>。
Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。
这些位于同一位置的容器可能形成单个内聚的服务单元 —— 一个容器将文件从共享卷提供给公众，
而另一个单独的“边车”（sidecar）容器则刷新或更新这些文件。
Pod 将这些容器和存储资源打包为一个可管理的实体。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 将多个并置、同管的容器组织到一个 Pod 中是一种相对高级的使用场景。
只有在一些场景中，容器之间紧密关联时你才应该使用这种模式。
</div>
</li>
</ul>
<!--
Each Pod is meant to run a single instance of a given application. If you want to
scale your application horizontally (to provide more overall resources by running
more instances), you should use multiple Pods, one for each instance. In
Kubernetes, this is typically referred to as _replication_.
Replicated Pods are usually created and managed as a group by a workload resource
and its <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a>.

See [Pods and controllers](#pods-and-controllers) for more information on how
Kubernetes uses workload resources, and their controllers, to implement application
scaling and auto-healing.
-->
<p>每个 Pod 都旨在运行给定应用程序的单个实例。如果希望横向扩展应用程序（例如，运行多个实例
以提供更多的资源），则应该使用多个 Pod，每个实例使用一个 Pod。
在 Kubernetes 中，这通常被称为 <em>副本（Replication）</em>。
通常使用一种工作负载资源及其<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>
来创建和管理一组 Pod 副本。</p>
<p>参见 <a href="#pods-and-controllers">Pod 和控制器</a>以了解 Kubernetes
如何使用工作负载资源及其控制器以实现应用的扩缩和自动修复。</p>
<!--
### How Pods manage multiple containers

Pods are designed to support multiple cooperating processes (as containers) that form
a cohesive unit of service. The containers in a Pod are automatically co-located and
co-scheduled on the same physical or virtual machine in the cluster. The containers
can share resources and dependencies, communicate with one another, and coordinate
when and how they are terminated.
-->
<h3 id="pod-怎样管理多个容器">Pod 怎样管理多个容器</h3>
<p>Pod 被设计成支持形成内聚服务单元的多个协作过程（形式为容器）。
Pod 中的容器被自动安排到集群中的同一物理机或虚拟机上，并可以一起进行调度。
容器之间可以共享资源和依赖、彼此通信、协调何时以及何种方式终止自身。</p>
<!--
For example, you might have a container that
acts as a web server for files in a shared volume, and a separate "sidecar" container
that updates those files from a remote source, as in the following diagram:
-->
<p>例如，你可能有一个容器，为共享卷中的文件提供 Web 服务器支持，以及一个单独的
&quot;边车 (sidercar)&quot; 容器负责从远端更新这些文件，如下图所示：</p>

<figure class="diagram-medium">
    <img src="/images/docs/pod.svg"
         alt="Pod creation diagram"/> 
</figure>

<!--
Some Pods have <a class='glossary-tooltip' title='应用容器运行前必须先运行完成的一个或多个初始化容器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-init-container' target='_blank' aria-label='init containers'>init containers</a>
as well as <a class='glossary-tooltip' title='用于运行部分工作负载的容器。与初始化容器比较而言。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-app-container' target='_blank' aria-label='app containers'>app containers</a>.
Init containers run and complete before the app containers are started.

Pods natively provide two kinds of shared resources for their constituent containers:
[networking](#pod-networking) and [storage](#pod-storage).
-->
<p>有些 Pod 具有 <a class='glossary-tooltip' title='应用容器运行前必须先运行完成的一个或多个初始化容器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-init-container' target='_blank' aria-label='Init 容器'>Init 容器</a> 和
<a class='glossary-tooltip' title='用于运行部分工作负载的容器。与初始化容器比较而言。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-app-container' target='_blank' aria-label='应用容器'>应用容器</a>。
Init 容器会在启动应用容器之前运行并完成。</p>
<p>Pod 天生地为其成员容器提供了两种共享资源：<a href="#pod-networking">网络</a>和
<a href="#pod-storage">存储</a>。</p>
<!--
## Working with Pods

You'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This
is because Pods are designed as relatively ephemeral, disposable entities. When
a Pod gets created (directly by you, or indirectly by a
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a>), the new Pod is
scheduled to run on a <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点（Node）'>节点（Node）</a> in your cluster.
The Pod remains on that node until the Pod finishes execution, the Pod object is deleted,
the Pod is *evicted* for lack of resources, or the node fails.
-->
<h2 id="working-with-pods">使用 Pod  </h2>
<p>你很少在 Kubernetes 中直接创建一个个的 Pod，甚至是单实例（Singleton）的 Pod。
这是因为 Pod 被设计成了相对临时性的、用后即抛的一次性实体。
当 Pod 由你或者间接地由 <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>
创建时，它被调度在集群中的<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>上运行。
Pod 会保持在该节点上运行，直到 Pod 结束执行、Pod 对象被删除、Pod 因资源不足而被
<em>驱逐</em> 或者节点失效为止。</p>
<!--
Restarting a container in a Pod should not be confused with restarting a Pod. A Pod
is not a process, but an environment for running container(s). A Pod persists until
it is deleted.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 重启 Pod 中的容器不应与重启 Pod 混淆。
Pod 不是进程，而是容器运行的环境。
在被删除之前，Pod 会一直存在。
</div>
<!--
When you create the manifest for a Pod object, make sure the name specified is a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>当你为 Pod 对象创建清单时，要确保所指定的 Pod 名称是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
### Pods and controllers

You can use workload resources to create and manage multiple Pods for you. A controller
for the resource handles replication and rollout and automatic healing in case of
Pod failure. For example, if a Node fails, a controller notices that Pods on that
Node have stopped working and creates a replacement Pod. The scheduler places the
replacement Pod onto a healthy Node.

Here are some examples of workload resources that manage one or more Pods:
-->
<h3 id="pods-and-controllers">Pod 和控制器   </h3>
<p>你可以使用工作负载资源来创建和管理多个 Pod。
资源的控制器能够处理副本的管理、上线，并在 Pod 失效时提供自愈能力。
例如，如果一个节点失败，控制器注意到该节点上的 Pod 已经停止工作，
就可以创建替换性的 Pod。调度器会将替身 Pod 调度到一个健康的节点执行。</p>
<p>下面是一些管理一个或者多个 Pod 的工作负载资源的示例：</p>
<ul>
<li><a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a></li>
<li><a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a></li>
<li><a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a></li>
</ul>
<!--
### Pod templates

Controllers for <a class='glossary-tooltip' title='工作负载是在 Kubernetes 上运行的应用程序。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/' target='_blank' aria-label='workload'>workload</a> resources create Pods
from a _pod template_ and manage those Pods on your behalf.

PodTemplates are specifications for creating Pods, and are included in workload resources such as
[Deployments](/docs/concepts/workloads/controllers/deployment/),
[Jobs](/docs/concepts/workloads/controllers/job/), and
[DaemonSets](/docs/concepts/workloads/controllers/daemonset/).
-->
<h3 id="pod-templates">Pod 模版   </h3>
<p><a class='glossary-tooltip' title='工作负载是在 Kubernetes 上运行的应用程序。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/' target='_blank' aria-label='负载'>负载</a>资源的控制器通常使用
<em>Pod 模板（Pod Template）</em> 来替你创建 Pod 并管理它们。</p>
<p>Pod 模板是包含在工作负载对象中的规范，用来创建 Pod。这类负载资源包括
<a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a>、
<a href="/zh/docs/concepts/workloads/controllers/job/">Job</a> 和
<a href="/zh/docs/concepts/workloads/controllers/daemonset/">DaemonSets</a> 等。</p>
<!--
Each controller for a workload resource uses the `PodTemplate` inside the workload
object to make actual Pods. The `PodTemplate` is part of the desired state of whatever
workload resource you used to run your app.

The sample below is a manifest for a simple Job with a `template` that starts one
container. The container in that Pod prints a message then pauses.
-->
<p>工作负载的控制器会使用负载对象中的 <code>PodTemplate</code> 来生成实际的 Pod。
<code>PodTemplate</code> 是你用来运行应用时指定的负载资源的目标状态的一部分。</p>
<p>下面的示例是一个简单的 Job 的清单，其中的 <code>template</code> 指示启动一个容器。
该 Pod 中的容器会打印一条消息之后暂停。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 这里是 Pod 模版</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#39;sh&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;-c&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;echo &#34;Hello, Kubernetes!&#34; &amp;&amp; sleep 3600&#39;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 以上为 Pod 模版</span><span style="color:#bbb">
</span></code></pre></div><!--
Modifying the pod template or switching to a new pod template has no effect on the
Pods that already exist. Pods do not receive template updates directly. Instead,
a new Pod is created to match the revised pod template.

For example, the deployment controller ensures that the running Pods match the current
pod template for each Deployment object. If the template is updated, the Deployment has
to remove the existing Pods and create new Pods based on the updated template. Each workload
resource implements its own rules for handling changes to the Pod template.
-->
<p>修改 Pod 模版或者切换到新的 Pod 模版都不会对已经存在的 Pod 起作用。
Pod 不会直接收到模版的更新。相反，
新的 Pod 会被创建出来，与更改后的 Pod 模版匹配。</p>
<p>例如，Deployment 控制器针对每个 Deployment 对象确保运行中的 Pod 与当前的 Pod
模版匹配。如果模版被更新，则 Deployment 必须删除现有的 Pod，基于更新后的模版
创建新的 Pod。每个工作负载资源都实现了自己的规则，用来处理对 Pod 模版的更新。</p>
<!--
On Nodes, the <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> does not
directly observe or manage any of the details around pod templates and updates; those
details are abstracted away. That abstraction and separation of concerns simplifies
system semantics, and makes it feasible to extend the cluster's behavior without
changing existing code.
-->
<p>在节点上，<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 并不直接监测
或管理与 Pod 模版相关的细节或模版的更新，这些细节都被抽象出来。
这种抽象和关注点分离简化了整个系统的语义，并且使得用户可以在不改变现有代码的
前提下就能扩展集群的行为。</p>
<!--
## Pod update and replacement

As mentioned in the previous section, when the Pod template for a workload
resource is changed, the controller creates new Pods based on the updated
template instead of updating or patching the existing Pods.
-->
<h2 id="pod-update-and-replacement">Pod 更新与替换  </h2>
<p>正如前面章节所述，当某工作负载的 Pod 模板被改变时，控制器会基于更新的模板
创建新的 Pod 对象而不是对现有 Pod 执行更新或者修补操作。</p>
<!--
Kubernetes doesn't prevent you from managing Pods directly. It is possible to
update some fields of a running Pod, in place. However, Pod update operations
like 
[`patch`](/docs/reference/generated/kubernetes-api/v1.23/#patch-pod-v1-core), and
[`replace`](/docs/reference/generated/kubernetes-api/v1.23/#replace-pod-v1-core)
have some limitations:
-->
<p>Kubernetes 并不禁止你直接管理 Pod。对运行中的 Pod 的某些字段执行就地更新操作
还是可能的。不过，类似
<a href="/docs/reference/generated/kubernetes-api/v1.23/#patch-pod-v1-core"><code>patch</code></a> 和
<a href="/docs/reference/generated/kubernetes-api/v1.23/#replace-pod-v1-core"><code>replace</code></a>
这类更新操作有一些限制：</p>
<!--
- Most of the metadata about a Pod is immutable. For example, you cannot
  change the `namespace`, `name`, `uid`, or `creationTimestamp` fields;
  the `generation` field is unique. It only accepts updates that increment the
  field's current value.
- If the `metadata.deletionTimestamp` is set, no new entry can be added to the
  `metadata.finalizers` list.
- Pod updates may not change fields other than `spec.containers[*].image`,
  `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or
  `spec.tolerations`. For `spec.tolerations`, you can only add new entries.
- When updating the `spec.activeDeadlineSeconds` field, two types of updates
  are allowed:

  1. setting the unassigned field to a positive number; 
  1. updating the field from a positive number to a smaller, non-negative
     number.
-->
<ul>
<li>
<p>Pod 的绝大多数元数据都是不可变的。例如，你不可以改变其 <code>namespace</code>、<code>name</code>、
<code>uid</code> 或者 <code>creationTimestamp</code> 字段；<code>generation</code> 字段是比较特别的，如果更新
该字段，只能增加字段取值而不能减少。</p>
</li>
<li>
<p>如果 <code>metadata.deletionTimestamp</code> 已经被设置，则不可以向 <code>metadata.finalizers</code>
列表中添加新的条目。</p>
</li>
<li>
<p>Pod 更新不可以改变除 <code>spec.containers[*].image</code>、<code>spec.initContainers[*].image</code>、
<code>spec.activeDeadlineSeconds</code> 或 <code>spec.tolerations</code> 之外的字段。
对于 <code>spec.tolerations</code>，你只被允许添加新的条目到其中。</p>
</li>
<li>
<p>在更新<code>spec.activeDeadlineSeconds</code> 字段时，以下两种更新操作是被允许的：</p>
<ol>
<li>如果该字段尚未设置，可以将其设置为一个正数；</li>
<li>如果该字段已经设置为一个正数，可以将其设置为一个更小的、非负的整数。</li>
</ol>
</li>
</ul>
<!--
## Resource sharing and communication

Pods enable data sharing and communication among their constituent
containters.
-->
<h3 id="resource-sharing-and-communication">资源共享和通信</h3>
<p>Pod 使它的成员容器间能够进行数据共享和通信。</p>
<!--
### Storage in Pods {#pod-storage}

A Pod can specify a set of shared storage
<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='volumes'>volumes</a>. All containers
in the Pod can access the shared volumes, allowing those containers to
share data. Volumes also allow persistent data in a Pod to survive
in case one of the containers within needs to be restarted. See
[Storage](/docs/concepts/storage/) for more information on how
Kubernetes implements shared storage and makes it available to Pods.
-->
<h3 id="pod-storage">Pod 中的存储</h3>
<p>一个 Pod 可以设置一组共享的存储<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>。
Pod 中的所有容器都可以访问该共享卷，从而允许这些容器共享数据。
卷还允许 Pod 中的持久数据保留下来，即使其中的容器需要重新启动。
有关 Kubernetes 如何在 Pod 中实现共享存储并将其提供给 Pod 的更多信息，
请参考<a href="/zh/docs/concepts/storage/">卷</a>。</p>
<!--
### Pod networking

Each Pod is assigned a unique IP address for each address family. Every
container in a Pod shares the network namespace, including the IP address and
network ports. Inside a Pod (and **only** then), the containers that belong to the Pod
can communicate with one another using `localhost`. When containers in a Pod communicate
with entities *outside the Pod*,
they must coordinate how they use the shared network resources (such as ports).
-->
<h3 id="pod-networking">Pod 联网   </h3>
<p>每个 Pod 都在每个地址族中获得一个唯一的 IP 地址。
Pod 中的每个容器共享网络名字空间，包括 IP 地址和网络端口。
<em>Pod 内</em> 的容器可以使用 <code>localhost</code> 互相通信。
当 Pod 中的容器与 <em>Pod 之外</em> 的实体通信时，它们必须协调如何使用共享的网络资源
（例如端口）。</p>
<!--
Within a Pod, containers share an IP address and port space, and
can find each other via `localhost`. The containers in a Pod can also communicate
with each other using standard inter-process communications like SystemV semaphores
or POSIX shared memory.  Containers in different Pods have distinct IP addresses
and can not communicate by IPC without
and can not communicate by OS-level IPC without special configuration.
Containers that want to interact with a container running in a different Pod can
use IP networking to communicate.
-->
<p>在同一个 Pod 内，所有容器共享一个 IP 地址和端口空间，并且可以通过 <code>localhost</code> 发现对方。
他们也能通过如 SystemV 信号量或 POSIX 共享内存这类标准的进程间通信方式互相通信。
不同 Pod 中的容器的 IP 地址互不相同，没有特殊配置，无法通过 OS 级 IPC 进行通信就不能使用 IPC 进行通信。
如果某容器希望与运行于其他 Pod 中的容器通信，可以通过 IP 联网的方式实现。</p>
<!--
Containers within the Pod see the system hostname as being the same as the configured
`name` for the Pod. There's more about this in the [networking](/docs/concepts/cluster-administration/networking/)
section.
-->
<p>Pod 中的容器所看到的系统主机名与为 Pod 配置的 <code>name</code> 属性值相同。
<a href="/zh/docs/concepts/cluster-administration/networking/">网络</a>部分提供了更多有关此内容的信息。</p>
<!--
## Privileged mode for containers

In Linux, any container in a Pod can enable privileged mode using the `privileged` (Linux) flag on the [security context](/docs/tasks/configure-pod-container/security-context/) of the container spec. This is useful for containers that want to use operating system administrative capabilities such as manipulating the network stack or accessing hardware devices.

If your cluster has the `WindowsHostProcessContainers` feature enabled, you can create a [Windows HostProcess pod](/docs/tasks/configure-pod-container/create-hostprocess-pod) by setting the `windowsOptions.hostProcess` flag on the security context of the pod spec. All containers in these pods must run as Windows HostProcess containers. HostProcess pods run directly on the host and can also be used to perform administrative tasks as is done with Linux privileged containers.
-->
<h2 id="privileged-mode-for-containers">容器的特权模式    </h2>
<p>在 Linux 中，Pod 中的任何容器都可以使用容器规约中的
<a href="/zh/docs/tasks/configure-pod-container/security-context/">安全性上下文</a>中的
<code>privileged</code>（Linux）参数启用特权模式。
这对于想要使用操作系统管理权能（Capabilities，如操纵网络堆栈和访问设备）
的容器很有用。</p>
<p>如果你的集群启用了 <code>WindowsHostProcessContainers</code> 特性，你可以使用 Pod 规约中安全上下文的
<code>windowsOptions.hostProcess</code> 参数来创建
<a href="/zh/docs/tasks/configure-pod-container/create-hostprocess-pod/">Windows HostProcess Pod</a>。
这些 Pod 中的所有容器都必须以 Windows HostProcess 容器方式运行。
HostProcess Pod 可以直接运行在主机上，它也能像 Linux 特权容器一样，用于执行管理任务。</p>
<!--
Your <a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a> must support the concept of a privileged container for this setting to be relevant.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你的<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>必须支持
特权容器的概念才能使用这一配置。
</div>
<!--
## Static Pods

_Static Pods_ are managed directly by the kubelet daemon on a specific node,
without the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a>
observing them.
Whereas most Pods are managed by the control plane (for example, a
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>), for static
Pods, the kubelet directly supervises each static Pod (and restarts it if it fails).
-->
<h2 id="static-pods">静态 Pod   </h2>
<p><em>静态 Pod（Static Pod）</em> 直接由特定节点上的 <code>kubelet</code> 守护进程管理，
不需要<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>看到它们。
尽管大多数 Pod 都是通过控制面（例如，<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>）
来管理的，对于静态 Pod 而言，<code>kubelet</code> 直接监控每个 Pod，并在其失效时重启之。</p>
<!--
Static Pods are always bound to one <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='Kubelet'>Kubelet</a> on a specific node.
The main use for static Pods is to run a self-hosted control plane: in other words,
using the kubelet to supervise the individual [control plane components](/docs/concepts/overview/components/#control-plane-components).

The kubelet automatically tries to create a <a class='glossary-tooltip' title='API 服务器中的一个对象，用于跟踪 kubelet 上的静态 pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-mirror-pod' target='_blank' aria-label='mirror Pod'>mirror Pod</a>
on the Kubernetes API server for each static Pod.
This means that the Pods running on a node are visible on the API server,
but cannot be controlled from there.
-->
<p>静态 Pod 通常绑定到某个节点上的 <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>。
其主要用途是运行自托管的控制面。
在自托管场景中，使用 <code>kubelet</code> 来管理各个独立的
<a href="/zh/docs/concepts/overview/components/#control-plane-components">控制面组件</a>。</p>
<p><code>kubelet</code> 自动尝试为每个静态 Pod 在 Kubernetes API 服务器上创建一个
<a class='glossary-tooltip' title='API 服务器中的一个对象，用于跟踪 kubelet 上的静态 pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-mirror-pod' target='_blank' aria-label='镜像 Pod'>镜像 Pod</a>。
这意味着在节点上运行的 Pod 在 API 服务器上是可见的，但不可以通过 API
服务器来控制。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The `spec` of a static Pod cannot refer to other API objects
(e.g., <a class='glossary-tooltip' title='为在 Pod 中运行的进程提供标识。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-service-account/' target='_blank' aria-label='ServiceAccount'>ServiceAccount</a>,
<a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a>,
<a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>, etc).
-->
<p>静态 Pod 的 <code>spec</code> 不能引用其他的 API 对象（例如：<a class='glossary-tooltip' title='为在 Pod 中运行的进程提供标识。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-service-account/' target='_blank' aria-label='ServiceAccount'>ServiceAccount</a>、<a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a>、<a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>等）。
</div>
<!--
## Container probes

A _probe_ is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions:

- `ExecAction` (performed with the help of the container runtime)
- `TCPSocketAction` (checked directly by the kubelet)
- `HTTPGetAction` (checked directly by the kubelet)

You can read more about [probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes) 
in the Pod Lifecycle documentation.
-->
<h2 id="container-probes">容器探针  </h2>
<p><em>Probe</em> 是由 kubelet 对容器执行的定期诊断。要执行诊断，kubelet 可以执行三种动作：</p>
<ul>
<li><code>ExecAction</code>（借助容器运行时执行）</li>
<li><code>TCPSocketAction</code>（由 kubelet 直接检测）</li>
<li><code>HTTPGetAction</code>（由 kubelet 直接检测）</li>
</ul>
<p>你可以参阅 Pod 的生命周期文档中的<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">探针</a>部分。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about the [lifecycle of a Pod](/docs/concepts/workloads/pods/pod-lifecycle/).
* Learn about [RuntimeClass](/docs/concepts/containers/runtime-class/) and how you can use it to
  configure different Pods with different container runtime configurations.
* Read about [Pod topology spread constraints](/docs/concepts/workloads/pods/pod-topology-spread-constraints/).
* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how you can use it to manage application availability during disruptions.
* Pod is a top-level resource in the Kubernetes REST API.
  The 





<a href=""></a>
  object definition describes the object in detail.
* [The Distributed System Toolkit: Patterns for Composite Containers](/blog/2015/06/the-distributed-system-toolkit-patterns/) explains common layouts for Pods with more than one container.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">Pod 生命周期</a></li>
<li>了解 <a href="/zh/docs/concepts/containers/runtime-class/">RuntimeClass</a>，以及如何使用它
来配置不同的 Pod 使用不同的容器运行时配置</li>
<li>了解 <a href="/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/">Pod 拓扑分布约束</a></li>
<li>了解 <a href="/zh/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>，以及你
如何可以利用它在出现干扰因素时管理应用的可用性。</li>
<li>Pod 在 Kubernetes REST API 中是一个顶层资源。






<a href=""></a>
对象的定义中包含了更多的细节信息。</li>
<li>博客 <a href="/blog/2015/06/the-distributed-system-toolkit-patterns/">分布式系统工具箱：复合容器模式</a>
中解释了在同一 Pod 中包含多个容器时的几种常见布局。</li>
</ul>
<!--
To understand the context for why Kubernetes wraps a common Pod API in other resources (such as <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSets'>StatefulSets</a> or <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployments'>Deployments</a>), you can read about the prior art, including:
-->
<p>要了解为什么 Kubernetes 会在其他资源
（如 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>
或 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>）
封装通用的 Pod API，相关的背景信息可以在前人的研究中找到。具体包括：</p>
<ul>
<li><a href="https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema">Aurora</a></li>
<li><a href="https://research.google.com/pubs/pub43438.html">Borg</a></li>
<li><a href="https://mesosphere.github.io/marathon/docs/rest-api.html">Marathon</a></li>
<li><a href="https://research.google/pubs/pub41684/">Omega</a></li>
<li><a href="https://engineering.fb.com/data-center-engineering/tupperware/">Tupperware</a>.</li>
</ul>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c3c2b9cf30915ec9d46c147201da3332">4.1.1 - Pod 的生命周期</h1>
    
	<!--
title: Pod Lifecycle
content_type: concept
weight: 30
-->
<!-- overview -->
<!--
This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting
in the `Pending` [phase](#pod-phase), moving through `Running` if at least one
of its primary containers starts OK, and then through either the `Succeeded` or
`Failed` phases depending on whether any container in the Pod terminated in failure.

Whilst a Pod is running, the kubelet is able to restart containers to handle some
kind of faults. Within a Pod, Kubernetes tracks different container
[states](#container-states) and determines what action to take to make the Pod
healthy again.
-->
<p>本页面讲述 Pod 的生命周期。
Pod 遵循一个预定义的生命周期，起始于 <code>Pending</code> <a href="#pod-phase">阶段</a>，如果至少
其中有一个主要容器正常启动，则进入 <code>Running</code>，之后取决于 Pod 中是否有容器以
失败状态结束而进入 <code>Succeeded</code> 或者 <code>Failed</code> 阶段。</p>
<p>在 Pod 运行期间，<code>kubelet</code> 能够重启容器以处理一些失效场景。
在 Pod 内部，Kubernetes 跟踪不同容器的<a href="#container-states">状态</a>
并确定使 Pod 重新变得健康所需要采取的动作。</p>
<!--
In the Kubernetes API, Pods have both a specification and an actual status. The
status for a Pod object consists of a set of [Pod conditions](#pod-conditions).
You can also inject [custom readiness information](#pod-readiness-gate) into the
condition data for a Pod, if that is useful to your application.

Pods are only [scheduled](/docs/concepts/scheduling-eviction/) once in their lifetime.
Once a Pod is scheduled (assigned) to a Node, the Pod runs on that Node until it stops
or is [terminated](#pod-termination).
-->
<p>在 Kubernetes API 中，Pod 包含规约部分和实际状态部分。
Pod 对象的状态包含了一组 <a href="#pod-conditions">Pod 状况（Conditions）</a>。
如果应用需要的话，你也可以向其中注入<a href="#pod-readiness-gate">自定义的就绪性信息</a>。</p>
<p>Pod 在其生命周期中只会被<a href="/zh/docs/concepts/scheduling-eviction/">调度</a>一次。
一旦 Pod 被调度（分派）到某个节点，Pod 会一直在该节点运行，直到 Pod 停止或者
被<a href="#pod-termination">终止</a>。</p>
<!-- body -->
<!--
## Pod lifetime

Like individual application containers, Pods are considered to be relatively
ephemeral (rather than durable) entities. Pods are created, assigned a unique
ID ([UID](/docs/concepts/overview/working-with-objects/names/#uids)), and scheduled
to nodes where they remain until termination (according to restart policy) or
deletion.  
If a <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点（Node）'>节点（Node）</a> dies, the Pods scheduled to that node
are [scheduled for deletion](#pod-garbage-collection) after a timeout period.
-->
<h2 id="pod-lifetime">Pod 生命期  </h2>
<p>和一个个独立的应用容器一样，Pod 也被认为是相对临时性（而不是长期存在）的实体。
Pod 会被创建、赋予一个唯一的
ID（<a href="/zh/docs/concepts/overview/working-with-objects/names/#uids">UID</a>），
并被调度到节点，并在终止（根据重启策略）或删除之前一直运行在该节点。</p>
<p>如果一个<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>死掉了，调度到该节点
的 Pod 也被计划在给定超时期限结束后<a href="#pod-garbage-collection">删除</a>。</p>
<!--
Pods do not, by themselves, self-heal. If a Pod is scheduled to a
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a> that then fails, the Pod is deleted; likewise, a Pod won't
survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a
higher-level abstraction, called a
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a>, that handles the work of
managing the relatively disposable Pod instances.
-->
<p>Pod 自身不具有自愈能力。如果 Pod 被调度到某<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>
而该节点之后失效，Pod 会被删除；类似地，Pod 无法在因节点资源
耗尽或者节点维护而被驱逐期间继续存活。Kubernetes 使用一种高级抽象
来管理这些相对而言可随时丢弃的 Pod 实例，称作
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>。</p>
<!--
A given Pod (as defined by a UID) is never "rescheduled" to a different node; instead,
that Pod can be replaced by a new, near-identical Pod, with even the same name if
desired, but with a different UID.

When something is said to have the same lifetime as a Pod, such as a
<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='volume'>volume</a>,
that means that the thing exists as long as that specific Pod (with that exact UID)
exists. If that Pod is deleted for any reason, and even if an identical replacement
is created, the related thing (a volume, in this example) is also destroyed and
created anew.
-->
<p>任何给定的 Pod （由 UID 定义）从不会被“重新调度（rescheduled）”到不同的节点；
相反，这一 Pod 可以被一个新的、几乎完全相同的 Pod 替换掉。
如果需要，新 Pod 的名字可以不变，但是其 UID 会不同。</p>
<p>如果某物声称其生命期与某 Pod 相同，例如存储<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>，
这就意味着该对象在此 Pod （UID 亦相同）存在期间也一直存在。
如果 Pod 因为任何原因被删除，甚至某完全相同的替代 Pod 被创建时，
这个相关的对象（例如这里的卷）也会被删除并重建。</p>

<figure class="diagram-medium">
    <img src="/images/docs/pod.svg"/> <figcaption>
            <h4>Pod 结构图例</h4>
        </figcaption>
</figure>

<p><em>一个包含多个容器的 Pod 中包含一个用来拉取文件的程序和一个 Web 服务器，
均使用持久卷作为容器间共享的存储。</em></p>
<!--
## Pod phase

A Pod's `status` field is a
[PodStatus](/docs/reference/generated/kubernetes-api/v1.23/#podstatus-v1-core)
object, which has a `phase` field.

The phase of a Pod is a simple, high-level summary of where the Pod is in its
lifecycle. The phase is not intended to be a comprehensive rollup of observations
of container or Pod state, nor is it intended to be a comprehensive state machine.

The number and meanings of Pod phase values are tightly guarded.
Other than what is documented here, nothing should be assumed about Pods that
have a given `phase` value.

Here are the possible values for `phase`:
-->
<h2 id="pod-phase">Pod 阶段    </h2>
<p>Pod 的 <code>status</code> 字段是一个
<a href="/docs/reference/generated/kubernetes-api/v1.23/#podstatus-v1-core">PodStatus</a>
对象，其中包含一个 <code>phase</code> 字段。</p>
<p>Pod 的阶段（Phase）是 Pod 在其生命周期中所处位置的简单宏观概述。
该阶段并不是对容器或 Pod 状态的综合汇总，也不是为了成为完整的状态机。</p>
<p>Pod 阶段的数量和含义是严格定义的。
除了本文档中列举的内容外，不应该再假定 Pod 有其他的 <code>phase</code> 值。</p>
<p>下面是 <code>phase</code> 可能的值：</p>
<!--
Value | Description
`Pending` | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to bescheduled as well as the time spent downloading container images over the network.
`Running` | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.
`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.
`Failed` | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system.
`Unknown` | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.
-->
<table>
<thead>
<tr>
<th style="text-align:left">取值</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>Pending</code>（悬决）</td>
<td style="text-align:left">Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间。</td>
</tr>
<tr>
<td style="text-align:left"><code>Running</code>（运行中）</td>
<td style="text-align:left">Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。</td>
</tr>
<tr>
<td style="text-align:left"><code>Succeeded</code>（成功）</td>
<td style="text-align:left">Pod 中的所有容器都已成功终止，并且不会再重启。</td>
</tr>
<tr>
<td style="text-align:left"><code>Failed</code>（失败）</td>
<td style="text-align:left">Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。</td>
</tr>
<tr>
<td style="text-align:left"><code>Unknown</code>（未知）</td>
<td style="text-align:left">因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。</td>
</tr>
</tbody>
</table>
<!--
If a node dies or is disconnected from the rest of the cluster, Kubernetes
applies a policy for setting the `phase` of all Pods on the lost node to Failed.
-->
<p>如果某节点死掉或者与集群中其他节点失联，Kubernetes
会实施一种策略，将失去的节点上运行的所有 Pod 的 <code>phase</code> 设置为 <code>Failed</code>。</p>
<!--
## Container states

As well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of
each container inside a Pod. You can use
[container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/) to
trigger events to run at certain points in a container's lifecycle.

Once the <a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='scheduler'>scheduler</a>
assigns a Pod to a Node, the kubelet starts creating containers for that Pod
using a <a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>.
There are three possible container states: `Waiting`, `Running`, and `Terminated`.
-->
<h2 id="container-states">容器状态 </h2>
<p>Kubernetes 会跟踪 Pod 中每个容器的状态，就像它跟踪 Pod 总体上的<a href="#pod-phase">阶段</a>一样。
你可以使用<a href="/zh/docs/concepts/containers/container-lifecycle-hooks/">容器生命周期回调</a>
来在容器生命周期中的特定时间点触发事件。</p>
<p>一旦<a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='调度器'>调度器</a>将 Pod
分派给某个节点，<code>kubelet</code> 就通过
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>
开始为 Pod 创建容器。
容器的状态有三种：<code>Waiting</code>（等待）、<code>Running</code>（运行中）和
<code>Terminated</code>（已终止）。</p>
<!--
To check the state of a Pod's containers, you can use
`kubectl describe pod <name-of-pod>`. The output shows the state for each container
within that Pod.

Each state has a specific meaning:
-->
<p>要检查 Pod 中容器的状态，你可以使用 <code>kubectl describe pod &lt;pod 名称&gt;</code>。
其输出中包含 Pod 中每个容器的状态。</p>
<p>每种状态都有特定的含义：</p>
<!--
### `Waiting` {#container-state-waiting}

If a container is not in either the `Running` or `Terminated` state, it is `Waiting`.
A container in the `Waiting` state is still running the operations it requires in
order to complete start up: for example, pulling the container image from a container
image registry, or applying <a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>
data.
When you use `kubectl` to query a Pod with a container that is `Waiting`, you also see
a Reason field to summarize why the container is in that state.
-->
<h3 id="container-state-waiting"><code>Waiting</code> （等待） </h3>
<p>如果容器并不处在 <code>Running</code> 或 <code>Terminated</code> 状态之一，它就处在 <code>Waiting</code> 状态。
处于 <code>Waiting</code> 状态的容器仍在运行它完成启动所需要的操作：例如，从某个容器镜像
仓库拉取容器镜像，或者向容器应用 <a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>
数据等等。
当你使用 <code>kubectl</code> 来查询包含 <code>Waiting</code> 状态的容器的 Pod 时，你也会看到一个
Reason 字段，其中给出了容器处于等待状态的原因。</p>
<!--
### `Running` {#container-state-running}

The `Running` status indicates that a container is executing without issues. If there
was a `postStart` hook configured, it has already executed and finished. When you use
`kubectl` to query a Pod with a container that is `Running`, you also see information
about when the container entered the `Running` state.
-->
<h3 id="container-state-running"><code>Running</code>（运行中）    </h3>
<p><code>Running</code> 状态表明容器正在执行状态并且没有问题发生。
如果配置了 <code>postStart</code> 回调，那么该回调已经执行且已完成。
如果你使用 <code>kubectl</code> 来查询包含 <code>Running</code> 状态的容器的 Pod 时，你也会看到
关于容器进入 <code>Running</code> 状态的信息。</p>
<!--
### `Terminated` {#container-state-terminated}

A container in the `Terminated` state began execution and then either ran to
completion or failed for some reason. When you use `kubectl` to query a Pod with
a container that is `Terminated`, you see a reason, an exit code, and the start and
finish time for that container's period of execution.

If a container has a `preStop` hook configured, this hook runs before the container enters
the `Terminated` state.
-->
<h3 id="container-state-terminated"><code>Terminated</code>（已终止）  </h3>
<p>处于 <code>Terminated</code> 状态的容器已经开始执行并且或者正常结束或者因为某些原因失败。
如果你使用 <code>kubectl</code> 来查询包含 <code>Terminated</code> 状态的容器的 Pod 时，你会看到
容器进入此状态的原因、退出代码以及容器执行期间的起止时间。</p>
<p>如果容器配置了 <code>preStop</code> 回调，则该回调会在容器进入 <code>Terminated</code>
状态之前执行。</p>
<!--
## Container restart policy {#restart-policy}

The `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,
and Never. The default value is Always.

The `restartPolicy` applies to all containers in the Pod. `restartPolicy` only
refers to restarts of the containers by the kubelet on the same node. After containers
in a Pod exit, the kubelet restarts them with an exponential back-off delay (10s, 20s,
40s, …), that is capped at five minutes. Once a container has executed for 10 minutes
without any problems, the kubelet resets the restart backoff timer for
that container.
-->
<h2 id="restart-policy">容器重启策略</h2>
<p>Pod 的 <code>spec</code> 中包含一个 <code>restartPolicy</code> 字段，其可能取值包括
Always、OnFailure 和 Never。默认值是 Always。</p>
<p><code>restartPolicy</code> 适用于 Pod 中的所有容器。<code>restartPolicy</code> 仅针对同一节点上
<code>kubelet</code> 的容器重启动作。当 Pod 中的容器退出时，<code>kubelet</code> 会按指数回退
方式计算重启的延迟（10s、20s、40s、...），其最长延迟为 5 分钟。
一旦某容器执行了 10 分钟并且没有出现问题，<code>kubelet</code> 对该容器的重启回退计时器执行
重置操作。</p>
<!--
## Pod conditions

A Pod has a PodStatus, which has an array of
[PodConditions](/docs/reference/generated/kubernetes-api/v1.23/#podcondition-v1-core)
through which the Pod has or has not passed:
-->
<h2 id="pod-conditions">Pod 状况 </h2>
<p>Pod 有一个 PodStatus 对象，其中包含一个
<a href="/docs/reference/generated/kubernetes-api/v1.23/#podcondition-v1-core">PodConditions</a>
数组。Pod 可能通过也可能未通过其中的一些状况测试。</p>
<!--
* `PodScheduled`: the Pod has been scheduled to a node.
* `ContainersReady`: all containers in the Pod are ready.
* `Initialized`: all [init containers](/docs/concepts/workloads/pods/init-containers/)
  have completed successfully.
* `Ready`: the Pod is able to serve requests and should be added to the load
  balancing pools of all matching Services.
-->
<ul>
<li><code>PodScheduled</code>：Pod 已经被调度到某节点；</li>
<li><code>ContainersReady</code>：Pod 中所有容器都已就绪；</li>
<li><code>Initialized</code>：所有的 <a href="/zh/docs/concepts/workloads/pods/init-containers/">Init 容器</a>
都已成功完成；</li>
<li><code>Ready</code>：Pod 可以为请求提供服务，并且应该被添加到对应服务的负载均衡池中。</li>
</ul>
<!--
Field name           | Description
`type`               | Name of this Pod condition.
`status`             | Indicates whether that condition is applicable, with possible values "`True`", "`False`", or "`Unknown`".
`lastProbeTime`      | Timestamp of when the Pod condition was last probed.
`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.
`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.
`message`            | Human-readable message indicating details about the last status transition.
-->
<table>
<thead>
<tr>
<th style="text-align:left">字段名称</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>type</code></td>
<td style="text-align:left">Pod 状况的名称</td>
</tr>
<tr>
<td style="text-align:left"><code>status</code></td>
<td style="text-align:left">表明该状况是否适用，可能的取值有 &quot;<code>True</code>&quot;, &quot;<code>False</code>&quot; 或 &quot;<code>Unknown</code>&quot;</td>
</tr>
<tr>
<td style="text-align:left"><code>lastProbeTime</code></td>
<td style="text-align:left">上次探测 Pod 状况时的时间戳</td>
</tr>
<tr>
<td style="text-align:left"><code>lastTransitionTime</code></td>
<td style="text-align:left">Pod 上次从一种状态转换到另一种状态时的时间戳</td>
</tr>
<tr>
<td style="text-align:left"><code>reason</code></td>
<td style="text-align:left">机器可读的、驼峰编码（UpperCamelCase）的文字，表述上次状况变化的原因</td>
</tr>
<tr>
<td style="text-align:left"><code>message</code></td>
<td style="text-align:left">人类可读的消息，给出上次状态转换的详细信息</td>
</tr>
</tbody>
</table>
<!--
### Pod readiness {#pod-readiness-gate}

Your application can inject extra feedback or signals into PodStatus:
_Pod readiness_. To use this, set `readinessGates` in the Pod's `spec` to
specify a list of additional conditions that the kubelet evaluates for Pod readiness.
-->
<h3 id="pod-readiness-gate">Pod 就绪态       </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code>
</div>


<p>你的应用可以向 PodStatus 中注入额外的反馈或者信号：<em>Pod Readiness（Pod 就绪态）</em>。
要使用这一特性，可以设置 Pod 规约中的 <code>readinessGates</code> 列表，为 kubelet
提供一组额外的状况供其评估 Pod 就绪态时使用。</p>
<!--
Readiness gates are determined by the current state of `status.condition`
fields for the Pod. If Kubernetes cannot find such a condition in the
`status.conditions` field of a Pod, the status of the condition
is defaulted to "`False`".

Here is an example:
-->
<p>就绪态门控基于 Pod 的 <code>status.conditions</code> 字段的当前值来做决定。
如果 Kubernetes 无法在 <code>status.conditions</code> 字段中找到某状况，则该状况的
状态值默认为 &quot;<code>False</code>&quot;。</p>
<p>这里是一个例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">readinessGates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">conditionType</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;www.example.com/feature-1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conditions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Ready                             <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 内置的 Pod 状况</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;False&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">lastProbeTime</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">lastTransitionTime</span>:<span style="color:#bbb"> </span>2018-01-01T00:00:00Z<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;www.example.com/feature-1&#34;</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 额外的 Pod 状况</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;False&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">lastProbeTime</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">lastTransitionTime</span>:<span style="color:#bbb"> </span>2018-01-01T00:00:00Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containerStatuses</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerID</span>:<span style="color:#bbb"> </span>docker://abcd...<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ready</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
The Pod conditions you add must have names that meet the Kubernetes [label key format](/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).
-->
<p>你所添加的 Pod 状况名称必须满足 Kubernetes
<a href="/zh/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">标签键名格式</a>。</p>
<!--
### Status for Pod readiness {#pod-readiness-status}

The `kubectl patch` command does not support patching object status.
To set these `status.conditions` for the pod, applications and
<a class='glossary-tooltip' title='一种用于管理自定义资源的专用控制器' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/operator/' target='_blank' aria-label='operators'>operators</a> should use
the `PATCH` action.
You can use a [Kubernetes client library](/docs/reference/using-api/client-libraries/) to
write code that sets custom Pod conditions for Pod readiness.
-->
<h3 id="pod-readiness-status">Pod 就绪态的状态</h3>
<p>命令 <code>kubectl patch</code> 不支持修改对象的状态。
如果需要设置 Pod 的 <code>status.conditions</code>，应用或者
<a class='glossary-tooltip' title='一种用于管理自定义资源的专用控制器' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/operator/' target='_blank' aria-label='Operators'>Operators</a>
需要使用 <code>PATCH</code> 操作。
你可以使用 <a href="/zh/docs/reference/using-api/client-libraries/">Kubernetes 客户端库</a>
之一来编写代码，针对 Pod 就绪态设置定制的 Pod 状况。</p>
<!--
For a Pod that uses custom conditions, that Pod is evaluated to be ready **only**
when both the following statements apply:

* All containers in the Pod are ready.
* All conditions specified in `readinessGates` are `True`.

When a Pod's containers are Ready but at least one custom condition is missing or
`False`, the kubelet sets the Pod's [condition](#pod-conditions) to `ContainersReady`.
-->
<p>对于使用定制状况的 Pod 而言，只有当下面的陈述都适用时，该 Pod 才会被评估为就绪：</p>
<ul>
<li>Pod 中所有容器都已就绪；</li>
<li><code>readinessGates</code> 中的所有状况都为 <code>True</code> 值。</li>
</ul>
<p>当 Pod 的容器都已就绪，但至少一个定制状况没有取值或者取值为 <code>False</code>，
<code>kubelet</code> 将 Pod 的<a href="#pod-conditions">状况</a>设置为 <code>ContainersReady</code>。</p>
<!--
## Container probes

A _probe_ is a diagnostic
performed periodically by the
[kubelet](/docs/reference/command-line-tools-reference/kubelet/)
on a container. To perform a diagnostic,
the kubelet either executes code within the container, or makes
a network request.
-->
<h2 id="container-probes">容器探针   </h2>
<p>probe 是由 <a href="/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> 对容器执行的定期诊断。
要执行诊断，kubelet 既可以在容器内执行代码，也可以发出一个网络请求。</p>
<!--
### Check mechanisms {#probe-check-methods}

There are four different ways to check a container using a probe.
Each probe must define exactly one of these four mechanisms:

`exec`
: Executes a specified command inside the container. The diagnostic
  is considered successful if the command exits with a status code of 0.

`grpc`
: Performs a remote procedure call using [gRPC](https://grpc.io/).
  The target should implement
  [gRPC health checks](https://grpc.io/grpc/core/md_doc_health-checking.html).
  The diagnostic is considered successful if the `status`
  of the response is `SERVING`.
  gRPC probes are an alpha feature and are only available if you
  enable the `GRPCContainerProbe`
  [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).

`httpGet`
: Performs an HTTP `GET` request against the Pod's IP
  address on a specified port and path. The diagnostic is
  considered successful if the response has a status code
  greater than or equal to 200 and less than 400.

`tcpSocket`
: Performs a TCP check against the Pod's IP address on
  a specified port. The diagnostic is considered successful if
  the port is open. If the remote system (the container) closes
  the connection immediately after it opens, this counts as healthy.

-->
<h3 id="probe-check-methods">检查机制   </h3>
<p>使用探针来检查容器有四种不同的方法。
每个探针都必须准确定义为这四种机制中的一种：</p>
<dl>
<dt><code>exec</code></dt>
<dd>在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。</dd>
<dt><code>grpc</code></dt>
<dd>使用 <a href="https://grpc.io/">gRPC</a> 执行一个远程过程调用。
目标应该实现
<a href="https://grpc.io/grpc/core/md_doc_health-checking.html">gRPC健康检查</a>。
如果响应的状态是 &quot;SERVING&quot;，则认为诊断成功。
gRPC 探针是一个 alpha 特性，只有在你启用了
&quot;GRPCContainerProbe&quot; <a href="/zh/docs/reference/command-line-tools-reference/feature-gate/">特性门控</a>时才能使用。</dd>
<dt><code>httpGet</code></dt>
<dd>对容器的 IP 地址上指定端口和路径执行 HTTP <code>GET</code> 请求。如果响应的状态码大于等于 200
且小于 400，则诊断被认为是成功的。</dd>
<dt><code>tcpSocket</code></dt>
<dd>对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。
如果远程系统（容器）在打开连接后立即将其关闭，这算作是健康的。</dd>
</dl>
<!--
### Probe outcome
Each probe has one of three results:

`Success`
: The container passed the diagnostic.

`Failure`
: The container failed the diagnostic.

`Unknown`
: The diagnostic failed (no action should be taken, and the kubelet
  will make further checks).

-->
<h3 id="probe-outcome">探测结果   </h3>
<p>每次探测都将获得以下三种结果之一：</p>
<dl>
<dt><code>Success</code>（成功）</dt>
<dd>容器通过了诊断。</dd>
<dt><code>Failure</code>（失败）</dt>
<dd>容器未通过诊断。</dd>
<dt><code>Unknown</code>（未知）</dt>
<dd>诊断失败，因此不会采取任何行动。</dd>
</dl>
<!--
### Types of probe
The kubelet can optionally perform and react to three kinds of probes on running
containers:
-->
<h3 id="types-of-probe">探测类型   </h3>
<p>针对运行中的容器，<code>kubelet</code> 可以选择是否执行以下三种探针，以及如何针对探测结果作出反应：</p>
<!--
`livenessProbe`
: Indicates whether the container is running. If
  the liveness probe fails, the kubelet kills the container, and the container
  is subjected to its [restart policy](#restart-policy). If a container does not
  provide a liveness probe, the default state is `Success`.

`readinessProbe`
: Indicates whether the container is ready to respond to requests.
  If the readiness probe fails, the endpoints controller removes the Pod's IP
  address from the endpoints of all Services that match the Pod. The default
  state of readiness before the initial delay is `Failure`. If a container does
  not provide a readiness probe, the default state is `Success`.

`startupProbe`
: Indicates whether the application within the container is started.
  All other probes are disabled if a startup probe is provided, until it succeeds.
  If the startup probe fails, the kubelet kills the container, and the container
  is subjected to its [restart policy](#restart-policy). If a container does not
  provide a startup probe, the default state is `Success`.
-->
<dl>
<dt><code>livenessProbe</code></dt>
<dd>指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器，
并且容器将根据其<a href="#restart-policy">重启策略</a>决定未来。如果容器不提供存活探针，
则默认状态为 <code>Success</code>。</dd>
<dt><code>readinessProbe</code></dt>
<dd>指示容器是否准备好为请求提供服务。如果就绪态探测失败，
端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。
初始延迟之前的就绪态的状态值默认为 <code>Failure</code>。
如果容器不提供就绪态探针，则默认状态为 <code>Success</code>。</dd>
<dt><code>startupProbe</code></dt>
<dd>指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被
禁用，直到此探针成功为止。如果启动探测失败，<code>kubelet</code> 将杀死容器，而容器依其
<a href="#restart-policy">重启策略</a>进行重启。
如果容器没有提供启动探测，则默认状态为 <code>Success</code>。</dd>
</dl>
<!--
For more information about how to set up a liveness, readiness, or startup probe,
see [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).
-->
<p>如欲了解如何设置存活态、就绪态和启动探针的进一步细节，可以参阅
<a href="/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">配置存活态、就绪态和启动探针</a>。</p>
<!--
#### When should you use a liveness probe?
-->
<h4 id="when-should-you-use-a-liveness-probe">何时该使用存活态探针?   </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.0 [stable]</code>
</div>


<!--
If the process in your container is able to crash on its own whenever it
encounters an issue or becomes unhealthy, you do not necessarily need a liveness
probe; the kubelet will automatically perform the correct action in accordance
with the Pod's `restartPolicy`.

If you'd like your container to be killed and restarted if a probe fails, then
specify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.
-->
<p>如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活态探针;
<code>kubelet</code> 将根据 Pod 的<code>restartPolicy</code> 自动执行修复操作。</p>
<p>如果你希望容器在探测失败时被杀死并重新启动，那么请指定一个存活态探针，
并指定<code>restartPolicy</code> 为 &quot;<code>Always</code>&quot; 或 &quot;<code>OnFailure</code>&quot;。</p>
<!--
#### When should you use a readiness probe?
-->
<h4 id="when-should-you-use-a-readiness-probe">何时该使用就绪态探针?     </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.0 [stable]</code>
</div>


<!--
If you'd like to start sending traffic to a Pod only when a probe succeeds,
specify a readiness probe. In this case, the readiness probe might be the same
as the liveness probe, but the existence of the readiness probe in the spec means
that the Pod will start without receiving any traffic and only start receiving
traffic after the probe starts succeeding.
-->
<p>如果要仅在探测成功时才开始向 Pod 发送请求流量，请指定就绪态探针。
在这种情况下，就绪态探针可能与存活态探针相同，但是规约中的就绪态探针的存在意味着
Pod 将在启动阶段不接收任何数据，并且只有在探针探测成功后才开始接收数据。</p>
<!--
If you want your container to be able to take itself down for maintenance, you
can specify a readiness probe that checks an endpoint specific to readiness that
is different from the liveness probe.
-->
<p>如果你希望容器能够自行进入维护状态，也可以指定一个就绪态探针，检查某个特定于
就绪态的因此不同于存活态探测的端点。</p>
<!--
If your app has a strict dependency on back-end services, you can implement both
a liveness and a readiness probe. The liveness probe passes when the app itself
is healthy, but the readiness probe additionally checks that each required
back-end service is available. This helps you avoid directing traffic to Pods
that can only respond with error messages.

If your container needs to work on loading large data, configuration files, or
migrations during startup, you can use a
[startup probe](#when-should-you-use-a-startup-probe). However, if you want to
detect the difference between an app that has failed and an app that is still
processing its startup data, you might prefer a readiness probe.
-->
<p>如果你的应用程序对后端服务有严格的依赖性，你可以同时实现存活态和就绪态探针。
当应用程序本身是健康的，存活态探针检测通过后，就绪态探针会额外检查每个所需的后端服务是否可用。
这可以帮助你避免将流量导向只能返回错误信息的 Pod。</p>
<p>如果你的容器需要在启动期间加载大型数据、配置文件或执行迁移，你可以使用
<a href="#when-should-you-use-a-startup-probe">启动探针</a>。
然而，如果你想区分已经失败的应用和仍在处理其启动数据的应用，你可能更倾向于使用就绪探针。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you want to be able to drain requests when the Pod is deleted, you do not
necessarily need a readiness probe; on deletion, the Pod automatically puts itself
into an unready state regardless of whether the readiness probe exists.
The Pod remains in the unready state while it waits for the containers in the Pod
to stop.
-->
<p>请注意，如果你只是想在 Pod 被删除时能够排空请求，则不一定需要使用就绪态探针；
在删除 Pod 时，Pod 会自动将自身置于未就绪状态，无论就绪态探针是否存在。
等待 Pod 中的容器停止期间，Pod 会一直处于未就绪状态。
</div>
<!--
#### When should you use a startup probe?
-->
<h4 id="when-should-you-use-a-startup-probe">何时该使用启动探针？  </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<!--
Startup probes are useful for Pods that have containers that take a long time to
come into service. Rather than set a long liveness interval, you can configure
a separate configuration for probing the container as it starts up, allowing
a time longer than the liveness interval would allow.
-->
<p>对于所包含的容器需要较长时间才能启动就绪的 Pod 而言，启动探针是有用的。
你不再需要配置一个较长的存活态探测时间间隔，只需要设置另一个独立的配置选定，
对启动期间的容器执行探测，从而允许使用远远超出存活态时间间隔所允许的时长。</p>
<!--
If your container usually starts in more than
`initialDelaySeconds + failureThreshold × periodSeconds`, you should specify a
startup probe that checks the same endpoint as the liveness probe. The default for
`periodSeconds` is 10s. You should then set its `failureThreshold` high enough to
allow the container to start, without changing the default values of the liveness
probe. This helps to protect against deadlocks.
-->
<p>如果你的容器启动时间通常超出  <code>initialDelaySeconds + failureThreshold × periodSeconds</code>
总值，你应该设置一个启动探测，对存活态探针所使用的同一端点执行检查。
<code>periodSeconds</code> 的默认值是 10 秒。你应该将其 <code>failureThreshold</code> 设置得足够高，
以便容器有充足的时间完成启动，并且避免更改存活态探针所使用的默认值。
这一设置有助于减少死锁状况的发生。</p>
<!--
## Termination of Pods {#pod-termination}

Because Pods represent processes running on nodes in the cluster, it is important to
allow those processes to gracefully terminate when they are no longer needed (rather
than being abruptly stopped with a `KILL` signal and having no chance to clean up).
-->
<h2 id="pod-termination">Pod 的终止   </h2>
<p>由于 Pod 所代表的是在集群中节点上运行的进程，当不再需要这些进程时允许其体面地
终止是很重要的。一般不应武断地使用 <code>KILL</code> 信号终止它们，导致这些进程没有机会
完成清理操作。</p>
<!--
The design aim is for you to be able to request deletion and know when processes
terminate, but also be able to ensure that deletes eventually complete.
When you request deletion of a Pod, the cluster records and tracks the intended grace period
before the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in
place, the {< glossary_tooltip text="kubelet" term_id="kubelet" >}} attempts graceful
shutdown.
-->
<p>设计的目标是令你能够请求删除进程，并且知道进程何时被终止，同时也能够确保删除
操作终将完成。当你请求删除某个 Pod 时，集群会记录并跟踪 Pod 的体面终止周期，
而不是直接强制地杀死 Pod。在存在强制关闭设施的前提下，
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 会尝试体面地终止
Pod。</p>
<!--
Typically, the container runtime sends a TERM signal to the main process in each
container. Many container runtimes respect the `STOPSIGNAL` value defined in the container
image and send this instead of TERM.
Once the grace period has expired, the KILL signal is sent to any remainig
processes, and the Pod is then deleted from the
<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API Server'>API Server</a>. If the kubelet or the
container runtime's management service is restarted while waiting for processes to terminate, the
cluster retries from the start including the full original grace period.
-->
<p>通常情况下，容器运行时会发送一个 TERM 信号到每个容器中的主进程。
很多容器运行时都能够注意到容器镜像中 <code>STOPSIGNAL</code> 的值，并发送该信号而不是 TERM。
一旦超出了体面终止限期，容器运行时会向所有剩余进程发送 KILL 信号，之后
Pod 就会被从 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>
上移除。如果 <code>kubelet</code> 或者容器运行时的管理服务在等待进程终止期间被重启，
集群会从头开始重试，赋予 Pod 完整的体面终止限期。</p>
<!--
An example flow:

1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period
   (30 seconds).
1. The Pod in the API server is updated with the time beyond which the Pod is considered "dead"
   along with the grace period.
   If you use `kubectl describe` to check on the Pod you're deleting, that Pod shows up as
   "Terminating".
   On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked
   as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod
   shutdown process.
-->
<p>下面是一个例子：</p>
<ol>
<li>
<p>你使用 <code>kubectl</code> 工具手动删除某个特定的 Pod，而该 Pod 的体面终止限期是默认值（30 秒）。</p>
</li>
<li>
<p>API 服务器中的 Pod 对象被更新，记录涵盖体面终止限期在内 Pod
的最终死期，超出所计算时间点则认为 Pod 已死（dead）。
如果你使用 <code>kubectl describe</code> 来查验你正在删除的 Pod，该 Pod 会显示为
&quot;Terminating&quot; （正在终止）。
在 Pod 运行所在的节点上：<code>kubelet</code> 一旦看到 Pod
被标记为正在终止（已经设置了体面终止限期），<code>kubelet</code> 即开始本地的 Pod 关闭过程。</p>
<!--
1. If one of the Pod's containers has defined a `preStop`
   [hook](/docs/concepts/containers/container-lifecycle-hooks), the kubelet
   runs that hook inside of the container. If the `preStop` hook is still running after the
   grace period expires, the kubelet requests a small, one-off grace period extension of 2
   seconds.
   If the `preStop` hook needs longer to complete than the default grace period allows,
   you must modify `terminationGracePeriodSeconds` to suit this.
1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each
   container.
   The containers in the Pod receive the TERM signal at different times and in an arbitrary
   order. If the order of shutdowns matters, consider using a `preStop` hook to synchronize.
-->
<ol>
<li>
<p>如果 Pod 中的容器之一定义了 <code>preStop</code>
<a href="/zh/docs/concepts/containers/container-lifecycle-hooks">回调</a>，
<code>kubelet</code> 开始在容器内运行该回调逻辑。如果超出体面终止限期时，<code>preStop</code> 回调逻辑
仍在运行，<code>kubelet</code> 会请求给予该 Pod 的宽限期一次性增加 2 秒钟。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果 <code>preStop</code> 回调所需要的时间长于默认的体面终止限期，你必须修改
<code>terminationGracePeriodSeconds</code> 属性值来使其正常工作。
</div>
</li>
<li>
<p><code>kubelet</code> 接下来触发容器运行时发送 TERM 信号给每个容器中的进程 1。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Pod 中的容器会在不同时刻收到 TERM 信号，接收顺序也是不确定的。
如果关闭的顺序很重要，可以考虑使用 <code>preStop</code> 回调逻辑来协调。
</div>
</li>
</ol>
</li>
</ol>
<!--
1. At the same time as the kubelet is starting graceful shutdown, the control plane removes that
   shutting-down Pod from Endpoints (and, if enabled, EndpointSlice) objects where these represent
   a <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> with a configured
   <a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='selector'>selector</a>.
   <a class='glossary-tooltip' title='ReplicaSet 是下一代副本控制器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/replicaset/' target='_blank' aria-label='ReplicaSets'>ReplicaSets</a> and other workload resources
   no longer treat the shutting-down Pod as a valid, in-service replica. Pods that shut down slowly
   cannot continue to serve traffic as load balancers (like the service proxy) remove the Pod from
   the list of endpoints as soon as the termination grace period _begins_.
-->
<ol start="3">
<li>与此同时，<code>kubelet</code> 启动体面关闭逻辑，控制面会将 Pod 从对应的端点列表（以及端点切片列表，
如果启用了的话）中移除，过滤条件是 Pod 被对应的
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务'>服务</a>以某
<a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='选择算符'>选择算符</a>选定。
<a class='glossary-tooltip' title='ReplicaSet 是下一代副本控制器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/replicaset/' target='_blank' aria-label='ReplicaSets'>ReplicaSets</a>和其他工作负载资源
不再将关闭进程中的 Pod 视为合法的、能够提供服务的副本。关闭动作很慢的 Pod
也无法继续处理请求数据，因为负载均衡器（例如服务代理）已经在终止宽限期开始的时候
将其从端点列表中移除。</li>
</ol>
<!--
1. When the grace period expires, the kubelet triggers forcible shutdown. The container runtime sends
   `SIGKILL` to any processes still running in any container in the Pod.
   The kubelet also cleans up a hidden `pause` container if that container runtime uses one.
1. The kubelet triggers forcible removal of Pod object from the API server, by setting grace period
   to 0 (immediate deletion).
1. The API server deletes the Pod's API object, which is then no longer visible from any client.
-->
<ol start="4">
<li>
<p>超出终止宽限期限时，<code>kubelet</code> 会触发强制关闭过程。容器运行时会向 Pod 中所有容器内
仍在运行的进程发送 <code>SIGKILL</code> 信号。
<code>kubelet</code> 也会清理隐藏的 <code>pause</code> 容器，如果容器运行时使用了这种容器的话。</p>
</li>
<li>
<p><code>kubelet</code> 触发强制从 API 服务器上删除 Pod 对象的逻辑，并将体面终止限期设置为 0
（这意味着马上删除）。</p>
</li>
<li>
<p>API 服务器删除 Pod 的 API 对象，从任何客户端都无法再看到该对象。</p>
</li>
</ol>
<!--
### Forced Pod termination {#pod-termination-forced}

Forced deletions can be potentially disruptive for some workloads and their Pods.

By default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports
the `-grace-period=<seconds>` option which allows you to override the default and specify your
own value.
-->
<h3 id="pod-termination-forced">强制终止 Pod    </h3>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 对于某些工作负载及其 Pod 而言，强制删除很可能会带来某种破坏。
</div>

<p>默认情况下，所有的删除操作都会附有 30 秒钟的宽限期限。
<code>kubectl delete</code> 命令支持 <code>--grace-period=&lt;seconds&gt;</code> 选项，允许你重载默认值，
设定自己希望的期限值。</p>
<!--
Setting the grace period to `0` forcibly and immediately deletes the Pod from the API
server. If the pod was still running on a node, that forcible deletion triggers the kubelet to
begin immediate cleanup.
-->
<p>将宽限期限强制设置为 <code>0</code> 意味着立即从 API 服务器删除 Pod。
如果 Pod 仍然运行于某节点上，强制删除操作会触发 <code>kubelet</code> 立即执行清理操作。</p>
<!--
You must specify an additional flag `--force` along with `--grace-period=0` in order to perform force deletions.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你必须在设置 <code>--grace-period=0</code> 的同时额外设置 <code>--force</code>
参数才能发起强制删除请求。
</div>
<!--
When a force deletion is performed, the API server does not wait for confirmation
from the kubelet that the Pod has been terminated on the node it was running on. It
removes the Pod in the API immediately so a new Pod can be created with the same
name. On the node, Pods that are set to terminate immediately will still be given
a small grace period before being force killed.

If you need to force-delete Pods that are part of a StatefulSet, refer to the task
documentation for
[deleting Pods from a StatefulSet](/docs/tasks/run-application/force-delete-stateful-set-pod/).
-->
<p>执行强制删除操作时，API 服务器不再等待来自 <code>kubelet</code> 的、关于 Pod
已经在原来运行的节点上终止执行的确认消息。
API 服务器直接删除 Pod 对象，这样新的与之同名的 Pod 即可以被创建。
在节点侧，被设置为立即终止的 Pod 仍然会在被强行杀死之前获得一点点的宽限时间。</p>
<p>如果你需要强制删除 StatefulSet 的 Pod，请参阅
<a href="/zh/docs/tasks/run-application/force-delete-stateful-set-pod/">从 StatefulSet 中删除 Pod</a>
的任务文档。</p>
<!--
### Garbage collection of failed Pods {#pod-garbage-collection}

For failed Pods, the API objects remain in the cluster's API until a human or
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> process
explicitly removes them.

The control plane cleans up terminated Pods (with a phase of `Succeeded` or
`Failed`), when the number of Pods exceeds the configured threshold
(determined by `terminated-pod-gc-threshold` in the kube-controller-manager).
This avoids a resource leak as Pods are created and terminated over time.
-->
<h3 id="pod-garbage-collection">失效 Pod 的垃圾收集   </h3>
<p>对于已失败的 Pod 而言，对应的 API 对象仍然会保留在集群的 API 服务器上，直到
用户或者<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>进程显式地
将其删除。</p>
<p>控制面组件会在 Pod 个数超出所配置的阈值
（根据 <code>kube-controller-manager</code> 的 <code>terminated-pod-gc-threshold</code> 设置）时
删除已终止的 Pod（阶段值为 <code>Succeeded</code> 或 <code>Failed</code>）。
这一行为会避免随着时间演进不断创建和终止 Pod 而引起的资源泄露问题。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Get hands-on experience
  [attaching handlers to container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).

* Get hands-on experience
  [configuring Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).

* Learn more about [container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).

* For detailed information about Pod and container status in the API, see
  the API reference documentation covering
  [`.status`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus) for Pod.
-->
<ul>
<li>动手实践<a href="/zh/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">为容器生命周期时间关联处理程序</a>。</li>
<li>动手实践<a href="/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">配置存活态、就绪态和启动探针</a>。</li>
<li>进一步了解<a href="/zh/docs/concepts/containers/container-lifecycle-hooks/">容器生命周期回调</a>。</li>
<li>关于 API 中定义的有关 Pod 和容器状态的详细规范信息，
可参阅 API 参考文档中 Pod 的 <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus"><code>.status</code></a> 字段。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1ccbd4eeded6ab138d98b59175bd557e">4.1.2 - Init 容器</h1>
    
	<!---
reviewers:
- erictune
title: Init Containers
content_type: concept
weight: 40
-->
<!-- overview -->
<!--
This page provides an overview of init containers: specialized containers that run
before app containers in a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>.
Init containers can contain utilities or setup scripts not present in an app image.
-->
<p>本页提供了 Init 容器的概览。Init 容器是一种特殊容器，在 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
内的应用容器启动之前运行。Init 容器可以包括一些应用镜像中不存在的实用工具和安装脚本。</p>
<!--
You can specify init containers in the Pod specification alongside the `containers`
array (which describes app containers).
-->
<p>你可以在 Pod 的规约中与用来描述应用容器的 <code>containers</code> 数组平行的位置指定
Init 容器。</p>
<!-- body -->
<!--
## Understanding init containers

A <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> can have multiple containers
running apps within it, but it can also have one or more init containers, which are run
before the app containers are started.
-->
<h2 id="理解-init-容器">理解 Init 容器</h2>
<p>每个 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 中可以包含多个容器，
应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。</p>
<!--
Init containers are exactly like regular containers, except:

* Init containers always run to completion.
* Each init container must complete successfully before the next one starts.
-->
<p>Init 容器与普通的容器非常像，除了如下两点：</p>
<ul>
<li>它们总是运行到完成。</li>
<li>每个都必须在下一个启动之前成功完成。</li>
</ul>
<!--
If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.
However, if the Pod has a `restartPolicy` of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.
-->
<p>如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。
然而，如果 Pod 对应的 <code>restartPolicy</code> 值为 &quot;Never&quot;，并且 Pod 的 Init 容器失败，
则 Kubernetes 会将整个 Pod 状态设置为失败。</p>
<!--
To specify an init container for a Pod, add the `initContainers` field into
the [Pod specification](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec),
as an array of `container` items (similar to the app `containers` field and its contents).
See [Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container) in the
API reference for more details.

The status of the init containers is returned in `.status.initContainerStatuses`
field as an array of the container statuses (similar to the `.status.containerStatuses`
field).
-->
<p>为 Pod 设置 Init 容器需要在 <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec">Pod 规约</a>
中添加 <code>initContainers</code> 字段，
该字段以 <a href="/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core">Container</a>
类型对象数组的形式组织，和应用的 <code>containers</code> 数组同级相邻。
参阅 API 参考的<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">容器</a>章节了解详情。</p>
<p>Init 容器的状态在 <code>status.initContainerStatuses</code> 字段中以容器状态数组的格式返回
（类似 <code>status.containerStatuses</code> 字段）。</p>
<!--
### Differences from regular containers

Init containers support all the fields and features of app containers,
including resource limits, volumes, and security settings. However, the
resource requests and limits for an init container are handled differently,
as documented in [Resources](#resources).

Also, init containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or
`startupProbe` because they must run to completion before the Pod can be ready.

If you specify multiple init containers for a Pod, Kubelet runs each init
container sequentially. Each init container must succeed before the next can run.
When all of the init containers have run to completion, Kubelet initializes
the application containers for the Pod and runs them as usual.
-->
<h3 id="与普通容器的不同之处">与普通容器的不同之处</h3>
<p>Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。
然而，Init 容器对资源请求和限制的处理稍有不同，在下面<a href="#resources">资源</a>节有说明。</p>
<p>同时 Init 容器不支持 <code>lifecycle</code>、<code>livenessProbe</code>、<code>readinessProbe</code> 和 <code>startupProbe</code>，
因为它们必须在 Pod 就绪之前运行完成。</p>
<p>如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。
每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时，
Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。</p>
<!--
## Using init containers

Because init containers have separate images from app containers, they
have some advantages for start-up related code:

* Init containers can contain utilities or custom code for setup that are not present in an app
  image. For example, there is no need to make an image `FROM` another image just to use a tool like
  `sed`, `awk`, `python`, or `dig` during setup.
* The application image builder and deployer roles can work independently without
  the need to jointly build a single app image.
* Init containers can run with a different view of the filesystem than app containers in the
  same Pod. Consequently, they can be given access to
  <a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secrets'>Secrets</a> that app containers cannot access.
* Because init containers run to completion before any app containers start, init containers offer
  a mechanism to block or delay app container startup until a set of preconditions are met. Once
  preconditions are met, all of the app containers in a Pod can start in parallel.
* Init containers can securely run utilities or custom code that would otherwise make an app
  container image less secure. By keeping unnecessary tools separate you can limit the attack
  surface of your app container image.
-->
<h2 id="使用-init-容器">使用 Init 容器</h2>
<p>因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势：</p>
<ul>
<li>
<p>Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。
例如，没有必要仅为了在安装过程中使用类似 <code>sed</code>、<code>awk</code>、<code>python</code> 或 <code>dig</code>
这样的工具而去 <code>FROM</code> 一个镜像来生成一个新的镜像。</p>
</li>
<li>
<p>Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。</p>
</li>
<li>
<p>应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。</p>
</li>
<li>
<p>Init 容器能以不同于 Pod 内应用容器的文件系统视图运行。因此，Init 容器可以访问
应用容器不能访问的 <a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a> 的权限。</p>
</li>
<li>
<p>由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器
提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。
一旦前置条件满足，Pod 内的所有的应用容器会并行启动。</p>
</li>
</ul>
<!--
### Examples

Here are some ideas for how to use init containers:

* Wait for a <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a> to
  be created, using a shell one-line command like:
  ```shell
  for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1
  ```

 * Register this Pod with a remote server from the downward API with a command like:
  ```shell
  curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'
  ```
* Wait for some time before starting the app container with a command like
  ```shell
  sleep 60
  ```

* Clone a Git repository into a <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='Volume'>Volume</a>

* Place values into a configuration file and run a template tool to dynamically
  generate a configuration file for the main app container. For example,
  place the `POD_IP` value in a configuration and generate the main app
  configuration file using Jinja.
-->
<h3 id="examples">示例 </h3>
<p>下面是一些如何使用 Init 容器的想法：</p>
<ul>
<li>
<p>等待一个 Service 完成创建，通过类似如下 shell 命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#666">{</span>1..100<span style="color:#666">}</span>; <span style="color:#a2f;font-weight:bold">do</span> sleep 1; <span style="color:#a2f;font-weight:bold">if</span> dig myservice; <span style="color:#a2f;font-weight:bold">then</span> <span style="color:#a2f">exit</span> 0; <span style="color:#a2f;font-weight:bold">fi</span>; <span style="color:#a2f;font-weight:bold">done</span>; <span style="color:#a2f">exit</span> <span style="color:#666">1</span>
</code></pre></div></li>
<li>
<p>注册这个 Pod 到远程服务器，通过在命令中调用 API，类似如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -X POST http://<span style="color:#b8860b">$MANAGEMENT_SERVICE_HOST</span>:<span style="color:#b8860b">$MANAGEMENT_SERVICE_PORT</span>/register <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -d <span style="color:#b44">&#39;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&#39;</span>
</code></pre></div></li>
<li>
<p>在启动应用容器之前等一段时间，使用类似命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sleep <span style="color:#666">60</span>
</code></pre></div></li>
<li>
<p>克隆 Git 仓库到<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>中。</p>
</li>
<li>
<p>将配置值放到配置文件中，运行模板工具为主应用容器动态地生成配置文件。
例如，在配置文件中存放 <code>POD_IP</code> 值，并使用 Jinja 生成主应用配置文件。</p>
</li>
</ul>
<!--
#### Init containers in use

This example defines a simple Pod that has two init containers.
The first waits for `myservice`, and the second waits for `mydb`. Once both
init containers complete, the Pod runs the app container from its `spec` section.
-->
<h3 id="使用-init-容器的情况">使用 Init 容器的情况</h3>
<p>下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 <code>myservice</code> 启动，
第二个等待 <code>mydb</code> 启动。 一旦这两个 Init容器 都启动完成，Pod 将启动 <code>spec</code> 节中的应用容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myapp-pod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>myapp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myapp-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#39;sh&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;-c&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;echo The app is running! &amp;&amp; sleep 3600&#39;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">initContainers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>init-myservice<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#39;sh&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;-c&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>init-mydb<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#39;sh&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#39;-c&#39;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
You can start this Pod by running:
-->
<p>你通过运行下面的命令启动 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f myapp.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>pod/myapp-pod created
</code></pre><!--
And check on its status with:
-->
<p>使用下面的命令检查其状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get -f myapp.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
</code></pre><!--
or for more details:
-->
<p>或者查看更多详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe -f myapp.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Name:          myapp-pod
Namespace:     default
[...]
Labels:        app=myapp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &quot;busybox&quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &quot;busybox&quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634
</code></pre><!--
To see logs for the init containers in this Pod, run:
-->
<p>如需查看 Pod 内 Init 容器的日志，请执行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs myapp-pod -c init-myservice <span style="color:#080;font-style:italic"># 查看第一个 Init 容器</span>
kubectl logs myapp-pod -c init-mydb      <span style="color:#080;font-style:italic"># 查看第二个 Init 容器</span>
</code></pre></div><!--
At this point, those init containers will be waiting to discover Services named
`mydb` and `myservice`.

Here's a configuration you can use to make those Services appear:
-->
<p>在这一刻，Init 容器将会等待至发现名称为 <code>mydb</code> 和 <code>myservice</code> 的 Service。</p>
<p>如下为创建这些 Service 的配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myservice<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mydb<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9377</span><span style="color:#bbb">
</span></code></pre></div><!--
To create the `mydb` and `myservice` services:
-->
<p>创建 <code>mydb</code> 和 <code>myservice</code> 服务的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f services.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>service &quot;myservice&quot; created
service &quot;mydb&quot; created
</code></pre><!--
You'll then see that those init containers complete, and that the `myapp-pod`
Pod moves into the Running state:
-->
<p>这样你将能看到这些 Init 容器执行完毕，随后 <code>my-app</code> 的 Pod 进入 <code>Running</code> 状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get -f myapp.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
</code></pre><!--
This simple example should provide some inspiration for you to create your own
init containers. [What's next](#what-s-next) contains a link to a more detailed example.
-->
<p>这个简单例子应该能为你创建自己的 Init 容器提供一些启发。
<a href="#what-s-next">接下来</a>节提供了更详细例子的链接。</p>
<!--
## Detailed behavior

During Pod startup, the kubelet delays running init containers until the networking
and storage are ready. Then the kubelet runs the Pod's init containers in the order
they appear in the Pod's spec.

Each init container must exit successfully before
the next container starts. If a container fails to start due to the runtime or
exits with failure, it is retried according to the Pod `restartPolicy`. However,
if the Pod `restartPolicy` is set to Always, the init containers use
`restartPolicy` OnFailure.

A Pod cannot be `Ready` until all init containers have succeeded. The ports on an
init container are not aggregated under a Service. A Pod that is initializing
is in the `Pending` state but should have a condition `Initialized` set to false.

If the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers
must execute again.
-->
<h2 id="detailed-behavior">具体行为</h2>
<p>在 Pod 启动过程中，每个 Init 容器会在网络和数据卷初始化之后按顺序启动。
kubelet 运行依据 Init 容器在 Pod 规约中的出现顺序依次运行之。</p>
<p>每个 Init 容器成功退出后才会启动下一个 Init 容器。
如果某容器因为容器运行时的原因无法启动，或以错误状态退出，kubelet 会根据
Pod 的 <code>restartPolicy</code> 策略进行重试。
然而，如果 Pod 的 <code>restartPolicy</code> 设置为 &quot;Always&quot;，Init 容器失败时会使用
<code>restartPolicy</code> 的 &quot;OnFailure&quot; 策略。</p>
<p>在所有的 Init 容器没有成功之前，Pod 将不会变成 <code>Ready</code> 状态。
Init 容器的端口将不会在 Service 中进行聚集。正在初始化中的 Pod 处于 <code>Pending</code> 状态，
但会将状况 <code>Initializing</code> 设置为 false。</p>
<p>如果 Pod <a href="#pod-restart-reasons">重启</a>，所有 Init 容器必须重新执行。</p>
<!--
Changes to the init container spec are limited to the container image field.
Altering an init container image field is equivalent to restarting the Pod.

Because init containers can be restarted, retried, or re-executed, init container
code should be idempotent. In particular, code that writes to files on `EmptyDirs`
should be prepared for the possibility that an output file already exists.

Init containers have all of the fields of an app container. However, Kubernetes
prohibits `readinessProbe` from being used because init containers cannot
define readiness distinct from completion. This is enforced during validation.

-->
<p>对 Init 容器规约的修改仅限于容器的 <code>image</code> 字段。
更改 Init 容器的 <code>image</code> 字段，等同于重启该 Pod。</p>
<p>因为 Init 容器可能会被重启、重试或者重新执行，所以 Init 容器的代码应该是幂等的。
特别地，基于 <code>emptyDirs</code> 写文件的代码，应该对输出文件可能已经存在做好准备。</p>
<p>Init 容器具有应用容器的所有字段。然而 Kubernetes 禁止使用 <code>readinessProbe</code>，
因为 Init 容器不能定义不同于完成态（Completion）的就绪态（Readiness）。
Kubernetes 会在校验时强制执行此检查。</p>
<!--
Use `activeDeadlineSeconds` on the Pod to prevent init containers from failing forever.
The active deadline includes init containers.
However it is recommended to use `activeDeadlineSeconds` only if teams deploy their application
as a Job, because `activeDeadlineSeconds` has an effect even after initContainer finished.
The Pod which is already running correctly would be killed by `activeDeadlineSeconds` if you set.

The name of each app and init container in a Pod must be unique; a
validation error is thrown for any container sharing a name with another.
-->
<p>在 Pod 上使用 <code>activeDeadlineSeconds</code> 和在容器上使用 <code>livenessProbe</code> 可以避免
Init 容器一直重复失败。
<code>activeDeadlineSeconds</code> 时间包含了 Init 容器启动的时间。
但建议仅在团队将其应用程序部署为 Job 时才使用 <code>activeDeadlineSeconds</code>，
因为 <code>activeDeadlineSeconds</code> 在 Init 容器结束后仍有效果。
如果你设置了 <code>activeDeadlineSeconds</code>，已经在正常运行的 Pod 会被杀死。</p>
<p>在 Pod 中的每个应用容器和 Init 容器的名称必须唯一；
与任何其它容器共享同一个名称，会在校验时抛出错误。</p>
<!--
### Resources

Given the ordering and execution for init containers, the following rules
for resource usage apply:

* The highest of any particular resource request or limit defined on all init
  containers is the *effective init request/limit*. If any resource has no
  resource limit specified this is considered as the highest limit.
* The Pod's *effective request/limit* for a resource is the higher of:
  * the sum of all app containers request/limit for a resource
  * the effective init request/limit for a resource
* Scheduling is done based on effective requests/limits, which means
  init containers can reserve resources for initialization that are not used
  during the life of the Pod.
* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the
  QoS tier for init containers and app containers alike.
-->
<h3 id="resources">资源</h3>
<p>在给定的 Init 容器执行顺序下，资源使用适用于如下规则：</p>
<ul>
<li>所有 Init 容器上定义的任何特定资源的 limit 或 request 的最大值，作为 Pod <em>有效初始 request/limit</em>。
如果任何资源没有指定资源限制，这被视为最高限制。</li>
<li>Pod 对资源的 <em>有效 limit/request</em> 是如下两者的较大者：
<ul>
<li>所有应用容器对某个资源的 limit/request 之和</li>
<li>对某个资源的有效初始 limit/request</li>
</ul>
</li>
<li>基于有效 limit/request 完成调度，这意味着 Init 容器能够为初始化过程预留资源，
这些资源在 Pod 生命周期过程中并没有被使用。</li>
<li>Pod 的 <em>有效 QoS 层</em> ，与 Init 容器和应用容器的一样。</li>
</ul>
<!--
Quota and limits are applied based on the effective Pod request and limit.
Pod level control groups (cgroups) are based on the effective Pod request and limit, the same as the scheduler.
-->
<p>配额和限制适用于有效 Pod 的请求和限制值。
Pod 级别的 cgroups 是基于有效 Pod 的请求和限制值，和调度器相同。</p>
<!--
### Pod restart reasons

A Pod can restart, causing re-execution of init containers, for the following
reasons:

* The Pod infrastructure container is restarted. This is uncommon and would
  have to be done by someone with root access to nodes.
* All containers in a Pod are terminated while `restartPolicy` is set to Always,
  forcing a restart, and the init container completion record has been lost due
  to garbage collection.
-->
<h3 id="pod-restart-reasons">Pod 重启的原因 </h3>
<p>Pod 重启会导致 Init 容器重新执行，主要有如下几个原因：</p>
<ul>
<li>
<p>Pod 的基础设施容器 (译者注：如 <code>pause</code> 容器) 被重启。这种情况不多见，
必须由具备 root 权限访问节点的人员来完成。</p>
</li>
<li>
<p>当 <code>restartPolicy</code> 设置为 &quot;<code>Always</code>&quot;，Pod 中所有容器会终止而强制重启。
由于垃圾收集机制的原因，Init 容器的完成记录将会丢失。</p>
</li>
</ul>
<!--
The Pod will not be restarted when the init container image is changed, or the
init container completion record has been lost due to garbage collection. This
applies for Kubernetes v1.20 and later. If you are using an earlier version of
Kubernetes, consult the documentation for the version you are using.
-->
<p>当 Init 容器的镜像发生改变或者 Init 容器的完成记录因为垃圾收集等原因被丢失时，
Pod 不会被重启。这一行为适用于 Kubernetes v1.20 及更新版本。如果你在使用较早
版本的 Kubernetes，可查阅你所使用的版本对应的文档。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container)
* Learn how to [debug init containers](/docs/tasks/debug-application-cluster/debug-init-containers/)
-->
<ul>
<li>阅读<a href="/zh/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container">创建包含 Init 容器的 Pod</a></li>
<li>学习如何<a href="/zh/docs/tasks/debug-application-cluster/debug-init-containers/">调试 Init 容器</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c8d62295ca703fdcef1aaf89fb4c916a">4.1.3 - Pod 拓扑分布约束</h1>
    
	<!--
title: Pod Topology Spread Constraints
content_type: concept
weight: 40
-->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code>
</div>


<!--
leave this shortcode in place until the note about EvenPodsSpread is obsolete
-->
<!-- overview -->
<!--
You can use _topology spread constraints_ to control how <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource utilization.
-->
<p>你可以使用 <em>拓扑分布约束（Topology Spread Constraints）</em> 来控制
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> 在集群内故障域
之间的分布，例如区域（Region）、可用区（Zone）、节点和其他用户自定义拓扑域。
这样做有助于实现高可用并提升资源利用率。</p>
<!-- body -->
<!--
## Prerequisites

### Node Labels
-->
<h2 id="prerequisites">先决条件  </h2>
<h3 id="node-labels">节点标签  </h3>
<!--
Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in. For example, a Node might have labels: `node=node1,zone=us-east-1a,region=us-east-1`
-->
<p>拓扑分布约束依赖于节点标签来标识每个节点所在的拓扑域。
例如，某节点可能具有标签：<code>node=node1,zone=us-east-1a,region=us-east-1</code></p>
<!--
Suppose you have a 4-node cluster with the following labels:
-->
<p>假设你拥有具有以下标签的一个 4 节点集群：</p>
<pre><code>NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre><!--
Then the cluster is logically viewed as below:
-->
<p>那么，从逻辑上看集群如下：</p>
<figure>
<div class="mermaid">
    
graph TB
    subgraph "zoneB"
        n3(Node3)
        n4(Node4)
    end
    subgraph "zoneA"
        n1(Node1)
        n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4 k8s;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
Instead of manually applying labels, you can also reuse the [well-known labels](/docs/reference/labels-annotations-taints/) that are created and populated automatically on most clusters.
-->
<p>你可以复用在大多数集群上自动创建和填充的<a href="/zh/docs/reference/labels-annotations-taints/">常用标签</a>，
而不是手动添加标签。</p>
<!--
## Spread Constraints for Pods
-->
<h2 id="spread-constraints-for-pods">Pod 的分布约束   </h2>
<h3 id="api">API</h3>
<!--
The API field `pod.spec.topologySpreadConstraints` is defined as below:
-->
<p><code>pod.spec.topologySpreadConstraints</code> 字段定义如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologySpreadConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span>&lt;integer&gt;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>&lt;string&gt;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>&lt;string&gt;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb"> </span>&lt;object&gt;<span style="color:#bbb">
</span></code></pre></div><!--
You can define one or multiple `topologySpreadConstraint` to instruct the kube-scheduler how to place each incoming Pod in relation to the existing Pods across your cluster. The fields are:
-->
<p>你可以定义一个或多个 <code>topologySpreadConstraint</code> 来指示 kube-scheduler
如何根据与现有的 Pod 的关联关系将每个传入的 Pod 部署到集群中。字段包括：</p>
<!--
- **maxSkew** describes the degree to which Pods may be unevenly distributed.
  It's the maximum permitted difference between the number of matching Pods in
  any two topology domains of a given topology type. It must be greater than
  zero. Its semantics differs according to the value of `whenUnsatisfiable`:
  - when `whenUnsatisfiable` equals to "DoNotSchedule", `maxSkew` is the maximum
    permitted difference between the number of matching pods in the target
    topology and the global minimum.
  - when `whenUnsatisfiable` equals to "ScheduleAnyway", scheduler gives higher
    precedence to topologies that would help reduce the skew.
- **topologyKey** is the key of node labels. If two Nodes are labelled with this key and have identical values for that label, the scheduler treats both Nodes as being in the same topology. The scheduler tries to place a balanced number of Pods into each topology domain.
- **whenUnsatisfiable** indicates how to deal with a Pod if it doesn't satisfy the spread constraint:
    - `DoNotSchedule` (default) tells the scheduler not to schedule it.
    - `ScheduleAnyway` tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.
- **labelSelector** is used to find matching Pods. Pods that match this label selector are counted to determine the number of Pods in their corresponding topology domain. See [Label Selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors) for more details.
-->
<ul>
<li><strong>maxSkew</strong> 描述 Pod 分布不均的程度。这是给定拓扑类型中任意两个拓扑域中
匹配的 pod 之间的最大允许差值。它必须大于零。取决于 <code>whenUnsatisfiable</code> 的
取值，其语义会有不同。
<ul>
<li>当 <code>whenUnsatisfiable</code> 等于 &quot;DoNotSchedule&quot; 时，<code>maxSkew</code> 是目标拓扑域
中匹配的 Pod 数与全局最小值之间可存在的差异。</li>
<li>当 <code>whenUnsatisfiable</code> 等于 &quot;ScheduleAnyway&quot; 时，调度器会更为偏向能够降低
偏差值的拓扑域。</li>
</ul>
</li>
<li><strong>topologyKey</strong> 是节点标签的键。如果两个节点使用此键标记并且具有相同的标签值，
则调度器会将这两个节点视为处于同一拓扑域中。调度器试图在每个拓扑域中放置数量
均衡的 Pod。</li>
<li><strong>whenUnsatisfiable</strong> 指示如果 Pod 不满足分布约束时如何处理：
<ul>
<li><code>DoNotSchedule</code>（默认）告诉调度器不要调度。</li>
<li><code>ScheduleAnyway</code> 告诉调度器仍然继续调度，只是根据如何能将偏差最小化来对
节点进行排序。</li>
</ul>
</li>
<li><strong>labelSelector</strong> 用于查找匹配的 pod。匹配此标签的 Pod 将被统计，以确定相应
拓扑域中 Pod 的数量。
有关详细信息，请参考<a href="/zh/docs/concepts/overview/working-with-objects/labels/#label-selectors">标签选择算符</a>。</li>
</ul>
<!--
When a Pod defines more than one `topologySpreadConstraint`, those constraints are ANDed: The kube-scheduler looks for a node for the incoming Pod that satisfies all the constraints.
-->
<p>当 Pod 定义了不止一个 <code>topologySpreadConstraint</code>，这些约束之间是逻辑与的关系。
kube-scheduler 会为新的 Pod 寻找一个能够满足所有约束的节点。</p>
<!--
You can read more about this field by running `kubectl explain Pod.spec.topologySpreadConstraints`.
-->
<p>你可以执行 <code>kubectl explain Pod.spec.topologySpreadConstraints</code> 命令以
了解关于 topologySpreadConstraints 的更多信息。</p>
<!--
### Example: One TopologySpreadConstraint

Suppose you have a 4-node cluster where 3 Pods labeled `foo:bar` are located in node1, node2 and node3 respectively:
-->
<h3 id="例子-单个-topologyspreadconstraint">例子：单个 TopologySpreadConstraint</h3>
<p>假设你拥有一个 4 节点集群，其中标记为 <code>foo:bar</code> 的 3 个 Pod 分别位于
node1、node2 和 node3 中：</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p3(Pod) --> n3(Node3)
        n4(Node4)
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4,p1,p2,p3 k8s;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
If we want an incoming Pod to be evenly spread with existing Pods across zones, the spec can be given as:
-->
<p>如果希望新来的 Pod 均匀分布在现有的可用区域，则可以按如下设置其规约：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/topology-spread-constraints/one-constraint.yaml" download="pods/topology-spread-constraints/one-constraint.yaml"><code>pods/topology-spread-constraints/one-constraint.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-topology-spread-constraints-one-constraint-yaml')" title="Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-topology-spread-constraints-one-constraint-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologySpreadConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:3.1</code></pre></div>
    </div>
</div>


<!--
`topologyKey: zone` implies the even distribution will only be applied to the nodes which have label pair "zone:&lt;any value&gt;" present. `whenUnsatisfiable: DoNotSchedule` tells the scheduler to let it stay pending if the incoming Pod can’t satisfy the constraint.
-->
<p><code>topologyKey: zone</code> 意味着均匀分布将只应用于存在标签键值对为
&quot;zone:&lt;any value&gt;&quot; 的节点。
<code>whenUnsatisfiable: DoNotSchedule</code> 告诉调度器如果新的 Pod 不满足约束，
则让它保持悬决状态。</p>
<!--
If the scheduler placed this incoming Pod into "zoneA", the Pods distribution would become [3, 1],
hence the actual skew is 2 (3 - 1) - which violates `maxSkew: 1`. In this example, the incoming Pod can only be placed onto "zoneB":
-->
<p>如果调度器将新的 Pod 放入 &quot;zoneA&quot;，Pods 分布将变为 [3, 1]，因此实际的偏差
为 2（3 - 1）。这违反了 <code>maxSkew: 1</code> 的约定。此示例中，新 Pod 只能放置在
&quot;zoneB&quot; 上：</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p3(Pod) --> n3(Node3)
        p4(mypod) --> n4(Node4)
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4,p1,p2,p3 k8s;
    class p4 plain;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<p>或者</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p3(Pod) --> n3(Node3)
        p4(mypod) --> n3
        n4(Node4)
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4,p1,p2,p3 k8s;
    class p4 plain;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
You can tweak the Pod spec to meet various kinds of requirements:
-->
<p>你可以调整 Pod 规约以满足各种要求：</p>
<!--
- Change `maxSkew` to a bigger value like "2" so that the incoming Pod can be placed onto "zoneA" as well.
- Change `topologyKey` to "node" so as to distribute the Pods evenly across nodes instead of zones. In the above example, if `maxSkew` remains "1", the incoming Pod can only be placed onto "node4".
- Change `whenUnsatisfiable: DoNotSchedule` to `whenUnsatisfiable: ScheduleAnyway` to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs are satisfied). However, it’s preferred to be placed onto the topology domain which has fewer matching Pods. (Be aware that this preferability is jointly normalized with other internal scheduling priorities like resource usage ratio, etc.)
-->
<ul>
<li>将 <code>maxSkew</code> 更改为更大的值，比如 &quot;2&quot;，这样新的 Pod 也可以放在 &quot;zoneA&quot; 上。</li>
<li>将 <code>topologyKey</code> 更改为 &quot;node&quot;，以便将 Pod 均匀分布在节点上而不是区域中。
在上面的例子中，如果 <code>maxSkew</code> 保持为 &quot;1&quot;，那么传入的 Pod 只能放在 &quot;node4&quot; 上。</li>
<li>将 <code>whenUnsatisfiable: DoNotSchedule</code> 更改为 <code>whenUnsatisfiable: ScheduleAnyway</code>，
以确保新的 Pod 始终可以被调度（假设满足其他的调度 API）。
但是，最好将其放置在匹配 Pod 数量较少的拓扑域中。
（请注意，这一优先判定会与其他内部调度优先级（如资源使用率等）排序准则一起进行标准化。）</li>
</ul>
<!--
### Example: Multiple TopologySpreadConstraints
-->
<h3 id="例子-多个-topologyspreadconstraints">例子：多个 TopologySpreadConstraints</h3>
<!--
This builds upon the previous example. Suppose you have a 4-node cluster where 3 Pods labeled `foo:bar` are located in node1, node2 and node3 respectively (`P` represents Pod):
-->
<p>下面的例子建立在前面例子的基础上。假设你拥有一个 4 节点集群，其中 3 个标记为 <code>foo:bar</code> 的
Pod 分别位于 node1、node2 和 node3 上：</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p3(Pod) --> n3(Node3)
        n4(Node4)
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4,p1,p2,p3 k8s;
    class p4 plain;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
You can use 2 TopologySpreadConstraints to control the Pods spreading on both zone and node:
-->
<p>可以使用 2 个 TopologySpreadConstraint 来控制 Pod 在 区域和节点两个维度上的分布：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/topology-spread-constraints/two-constraints.yaml" download="pods/topology-spread-constraints/two-constraints.yaml"><code>pods/topology-spread-constraints/two-constraints.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-topology-spread-constraints-two-constraints-yaml')" title="Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-topology-spread-constraints-two-constraints-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologySpreadConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>node<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:3.1</code></pre></div>
    </div>
</div>


<!--
In this case, to match the first constraint, the incoming Pod can only be placed onto "zoneB"; while in terms of the second constraint, the incoming Pod can only be placed onto "node4". Then the results of 2 constraints are ANDed, so the only viable option is to place on "node4".
-->
<p>在这种情况下，为了匹配第一个约束，新的 Pod 只能放置在 &quot;zoneB&quot; 中；而在第二个约束中，
新的 Pod 只能放置在 &quot;node4&quot; 上。最后两个约束的结果加在一起，唯一可行的选择是放置
在 &quot;node4&quot; 上。</p>
<!--
Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:
-->
<p>多个约束之间可能存在冲突。假设有一个跨越 2 个区域的 3 节点集群：</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p4(Pod) --> n3(Node3)
        p5(Pod) --> n3
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n1
        p3(Pod) --> n2(Node2)
    end

    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
    class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s;
    class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
If you apply "two-constraints.yaml" to this cluster, you will notice "mypod" stays in `Pending` state. This is because: to satisfy the first constraint, "mypod" can only be put to "zoneB"; while in terms of the second constraint, "mypod" can only put to "node2". Then a joint result of "zoneB" and "node2" returns nothing.
-->
<p>如果对集群应用 &quot;two-constraints.yaml&quot;，会发现 &quot;mypod&quot; 处于 <code>Pending</code> 状态。
这是因为：为了满足第一个约束，&quot;mypod&quot; 只能放在 &quot;zoneB&quot; 中，而第二个约束要求
&quot;mypod&quot; 只能放在 &quot;node2&quot; 上。Pod 调度无法满足两种约束。</p>
<!--
To overcome this situation, you can either increase the `maxSkew` or modify one of the constraints to use `whenUnsatisfiable: ScheduleAnyway`.
-->
<p>为了克服这种情况，你可以增加 <code>maxSkew</code> 或修改其中一个约束，让其使用
<code>whenUnsatisfiable: ScheduleAnyway</code>。</p>
<!--
### Interaction With Node Affinity and Node Selectors

The scheduler will skip the non-matching nodes from the skew calculations if the incoming Pod has `spec.nodeSelector` or `spec.affinity.nodeAffinity` defined.
-->
<h3 id="interaction-with-node-affinity-and-node-selectors">节点亲和性与节点选择器的相互作用  </h3>
<p>如果 Pod 定义了 <code>spec.nodeSelector</code> 或 <code>spec.affinity.nodeAffinity</code>，
调度器将在偏差计算中跳过不匹配的节点。</p>
<!--
### Example: TopologySpreadConstraints with NodeAffinity

Suppose you have a 5-node cluster ranging from zoneA to zoneC:
-->
<h3 id="示例-topologyspreadconstraints-与-nodeaffinity">示例：TopologySpreadConstraints 与 NodeAffinity</h3>
<p>假设你有一个跨越 zoneA 到 zoneC 的 5 节点集群：</p>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneB"
        p3(Pod) --> n3(Node3)
        n4(Node4)
    end
    subgraph "zoneA"
        p1(Pod) --> n1(Node1)
        p2(Pod) --> n2(Node2)
    end

classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<figure>
<div class="mermaid">
    
graph BT
    subgraph "zoneC"
        n5(Node5)
    end

classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n5 k8s;
class zoneC cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
and you know that "zoneC" must be excluded. In this case, you can compose the yaml as below, so that "mypod" will be placed onto "zoneB" instead of "zoneC". Similarly `spec.nodeSelector` is also respected.
-->
<p>而且你知道 &quot;zoneC&quot; 必须被排除在外。在这种情况下，可以按如下方式编写 YAML，
以便将 &quot;mypod&quot; 放置在 &quot;zoneB&quot; 上，而不是 &quot;zoneC&quot; 上。同样，<code>spec.nodeSelector</code>
也要一样处理。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml" download="pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml"><code>pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml')" title="Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologySpreadConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>DoNotSchedule<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>zone<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>NotIn<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- zoneC<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pause<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:3.1</code></pre></div>
    </div>
</div>


<!--
The scheduler doesn't have prior knowledge of all the zones or other topology domains that a cluster has. They are determined from the existing nodes in the cluster. This could lead to a problem in autoscaled clusters, when a node pool (or node group) is scaled to zero nodes and the user is expecting them to scale up, because, in this case, those topology domains won't be considered until there is at least one node in them.
-->
<p>调度器不会预先知道集群拥有的所有区域和其他拓扑域。拓扑域由集群中存在的节点确定。
在自动伸缩的集群中，如果一个节点池（或节点组）的节点数量为零，
而用户正期望其扩容时，可能会导致调度出现问题。
因为在这种情况下，调度器不会考虑这些拓扑域信息，因为它们是空的，没有节点。</p>
<!--
### Other Noticeable Semantics

There are some implicit conventions worth noting here:
-->
<h3 id="other-noticeable-semantics">其他值得注意的语义  </h3>
<p>这里有一些值得注意的隐式约定：</p>
<!--
- Only the Pods holding the same namespace as the incoming Pod can be matching candidates.

- The scheduler will bypass the nodes without `topologySpreadConstraints[*].topologyKey` present. This implies that:

  1. the Pods located on those nodes do not impact `maxSkew` calculation - in the above example, suppose "node1" does not have label "zone", then the 2 Pods will be disregarded, hence the incoming Pod will be scheduled into "zoneA".
  2. the incoming Pod has no chances to be scheduled onto this kind of nodes - in the above example, suppose a "node5" carrying label `{zone-typo: zoneC}` joins the cluster, it will be bypassed due to the absence of label key "zone".
-->
<ul>
<li>只有与新的 Pod 具有相同命名空间的 Pod 才能作为匹配候选者。</li>
<li>调度器会忽略没有 <code>topologySpreadConstraints[*].topologyKey</code> 的节点。这意味着：
<ol>
<li>
<p>位于这些节点上的 Pod 不影响 <code>maxSkew</code> 的计算。
在上面的例子中，假设 &quot;node1&quot; 没有标签 &quot;zone&quot;，那么 2 个 Pod 将被忽略，
因此传入的 Pod 将被调度到 &quot;zoneA&quot; 中。</p>
</li>
<li>
<p>新的 Pod 没有机会被调度到这类节点上。
在上面的例子中，假设一个带有标签 <code>{zone-typo: zoneC}</code> 的 &quot;node5&quot; 加入到集群，
它将由于没有标签键 &quot;zone&quot; 而被忽略。</p>
</li>
</ol>
</li>
</ul>
<!--
- Be aware of what will happen if the incomingPod’s `topologySpreadConstraints[*].labelSelector` doesn’t match its own labels. In the above example, if we remove the incoming Pod’s labels, it can still be placed onto "zoneB" since the constraints are still satisfied. However, after the placement, the degree of imbalance of the cluster remains unchanged - it’s still zoneA having 2 Pods which hold label {foo:bar}, and zoneB having 1 Pod which holds label {foo:bar}. So if this is not what you expect, we recommend the workload’s `topologySpreadConstraints[*].labelSelector` to match its own labels.
-->
<ul>
<li>注意，如果新 Pod 的 <code>topologySpreadConstraints[*].labelSelector</code> 与自身的
标签不匹配，将会发生什么。
在上面的例子中，如果移除新 Pod 上的标签，Pod 仍然可以调度到 &quot;zoneB&quot;，因为约束仍然满足。
然而，在调度之后，集群的不平衡程度保持不变。zoneA 仍然有 2 个带有 {foo:bar} 标签的 Pod，
zoneB 有 1 个带有 {foo:bar} 标签的 Pod。
因此，如果这不是你所期望的，建议工作负载的 <code>topologySpreadConstraints[*].labelSelector</code>
与其自身的标签匹配。</li>
</ul>
<!--
### Cluster-level default constraints

It is possible to set default topology spread constraints for a cluster. Default
topology spread constraints are applied to a Pod if, and only if:

- It doesn't define any constraints in its `.spec.topologySpreadConstraints`.
- It belongs to a service, replication controller, replica set or stateful set.
-->
<h3 id="cluster-level-default-constraints">集群级别的默认约束  </h3>
<p>为集群设置默认的拓扑分布约束也是可能的。默认拓扑分布约束在且仅在以下条件满足
时才会应用到 Pod 上：</p>
<ul>
<li>Pod 没有在其 <code>.spec.topologySpreadConstraints</code> 设置任何约束；</li>
<li>Pod 隶属于某个服务、副本控制器、ReplicaSet 或 StatefulSet。</li>
</ul>
<!--
Default constraints can be set as part of the `PodTopologySpread` plugin args
in a [scheduling profile](/docs/reference/scheduling/config/#profiles).
The constraints are specified with the same [API above](#api), except that
`labelSelector` must be empty. The selectors are calculated from the services,
replication controllers, replica sets or stateful sets that the Pod belongs to.

An example configuration might look like follows:
-->
<p>你可以在 <a href="/zh/docs/reference/scheduling/config/#profiles">调度方案（Scheduling Profile）</a>
中将默认约束作为 <code>PodTopologySpread</code> 插件参数的一部分来设置。
约束的设置采用<a href="#api">如前所述的 API</a>，只是 <code>labelSelector</code> 必须为空。
选择算符是根据 Pod 所属的服务、副本控制器、ReplicaSet 或 StatefulSet 来设置的。</p>
<p>配置的示例可能看起来像下面这个样子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">profiles</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">schedulerName</span>:<span style="color:#bbb"> </span>default-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pluginConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>PodTopologySpread<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">defaultConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>ScheduleAnyway<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">defaultingType</span>:<span style="color:#bbb"> </span>List<span style="color:#bbb">
</span></code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The score produced by default scheduling constraints might conflict with the
score produced by the
[`SelectorSpread` plugin](/docs/reference/scheduling/config/#scheduling-plugins).
It is recommended that you disable this plugin in the scheduling profile when
using default constraints for `PodTopologySpread`.
-->
<p>默认调度约束所生成的评分可能与
<a href="/zh/docs/reference/scheduling/config/#scheduling-plugins"><code>SelectorSpread</code> 插件</a>
所生成的评分有冲突。
建议你在为 <code>PodTopologySpread</code> 设置默认约束是禁用调度方案中的该插件。
</div>
<!--
#### Internal default constraints
-->
<h4 id="internal-default-constraints">内部默认约束   </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code>
</div>


<!--
With the `DefaultPodTopologySpread` feature gate, enabled by default, the
legacy `SelectorSpread` plugin is disabled.
kube-scheduler uses the following default topology constraints for the
`PodTopologySpread` plugin configuration:
-->
<p>当你使用了默认启用的 <code>DefaultPodTopologySpread</code> 特性门控时，原来的
<code>SelectorSpread</code> 插件会被禁用。
kube-scheduler 会使用下面的默认拓扑约束作为 <code>PodTopologySpread</code> 插件的
配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">defaultConstraints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>ScheduleAnyway<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">maxSkew</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;topology.kubernetes.io/zone&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">whenUnsatisfiable</span>:<span style="color:#bbb"> </span>ScheduleAnyway<span style="color:#bbb">
</span></code></pre></div><!--
Also, the legacy `SelectorSpread` plugin, which provides an equivalent behavior,
is disabled.
-->
<p>此外，原来用于提供等同行为的 <code>SelectorSpread</code> 插件也会被禁用。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The `PodTopologySpread` plugin does not score the nodes that don't have
the topology keys specified in the spreading constraints. This might result
in a different default behavior compared to the legacy `SelectorSpread` plugin when
using the default topology constraints.
-->
<p>对于分布约束中所指定的拓扑键而言，<code>PodTopologySpread</code> 插件不会为不包含这些主键的节点评分。
这可能导致在使用默认拓扑约束时，其行为与原来的 <code>SelectorSpread</code> 插件的默认行为不同，</p>
<!--
If your nodes are not expected to have **both** `kubernetes.io/hostname` and
`topology.kubernetes.io/zone` labels set, define your own constraints
instead of using the Kubernetes defaults.
-->
<p>如果你的节点不会 <strong>同时</strong> 设置 <code>kubernetes.io/hostname</code> 和
<code>topology.kubernetes.io/zone</code> 标签，你应该定义自己的约束而不是使用
Kubernetes 的默认约束。</p>

</div>
<!--
If you don't want to use the default Pod spreading constraints for your cluster,
you can disable those defaults by setting `defaultingType` to `List` and leaving
empty `defaultConstraints` in the `PodTopologySpread` plugin configuration:
-->
<p>如果你不想为集群使用默认的 Pod 分布约束，你可以通过设置 <code>defaultingType</code> 参数为 <code>List</code>
并将 <code>PodTopologySpread</code> 插件配置中的 <code>defaultConstraints</code> 参数置空来禁用默认 Pod 分布约束。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">profiles</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">schedulerName</span>:<span style="color:#bbb"> </span>default-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pluginConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>PodTopologySpread<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">defaultConstraints</span>:<span style="color:#bbb"> </span>[]<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">defaultingType</span>:<span style="color:#bbb"> </span>List<span style="color:#bbb">
</span></code></pre></div><!--
## Comparison with PodAffinity/PodAntiAffinity

In Kubernetes, directives related to "Affinity" control how Pods are
scheduled - more packed or more scattered.
-->
<h2 id="与-podaffinity-podantiaffinity-相比较">与 PodAffinity/PodAntiAffinity 相比较</h2>
<p>在 Kubernetes 中，与“亲和性”相关的指令控制 Pod 的调度方式（更密集或更分散）。</p>
<!--
- For `PodAffinity`, you can try to pack any number of Pods into qualifying
  topology domain(s)
- For `PodAntiAffinity`, only one Pod can be scheduled into a
  single topology domain.
-->
<ul>
<li>对于 <code>PodAffinity</code>，你可以尝试将任意数量的 Pod 集中到符合条件的拓扑域中。</li>
<li>对于 <code>PodAntiAffinity</code>，只能将一个 Pod 调度到某个拓扑域中。</li>
</ul>
<!--
For finer control, you can specify topology spread constraints to distribute
Pods across different topology domains - to achieve either high availability or
cost-saving. This can also help on rolling update workloads and scaling out
replicas smoothly. See
[Motivation](https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation)
for more details.
-->
<p>要实现更细粒度的控制，你可以设置拓扑分布约束来将 Pod 分布到不同的拓扑域下，
从而实现高可用性或节省成本。这也有助于工作负载的滚动更新和平稳地扩展副本规模。
有关详细信息，请参考
<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20190221-pod-topology-spread.md#motivation">动机</a>文档。</p>
<!--
## Known Limitations

- There's no guarantee that the constraints remain satisfied when Pods are removed. For example, scaling down a Deployment may result in imbalanced Pods distribution.
You can use [Descheduler](https://github.com/kubernetes-sigs/descheduler) to rebalance the Pods distribution.

- Pods matched on tainted nodes are respected. See [Issue 80921](https://github.com/kubernetes/kubernetes/issues/80921)
-->
<h2 id="已知局限性">已知局限性</h2>
<ul>
<li>
<p>当 Pod 被移除时，无法保证约束仍被满足。例如，缩减某 Deployment 的规模时，
Pod 的分布可能不再均衡。
你可以使用 <a href="https://github.com/kubernetes-sigs/descheduler">Descheduler</a>
来重新实现 Pod 分布的均衡。</p>
</li>
<li>
<p>具有污点的节点上匹配的 Pods 也会被统计。
参考 <a href="https://github.com/kubernetes/kubernetes/issues/80921">Issue 80921</a>。</p>
</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
- [Blog: Introducing PodTopologySpread](https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/)
  explains `maxSkew` in details, as well as bringing up some advanced usage examples.
-->
<ul>
<li><a href="https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/">博客: PodTopologySpread介绍</a>
详细解释了 <code>maxSkew</code>，并给出了一些高级的使用示例。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4aaf43c715cd764bc8ed4436f3537e68">4.1.4 - 干扰（Disruptions）</h1>
    
	<!--
reviewers:
- erictune
- foxish
- davidopp
title: Disruptions
content_type: concept
weight: 60
-->
<!-- overview -->
<!--
This guide is for application owners who want to build
highly available applications, and thus need to understand
what types of Disruptions can happen to Pods.
-->
<p>本指南针对的是希望构建高可用性应用程序的应用所有者，他们有必要了解可能发生在 Pod 上的干扰类型。</p>
<!--
It is also for Cluster Administrators who want to perform automated
cluster actions, like upgrading and autoscaling clusters.
-->
<p>文档同样适用于想要执行自动化集群操作（例如升级和自动扩展集群）的集群管理员。</p>
<!-- body -->
<!--
## Voluntary and Involuntary Disruptions

Pods do not disappear until someone (a person or a controller) destroys them, or
there is an unavoidable hardware or system software error.
-->
<h2 id="voluntary-and-involuntary-disruptions">自愿干扰和非自愿干扰    </h2>
<p>Pod 不会消失，除非有人（用户或控制器）将其销毁，或者出现了不可避免的硬件或软件系统错误。</p>
<!--
We call these unavoidable cases *involuntary disruptions* to
an application.  Examples are:
-->
<p>我们把这些不可避免的情况称为应用的<em>非自愿干扰（Involuntary Disruptions）</em>。例如：</p>
<!--
- a hardware failure of the physical machine backing the node
- cluster administrator deletes VM (instance) by mistake
- cloud provider or hypervisor failure makes VM disappear
- a kernel panic
- the node disappears from the cluster due to cluster network partition
- eviction of a pod due to the node being [out-of-resources](/docs/concepts/scheduling-eviction/node-pressure-eviction/).
-->
<ul>
<li>节点下层物理机的硬件故障</li>
<li>集群管理员错误地删除虚拟机（实例）</li>
<li>云提供商或虚拟机管理程序中的故障导致的虚拟机消失</li>
<li>内核错误</li>
<li>节点由于集群网络隔离从集群中消失</li>
<li>由于节点<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/">资源不足</a>导致 pod 被驱逐。</li>
</ul>
<!--
Except for the out-of-resources condition, all these conditions
should be familiar to most users; they are not specific
to Kubernetes.
-->
<p>除了资源不足的情况，大多数用户应该都熟悉这些情况；它们不是特定于 Kubernetes 的。</p>
<!--
We call other cases *voluntary disruptions*.  These include both
actions initiated by the application owner and those initiated by a Cluster
Administrator.  Typical application owner actions include:
-->
<p>我们称其他情况为<em>自愿干扰（Voluntary Disruptions）</em>。
包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者的操
作包括：</p>
<!--
- deleting the deployment or other controller that manages the pod
- updating a deployment's pod template causing a restart
- directly deleting a pod (e.g. by accident)
-->
<ul>
<li>删除 Deployment 或其他管理 Pod 的控制器</li>
<li>更新了 Deployment 的 Pod 模板导致 Pod 重启</li>
<li>直接删除 Pod（例如，因为误操作）</li>
</ul>
<!--
Cluster Administrator actions include:

- [Draining a node](/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.
- Draining a node from a cluster to scale the cluster down (learn about
[Cluster Autoscaling](https://github.com/kubernetes/autoscaler/#readme)
).
- Removing a pod from a node to permit something else to fit on that node.
-->
<p>集群管理员操作包括：</p>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/safely-drain-node/">排空（drain）节点</a>进行修复或升级。</li>
<li>从集群中排空节点以缩小集群（了解<a href="https://github.com/kubernetes/autoscaler/#readme">集群自动扩缩</a>）。</li>
<li>从节点中移除一个 Pod，以允许其他 Pod 使用该节点。</li>
</ul>
<!--
These actions might be taken directly by the cluster administrator, or by automation
run by the cluster administrator, or by your cluster hosting provider.
-->
<p>这些操作可能由集群管理员直接执行，也可能由集群管理员所使用的自动化工具执行，或者由集群托管提供商自动执行。</p>
<!--
Ask your cluster administrator or consult your cloud provider or distribution documentation
to determine if any sources of voluntary disruptions are enabled for your cluster.
If none are enabled, you can skip creating Pod Disruption Budgets.
-->
<p>咨询集群管理员或联系云提供商，或者查询发布文档，以确定是否为集群启用了任何资源干扰源。
如果没有启用，可以不用创建 Pod Disruption Budgets（Pod 干扰预算）</p>
<!--
Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example,
deleting deployments or pods bypasses Pod Disruption Budgets.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 并非所有的自愿干扰都会受到 Pod 干扰预算的限制。
例如，删除 Deployment 或 Pod 的删除操作就会跳过 Pod 干扰预算检查。
</div>

<!--
## Dealing with Disruptions

Here are some ways to mitigate involuntary disruptions:
-->
<h2 id="处理干扰">处理干扰</h2>
<p>以下是减轻非自愿干扰的一些方法：</p>
<!--
- Ensure your pod [requests the resources](/docs/tasks/configure-pod-container/assign-cpu-ram-container) it needs.
- Replicate your application if you need higher availability.  (Learn about running replicated
[stateless](/docs/tasks/run-application/run-stateless-application-deployment/)
and [stateful](/docs/tasks/run-application/run-replicated-stateful-application/) applications.)
- For even higher availability when running replicated applications,
  spread applications across racks (using
  [anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity))
  or across zones (if using a
  [multi-zone cluster](/docs/setup/multiple-zones).)
-->
<ul>
<li>确保 Pod 在请求中给出<a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">所需资源</a>。</li>
<li>如果需要更高的可用性，请复制应用程序。
（了解有关运行多副本的<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">无状态</a>
和<a href="/zh/docs/tasks/run-application/run-replicated-stateful-application/">有状态</a>应用程序的信息。）</li>
<li>为了在运行复制应用程序时获得更高的可用性，请跨机架（使用
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">反亲和性</a>
或跨区域（如果使用<a href="/zh/docs/setup/best-practices/multiple-zones/">多区域集群</a>）扩展应用程序。</li>
</ul>
<!--
The frequency of voluntary disruptions varies.  On a basic Kubernetes cluster, there are
no automated voluntary disruptions (only user-triggered ones).  However, your cluster administrator or hosting provider
may run some additional services which cause voluntary disruptions. For example,
rolling out node software updates can cause voluntary disruptions. Also, some implementations
of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.
Your cluster administrator or hosting provider should have documented what level of voluntary
disruptions, if any, to expect. Certain configuration options, such as
[using PriorityClasses](/docs/concepts/scheduling-eviction/pod-priority-preemption/)
in your pod spec can also cause voluntary (and involuntary) disruptions.
-->
<p>自愿干扰的频率各不相同。在一个基本的 Kubernetes 集群中，没有自愿干扰（只有用户触发的干扰）。
然而，集群管理员或托管提供商可能运行一些可能导致自愿干扰的额外服务。例如，节点软
更新可能导致自愿干扰。另外，集群（节点）自动缩放的某些
实现可能导致碎片整理和紧缩节点的自愿干扰。集群
管理员或托管提供商应该已经记录了各级别的自愿干扰（如果有的话）。
有些配置选项，例如在 pod spec 中
<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">使用 PriorityClasses</a>
也会产生自愿（和非自愿）的干扰。</p>
<!--
Kubernetes offers features to help run highly available applications at the same
time as frequent voluntary disruptions.  We call this set of features
*Disruption Budgets*.
-->
<p>Kubernetes 提供特性来满足在出现频繁自愿干扰的同时运行高可用的应用程序。我们称这些特性为
<em>干扰预算（Disruption Budget）</em>。</p>
<!--
## Pod disruption budgets

Kubernetes offers features to help you run highly available applications even when you
introduce frequent voluntary disruptions.

An Application Owner can create a `PodDisruptionBudget` object (PDB) for each application.
A PDB limits the number of pods of a replicated application that are down simultaneously from
voluntary disruptions.  For example, a quorum-based application would
like to ensure that the number of replicas running is never brought below the
number needed for a quorum. A web front end might want to
ensure that the number of replicas serving load never falls below a certain
percentage of the total.
-->
<h2 id="pod-disruption-budgets">干扰预算  </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>


<p>即使你会经常引入自愿性干扰，Kubernetes 也能够支持你运行高度可用的应用。</p>
<p>应用程序所有者可以为每个应用程序创建 <code>PodDisruptionBudget</code> 对象（PDB）。
PDB 将限制在同一时间因自愿干扰导致的复制应用程序中宕机的 pod 数量。
例如，基于票选机制的应用程序希望确保运行的副本数永远不会低于仲裁所需的数量。
Web 前端可能希望确保提供负载的副本数量永远不会低于总数的某个百分比。</p>
<!--
Cluster managers and hosting providers should use tools which
respect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#eviction-api)
instead of directly deleting pods or deployments.  Examples are the `kubectl drain` command
and the Kubernetes-on-GCE cluster upgrade script (`cluster/gce/upgrade.sh`).
-->
<p>集群管理员和托管提供商应该使用遵循 PodDisruptionBudgets 的接口
（通过调用<a href="/zh/docs/tasks/administer-cluster/safely-drain-node/#the-eviction-api">Eviction API</a>），
而不是直接删除 Pod 或 Deployment。</p>
<!--
For example, the `kubectl drain` subcommand lets you mark a node as going out of
service. When you run `kubectl drain`, the tool tries to evict all of the Pods on
the Node you'are taking out of service. The eviction request may be temporarily rejected,
and the tool periodically retries all failed requests until all pods
are terminated, or until a configurable timeout is reached.
-->
<p>例如，<code>kubectl drain</code> 命令可以用来标记某个节点即将停止服务。
运行 <code>kubectl drain</code> 命令时，工具会尝试驱逐机器上的所有 Pod。
<code>kubectl</code> 所提交的驱逐请求可能会暂时被拒绝，所以该工具会定时重试失败的请求，
直到所有的 Pod 都被终止，或者达到配置的超时时间。</p>
<!--
A PDB specifies the number of replicas that an application can tolerate having, relative to how
many it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is
supposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,
then the Eviction API will allow voluntary disruption of one, but not two pods, at a time.
-->
<p>PDB 指定应用程序可以容忍的副本数量（相当于应该有多少副本）。
例如，具有 <code>.spec.replicas: 5</code> 的 Deployment 在任何时间都应该有 5 个 Pod。
如果 PDB 允许其在某一时刻有 4 个副本，那么驱逐 API 将允许同一时刻仅有一个而不是两个 Pod 自愿干扰。</p>
<!--
The group of pods that comprise the application is specified using a label selector, the same
as the one used by the application's controller (deployment, stateful-set, etc).
-->
<p>使用标签选择器来指定构成应用程序的一组 Pod，这与应用程序的控制器（Deployment，StatefulSet 等）
选择 Pod 的逻辑一样。</p>
<!--
The "intended" number of pods is computed from the `.spec.replicas` of the pods controller.
The controller is discovered from the pods using the `.metadata.ownerReferences` of the object.
-->
<p>Pod 控制器的 <code>.spec.replicas</code> 计算“预期的” Pod 数量。
根据 Pod 对象的 <code>.metadata.ownerReferences</code> 字段来发现控制器。</p>
<!--
[Involuntary disruptions](#voluntary-and-involuntary-disruptions) cannot be prevented by PDBs; however they
do count against the budget.
-->
<p>PDB 无法防止<a href="#voluntary-and-involuntary-disruptions">非自愿干扰</a>；
但它们确实计入预算。</p>
<!--
Pods which are deleted or unavailable due to a rolling upgrade to an application do count
against the disruption budget, but controllers (like deployment and stateful-set)
are not limited by PDBs when doing rolling upgrades - the handling of failures
during application updates is configured in spec for the specific workload resource.
-->
<p>由于应用程序的滚动升级而被删除或不可用的 Pod 确实会计入干扰预算，
但是控制器（如 Deployment 和 StatefulSet）在进行滚动升级时不受 PDB
的限制。应用程序更新期间的故障处理方式是在对应的工作负载资源的 <code>spec</code> 中配置的。</p>
<!--
When a pod is evicted using the eviction API, it is gracefully
[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination),
hornoring the
`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core).
-->
<p>当使用驱逐 API 驱逐 Pod 时，Pod 会被体面地
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">终止</a>，期间会
参考 <a href="/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core">PodSpec</a>
中的 <code>terminationGracePeriodSeconds</code> 配置值。</p>
<!--
## PDB Example

Consider a cluster with 3 nodes, `node-1` through `node-3`.
The cluster is running several applications.  One of them has 3 replicas initially called
`pod-a`, `pod-b`, and `pod-c`.  Another, unrelated pod without a PDB, called `pod-x`, is also shown.
Initially, the pods are laid out as follows:
-->
<h2 id="pdb-example">PDB 例子  </h2>
<p>假设集群有 3 个节点，<code>node-1</code> 到 <code>node-3</code>。集群上运行了一些应用。
其中一个应用有 3 个副本，分别是 <code>pod-a</code>，<code>pod-b</code> 和 <code>pod-c</code>。
另外，还有一个不带 PDB 的无关 pod <code>pod-x</code> 也同样显示出来。
最初，所有的 Pod 分布如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1</th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">pod-a  <em>available</em></td>
<td style="text-align:center">pod-b <em>available</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
</tr>
<tr>
<td style="text-align:center">pod-x  <em>available</em></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<!--
All 3 pods are part of a deployment, and they collectively have a PDB which requires
there be at least 2 of the 3 pods to be available at all times.
-->
<p>3 个 Pod 都是 deployment 的一部分，并且共同拥有同一个 PDB，要求 3 个 Pod 中至少有 2 个 Pod 始终处于可用状态。</p>
<!--
For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.
The cluster administrator first tries to drain `node-1` using the `kubectl drain` command.
That tool tries to evict `pod-a` and `pod-x`.  This succeeds immediately.
Both pods go into the `terminating` state at the same time.
This puts the cluster in this state:
-->
<p>例如，假设集群管理员想要重启系统，升级内核版本来修复内核中的权限。
集群管理员首先使用 <code>kubectl drain</code> 命令尝试排空 <code>node-1</code> 节点。
命令尝试驱逐 <code>pod-a</code> 和 <code>pod-x</code>。操作立即就成功了。
两个 Pod 同时进入 <code>terminating</code> 状态。这时的集群处于下面的状态：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1 <em>draining</em></th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">pod-a  <em>terminating</em></td>
<td style="text-align:center">pod-b <em>available</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
</tr>
<tr>
<td style="text-align:center">pod-x  <em>terminating</em></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<!--
The deployment notices that one of the pods is terminating, so it creates a replacement
called `pod-d`.  Since `node-1` is cordoned, it lands on another node.  Something has
also created `pod-y` as a replacement for `pod-x`.
-->
<p>Deployment 控制器观察到其中一个 Pod 正在终止，因此它创建了一个替代 Pod <code>pod-d</code>。
由于 <code>node-1</code> 被封锁（cordon），<code>pod-d</code> 落在另一个节点上。
同样其他控制器也创建了 <code>pod-y</code> 作为 <code>pod-x</code> 的替代品。</p>
<!--
(Note: for a StatefulSet, `pod-a`, which would be called something like `pod-0`, would need
to terminate completely before its replacement, which is also called `pod-0` but has a
different UID, could be created.  Otherwise, the example applies to a StatefulSet as well.)
-->
<p>（注意：对于 StatefulSet 来说，<code>pod-a</code>（也称为 <code>pod-0</code>）需要在替换 Pod 创建之前完全终止，
替代它的也称为 <code>pod-0</code>，但是具有不同的 UID。除此之外，此示例也适用于 StatefulSet。）</p>
<!--
Now the cluster is in this state:
-->
<p>当前集群的状态如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1 <em>draining</em></th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">pod-a  <em>terminating</em></td>
<td style="text-align:center">pod-b <em>available</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
</tr>
<tr>
<td style="text-align:center">pod-x  <em>terminating</em></td>
<td style="text-align:center">pod-d <em>starting</em></td>
<td style="text-align:center">pod-y</td>
</tr>
</tbody>
</table>
<!--
At some point, the pods terminate, and the cluster looks like this:
-->
<p>在某一时刻，Pod 被终止，集群如下所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1 <em>drained</em></th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-b <em>available</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-d <em>starting</em></td>
<td style="text-align:center">pod-y</td>
</tr>
</tbody>
</table>
<!--
At this point, if an impatient cluster administrator tries to drain `node-2` or
`node-3`, the drain command will block, because there are only 2 available
pods for the deployment, and its PDB requires at least 2.  After some time passes, `pod-d` becomes available.
-->
<p>此时，如果一个急躁的集群管理员试图排空（drain）<code>node-2</code> 或 <code>node-3</code>，drain 命令将被阻塞，
因为对于 Deployment 来说只有 2 个可用的 Pod，并且它的 PDB 至少需要 2 个。
经过一段时间，<code>pod-d</code> 变得可用。</p>
<!--
The cluster state now looks like this:
-->
<p>集群状态如下所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1 <em>drained</em></th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-b <em>available</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-d <em>available</em></td>
<td style="text-align:center">pod-y</td>
</tr>
</tbody>
</table>
<!--
Now, the cluster administrator tries to drain `node-2`.
The drain command will try to evict the two pods in some order, say
`pod-b` first and then `pod-d`.  It will succeed at evicting `pod-b`.
But, when it tries to evict `pod-d`, it will be refused because that would leave only
one pod available for the deployment.
-->
<p>现在，集群管理员试图排空（drain）<code>node-2</code>。
drain 命令将尝试按照某种顺序驱逐两个 Pod，假设先是 <code>pod-b</code>，然后是 <code>pod-d</code>。
命令成功驱逐 <code>pod-b</code>，但是当它尝试驱逐 <code>pod-d</code>时将被拒绝，因为对于
Deployment 来说只剩一个可用的 Pod 了。</p>
<!--
The deployment creates a replacement for `pod-b` called `pod-e`.
Because there are not enough resources in the cluster to schedule
`pod-e` the drain will again block.  The cluster may end up in this
state:
-->
<p>Deployment 创建 <code>pod-b</code> 的替代 Pod <code>pod-e</code>。
因为集群中没有足够的资源来调度 <code>pod-e</code>，drain 命令再次阻塞。集群最终将是下面这种状态：</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1 <em>drained</em></th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
<th style="text-align:center"><em>no node</em></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-b <em>terminating</em></td>
<td style="text-align:center">pod-c <em>available</em></td>
<td style="text-align:center">pod-e <em>pending</em></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">pod-d <em>available</em></td>
<td style="text-align:center">pod-y</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<!--
At this point, the cluster administrator needs to
add a node back to the cluster to proceed with the upgrade.
-->
<p>此时，集群管理员需要增加一个节点到集群中以继续升级操作。</p>
<!--
You can see how Kubernetes varies the rate at which disruptions
can happen, according to:
-->
<p>可以看到 Kubernetes 如何改变干扰发生的速率，根据：</p>
<!--
- how many replicas an application needs
- how long it takes to gracefully shutdown an instance
- how long it takes a new instance to start up
- the type of controller
- the cluster's resource capacity
-->
<ul>
<li>应用程序需要多少个副本</li>
<li>优雅关闭应用实例需要多长时间</li>
<li>启动应用新实例需要多长时间</li>
<li>控制器的类型</li>
<li>集群的资源能力</li>
</ul>
<!--
## Separating Cluster Owner and Application Owner Roles

Often, it is useful to think of the Cluster Manager
and Application Owner as separate roles with limited knowledge
of each other.   This separation of responsibilities
may make sense in these scenarios:
-->
<h2 id="分离集群所有者和应用所有者角色">分离集群所有者和应用所有者角色</h2>
<p>通常，将集群管理者和应用所有者视为彼此了解有限的独立角色是很有用的。这种责任分离在下面这些场景下是有意义的：</p>
<!--
- when there are many application teams sharing a Kubernetes cluster, and
  there is natural specialization of roles
- when third-party tools or services are used to automate cluster management
-->
<ul>
<li>当有许多应用程序团队共用一个 Kubernetes 集群，并且有自然的专业角色</li>
<li>当第三方工具或服务用于集群自动化管理</li>
</ul>
<!--
Pod Disruption Budgets support this separation of roles by providing an
interface between the roles.
-->
<p>Pod 干扰预算通过在角色之间提供接口来支持这种分离。</p>
<!--
If you do not have such a separation of responsibilities in your organization,
you may not need to use Pod Disruption Budgets.
-->
<p>如果你的组织中没有这样的责任分离，则可能不需要使用 Pod 干扰预算。</p>
<!--
## How to perform Disruptive Actions on your Cluster

If you are a Cluster Administrator, and you need to perform a disruptive action on all
the nodes in your cluster, such as a node or system software upgrade, here are some options:
-->
<h2 id="如何在集群上执行干扰性操作">如何在集群上执行干扰性操作</h2>
<p>如果你是集群管理员，并且需要对集群中的所有节点执行干扰操作，例如节点或系统软件升级，则可以使用以下选项</p>
<!--
- Accept downtime during the upgrade.
- Failover to another complete replica cluster.
   -  No downtime, but may be costly both for the duplicated nodes
     and for human effort to orchestrate the switchover.
- Write disruption tolerant applications and use PDBs.
   - No downtime.
   - Minimal resource duplication.
   - Allows more automation of cluster administration.
   - Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary
     disruptions largely overlaps with work to support autoscaling and tolerating
     involuntary disruptions.
-->
<ul>
<li>接受升级期间的停机时间。</li>
<li>故障转移到另一个完整的副本集群。
<ul>
<li>没有停机时间，但是对于重复的节点和人工协调成本可能是昂贵的。</li>
</ul>
</li>
<li>编写可容忍干扰的应用程序和使用 PDB。
<ul>
<li>不停机。</li>
<li>最小的资源重复。</li>
<li>允许更多的集群管理自动化。</li>
<li>编写可容忍干扰的应用程序是棘手的，但对于支持容忍自愿干扰所做的工作，和支持自动扩缩和容忍非
自愿干扰所做工作相比，有大量的重叠</li>
</ul>
</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).
* Learn more about [draining nodes](/docs/tasks/administer-cluster/safely-drain-node/)
* Learn about [updating a deployment](/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)
  including steps to maintain its availability during the rollout.
-->
<ul>
<li>参考<a href="/zh/docs/tasks/run-application/configure-pdb/">配置 Pod 干扰预算</a>中的方法来保护你的应用。</li>
<li>进一步了解<a href="/zh/docs/tasks/administer-cluster/safely-drain-node/">排空节点</a>的信息。</li>
<li>了解<a href="/zh/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">更新 Deployment</a>
的过程，包括如何在其进程中维持应用的可用性</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-53a1005011e1bda2ce81819aad7c8b32">4.1.5 - 临时容器</h1>
    
	<!--
title: Ephemeral Containers
content_type: concept
weight: 80
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!--
This page provides an overview of ephemeral containers: a special type of container
that runs temporarily in an existing <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> to
accomplish user-initiated actions such as troubleshooting. You use ephemeral
containers to inspect services rather than to build applications.
-->
<p>本页面概述了临时容器：一种特殊的容器，该容器在现有
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
中临时运行，以便完成用户发起的操作，例如故障排查。
你会使用临时容器来检查服务，而不是用它来构建应用程序。</p>
<!-- body -->
<!--
## Understanding ephemeral containers

<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> are the fundamental building
block of Kubernetes applications. Since Pods are intended to be disposable and
replaceable, you cannot add a container to a Pod once it has been created.
Instead, you usually delete and replace Pods in a controlled fashion using
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='deployments'>deployments</a>.
-->
<h2 id="understanding-ephemeral-containers">了解临时容器  </h2>
<p><a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 是 Kubernetes 应用程序的基本构建块。
由于 Pod 是一次性且可替换的，因此一旦 Pod 创建，就无法将容器加入到 Pod 中。
取而代之的是，通常使用 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>
以受控的方式来删除并替换 Pod。</p>
<!--
Sometimes it's necessary to inspect the state of an existing Pod, however, for
example to troubleshoot a hard-to-reproduce bug. In these cases you can run
an ephemeral container in an existing Pod to inspect its state and run
arbitrary commands.
-->
<p>有时有必要检查现有 Pod 的状态。例如，对于难以复现的故障进行排查。
在这些场景中，可以在现有 Pod 中运行临时容器来检查其状态并运行任意命令。</p>
<!--
### What is an ephemeral container?

Ephemeral containers differ from other containers in that they lack guarantees
for resources or execution, and they will never be automatically restarted, so
they are not appropriate for building applications.  Ephemeral containers are
described using the same `ContainerSpec` as regular containers, but many fields
are incompatible and disallowed for ephemeral containers.
-->
<h3 id="what-is-an-ephemeral-container">什么是临时容器？   </h3>
<p>临时容器与其他容器的不同之处在于，它们缺少对资源或执行的保证，并且永远不会自动重启，
因此不适用于构建应用程序。
临时容器使用与常规容器相同的 <code>ContainerSpec</code> 节来描述，但许多字段是不兼容和不允许的。</p>
<!--
- Ephemeral containers may not have ports, so fields such as `ports`,
  `livenessProbe`, `readinessProbe` are disallowed.
- Pod resource allocations are immutable, so setting `resources` is disallowed.
- For a complete list of allowed fields, see the [EphemeralContainer reference
  documentation](/docs/reference/generated/kubernetes-api/v1.23/#ephemeralcontainer-v1-core).
-->
<ul>
<li>临时容器没有端口配置，因此像 <code>ports</code>，<code>livenessProbe</code>，<code>readinessProbe</code>
这样的字段是不允许的。</li>
<li>Pod 资源分配是不可变的，因此 <code>resources</code> 配置是不允许的。</li>
<li>有关允许字段的完整列表，请参见
<a href="/docs/reference/generated/kubernetes-api/v1.23/#ephemeralcontainer-v1-core">EphemeralContainer 参考文档</a>。</li>
</ul>
<!--
Ephemeral containers are created using a special `ephemeralcontainers` handler
in the API rather than by adding them directly to `pod.spec`, so it's not
possible to add an ephemeral container using `kubectl edit`.
-->
<p>临时容器是使用 API 中的一种特殊的 <code>ephemeralcontainers</code> 处理器进行创建的，
而不是直接添加到 <code>pod.spec</code> 段，因此无法使用 <code>kubectl edit</code> 来添加一个临时容器。</p>
<!--
Like regular containers, you may not change or remove an ephemeral container
after you have added it to a Pod.
-->
<p>与常规容器一样，将临时容器添加到 Pod 后，将不能更改或删除临时容器。</p>
<!--
## Uses for ephemeral containers

Ephemeral containers are useful for interactive troubleshooting when `kubectl
exec` is insufficient because a container has crashed or a container image
doesn't include debugging utilities.
-->
<h2 id="uses-for-ephemeral-containers">临时容器的用途  </h2>
<p>当由于容器崩溃或容器镜像不包含调试工具而导致 <code>kubectl exec</code> 无用时，
临时容器对于交互式故障排查很有用。</p>
<!--
In particular, [distroless images](https://github.com/GoogleContainerTools/distroless)
enable you to deploy minimal container images that reduce attack surface
and exposure to bugs and vulnerabilities. Since distroless images do not include a
shell or any debugging utilities, it's difficult to troubleshoot distroless
images using `kubectl exec` alone.
-->
<p>尤其是，<a href="https://github.com/GoogleContainerTools/distroless">Distroless 镜像</a>
允许用户部署最小的容器镜像，从而减少攻击面并减少故障和漏洞的暴露。
由于 distroless 镜像不包含 Shell 或任何的调试工具，因此很难单独使用
<code>kubectl exec</code> 命令进行故障排查。</p>
<!--
When using ephemeral containers, it's helpful to enable [process namespace
sharing](/docs/tasks/configure-pod-container/share-process-namespace/) so
you can view processes in other containers.
-->
<p>使用临时容器时，启用
<a href="/zh/docs/tasks/configure-pod-container/share-process-namespace/">进程名字空间共享</a>
很有帮助，可以查看其他容器中的进程。</p>
<p>What's next</p>
<!--
* Learn how to [debug pods using ephemeral containers](/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container).
-->
<ul>
<li>了解如何<a href="/zh/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container">使用临时调试容器来进行调试</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-89637410cacae45a36ab1cc278c482eb">4.2 - 工作负载资源</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-a2dc0393e0c4079e1c504b6429844e86">4.2.1 - Deployments</h1>
    
	<!--
title: Deployments
feature:
  title: Automated rollouts and rollbacks
  description: >
    Kubernetes progressively rolls out changes to your application or its configuration, while monitoring application health to ensure it doesn't kill all your instances at the same time. If something goes wrong, Kubernetes will rollback the change for you. Take advantage of a growing ecosystem of deployment solutions.

content_type: concept
weight: 10
-->
<!-- overview -->
<!--
A _Deployment_ provides declarative updates for [Pods](/docs/concepts/workloads/pods/pod/) and
[ReplicaSets](/docs/concepts/workloads/controllers/replicaset/).
-->
<p>一个 Deployment 为 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
和 <a class='glossary-tooltip' title='ReplicaSet 是下一代副本控制器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/replicaset/' target='_blank' aria-label='ReplicaSet'>ReplicaSet</a>
提供声明式的更新能力。</p>
<!--
You describe a _desired state_ in a Deployment, and the Deployment <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.
-->
<p>你负责描述 Deployment 中的 <em>目标状态</em>，而 Deployment <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a>
以受控速率更改实际状态，
使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment，
并通过新的 Deployment 收养其资源。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.
-->
<p>不要管理 Deployment 所拥有的 ReplicaSet 。
如果存在下面未覆盖的使用场景，请考虑在 Kubernetes 仓库中提出 Issue。
</div>
<!-- body -->
<!--
## Use Case

The following are typical use cases for Deployments:
-->
<h2 id="用例">用例</h2>
<p>以下是 Deployments 的典型用例：</p>
<!--
* [Create a Deployment to rollout a ReplicaSet](#creating-a-deployment). The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.
* [Declare the new state of the Pods](#updating-a-deployment) by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.
-->
<ul>
<li><a href="#creating-a-deployment">创建 Deployment 以将 ReplicaSet 上线</a>。 ReplicaSet 在后台创建 Pods。
检查 ReplicaSet 的上线状态，查看其是否成功。</li>
<li>通过更新 Deployment 的 PodTemplateSpec，<a href="#updating-a-deployment">声明 Pod 的新状态</a> 。
新的 ReplicaSet 会被创建，Deployment 以受控速率将 Pod 从旧 ReplicaSet 迁移到新 ReplicaSet。
每个新的 ReplicaSet 都会更新 Deployment 的修订版本。</li>
</ul>
<!--
* [Rollback to an earlier Deployment revision](#rolling-back-a-deployment) if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.
* [Scale up the Deployment to facilitate more load](#scaling-a-deployment).
* [Pause the Deployment](#pausing-and-resuming-a-deployment) to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.
* [Use the status of the Deployment](#deployment-status) as an indicator that a rollout has stuck.
* [Clean up older ReplicaSets](#clean-up-policy) that you don't need anymore.
-->
<ul>
<li>如果 Deployment 的当前状态不稳定，<a href="#rolling-back-a-deployment">回滚到较早的 Deployment 版本</a>。
每次回滚都会更新 Deployment 的修订版本。</li>
<li><a href="#scaling-a-deployment">扩大 Deployment 规模以承担更多负载</a>。</li>
<li><a href="#pausing-and-resuming-a-deployment">暂停 Deployment </a> 以应用对 PodTemplateSpec 所作的多项修改，
然后恢复其执行以启动新的上线版本。</li>
<li><a href="#deployment-status">使用 Deployment 状态</a> 来判定上线过程是否出现停滞。</li>
<li><a href="#clean-up-policy">清理较旧的不再需要的 ReplicaSet</a> 。</li>
</ul>
<!--
 ## Creating a Deployment

The following is an example of a Deployment. It creates a ReplicaSet to bring up three `nginx` Pods:
-->
<h2 id="creating-a-deployment">创建 Deployment </h2>
<p>下面是一个 Deployment 示例。其中创建了一个 ReplicaSet，负责启动三个 <code>nginx</code> Pods：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/nginx-deployment.yaml" download="controllers/nginx-deployment.yaml"><code>controllers/nginx-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-nginx-deployment-yaml')" title="Copy controllers/nginx-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-nginx-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In this example:
-->
<p>在该例中：</p>
<!--
 * A Deployment named `nginx-deployment` is created, indicated by the `.metadata.name` field.
 * The Deployment creates three replicated Pods, indicated by the `replicas` field.
-->
<ul>
<li>创建名为 <code>nginx-deployment</code>（由 <code>.metadata.name</code> 字段标明）的 Deployment。</li>
<li>该 Deployment 创建三个（由 <code>replicas</code> 字段标明）Pod 副本。</li>
</ul>
<!--
* The `selector` field defines how the Deployment finds which Pods to manage.
  In this case, you select a label that is defined in the Pod template (`app: nginx`).
  However, more sophisticated selection rules are possible,
  as long as the Pod template itself satisfies the rule.
-->
<ul>
<li>
<p><code>selector</code> 字段定义 Deployment 如何查找要管理的 Pods。
在这里，你选择在 Pod 模板中定义的标签（<code>app: nginx</code>）。
不过，更复杂的选择规则是也可能的，只要 Pod 模板本身满足所给规则即可。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
  The `spec.selector.matchLabels` field is a map of {key,value} pairs.
  A single {key,value} in the `matchLabels` map is equivalent to an element of `matchExpressions`,
  whose `key` field is "key", the `operator` is "In", and the `values` array contains only "value".
  All of the requirements, from both `matchLabels` and `matchExpressions`, must be satisfied in order to match.
  -->
<p><code>spec.selector.matchLabels</code> 字段是 <code>{key,value}</code> 键值对映射。
在 <code>matchLabels</code> 映射中的每个 <code>{key,value}</code> 映射等效于 <code>matchExpressions</code> 中的一个元素，
即其 <code>key</code> 字段是 “key”，<code>operator</code> 为 “In”，<code>values</code> 数组仅包含 “value”。
在 <code>matchLabels</code> 和 <code>matchExpressions</code> 中给出的所有条件都必须满足才能匹配。
</div>
</li>
</ul>
<!--
* The `template` field contains the following sub-fields:
  * The Pods are labeled `app: nginx`using the `.meatadata.labels` field.
  * The Pod template's specification, or `.template.spec` field, indicates that
    the Pods run one container, `nginx`, which runs the `nginx`
    [Docker Hub](https://hub.docker.com/) image at version 1.14.2.
  * Create one container and name it `nginx` using the `.spec.template.spec.containers[0].name` field.
-->
<ul>
<li><code>template</code> 字段包含以下子字段：
<ul>
<li>Pod 被使用 <code>.metadata.labels</code> 字段打上 <code>app: nginx</code> 标签。</li>
<li>Pod 模板规约（即 <code>.template.spec</code> 字段）指示 Pods 运行一个 <code>nginx</code> 容器，
该容器运行版本为 1.14.2 的 <code>nginx</code> <a href="https://hub.docker.com/">Docker Hub</a>镜像。</li>
<li>创建一个容器并使用 <code>.spec.template.spec.containers[0].name</code> 字段将其命名为 <code>nginx</code>。</li>
</ul>
</li>
</ul>
<!--
Before you begin, make sure your Kubernetes cluster is up and running.
Follow the steps given below to create the above Deployment:
-->
<p>开始之前，请确保的 Kubernetes 集群已启动并运行。
按照以下步骤创建上述 Deployment ：</p>
<!--
1. Create the Deployment by running the following command:
-->
<ol>
<li>
<p>通过运行以下命令创建 Deployment ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
</code></pre></div></li>
</ol>
<!--
 2. Run `kubectl get deployments` to check if the Deployment was created.

    If the Deployment is still being created, the output is similar to the following:
-->
<ol start="2">
<li>
<p>运行 <code>kubectl get deployments</code> 检查 Deployment 是否已创建。
如果仍在创建 Deployment，则输出类似于：</p>
<pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/3     0            0           1s
</code></pre><!--
When you inspect the Deployments in your cluster, the following fields are displayed:
-->
<p>在检查集群中的 Deployment 时，所显示的字段有：</p>
<!--
* `NAME` lists the names of the Deployments in the cluster.
* `READY` displays how many replicas of the application are available to your users. It follows the pattern ready/desired.
* `UP-TO-DATE` displays the number of replicas that have been updated to achieve the desired state.
* `AVAILABLE` displays how many replicas of the application are available to your users.
* `AGE` displays the amount of time that the application has been running.
-->
<ul>
<li><code>NAME</code> 列出了集群中 Deployment 的名称。</li>
<li><code>READY</code> 显示应用程序的可用的“副本”数。显示的模式是“就绪个数/期望个数”。</li>
<li><code>UP-TO-DATE</code> 显示为了达到期望状态已经更新的副本数。</li>
<li><code>AVAILABLE</code> 显示应用可供用户使用的副本数。</li>
<li><code>AGE</code> 显示应用程序运行的时间。</li>
</ul>
<!--
Notice how the number of desired replicas is 3 according to `.spec.replicas` field.
-->
<p>请注意期望副本数是根据 <code>.spec.replicas</code> 字段设置 3。</p>
</li>
</ol>
<!--
3. To see the Deployment rollout status, run `kubectl rollout status deployment/nginx-deployment`.

   The output is similar to:
-->
<ol start="3">
<li>
<p>要查看 Deployment 上线状态，运行 <code>kubectl rollout status deployment/nginx-deployment</code>。</p>
<p>输出类似于：</p>
<pre><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment &quot;nginx-deployment&quot; successfully rolled out
</code></pre></li>
</ol>
<!--
4. Run the `kubectl get deployments` again a few seconds later. The output is similar to this:
-->
<ol start="4">
<li>
<p>几秒钟后再次运行 <code>kubectl get deployments</code>。输出类似于：</p>
<pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           18s
</code></pre><!--
Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.
-->
<p>注意 Deployment 已创建全部三个副本，并且所有副本都是最新的（它们包含最新的 Pod 模板）
并且可用。</p>
</li>
</ol>
<!--
5. To see the ReplicaSet (`rs`) created by the Deployment, run `kubectl get rs`. The output is similar to this:
-->
<ol start="5">
<li>
<p>要查看 Deployment 创建的 ReplicaSet（<code>rs</code>），运行 <code>kubectl get rs</code>。
输出类似于：</p>
<pre><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75675f5897   3         3         3       18s
</code></pre><!--
ReplicaSet output shows the following fields:

* `NAME` lists the names of the ReplicaSets in the namespace.
* `DESIRED` displays the desired number of _replicas_ of the application, which you define when you create the Deployment. This is the _desired state_.
* `CURRENT` displays how many replicas are currently running.
* `READY` displays how many replicas of the application are available to your users.
* `AGE` displays the amount of time that the application has been running.
-->
<p>ReplicaSet 输出中包含以下字段：</p>
<ul>
<li><code>NAME</code> 列出名字空间中 ReplicaSet 的名称；</li>
<li><code>DESIRED</code> 显示应用的期望副本个数，即在创建 Deployment 时所定义的值。
此为期望状态；</li>
<li><code>CURRENT</code> 显示当前运行状态中的副本个数；</li>
<li><code>READY</code> 显示应用中有多少副本可以为用户提供服务；</li>
<li><code>AGE</code> 显示应用已经运行的时间长度。</li>
</ul>
<!--
Notice that the name of the ReplicaSet is always formatted as `[DEPLOYMENT-NAME]-[RANDOM-STRING]`. The random string is
randomly generated and uses the `pod-template-hash` as a seed.
-->
<p>注意 ReplicaSet 的名称始终被格式化为<code>[Deployment名称]-[随机字符串]</code>。
其中的随机字符串是使用 <code>pod-template-hash</code> 作为种子随机生成的。</p>
</li>
</ol>
<!--
6. To see the labels automatically generated for each Pod, run `kubectl get pods -show-labels`.
   The following output is returned:
-->
<ol start="6">
<li>
<p>要查看每个 Pod 自动生成的标签，运行 <code>kubectl get pods --show-labels</code>。返回以下输出：</p>
<pre><code>NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
</code></pre><!--
The created ReplicaSet ensures that there are three `nginx` Pods.
-->
<p>所创建的 ReplicaSet 确保总是存在三个 <code>nginx</code> Pod。</p>
</li>
</ol>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
You must specify an appropriate selector and Pod template labels in a Deployment
(in this case, `app: nginx`).

Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.
-->
<p>你必须在 Deployment 中指定适当的选择算符和 Pod 模板标签（在本例中为 <code>app: nginx</code>）。
标签或者选择算符不要与其他控制器（包括其他 Deployment 和 StatefulSet）重叠。
Kubernetes 不会阻止你这样做，但是如果多个控制器具有重叠的选择算符，
它们可能会发生冲突执行难以预料的操作。
</div>
<!--
 ### Pod-template-hash label
-->
<h3 id="pod-template-hash-标签">Pod-template-hash 标签</h3>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Do not change this label.
-->
<p>不要更改此标签。
</div>
<!--
The `pod-template-hash` label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.
-->
<p>Deployment 控制器将 <code>pod-template-hash</code> 标签添加到 Deployment
所创建或收留的每个 ReplicaSet 。</p>
<!--
This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the `PodTemplate` of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,
and in any existing Pods that the ReplicaSet might have.
-->
<p>此标签可确保 Deployment 的子 ReplicaSets 不重叠。
标签是通过对 ReplicaSet 的 <code>PodTemplate</code> 进行哈希处理。
所生成的哈希值被添加到 ReplicaSet 选择算符、Pod 模板标签，并存在于在 ReplicaSet
可能拥有的任何现有 Pod 中。</p>
<!--
 ## Updating a Deployment
-->
<h2 id="updating-a-deployment">更新 Deployment  </h2>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
 A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, `.spec.template`)
is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.
-->
<p>仅当 Deployment Pod 模板（即 <code>.spec.template</code>）发生改变时，例如模板的标签或容器镜像被更新，
才会触发 Deployment 上线。其他更新（如对 Deployment 执行扩缩容的操作）不会触发上线动作。
</div>
<!--
Follow the steps given below to update your Deployment:
-->
<p>按照以下步骤更新 Deployment：</p>
<!--
1. Let's update the nginx Pods to use the `nginx:1.16.1` image instead of the `nginx:1.14.2` image.
-->
<ol>
<li>
<p>先来更新 nginx Pod 以使用 <code>nginx:1.16.1</code> 镜像，而不是 <code>nginx:1.14.2</code> 镜像。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image deployment.v1.apps/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.16.1
</code></pre></div><!--
or use the following command:
-->
<p>或者使用下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.16.1
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>deployment/nginx-deployment image updated
</code></pre><!--
Alternatively, you can `edit` the Deployment and change `.spec.template.spec.containers[0].image` from `nginx:1.14.2` to `nginx:1.16.1`:
-->
<p>或者，可以对 Deployment 执行 <code>edit</code> 操作并将 <code>.spec.template.spec.containers[0].image</code> 从
<code>nginx:1.14.2</code> 更改至 <code>nginx:1.16.1</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment/nginx-deployment edited
</code></pre></li>
</ol>
<!--
2. To see the rollout status, run:
-->
<ol start="2">
<li>
<p>要查看上线状态，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
</code></pre><!-- or -->
<p>或者</p>
<pre><code>deployment &quot;nginx-deployment&quot; successfully rolled out
</code></pre></li>
</ol>
<!--
Get more details on your updated Deployment:
-->
<p>获取关于已更新的 Deployment 的更多信息：</p>
<!--
* After the rollout succeeds, you can view the Deployment by running `kubectl get deployments`.
  The output is similar to this:
-->
<ul>
<li>
<p>在上线成功后，可以通过运行 <code>kubectl get deployments</code> 来查看 Deployment：
输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#b44">NAME               READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span style="color:#b44">nginx-deployment   3/3     3            3           36s</span>
</code></pre></div></li>
</ul>
<!--
* Run `kubectl get rs` to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it
up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.
-->
<ul>
<li>
<p>运行 <code>kubectl get rs</code> 以查看 Deployment 通过创建新的 ReplicaSet 并将其扩容到
3 个副本并将旧 ReplicaSet 缩容到 0 个副本完成了 Pod 的更新操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s
</code></pre></li>
</ul>
<!--
* Running `get pods` should now show only the new Pods:
-->
<ul>
<li>
<p>现在运行 <code>get pods</code> 应仅显示新的 Pods:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
</code></pre><!--
Next time you want to update these Pods, you only need to update the Deployment's Pod template again.

Deployment ensures that only a certain number of Pods are down while they are being updated. By default,
it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).
-->
<p>下次要更新这些 Pods 时，只需再次更新 Deployment Pod 模板即可。</p>
<p>Deployment 可确保在更新时仅关闭一定数量的 Pod。默认情况下，它确保至少所需 Pods
75% 处于运行状态（最大不可用比例为 25%）。</p>
<!--
Deployment also ensures that only a certain number of Pods are created above the desired number of Pods.
By default, it ensures that at most 25% of the desired number of Pods are up (25% max surge).
-->
<p>Deployment 还确保仅所创建 Pod 数量只可能比期望 Pods 数高一点点。
默认情况下，它可确保启动的 Pod 个数比期望个数最多多出 25%（最大峰值 25%）。</p>
<!--   
For example, if you look at the above Deployment closely, you will see that it first created a new Pod,
then deleted some old Pods, and created new ones. It does not kill old Pods until a sufficient number of
new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.
It makes sure that at least 2 Pods are available and that at max 4 Pods in total are available. In case of
  a Deployment with 4 replicas, the number of Pods would be between 3 and 5.
-->
<p>例如，如果仔细查看上述 Deployment ，将看到它首先创建了一个新的 Pod，然后删除了一些旧的 Pods，
并创建了新的 Pods。它不会杀死老 Pods，直到有足够的数量新的 Pods 已经出现。
在足够数量的旧 Pods 被杀死前并没有创建新 Pods。它确保至少 2 个 Pod 可用，
同时最多总共 4 个 Pod 可用。
当 Deployment 设置为 4 个副本时，Pod 的个数会介于 3 和 5 之间。</p>
</li>
</ul>
<!--
* Get details of your Deployment:
-->
<ul>
<li>
<p>获取 Deployment 的更多信息</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployments
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
   Containers:
    nginx:
      Image:        nginx:1.16.1
      Port:         80/TCP
      Environment:  &lt;none&gt;
      Mounts:       &lt;none&gt;
    Volumes:        &lt;none&gt;
  Conditions:
    Type           Status  Reason
    ----           ------  ------
    Available      True    MinimumReplicasAvailable
    Progressing    True    NewReplicaSetAvailable
  OldReplicaSets:  &lt;none&gt;
  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
  Events:
    Type    Reason             Age   From                   Message
    ----    ------             ----  ----                   -------
    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
</code></pre><!--
Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)
and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet
(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet
to 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.
It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.
Finally, you'll have 3 available replicas
in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.
-->
<p>可以看到，当第一次创建 Deployment 时，它创建了一个 ReplicaSet（<code>nginx-deployment-2035384211</code>）
并将其直接扩容至 3 个副本。更新 Deployment 时，它创建了一个新的 ReplicaSet
（nginx-deployment-1564180365），并将其扩容为 1，等待其就绪；然后将旧 ReplicaSet 缩容到 2，
将新的 ReplicaSet 扩容到 2 以便至少有 3 个 Pod 可用且最多创建 4 个 Pod。
然后，它使用相同的滚动更新策略继续对新的 ReplicaSet 扩容并对旧的 ReplicaSet 缩容。
最后，你将有 3 个可用的副本在新的 ReplicaSet 中，旧 ReplicaSet 将缩容到 0。</p>
</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Kubernetes doesn't count terminating Pods when calculating the number of `availableReplicas`, which must be between
`replicas - maxUnavailable` and `replicas + maxSurge`. As a result, you might notice that there are more Pods than
expected during a rollout, and that the total resources consumed by the Deployment is more than `replicas + maxSurge`
until the `terminationGracePeriodSeconds` of the terminating Pods expires.
-->
<p>Kubernetes 在计算 <code>availableReplicas</code> 数值时不考虑终止过程中的 Pod，
<code>availableReplicas</code> 的值一定介于 <code>replicas - maxUnavailable</code> 和 <code>replicas + maxSurge</code> 之间。
因此，你可能在上线期间看到 Pod 个数比预期的多，Deployment 所消耗的总的资源也大于
<code>replicas + maxSurge</code> 个 Pod 所用的资源，直到被终止的 Pod 所设置的
<code>terminationGracePeriodSeconds</code> 到期为止。
</div>
<!--
### Rollover (aka multiple updates in-flight)

Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up
the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels
match `.spec.selector` but whose template does not match `.spec.template` are scaled down. Eventually, the new
ReplicaSet is scaled to `.spec.replicas` and all old ReplicaSets is scaled to 0.
-->
<h3 id="翻转-多-deployment-动态更新">翻转（多 Deployment 动态更新）</h3>
<p>Deployment 控制器每次注意到新的 Deployment 时，都会创建一个 ReplicaSet 以启动所需的 Pods。
如果更新了 Deployment，则控制标签匹配 <code>.spec.selector</code> 但模板不匹配 <code>.spec.template</code> 的
Pods 的现有 ReplicaSet 被缩容。最终，新的 ReplicaSet 缩放为 <code>.spec.replicas</code> 个副本，
所有旧 ReplicaSets 缩放为 0 个副本。</p>
<!--
If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet
as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously
- it will add it to its list of old ReplicaSets and start scaling it down.
-->
<p>当 Deployment 正在上线时被更新，Deployment 会针对更新创建一个新的 ReplicaSet
并开始对其扩容，之前正在被扩容的 ReplicaSet 会被翻转，添加到旧 ReplicaSets 列表
并开始缩容。</p>
<!--
For example, suppose you create a Deployment to create 5 replicas of `nginx:1.14.2`,
but then update the Deployment to create 5 replicas of `nginx:1.16.1`, when only 3
replicas of `nginx:1.7.9` had been created. In that case, the Deployment immediately starts
killing the 3 `nginx:1.7.9` Pods that it had created, and starts creating
`nginx:1.9.1` Pods. It does not wait for the 5 replicas of `nginx:1.14.2` to be created
before changing course.
-->
<p>例如，假定你在创建一个 Deployment 以生成 <code>nginx:1.14.2</code> 的 5 个副本，但接下来
更新 Deployment 以创建 5 个 <code>nginx:1.16.1</code> 的副本，而此时只有 3 个<code>nginx:1.14.2</code>
副本已创建。在这种情况下，Deployment 会立即开始杀死 3 个 <code>nginx:1.14.2</code> Pods，
并开始创建 <code>nginx:1.16.1</code> Pods。它不会等待 <code>nginx:1.14.2</code> 的 5
个副本都创建完成后才开始执行变更动作。</p>
<!--
### Label selector updates

 It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.
In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped
all of the implications.
-->
<h3 id="label-selector-updates">更改标签选择算符  </h3>
<p>通常不鼓励更新标签选择算符。建议你提前规划选择算符。
在任何情况下，如果需要更新标签选择算符，请格外小心，
并确保自己了解这背后可能发生的所有事情。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In API version `apps/v1`, a Deployment's label selector is immutable after it gets created.
-->
<p>在 API 版本 <code>apps/v1</code> 中，Deployment 标签选择算符在创建后是不可变的。
</div>
<!--
 * Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,
otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does
not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and
creating a new ReplicaSet.
* Selector updates changes the existing value in a selector key - result in the same behavior as additions.
* Selector removals removes an existing key from the Deployment selector - do not require any changes in the
Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the
removed label still exists in any existing Pods and ReplicaSets.
-->
<ul>
<li>添加选择算符时要求使用新标签更新 Deployment 规约中的 Pod 模板标签，否则将返回验证错误。
此更改是非重叠的，也就是说新的选择算符不会选择使用旧选择算符所创建的 ReplicaSet 和 Pod，
这会导致创建新的 ReplicaSet 时所有旧 ReplicaSet 都会被孤立。</li>
<li>选择算符的更新如果更改了某个算符的键名，这会导致与添加算符时相同的行为。</li>
<li>删除选择算符的操作会删除从 Deployment 选择算符中删除现有算符。
此操作不需要更改 Pod 模板标签。现有 ReplicaSet 不会被孤立，也不会因此创建新的 ReplicaSet，
但请注意已删除的标签仍然存在于现有的 Pod 和 ReplicaSet 中。</li>
</ul>
<!--
## Rolling Back a Deployment
-->
<h2 id="rolling-back-a-deployment">回滚 Deployment</h2>
<!--
 Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.
By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want
(you can change that by modifying revision history limit).
-->
<p>有时，你可能想要回滚 Deployment；例如，当 Deployment 不稳定时（例如进入反复崩溃状态）。
默认情况下，Deployment 的所有上线记录都保留在系统中，以便可以随时回滚
（你可以通过修改修订历史记录限制来更改这一约束）。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
A Deployment's revision is created when a Deployment's rollout is triggered. This means that the
new revision is created if and only if the Deployment's Pod template (`.spec.template`) is changed,
for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,
do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.
This means that when you roll back to an earlier revision, only the Deployment's Pod template part is
rolled back.
-->
<p>Deployment 被触发上线时，系统就会创建 Deployment 的新的修订版本。
这意味着仅当 Deployment 的 Pod 模板（<code>.spec.template</code>）发生更改时，才会创建新修订版本
-- 例如，模板的标签或容器镜像发生变化。
其他更新，如 Deployment 的扩缩容操作不会创建 Deployment 修订版本。
这是为了方便同时执行手动缩放或自动缩放。
换言之，当你回滚到较早的修订版本时，只有 Deployment 的 Pod 模板部分会被回滚。
</div>
<!--
* Suppose that you made a typo while updating the Deployment, by putting the image name as `nginx:1.161` instead of `nginx:1.16.1`:
-->
<ul>
<li>
<p>假设你在更新 Deployment 时犯了一个拼写错误，将镜像名称命名设置为
<code>nginx:1.161</code> 而不是 <code>nginx:1.16.1</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.161 --record<span style="color:#666">=</span><span style="color:#a2f">true</span>
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>deployment/nginx-deployment image updated
</code></pre></li>
</ul>
<!--
* The rollout gets stuck. You can verify it by checking the rollout status:
-->
<ul>
<li>
<p>此上线进程会出现停滞。你可以通过检查上线状态来验证：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status deployment/nginx-deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
</code></pre></li>
</ul>
<!--
* Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,
[read more here](#deployment-status).
-->
<ul>
<li>按 Ctrl-C 停止上述上线状态观测。有关上线停滞的详细信息，<a href="#deployment-status">参考这里</a>。</li>
</ul>
<!--
 * You see that the number of old replicas (`nginx-deployment-1564180365` and `nginx-deployment-2035384211`) is 2, and new replicas (nginx-deployment-3066724191) is 1.
-->
<ul>
<li>
<p>你可以看到旧的副本有两个（<code>nginx-deployment-1564180365</code> 和 <code>nginx-deployment-2035384211</code>），
新的副本有 1 个（<code>nginx-deployment-3066724191</code>）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   <span style="color:#666">3</span>         <span style="color:#666">3</span>         <span style="color:#666">3</span>       25s
nginx-deployment-2035384211   <span style="color:#666">0</span>         <span style="color:#666">0</span>         <span style="color:#666">0</span>       36s
nginx-deployment-3066724191   <span style="color:#666">1</span>         <span style="color:#666">1</span>         <span style="color:#666">0</span>       6s
</code></pre></div></li>
</ul>
<!--
* Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.
-->
<ul>
<li>
<p>查看所创建的 Pod，你会注意到新 ReplicaSet 所创建的 1 个 Pod 卡顿在镜像拉取循环中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            <span style="color:#666">0</span>          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            <span style="color:#666">0</span>          25s
nginx-deployment-1564180365-hysrc   1/1       Running            <span style="color:#666">0</span>          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   <span style="color:#666">0</span>          6s
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
  The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (`maxUnavailable` specifically) that you have specified. Kubernetes by default sets the value to 25%.
  -->
<p>Deployment 控制器自动停止有问题的上线过程，并停止对新的 ReplicaSet 扩容。
这行为取决于所指定的 rollingUpdate 参数（具体为 <code>maxUnavailable</code>）。
默认情况下，Kubernetes 将此值设置为 25%。
</div>
</li>
</ul>
<!--
* Get the description of the Deployment:
-->
<ul>
<li>
<p>获取 Deployment 描述信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.91
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubobjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
</code></pre><!--
To fix this, you need to rollback to a previous revision of Deployment that is stable.
-->
<p>要解决此问题，需要回滚到以前稳定的 Deployment 版本。</p>
</li>
</ul>
<!--
### Checking Rollout History of a Deployment

Follow the steps given below to check the rollout history:
-->
<h3 id="检查-deployment-上线历史">检查 Deployment 上线历史</h3>
<p>按照如下步骤检查回滚历史：</p>
<!--
 1. First, check the revisions of this Deployment:
-->
<ol>
<li>
<p>首先，检查 Deployment 修订历史：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout <span style="color:#a2f">history</span> deployment/nginx-deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>deployments &quot;nginx-deployment&quot;
REVISION    CHANGE-CAUSE
1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml
2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161
</code></pre><!--
`CHANGE-CAUSE` is copied from the Deployment annotation `kubernetes.io/change-cause` to its revisions upon creation. You can specify the`CHANGE-CAUSE` message by:
-->
<p><code>CHANGE-CAUSE</code> 的内容是从 Deployment 的 <code>kubernetes.io/change-cause</code> 注解复制过来的。
复制动作发生在修订版本创建时。你可以通过以下方式设置 <code>CHANGE-CAUSE</code> 消息：</p>
<!--
* Annotating the Deployment with `kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.9.1"`
* Manually editing the manifest of the resource.
-->
<ul>
<li>使用 <code>kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=&quot;image updated to 1.16.1&quot;</code>
为 Deployment 添加注解。</li>
<li>手动编辑资源的清单。</li>
</ul>
</li>
</ol>
<!--
2. To see the details of each revision, run:
-->
<ol start="2">
<li>
<p>要查看修订历史的详细信息，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout <span style="color:#a2f">history</span> deployment/nginx-deployment --revision<span style="color:#666">=</span><span style="color:#666">2</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployments &quot;nginx-deployment&quot; revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
  Containers:
   nginx:
    Image:      nginx:1.16.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      &lt;none&gt;
  No volumes.
</code></pre></li>
</ol>
<!--
### Rolling Back to a Previous Revision
Follow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.
-->
<h3 id="rolling-back-to-a-previous-revision">回滚到之前的修订版本  </h3>
<p>按照下面给出的步骤将 Deployment 从当前版本回滚到以前的版本（即版本 2）。</p>
<!--
 1. Now you've decided to undo the current rollout and rollback to the previous revision:
-->
<ol>
<li>
<p>假定现在你已决定撤消当前上线并回滚到以前的修订版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout undo deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment
</code></pre><!--
Alternatively, you can rollback to a specific revision by specifying it with `-to-revision`:
-->
<p>或者，你也可以通过使用 <code>--to-revision</code> 来回滚到特定修订版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout undo deployment/nginx-deployment --to-revision<span style="color:#666">=</span><span style="color:#666">2</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment
</code></pre><!--
For more details about rollout related commands, read [`kubectl rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout).
-->
<p>与回滚相关的指令的更详细信息，请参考
<a href="/docs/reference/generated/kubectl/kubectl-commands#rollout"><code>kubectl rollout</code></a>。</p>
<!--
The Deployment is now rolled back to a previous stable revision. As you can see, a `DeploymentRollback` event
for rolling back to revision 2 is generated from Deployment controller.
-->
<p>现在，Deployment 正在回滚到以前的稳定版本。正如你所看到的，Deployment
控制器生成了回滚到修订版本 2 的 <code>DeploymentRollback</code> 事件。</p>
</li>
</ol>
<!--
 2. Check if the rollback was successful and the Deployment is running as expected, run:
-->
<ol start="2">
<li>
<p>检查回滚是否成功以及 Deployment 是否正在运行，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30m
</code></pre></li>
</ol>
<!--
3. Get the description of the Deployment:
-->
<ol start="3">
<li>
<p>获取 Deployment 描述信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployment nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
                        kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.16.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &quot;nginx-deployment&quot; to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
</code></pre></li>
</ol>
<!--
## Scaling a Deployment

You can scale a Deployment by using the following command:
-->
<h2 id="scaling-a-deployment">缩放 Deployment  </h2>
<p>你可以使用如下指令缩放 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale deployment/nginx-deployment --replicas<span style="color:#666">=</span><span style="color:#666">10</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment scaled
</code></pre><!--
Assuming [horizontal Pod autoscaling](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) is enabled
in your cluster, you can setup an autoscaler for your Deployment and choose the minimum and maximum number of
Pods you want to run based on the CPU utilization of your existing Pods.
-->
<p>假设集群启用了<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">Pod 的水平自动缩放</a>，
你可以为 Deployment 设置自动缩放器，并基于现有 Pod 的 CPU 利用率选择要运行的
Pod 个数下限和上限。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl autoscale deployment/nginx-deployment --min<span style="color:#666">=</span><span style="color:#666">10</span> --max<span style="color:#666">=</span><span style="color:#666">15</span> --cpu-percent<span style="color:#666">=</span><span style="color:#666">80</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment scaled
</code></pre><!--
### Proportional scaling

RollingUpdate Deployments support running multiple versions of an application at the same time. When you
or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress
or paused), the Deployment controller balances the additional replicas in the existing active
ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called *proportional scaling*.
-->
<h3 id="proportional-scaling">比例缩放 </h3>
<p>RollingUpdate 的 Deployment 支持同时运行应用程序的多个版本。
当自动缩放器缩放处于上线进程（仍在进行中或暂停）中的 RollingUpdate Deployment 时，
Deployment 控制器会平衡现有的活跃状态的 ReplicaSets（含 Pods 的 ReplicaSets）中的额外副本，
以降低风险。这称为 <em>比例缩放（Proportional Scaling）</em>。</p>
<!--
For example, you are running a Deployment with 10 replicas, [maxSurge](#max-surge)=3, and [maxUnavailable](#max-unavailable)=2.
-->
<p>例如，你正在运行一个 10 个副本的 Deployment，其
<a href="#max-surge">maxSurge</a>=3，<a href="#max-unavailable">maxUnavailable</a>=2。</p>
<!--
* Ensure that the 10 replicas in your Deployment are running.
-->
<ul>
<li>
<p>确保 Deployment 的这 10 个副本都在运行。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deploy
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s
</code></pre></li>
</ul>
<!--
* You update to a new image which happens to be unresolvable from inside the cluster.
-->
<ul>
<li>
<p>更新 Deployment 使用新镜像，碰巧该镜像无法从集群内部解析。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:sometag
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment image updated
</code></pre></li>
</ul>
<!--
* The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the
`maxUnavailable` requirement that you mentioned above. Check out the rollout status:
-->
<ul>
<li>
<p>镜像更新使用 ReplicaSet <code>nginx-deployment-1989198191</code> 启动新的上线过程，
但由于上面提到的 <code>maxUnavailable</code> 要求，该进程被阻塞了。检查上线状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
</code></pre></li>
</ul>
<!--
* Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas
to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using
proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you
spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the
most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the
ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.
-->
<ul>
<li>然后，出现了新的 Deployment 扩缩请求。自动缩放器将 Deployment 副本增加到 15。
Deployment 控制器需要决定在何处添加 5 个新副本。如果未使用比例缩放，所有 5 个副本
都将添加到新的 ReplicaSet 中。使用比例缩放时，可以将额外的副本分布到所有 ReplicaSet。
较大比例的副本会被添加到拥有最多副本的 ReplicaSet，而较低比例的副本会进入到
副本较少的 ReplicaSet。所有剩下的副本都会添加到副本最多的 ReplicaSet。
具有零副本的 ReplicaSets 不会被扩容。</li>
</ul>
<!--
In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the
new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming
the new replicas become healthy. To confirm this, run:
-->
<p>在上面的示例中，3 个副本被添加到旧 ReplicaSet 中，2 个副本被添加到新 ReplicaSet。
假定新的副本都很健康，上线过程最终应将所有副本迁移到新的 ReplicaSet 中。
要确认这一点，请运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deploy
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
</code></pre><!--
The rollout status confirms how the replicas were added to each ReplicaSet.
-->
<p>上线状态确认了副本是如何被添加到每个 ReplicaSet 的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   <span style="color:#666">7</span>         <span style="color:#666">7</span>         <span style="color:#666">0</span>         7m
nginx-deployment-618515232    <span style="color:#666">11</span>        <span style="color:#666">11</span>        <span style="color:#666">11</span>        7m
</code></pre></div><!--
## Pausing and Resuming a rollout of a Deployment {#pausing-and-resuming-a-deployment}

When you update a Deployment, or plan to, you can pause rollouts
for that Deployment before you trigger one or more updates. When
you're ready to apply those changes, you resume rollouts for the
Deployment. This approach allows you to
apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.
-->
<h2 id="pausing-and-resuming-a-deployment">暂停、恢复 Deployment 的上线过程 </h2>
<p>在你更新一个 Deployment 的时候，或者计划更新它的时候，
你可以在触发一个或多个更新之前暂停 Deployment 的上线过程。
当你准备行应用这些变更时，你可以重新恢复 Deployment 上线过程。
这样做使得你能够在暂停和恢复执行之间应用多个修补程序，而不会触发不必要的上线操作。</p>
<!--
* For example, with a Deployment that was created:

  Get the Deployment details:
-->
<ul>
<li>
<p>例如，对于一个刚刚创建的 Deployment：</p>
<p>获取该 Deployment 信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deploy
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m
</code></pre><!--
Get the rollout status:
-->
<p>获取上线状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
</code></pre></li>
</ul>
<!--
* Pause by running the following command:
-->
<ul>
<li>
<p>使用如下指令暂停上线：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout pause deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">deployment.apps/nginx-deployment paused
</code></pre></div></li>
</ul>
<!--
* Then update the image of the Deployment:
-->
<ul>
<li>
<p>接下来更新 Deployment 镜像：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image deployment/nginx-deployment <span style="color:#b8860b">nginx</span><span style="color:#666">=</span>nginx:1.16.1
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment image updated
</code></pre></li>
</ul>
<!--
 * Notice that no new rollout started:
-->
<ul>
<li>
<p>注意没有新的上线被触发：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout <span style="color:#a2f">history</span> deployment/nginx-deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>deployments &quot;nginx&quot;
REVISION  CHANGE-CAUSE
1   &lt;none&gt;
</code></pre></li>
</ul>
<!--
* Get the rollout status to verify that the existing ReplicaSet has not changed:
-->
<ul>
<li>
<p>获取上线状态验证现有的 ReplicaSet 没有被更改：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   <span style="color:#666">3</span>         <span style="color:#666">3</span>         <span style="color:#666">3</span>         2m
</code></pre></div></li>
</ul>
<!--
* You can make as many updates as you wish, for example, update the resources that will be used:
-->
<ul>
<li>
<p>你可以根据需要执行很多更新操作，例如，可以要使用的资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> resources deployment/nginx-deployment -c<span style="color:#666">=</span>nginx --limits<span style="color:#666">=</span><span style="color:#b8860b">cpu</span><span style="color:#666">=</span>200m,memory<span style="color:#666">=</span>512Mi
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment resource requirements updated
</code></pre><!--
The initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to
the Deployment will not have any effect as long as the Deployment is paused.
-->
<p>暂停 Deployment 上线之前的初始状态将继续发挥作用，但新的更新在 Deployment
上线被暂停期间不会产生任何效果。</p>
</li>
</ul>
<!--
* Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:
-->
<ul>
<li>
<p>最终，恢复 Deployment 上线并观察新的 ReplicaSet 的创建过程，其中包含了所应用的所有更新：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout resume deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于这样：</p>
<pre><code>deployment.apps/nginx-deployment resumed
</code></pre></li>
</ul>
<!--
* Watch the status of the rollout until it's done.
-->
<ul>
<li>
<p>观察上线的状态，直到完成。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs -w
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
</code></pre></li>
</ul>
<!--
* Get the status of the latest rollout:
-->
<ul>
<li>
<p>获取最近上线的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
</code></pre></li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
You cannot rollback a paused Deployment until you resume it.
-->
<p>你不可以回滚处于暂停状态的 Deployment，除非先恢复其执行状态。
</div>
<!--
## Deployment status

A Deployment enters various states during its lifecycle. It can be [progressing](#progressing-deployment) while
rolling out a new ReplicaSet, it can be [complete](#complete-deployment), or it can [fail to progress](#failed-deployment).
-->
<h2 id="deployment-status">Deployment 状态</h2>
<p>Deployment 的生命周期中会有许多状态。上线新的 ReplicaSet 期间可能处于
<a href="#progressing-deployment">Progressing（进行中）</a>，可能是
<a href="#complete-deployment">Complete（已完成）</a>，也可能是
<a href="#failed-deployment">Failed（失败）</a>以至于无法继续进行。</p>
<!--
### Progressing Deployment

Kubernetes marks a Deployment as _progressing_ when one of the following tasks is performed:
-->
<h3 id="progressing-deployment">进行中的 Deployment </h3>
<p>执行下面的任务期间，Kubernetes 标记 Deployment 为 <em>进行中（Progressing）</em>：</p>
<!--
* The Deployment creates a new ReplicaSet.
* The Deployment is scaling up its newest ReplicaSet.
* The Deployment is scaling down its older ReplicaSet(s).
* New Pods become ready or available (ready for at least [MinReadySeconds](#min-ready-seconds)).
-->
<ul>
<li>Deployment 创建新的 ReplicaSet</li>
<li>Deployment 正在为其最新的 ReplicaSet 扩容</li>
<li>Deployment 正在为其旧有的 ReplicaSet(s) 缩容</li>
<li>新的 Pods 已经就绪或者可用（就绪至少持续了 <a href="#min-ready-seconds">MinReadySeconds</a> 秒）。</li>
</ul>
<!--
When the rollout becomes “progressing”, the Deployment controller adds a condition with the following
attributes to the Deployment's `.status.conditions`:
-->
<p>当上线过程进入“Progressing”状态时，Deployment 控制器会向 Deployment 的
<code>.status.conditions</code> 中添加包含下面属性的状况条目：</p>
<ul>
<li><code>type: Progressing</code></li>
<li><code>status: &quot;True&quot;</code></li>
<li><code>reason: NewReplicaSetCreated</code> | <code>reason: FoundNewReplicaSet</code> | <code>reason: ReplicaSetUpdated</code></li>
</ul>
<!--
You can monitor the progress for a Deployment by using `kubectl rollout status`.
-->
<p>你可以使用 <code>kubectl rollout status</code> 监视 Deployment 的进度。</p>
<!--
### Complete Deployment

Kubernetes marks a Deployment as _complete_ when it has the following characteristics:
-->
<h3 id="complete-deployment">完成的 Deployment   </h3>
<p>当 Deployment 具有以下特征时，Kubernetes 将其标记为 <em>完成（Complete）</em>：</p>
<!--
* All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any
updates you've requested have been completed.
* All of the replicas associated with the Deployment are available.
* No old replicas for the Deployment are running.
-->
<ul>
<li>与 Deployment 关联的所有副本都已更新到指定的最新版本，这意味着之前请求的所有更新都已完成。</li>
<li>与 Deployment 关联的所有副本都可用。</li>
<li>未运行 Deployment 的旧副本。</li>
</ul>
<!--
When the rollout becomes “complete”, the Deployment controller sets a condition with the following
attributes to the Deployment's `.status.conditions`:
-->
<p>当上线过程进入“Complete”状态时，Deployment 控制器会向 Deployment 的
<code>.status.conditions</code> 中添加包含下面属性的状况条目：</p>
<ul>
<li><code>type: Progressing</code></li>
<li><code>status: &quot;True&quot;</code></li>
<li><code>reason: NewReplicaSetAvailable</code></li>
</ul>
<!--
This `Progressing` condition will retain a status value of `"True"` until a new rollout
is initiated. The condition holds even when availability of replicas changes (which
does instead affect the `Available` condition).
-->
<p>这一 <code>Progressing</code> 状况的状态值会持续为 <code>&quot;True&quot;</code>，直至新的上线动作被触发。
即使副本的可用状态发生变化（进而影响 <code>Available</code> 状况），<code>Progressing</code> 状况的值也不会变化。</p>
<!--
You can check if a Deployment has completed by using `kubectl rollout status`. If the rollout completed
successfully, `kubectl rollout status` returns a zero exit code.
-->
<p>你可以使用 <code>kubectl rollout status</code> 检查 Deployment 是否已完成。
如果上线成功完成，<code>kubectl rollout status</code> 返回退出代码 0。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status deployment/nginx-deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Waiting <span style="color:#a2f;font-weight:bold">for</span> rollout to finish: <span style="color:#666">2</span> of <span style="color:#666">3</span> updated replicas are available...
deployment <span style="color:#b44">&#34;nginx-deployment&#34;</span> successfully rolled out
</code></pre></div><p>从 <code>kubectl rollout</code> 命令获得的返回状态为 0（成功）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ <span style="color:#a2f">echo</span> <span style="color:#b8860b">$?</span>
</code></pre></div><pre><code>0
</code></pre><!--
### Failed Deployment
-->
<h3 id="failed-deployment">失败的 Deployment  </h3>
<!--
Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur
due to some of the following factors:
-->
<p>你的 Deployment 可能会在尝试部署其最新的 ReplicaSet 受挫，一直处于未完成状态。
造成此情况一些可能因素如下：</p>
<!--
 * Insufficient quota
* Readiness probe failures
* Image pull errors
* Insufficient permissions
* Limit ranges
* Application runtime misconfiguration
-->
<ul>
<li>配额（Quota）不足</li>
<li>就绪探测（Readiness Probe）失败</li>
<li>镜像拉取错误</li>
<li>权限不足</li>
<li>限制范围（Limit Ranges）问题</li>
<li>应用程序运行时的配置错误</li>
</ul>
<!--
One way you can detect this condition is to specify a deadline parameter in your Deployment spec:
([`.spec.progressDeadlineSeconds`](#progress-deadline-seconds)). `.spec.progressDeadlineSeconds` denotes the
number of seconds the Deployment controller waits before indicating (in the Deployment status) that the
Deployment progress has stalled.
-->
<p>检测此状况的一种方法是在 Deployment 规约中指定截止时间参数：
（[<code>.spec.progressDeadlineSeconds</code>]（#progress-deadline-seconds））。
<code>.spec.progressDeadlineSeconds</code> 给出的是一个秒数值，Deployment 控制器在（通过 Deployment 状态）
标示 Deployment 进展停滞之前，需要等待所给的时长。</p>
<!--
The following `kubectl` command sets the spec with `progressDeadlineSeconds` to make the controller report
lack of progress for a Deployment after 10 minutes:
-->
<p>以下 <code>kubectl</code> 命令设置规约中的 <code>progressDeadlineSeconds</code>，从而告知控制器
在 10 分钟后报告 Deployment 没有进展：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch deployment/nginx-deployment -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;progressDeadlineSeconds&#34;:600}}&#39;</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>deployment.apps/nginx-deployment patched
</code></pre><!--
Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following
attributes to the Deployment's `.status.conditions`:
-->
<p>超过截止时间后，Deployment 控制器将添加具有以下属性的 Deployment 状况到
Deployment 的 <code>.status.conditions</code> 中：</p>
<ul>
<li>Type=Progressing</li>
<li>Status=False</li>
<li>Reason=ProgressDeadlineExceeded</li>
</ul>
<!--
This condition can also fail early and is then set to status value of `"False"` due to reasons as `ReplicaSetCreateError`.
Also, the deadline is not taken into account anymore once the Deployment rollout completes.
-->
<p>这一状况也可能会比较早地失败，因而其状态值被设置为 <code>&quot;False&quot;</code>，
其原因为 <code>ReplicaSetCreateError</code>。
一旦 Deployment 上线完成，就不再考虑其期限。</p>
<!--
See the [Kubernetes API conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties) for more information on status conditions.
-->
<p>参考
<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties">Kubernetes API Conventions</a>
获取更多状态状况相关的信息。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
 Kubernetes takes no action on a stalled Deployment other than to report a status condition with
`Reason=ProgressDeadlineExceeded`. Higher level orchestrators can take advantage of it and act accordingly, for
example, rollback the Deployment to its previous version.
-->
<p>除了报告 <code>Reason=ProgressDeadlineExceeded</code> 状态之外，Kubernetes 对已停止的
Deployment 不执行任何操作。更高级别的编排器可以利用这一设计并相应地采取行动。
例如，将 Deployment 回滚到其以前的版本。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline. You can
safely pause a Deployment in the middle of a rollout and resume without triggering the condition for
exceeding the deadline.
-->
<p>如果你暂停了某个 Deployment 上线，Kubernetes 不再根据指定的截止时间检查 Deployment 进展。
你可以在上线过程中间安全地暂停 Deployment 再恢复其执行，这样做不会导致超出最后时限的问题。
</div>
<!--
You may experience transient errors with your Deployments, either due to a low timeout that you have set or
due to any other kind of error that can be treated as transient. For example, let's suppose you have
insufficient quota. If you describe the Deployment you will notice the following section:
-->
<p>Deployment 可能会出现瞬时性的错误，可能因为设置的超时时间过短，
也可能因为其他可认为是临时性的问题。例如，假定所遇到的问题是配额不足。
如果描述 Deployment，你将会注意到以下部分：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployment nginx-deployment
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code>&lt;...&gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&lt;...&gt;
</code></pre><!--
If you run `kubectl get deployment nginx-deployment -o yaml`, the Deployment status is similar to this:
-->
<p>如果运行 <code>kubectl get deployment nginx-deployment -o yaml</code>，Deployment 状态输出
将类似于这样：</p>
<pre><code>status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set &quot;nginx-deployment-4262182780&quot; is progressing.
    reason: ReplicaSetUpdated
    status: &quot;True&quot;
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &quot;True&quot;
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: 'Error creating: pods &quot;nginx-deployment-4262182780-&quot; is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2'
    reason: FailedCreate
    status: &quot;True&quot;
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
</code></pre><!--
Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the
reason for the Progressing condition:
-->
<p>最终，一旦超过 Deployment 进度限期，Kubernetes 将更新状态和进度状况的原因：</p>
<pre><code>Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
</code></pre><!--
You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other
controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota
conditions and the Deployment controller then completes the Deployment rollout, you'll see the
Deployment's status update with a successful condition (`Status=True` and `Reason=NewReplicaSetAvailable`).
-->
<p>可以通过缩容 Deployment 或者缩容其他运行状态的控制器，或者直接在命名空间中增加配额
来解决配额不足的问题。如果配额条件满足，Deployment 控制器完成了 Deployment 上线操作，
Deployment 状态会更新为成功状况（<code>Status=True</code> and <code>Reason=NewReplicaSetAvailable</code>）。</p>
<pre><code>Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
</code></pre><!--
`type: Available` with `status: "True"` means that your Deployment has minimum availability. Minimum availability is dictated
by the parameters specified in the deployment strategy. `type: Progressing` with `status: "True"` means that your Deployment
is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum
required new replicas are available (see the Reason of the condition for the particulars - in our case
`reason: NewReplicaSetAvailable` means that the Deployment is complete).
-->
<p><code>type: Available</code> 加上 <code>status: True</code> 意味着 Deployment 具有最低可用性。
最低可用性由 Deployment 策略中的参数指定。
<code>type: Progressing</code> 加上 <code>status: True</code> 表示 Deployment 处于上线过程中，并且正在运行，
或者已成功完成进度，最小所需新副本处于可用。
请参阅对应状况的 Reason 了解相关细节。
在我们的案例中 <code>reason: NewReplicaSetAvailable</code> 表示 Deployment 已完成。</p>
<!--
You can check if a Deployment has failed to progress by using `kubectl rollout status`. `kubectl rollout status`
returns a non-zero exit code if the Deployment has exceeded the progression deadline.
-->
<p>你可以使用 <code>kubectl rollout status</code> 检查 Deployment 是否未能取得进展。
如果 Deployment 已超过进度限期，<code>kubectl rollout status</code> 返回非零退出代码。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status deployment/nginx-deployment
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment &quot;nginx&quot; exceeded its progress deadline
</code></pre><!--
and the exit status from `kubectl rollout` is 1 (indicating an error):
-->
<p><code>kubectl rollout</code> 命令的退出状态为 1（表明发生了错误）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ <span style="color:#a2f">echo</span> <span style="color:#b8860b">$?</span>
</code></pre></div><pre><code>1
</code></pre><!--
### Operating on a failed deployment

All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back
to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment
Pod template.
-->
<h3 id="operating-on-a-failed-deployment">对失败 Deployment 的操作  </h3>
<p>可应用于已完成的 Deployment 的所有操作也适用于失败的 Deployment。
你可以对其执行扩缩容、回滚到以前的修订版本等操作，或者在需要对 Deployment 的
Pod 模板应用多项调整时，将 Deployment 暂停。</p>
<!--
## Clean up Policy

You can set `.spec.revisionHistoryLimit` field in a Deployment to specify how many old ReplicaSets for
this Deployment you want to retain. The rest will be garbage-collected in the background. By default,
it is 10.
-->
<h2 id="clean-up-policy">清理策略  </h2>
<p>你可以在 Deployment 中设置 <code>.spec.revisionHistoryLimit</code> 字段以指定保留此
Deployment 的多少个旧有 ReplicaSet。其余的 ReplicaSet 将在后台被垃圾回收。
默认情况下，此值为 10。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment
thus that Deployment will not be able to roll back.
-->
<p>显式将此字段设置为 0 将导致 Deployment 的所有历史记录被清空，因此 Deployment 将无法回滚。
</div>
<!--
## Canary Deployment

If you want to roll out releases to a subset of users or servers using the Deployment, you
can create multiple Deployments, one for each release, following the canary pattern described in
[managing resources](/docs/concepts/cluster-administration/manage-deployment/#canary-deployments).
-->
<h2 id="canary-deployment">金丝雀部署</h2>
<p>如果要使用 Deployment 向用户子集或服务器子集上线版本，
则可以遵循<a href="/zh/docs/concepts/cluster-administration/manage-deployment/#canary-deployments">资源管理</a>
所描述的金丝雀模式，创建多个 Deployment，每个版本一个。</p>
<!--
## Writing a Deployment Spec

As with all other Kubernetes configs, a Deployment needs `apiVersion`, `kind`, and `metadata` fields.
For general information about working with config files, see
[deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/),
configuring containers, and [using kubectl to manage resources](/docs/concepts/overview/working-with-objects/object-management/) documents.
-->
<h2 id="writing-a-deployment-spec">编写 Deployment 规约      </h2>
<p>同其他 Kubernetes 配置一样， Deployment 需要 <code>apiVersion</code>，<code>kind</code> 和 <code>metadata</code> 字段。
有关配置文件的其他信息，请参考<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">部署 Deployment</a>、
配置容器和<a href="/zh/docs/concepts/overview/working-with-objects/object-management/">使用 kubectl 管理资源</a>等相关文档。</p>
<!--
The name of a Deployment object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A Deployment also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
-->
<p>Deployment 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。
Deployment 还需要 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> 部分</a>。</p>
<!--
### Pod Template

The `.spec.template` and `.spec.selector` are the only required field of the `.spec`.
-->
<h3 id="pod-template">Pod 模板    </h3>
<p><code>.spec</code> 中只有 <code>.spec.template</code> 和 <code>.spec.selector</code> 是必需的字段。</p>
<!--

The `.spec.template` is a [Pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>, except it is nested and does not have an `apiVersion` or `kind`.
-->
<p><code>.spec.template</code> 是一个 <a href="/zh/docs/concepts/workloads/pods/#pod-templates">Pod 模板</a>。
它和 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 的语法规则完全相同。
只是这里它是嵌套的，因此不需要 <code>apiVersion</code> 或 <code>kind</code>。</p>
<!--
In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [selector](#selector)).
-->
<p>除了 Pod 的必填字段外，Deployment 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。
对于标签，请确保不要与其他控制器重叠。请参考<a href="#selector">选择算符</a>。</p>
<!--
Only a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is
allowed, which is the default if not specified.
-->
<p>只有 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>.spec.template.spec.restartPolicy</code></a>
等于 <code>Always</code> 才是被允许的，这也是在没有指定时的默认设置。</p>
<!--
### Replicas

`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.
-->
<h3 id="replicas">副本  </h3>
<p><code>.spec.replicas</code> 是指定所需 Pod 的可选字段。它的默认值是1。</p>
<!--
Should you manually scale a Deployment, example via `kubectl scale deployment
deployment --replicas=X`, and then you update that Deployment based on a manifest
(for example: by running `kubectl apply -f deployment.yaml`),
then applying that manifest overwrites the manual scaling that you previously did.
-->
<p>如果你对某个 Deployment 执行了手动扩缩操作（例如，通过
<code>kubectl scale deployment deployment --replicas=X</code>），
之后基于清单对 Deployment 执行了更新操作（例如通过运行
<code>kubectl apply -f deployment.yaml</code>），那么通过应用清单而完成的更新会覆盖之前手动扩缩所作的变更。</p>
<!--
If a [HorizontalPodAutoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/) (or any
similar API for horizontal scaling) is managing scaling for a Deployment, don't set `.spec.replicas`.
-->
<p>如果一个 <a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale/">HorizontalPodAutoscaler</a>
（或者其他执行水平扩缩操作的类似 API）在管理 Deployment 的扩缩，
则不要设置 <code>.spec.replicas</code>。</p>
<!--
Instead, allow the Kubernetes
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> to manage the
`.spec.replicas` field automatically.
-->
<p>恰恰相反，应该允许 Kubernetes
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a>来自动管理
<code>.spec.replicas</code> 字段。</p>
<!--
### Selector

`.spec.selector` is an required field that specifies a [label selector](/docs/concepts/overview/working-with-objects/labels/)
for the Pods targeted by this Deployment.

`.spec.selector` must match `.spec.template.metadata.labels`, or it will be rejected by the API.
-->
<h3 id="selector">选择算符  </h3>
<p><code>.spec.selector</code> 是指定本 Deployment 的 Pod
<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签选择算符</a>的必需字段。</p>
<p><code>.spec.selector</code> 必须匹配 <code>.spec.template.metadata.labels</code>，否则请求会被 API 拒绝。</p>
<!--
In API version `apps/v1`, `.spec.selector` and `.metadata.labels` do not default to `.spec.template.metadata.labels` if not set. So they must be set explicitly. Also note that `.spec.selector` is immutable after creation of the Deployment in `apps/v1`.
-->
<p>在 API <code>apps/v1</code>版本中，<code>.spec.selector</code> 和 <code>.metadata.labels</code> 如果没有设置的话，
不会被默认设置为 <code>.spec.template.metadata.labels</code>，所以需要明确进行设置。
同时在 <code>apps/v1</code>版本中，Deployment 创建后 <code>.spec.selector</code> 是不可变的。</p>
<!--
A Deployment may terminate Pods whose labels match the selector if their template is different
from `.spec.template` or if the total number of such Pods exceeds `.spec.replicas`. It brings up new
Pods with `.spec.template` if the number of Pods is less than the desired number.
-->
<p>当 Pod 的标签和选择算符匹配，但其模板和 <code>.spec.template</code> 不同时，或者此类 Pod
的总数超过 <code>.spec.replicas</code> 的设置时，Deployment 会终结之。
如果 Pods 总数未达到期望值，Deployment 会基于 <code>.spec.template</code> 创建新的 Pod。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
You should not create other Pods whose labels match this selector, either directly, by creating
another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you
do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.
-->
<p>你不应直接创建与此选择算符匹配的 Pod，也不应通过创建另一个 Deployment 或者类似于
ReplicaSet 或 ReplicationController 这类控制器来创建标签与此选择算符匹配的 Pod。
如果这样做，第一个 Deployment 会认为它创建了这些 Pod。
Kubernetes 不会阻止你这么做。
</div>
<!--
If you have multiple controllers that have overlapping selectors, the controllers will fight with each
other and won't behave correctly.
-->
<p>如果有多个控制器的选择算符发生重叠，则控制器之间会因冲突而无法正常工作。</p>
<!--
### Strategy

`.spec.strategy` specifies the strategy used to replace old Pods by new ones.
`.spec.strategy.type` can be "Recreate" or "RollingUpdate". "RollingUpdate" is
the default value.
-->
<h3 id="strategy">策略  </h3>
<p><code>.spec.strategy</code> 策略指定用于用新 Pods 替换旧 Pods 的策略。
<code>.spec.strategy.type</code> 可以是 “Recreate” 或 “RollingUpdate”。“RollingUpdate” 是默认值。</p>
<!--
#### Recreate Deployment

All existing Pods are killed before new ones are created when `.spec.strategy.type==Recreate`.
-->
<h4 id="recreate-deployment">重新创建 Deployment  </h4>
<p>如果 <code>.spec.strategy.type==Recreate</code>，在创建新 Pods 之前，所有现有的 Pods 会被杀死。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods 
of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new 
revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the 
replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an 
"at most" guarantee for your Pods, you should consider using a 
[StatefulSet](/docs/concepts/workloads/controllers/statefulset/).
-->
<p>这只会确保为了升级而创建新 Pod 之前其他 Pod 都已终止。如果你升级一个 Deployment，
所有旧版本的 Pod 都会立即被终止。控制器等待这些 Pod 被成功移除之后，
才会创建新版本的 Pod。如果你手动删除一个 Pod，其生命周期是由 ReplicaSet 来控制的，
后者会立即创建一个替换 Pod（即使旧的 Pod 仍然处于 Terminating 状态）。
如果你需要一种“最多 n 个”的 Pod 个数保证，你需要考虑使用
<a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>。
</div>
<!--
#### Rolling Update Deployment

The Deployment updates Pods in a rolling update
fashion when `.spec.strategy.type==RollingUpdate`. You can specify `maxUnavailable` and `maxSurge` to control
the rolling update process.
-->
<h4 id="rolling-update-deployment">滚动更新 Deployment  </h4>
<p>Deployment 会在 <code>.spec.strategy.type==RollingUpdate</code>时，采取
滚动更新的方式更新 Pods。你可以指定 <code>maxUnavailable</code> 和 <code>maxSurge</code> 来控制滚动更新
过程。</p>
<!--
##### Max Unavailable
-->
<h5 id="max-unavailable">最大不可用  </h5>
<!--
`.spec.strategy.rollingUpdate.maxUnavailable` is an optional field that specifies the maximum number
of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)
or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by
rounding down. The value cannot be 0 if `.spec.strategy.rollingUpdate.maxSurge` is 0. The default value is 25%.
-->
<p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> 是一个可选字段，用来指定
更新过程中不可用的 Pod 的个数上限。该值可以是绝对数字（例如，5），也可以是所需
Pods 的百分比（例如，10%）。百分比值会转换成绝对数并去除小数部分。
如果 <code>.spec.strategy.rollingUpdate.maxSurge</code> 为 0，则此值不能为 0。
默认值为 25%。</p>
<!--
For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired
Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled
down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available
at all times during the update is at least 70% of the desired Pods.
-->
<p>例如，当此值设置为 30% 时，滚动更新开始时会立即将旧 ReplicaSet 缩容到期望 Pod 个数的70%。
新 Pod 准备就绪后，可以继续缩容旧有的 ReplicaSet，然后对新的 ReplicaSet 扩容，
确保在更新期间可用的 Pods 总数在任何时候都至少为所需的 Pod 个数的 70%。</p>
<!--
##### Max Surge

 `.spec.strategy.rollingUpdate.maxSurge` is an optional field that specifies the maximum number of Pods
that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a
percentage of desired Pods (for example, 10%). The value cannot be 0 if `MaxUnavailable` is 0. The absolute number
is calculated from the percentage by rounding up. The default value is 25%.
-->
<h5 id="max-surge">最大峰值  </h5>
<p><code>.spec.strategy.rollingUpdate.maxSurge</code> 是一个可选字段，用来指定可以创建的超出期望
Pod 个数的 Pod 数量。此值可以是绝对数（例如，5）或所需 Pods 的百分比（例如，10%）。
如果 <code>MaxUnavailable</code> 为 0，则此值不能为 0。百分比值会通过向上取整转换为绝对数。
此字段的默认值为 25%。</p>
<!--
For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the
rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired
Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the
total number of Pods running at any time during the update is at most 130% of desired Pods.
-->
<p>例如，当此值为 30% 时，启动滚动更新后，会立即对新的 ReplicaSet 扩容，同时保证新旧 Pod
的总数不超过所需 Pod 总数的 130%。一旦旧 Pods 被杀死，新的 ReplicaSet 可以进一步扩容，
同时确保更新期间的任何时候运行中的 Pods 总数最多为所需 Pods 总数的 130%。</p>
<!--
 ### Progress Deadline Seconds

 `.spec.progressDeadlineSeconds` is an optional field that specifies the number of seconds you want
to wait for your Deployment to progress before the system reports back that the Deployment has
[failed progressing](#failed-deployment) - surfaced as a condition with `type: Progressing`, `status: False`.
and `reason: ProgressDeadlineExceeded` in the status of the resource. The Deployment controller will keep
retrying the Deployment. In the future, once automatic rollback will be implemented, the Deployment
controller will roll back a Deployment as soon as it observes such a condition.
-->
<h3 id="progress-deadline-seconds">进度期限秒数   </h3>
<p><code>.spec.progressDeadlineSeconds</code> 是一个可选字段，用于指定系统在报告 Deployment
<a href="#failed-deployment">进展失败</a> 之前等待 Deployment 取得进展的秒数。
这类报告会在资源状态中体现为 <code>type: Progressing</code>、<code>status: False</code>、
<code>reason: ProgressDeadlineExceeded</code>。Deployment 控制器将持续重试 Deployment。
将来，一旦实现了自动回滚，Deployment 控制器将在探测到这样的条件时立即回滚 Deployment。</p>
<!--
If specified, this field needs to be greater than `.spec.minReadySeconds`.
-->
<p>如果指定，则此字段值需要大于 <code>.spec.minReadySeconds</code> 取值。</p>
<!--
### Min Ready Seconds

 `.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be ready without any of its containers crashing, for it to be considered available.
This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).
-->
<h3 id="min-ready-seconds">最短就绪时间   </h3>
<p><code>.spec.minReadySeconds</code> 是一个可选字段，用于指定新创建的 Pod
在没有任意容器崩溃情况下的最小就绪时间，
只有超出这个时间 Pod 才被视为可用。默认值为 0（Pod 在准备就绪后立即将被视为可用）。
要了解何时 Pod 被视为就绪，
可参考<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">容器探针</a>。</p>
<!--
### Revision History Limit

A Deployment's revision history is stored in the ReplicaSets it controls.
-->
<h3 id="修订历史限制">修订历史限制</h3>
<p>Deployment 的修订历史记录存储在它所控制的 ReplicaSets 中。</p>
<!--
`.spec.revisionHistoryLimit` is an optional field that specifies the number of old ReplicaSets to retain
to allow rollback. These old ReplicaSets consume resources in `etcd` and crowd the output of `kubectl get rs`. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.
-->
<p><code>.spec.revisionHistoryLimit</code> 是一个可选字段，用来设定出于会滚目的所要保留的旧 ReplicaSet 数量。
这些旧 ReplicaSet 会消耗 etcd 中的资源，并占用 <code>kubectl get rs</code> 的输出。
每个 Deployment 修订版本的配置都存储在其 ReplicaSets 中；因此，一旦删除了旧的 ReplicaSet，
将失去回滚到 Deployment 的对应修订版本的能力。
默认情况下，系统保留 10 个旧 ReplicaSet，但其理想值取决于新 Deployment 的频率和稳定性。</p>
<!--
More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.
In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.
-->
<p>更具体地说，将此字段设置为 0 意味着将清理所有具有 0 个副本的旧 ReplicaSet。
在这种情况下，无法撤消新的 Deployment 上线，因为它的修订历史被清除了。</p>
<!--
### Paused

`.spec.paused` is an optional boolean field for pausing and resuming a Deployment. The only difference between
a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused
Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when
it is created.
-->
<h3 id="paused">paused（暂停的） </h3>
<p><code>.spec.paused</code> 是用于暂停和恢复 Deployment 的可选布尔字段。
暂停的 Deployment 和未暂停的 Deployment 的唯一区别是，Deployment 处于暂停状态时，
PodTemplateSpec 的任何修改都不会触发新的上线。
Deployment 在创建时是默认不会处于暂停状态。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods).
* [Run a Stateless Application Using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/).
* `Deployment` is a top-level resource in the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for deployments.
* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how
  you can use it to manage application availability during disruptions.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pod</a>。</li>
<li><a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">使用 Deployment 运行一个无状态应用</a>。</li>
<li><code>Deployment</code> 是 Kubernetes REST API 中的一个顶层资源。
阅读 





<a href=""></a>
对象定义，以了解 Deployment 的 API 细节。</li>
<li>阅读 <a href="/zh/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>
了解如何使用它来在可能出现干扰的情况下管理应用的可用性。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d459b930218774655fa7fd1620625539">4.2.2 - ReplicaSet</h1>
    
	<!--
reviewers:
- Kashomon
- bprashanth
- madhusudancs
title: ReplicaSet
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often
used to guarantee the availability of a specified number of identical Pods.
-->
<p>ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。
因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。</p>
<!-- body -->
<!--
## How a ReplicaSet works

A ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number
of replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods
it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating
and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod
template.
-->
<h2 id="how-a-replicaset-works">ReplicaSet 的工作原理</h2>
<p>ReplicaSet 是通过一组字段来定义的，包括一个用来识别可获得的 Pod
的集合的选择算符、一个用来标明应该维护的副本个数的数值、一个用来指定应该创建新 Pod
以满足副本个数条件时要使用的 Pod 模板等等。
每个 ReplicaSet 都通过根据需要创建和 删除 Pod 以使得副本个数达到期望值，
进而实现其存在价值。当 ReplicaSet 需要创建新的 Pod 时，会使用所提供的 Pod 模板。</p>
<!--
A ReplicaSet is linked to its Pods via the Pods' [metadata.ownerReferences](/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents)
field, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning
ReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet
knows of the state of the Pods it is maintaining and plans accordingly.
-->
<p>ReplicaSet 通过 Pod 上的
<a href="/zh/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents">metadata.ownerReferences</a>
字段连接到附属 Pod，该字段给出当前对象的属主资源。
ReplicaSet 所获得的 Pod 都在其 ownerReferences 字段中包含了属主 ReplicaSet
的标识信息。正是通过这一连接，ReplicaSet 知道它所维护的 Pod 集合的状态，
并据此计划其操作行为。</p>
<!--
A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no OwnerReference or the
OwnerReference is not a <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> and it matches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.
-->
<p>ReplicaSet 使用其选择算符来辨识要获得的 Pod 集合。如果某个 Pod 没有
OwnerReference 或者其 OwnerReference 不是一个
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>，且其匹配到
某 ReplicaSet 的选择算符，则该 Pod 立即被此 ReplicaSet 获得。</p>
<!--
## When to use a ReplicaSet

A ReplicaSet ensures that a specified number of pod replicas are running at any given
time. However, a Deployment is a higher-level concept that manages ReplicaSets and
provides declarative updates to pods along with a lot of other useful features.
Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless
you require custom update orchestration or don't require updates at all.

This actually means that you may never need to manipulate ReplicaSet objects:
use a Deployment instead, and define your application in the spec section.
-->
<h2 id="何时使用-replicaset">何时使用 ReplicaSet</h2>
<p>ReplicaSet 确保任何时间都有指定数量的 Pod 副本在运行。
然而，Deployment 是一个更高级的概念，它管理 ReplicaSet，并向 Pod
提供声明式的更新以及许多其他有用的功能。
因此，我们建议使用 Deployment 而不是直接使用 ReplicaSet，除非
你需要自定义更新业务流程或根本不需要更新。</p>
<p>这实际上意味着，你可能永远不需要操作 ReplicaSet 对象：而是使用
Deployment，并在 spec 部分定义你的应用。</p>
<!--
## Example
-->
<h2 id="示例">示例</h2>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/frontend.yaml" download="controllers/frontend.yaml"><code>controllers/frontend.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-frontend-yaml')" title="Copy controllers/frontend.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-frontend-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># modify replicas according to your case</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-redis<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google_samples/gb-frontend:v3<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Saving this manifest into `frontend.yaml` and submitting it to a Kubernetes cluster should
create the defined ReplicaSet and the pods that it manages.
-->
<p>将此清单保存到 <code>frontend.yaml</code> 中，并将其提交到 Kubernetes 集群，
应该就能创建 yaml 文件所定义的 ReplicaSet 及其管理的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</code></pre></div><!--
You can then get the current ReplicaSets deployed:
-->
<p>你可以看到当前被部署的 ReplicaSet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs
</code></pre></div><!--
And see the frontend one you created:
-->
<p>并看到你所创建的前端：</p>
<pre><code>NAME       DESIRED   CURRENT   READY   AGE
frontend   3         3         3       6s
</code></pre><!--
You can also check on the state of the ReplicaSet:
-->
<p>你也可以查看 ReplicaSet 的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe rs/frontend
</code></pre></div><!--
And you will see output similar to:
-->
<p>你会看到类似如下的输出：</p>
<pre><code>Name:		frontend
Namespace:	default
Selector:	tier=frontend
Labels:		app=guestbook
		tier=frontend
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;ReplicaSet&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;labels&quot;:{&quot;app&quot;:&quot;guestbook&quot;,&quot;tier&quot;:&quot;frontend&quot;},&quot;name&quot;:&quot;frontend&quot;,...
Replicas:	3 current / 3 desired
Pods Status:	3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       tier=frontend
  Containers:
   php-redis:
    Image:      gcr.io/google_samples/gb-frontend:v3
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts
</code></pre><!--
And lastly you can check for the Pods brought up:
-->
<p>最后可以查看启动了的 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
You should see Pod information similar to:
-->
<p>你会看到类似如下的 Pod 信息：</p>
<pre><code>NAME             READY   STATUS    RESTARTS   AGE
frontend-b2zdv   1/1     Running   0          6m36s
frontend-vcmts   1/1     Running   0          6m36s
frontend-wtsmm   1/1     Running   0          6m36s
</code></pre><!--
You can also verify that the owner reference of these pods is set to the frontend ReplicaSet.
To do this, get the yaml of one of the Pods running:
-->
<p>你也可以查看 Pods 的属主引用被设置为前端的 ReplicaSet。
要实现这点，可取回运行中的 Pods 之一的 YAML：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods frontend-b2zdv -o yaml
</code></pre></div><!--
The output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:
-->
<p>输出将类似这样，frontend ReplicaSet 的信息被设置在 metadata 的
<code>ownerReferences</code> 字段中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2020-02-12T07:06:16Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">generateName</span>:<span style="color:#bbb"> </span>frontend-<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend-b2zdv<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ownerReferences</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">blockOwnerDeletion</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">controller</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
## Non-Template Pod acquisitions

While you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have
labels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited
to owning Pods specified by its template - it can acquire other Pods in the manner specified in the previous sections.
-->
<h2 id="非模板-pod-的获得">非模板 Pod 的获得</h2>
<!--
While you can create bare Pods with no problems, it is strongly recommended to
make sure that the bare Pods do not have labels which match the selector of
one of your ReplicaSets. The reason for this is because a ReplicaSet is not
limited to owning Pods specified by its template - it can acquire other Pods
in the manner specified in the previous sections.

Take the previous frontend ReplicaSet example, and the Pods specified in the
following manifest:
-->
<p>尽管你完全可以直接创建裸的 Pods，强烈建议你确保这些裸的 Pods 并不包含可能与你
的某个 ReplicaSet 的选择算符相匹配的标签。原因在于 ReplicaSet 并不仅限于拥有
在其模板中设置的 Pods，它还可以像前面小节中所描述的那样获得其他 Pods。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-rs.yaml" download="pods/pod-rs.yaml"><code>pods/pod-rs.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-rs-yaml')" title="Copy pods/pod-rs.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-rs-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/hello-app:2.0<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/hello-app:1.0<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend
ReplicaSet, they will immediately be acquired by it.

Suppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to
fulfill its replica count requirement:
-->
<p>由于这些 Pod 没有控制器（Controller，或其他对象）作为其属主引用，并且
其标签与 frontend ReplicaSet 的选择算符匹配，它们会立即被该 ReplicaSet
获取。</p>
<p>假定你在 frontend ReplicaSet 已经被部署之后创建 Pods，并且你已经在 ReplicaSet
中设置了其初始的 Pod 副本数以满足其副本计数需要：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</code></pre></div><!--
The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over
its desired count.

Fetching the Pods:
-->
<p>新的 Pods 会被该 ReplicaSet 获取，并立即被 ReplicaSet 终止，因为
它们的存在会使得 ReplicaSet 中 Pod 个数超出其期望值。</p>
<p>取回 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The output shows that the new Pods are either already terminated, or in the process of being terminated:
-->
<p>输出显示新的 Pods 或者已经被终止，或者处于终止过程中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME             READY   STATUS        RESTARTS   AGE
frontend-b2zdv   1/1     Running       <span style="color:#666">0</span>          10m
frontend-vcmts   1/1     Running       <span style="color:#666">0</span>          10m
frontend-wtsmm   1/1     Running       <span style="color:#666">0</span>          10m
pod1             0/1     Terminating   <span style="color:#666">0</span>          1s
pod2             0/1     Terminating   <span style="color:#666">0</span>          1s
</code></pre></div><!--
If you create the Pods first:
-->
<p>如果你先行创建 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</code></pre></div><!--
And then create the ReplicaSet however:
-->
<p>之后再创建 ReplicaSet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</code></pre></div><!--
You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the
number of its new Pods and the original matches its desired count. As fetching the Pods:
-->
<p>你会看到 ReplicaSet 已经获得了该 Pods，并仅根据其规约创建新的 Pods，直到
新的 Pods 和原来的 Pods 的总数达到其预期个数。
这时取回 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
Will reveal in its output:
-->
<p>将会生成下面的输出：</p>
<pre><code>NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   0          9s
pod1             1/1     Running   0          36s
pod2             1/1     Running   0          36s
</code></pre><p>采用这种方式，一个 ReplicaSet 中可以包含异质的 Pods 集合。</p>
<!--
## Writing a ReplicaSet Spec

As with all other Kubernetes API objects, a ReplicaSet needs the `apiVersion`, `kind`, and `metadata` fields.
For ReplicaSets, the `kind` is always a ReplicaSet.

The name of a ReplicaSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A ReplicaSet also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status).
-->
<h2 id="编写-replicaset-的-spec">编写 ReplicaSet 的 spec</h2>
<p>与所有其他 Kubernetes API 对象一样，ReplicaSet 也需要 <code>apiVersion</code>、<code>kind</code>、和 <code>metadata</code> 字段。
对于 ReplicaSets 而言，其 <code>kind</code> 始终是 ReplicaSet。</p>
<p>ReplicaSet 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<p>ReplicaSet 也需要 <a href="https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status"><code>.spec</code></a>
部分。</p>
<!--
### Pod Template

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates) which is also
required to have labels in place. In our `frontend.yaml` example we had one label: `tier: frontend`.
Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.

For the template's [restart policy](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) field,
`.spec.template.spec.restartPolicy`, the only allowed value is `Always`, which is the default.
-->
<h3 id="pod-模版">Pod 模版</h3>
<p><code>.spec.template</code> 是一个<a href="/zh/docs/concepts/workloads/pods/#pod-templates">Pod 模版</a>，
要求设置标签。在 <code>frontend.yaml</code> 示例中，我们指定了标签 <code>tier: frontend</code>。
注意不要将标签与其他控制器的选择算符重叠，否则那些控制器会尝试收养此 Pod。</p>
<p>对于模板的<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">重启策略</a>
字段，<code>.spec.template.spec.restartPolicy</code>，唯一允许的取值是 <code>Always</code>，这也是默认值.</p>
<!--
### Pod Selector

The `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/). As discussed
[earlier](#how-a-replicaset-works) these are the labels used to identify potential Pods to acquire. In our
`frontend.yaml` example, the selector was:

```yaml
matchLabels:
  tier: frontend
```

In the ReplicaSet, `.spec.template.metadata.labels` must match `spec.selector`, or it will
be rejected by the API.
-->
<h3 id="pod-selector">Pod 选择算符  </h3>
<p><code>.spec.selector</code> 字段是一个<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签选择算符</a>。
如前文中<a href="#how-a-replicaset-works">所讨论的</a>，这些是用来标识要被获取的 Pods
的标签。在签名的 <code>frontend.yaml</code> 示例中，选择算符为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></code></pre></div><p>在 ReplicaSet 中，<code>.spec.template.metadata.labels</code> 的值必须与 <code>spec.selector</code> 值
相匹配，否则该配置会被 API 拒绝。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
For 2 ReplicaSets specifying the same `.spec.selector` but different `.spec.template.metadata.labels` and `.spec.template.spec` fields, each ReplicaSet ignores the Pods created by the other ReplicaSet.
-->
<p>对于设置了相同的 <code>.spec.selector</code>，但
<code>.spec.template.metadata.labels</code> 和 <code>.spec.template.spec</code> 字段不同的
两个 ReplicaSet 而言，每个 ReplicaSet 都会忽略被另一个 ReplicaSet 所
创建的 Pods。
</div>
<!--
### Replicas

You can specify how many Pods should run concurrently by setting `.spec.replicas`. The ReplicaSet will create/delete
its Pods to match this number.

If you do not specify `.spec.replicas`, then it defaults to 1.
-->
<h3 id="replicas">Replicas</h3>
<p>你可以通过设置 <code>.spec.replicas</code> 来指定要同时运行的 Pod 个数。
ReplicaSet 创建、删除 Pods 以与此值匹配。</p>
<p>如果你没有指定 <code>.spec.replicas</code>, 那么默认值为 1。</p>
<!--
## Working with ReplicaSets

### Deleting a ReplicaSet and its Pods

To delete a ReplicaSet and all of its Pods, use [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete). The [Garbage collector](/docs/concepts/workloads/controllers/garbage-collection/) automatically deletes all of the dependent Pods by default.

When using the REST API or the `client-go` library, you must set `propagationPolicy` to `Background` or `Foreground` in delete option. e.g. :
-->
<h2 id="使用-replicasets">使用 ReplicaSets</h2>
<h3 id="删除-replicaset-和它的-pod">删除 ReplicaSet 和它的 Pod</h3>
<p>要删除 ReplicaSet 和它的所有 Pod，使用
<a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a> 命令。
默认情况下，<a href="/zh/docs/concepts/workloads/controllers/garbage-collection/">垃圾收集器</a>
自动删除所有依赖的 Pod。</p>
<p>当使用 REST API 或 <code>client-go</code> 库时，你必须在删除选项中将 <code>propagationPolicy</code>
设置为 <code>Background</code> 或 <code>Foreground</code>。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
curl -X DELETE  <span style="color:#b44">&#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   -d <span style="color:#b44">&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   -H <span style="color:#b44">&#34;Content-Type: application/json&#34;</span>
</code></pre></div><!--
### Deleting just a ReplicaSet

You can delete a ReplicaSet without affecting any of its Pods using [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete) with the `-cascade=orphan` option.
When using the REST API or the `client-go` library, you must set `propagationPolicy` to `Orphan`.
For example:
-->
<h3 id="只删除-replicaset">只删除 ReplicaSet</h3>
<p>你可以只删除 ReplicaSet 而不影响它的 Pods，方法是使用
<a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>
命令并设置 <code>--cascade=orphan</code> 选项。</p>
<p>当使用 REST API 或 <code>client-go</code> 库时，你必须将 <code>propagationPolicy</code> 设置为 <code>Orphan</code>。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span>
curl -X DELETE  <span style="color:#b44">&#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -d <span style="color:#b44">&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -H <span style="color:#b44">&#34;Content-Type: application/json&#34;</span>
</code></pre></div><!--
Once the original is deleted, you can create a new ReplicaSet to replace it.  As long
as the old and new `.spec.selector` are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update Pods to a new spec in a controlled way, use a
[Deployment](/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), as ReplicaSets do not support a rolling update directly.
-->
<p>一旦删除了原来的 ReplicaSet，就可以创建一个新的来替换它。
由于新旧 ReplicaSet 的 <code>.spec.selector</code> 是相同的，新的 ReplicaSet 将接管老的 Pod。
但是，它不会努力使现有的 Pod 与新的、不同的 Pod 模板匹配。
若想要以可控的方式更新 Pod 的规约，可以使用
<a href="/zh/docs/concepts/workloads/controllers/deployment/#creating-a-deployment">Deployment</a>
资源，因为 ReplicaSet 并不直接支持滚动更新。</p>
<!--
### Isolating pods from a ReplicaSet

Pods may be removed from a ReplicaSet's target set by changing their labels. This technique may be used to remove pods 
from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (
  assuming that the number of replicas is not also changed).

-->
<h3 id="将-pod-从-replicaset-中隔离">将 Pod 从 ReplicaSet 中隔离</h3>
<p>可以通过改变标签来从 ReplicaSet 的目标集中移除 Pod。
这种技术可以用来从服务中去除 Pod，以便进行排错、数据恢复等。
以这种方式移除的 Pod 将被自动替换（假设副本的数量没有改变）。</p>
<!--
### Scaling a ReplicaSet

A ReplicaSet can be easily scaled up or down by simply updating the `.spec.replicas` field. The ReplicaSet controller
ensures that a desired number of pods with a matching label selector are available and operational.
-->
<h3 id="缩放-repliaset">缩放 RepliaSet</h3>
<p>通过更新 <code>.spec.replicas</code> 字段，ReplicaSet 可以被轻松的进行缩放。ReplicaSet
控制器能确保匹配标签选择器的数量的 Pod 是可用的和可操作的。</p>
<!--
When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to
prioritize scaling down pods based on the following general algorithm:
-->
<p>在降低集合规模时，ReplicaSet 控制器通过对可用的 Pods 进行排序来优先选择
要被删除的 Pods。其一般性算法如下：</p>
<!--
 1. Pending (and unschedulable) pods are scaled down first
 2. If `controller.kubernetes.io/pod-deletion-cost` annotation is set, then
    the pod with the lower value will come first.
 3. Pods on nodes with more replicas come before pods on nodes with fewer replicas.
 4. If the pods' creation times differ, the pod that was created more recently
    comes before the older pod (the creation times are bucketed on an integer log scale
    when the `LogarithmicScaleDown` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled)
-->
<ol>
<li>首先选择剔除悬决（Pending，且不可调度）的 Pods</li>
<li>如果设置了 <code>controller.kubernetes.io/pod-deletion-cost</code> 注解，则注解值
较小的优先被裁减掉</li>
<li>所处节点上副本个数较多的 Pod 优先于所处节点上副本较少者</li>
<li>如果 Pod 的创建时间不同，最近创建的 Pod 优先于早前创建的 Pod 被裁减。
（当 <code>LogarithmicScaleDown</code> 这一
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
被启用时，创建时间是按整数幂级来分组的）。</li>
</ol>
<p>如果以上比较结果都相同，则随机选择。</p>
<!--
### Pod deletion cost 
-->
<h3 id="pod-deletion-cost">Pod 删除开销  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
Using the [`controller.kubernetes.io/pod-deletion-cost`](/docs/reference/labels-annotations-taints/#pod-deletion-cost) 
annotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.
-->
<p>通过使用 <a href="/zh/docs/reference/labels-annotations-taints/#pod-deletion-cost"><code>controller.kubernetes.io/pod-deletion-cost</code></a>
注解，用户可以对 ReplicaSet 缩容时要先删除哪些 Pods 设置偏好。</p>
<!--
The annotation should be set on the pod, the range is [-2147483647, 2147483647]. It represents the cost of
deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion
cost are preferred to be deleted before pods with higher deletion cost. 
-->
<p>此注解要设置到 Pod 上，取值范围为 [-2147483647, 2147483647]。
所代表的的是删除同一 ReplicaSet 中其他 Pod 相比较而言的开销。
删除开销较小的 Pods 比删除开销较高的 Pods 更容易被删除。</p>
<!--
The implicit value for this annotation for pods that don't set it is 0; negative values are permitted.
Invalid values will be rejected by the API server.
-->
<p>Pods 如果未设置此注解，则隐含的设置值为 0。负值也是可接受的。
如果注解值非法，API 服务器会拒绝对应的 Pod。</p>
<!--
This feature is beta and enabled by default. You can disable it using the
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`PodDeletionCost` in both kube-apiserver and kube-controller-manager.
-->
<p>此功能特性处于 Beta 阶段，默认被禁用。你可以通过为 kube-apiserver 和
kube-controller-manager 设置
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>PodDeletionCost</code> 来启用此功能。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
- This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.
- Users should avoid updating the annotation frequently, such as updating it based on a metric value,
  because doing so will generate a significant number of pod updates on the apiserver.
-->
<ul>
<li>此机制实施时仅是尽力而为，并不能对 Pod 的删除顺序作出任何保证；</li>
<li>用户应避免频繁更新注解值，例如根据某观测度量值来更新此注解值是应该避免的。
这样做会在 API 服务器上产生大量的 Pod 更新操作。</li>
</ul>

</div>
<!--
#### Example Use Case

The different pods of an application could have different utilization levels. On scale down, the application 
may prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application
should update `controller.kubernetes.io/pod-deletion-cost` once before issuing a scale down (setting the 
annotation to a value proportional to pod utilization level). This works if the application itself controls
the down scaling; for example, the driver pod of a Spark deployment.
-->
<h4 id="使用场景示例">使用场景示例</h4>
<p>同一应用的不同 Pods 可能其利用率是不同的。在对应用执行缩容操作时，可能
希望移除利用率较低的 Pods。为了避免频繁更新 Pods，应用应该在执行缩容
操作之前更新一次 <code>controller.kubernetes.io/pod-deletion-cost</code> 注解值
（将注解值设置为一个与其 Pod 利用率对应的值）。
如果应用自身控制器缩容操作时（例如 Spark 部署的驱动 Pod），这种机制
是可以起作用的。</p>
<!--
### ReplicaSet as an Horizontal Pod Autoscaler Target

A ReplicaSet can also be a target for
[Horizontal Pod Autoscalers (HPA)](/docs/tasks/run-application/horizontal-pod-autoscale/). That is,
a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting
the ReplicaSet we created in the previous example.
-->
<h3 id="replicaset-作为水平的-pod-自动缩放器目标">ReplicaSet 作为水平的 Pod 自动缩放器目标</h3>
<p>ReplicaSet 也可以作为
<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale/">水平的 Pod 缩放器 (HPA)</a>
的目标。也就是说，ReplicaSet 可以被 HPA 自动缩放。
以下是 HPA 以我们在前一个示例中创建的副本集为目标的示例。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/hpa-rs.yaml" download="controllers/hpa-rs.yaml"><code>controllers/hpa-rs.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-hpa-rs-yaml')" title="Copy controllers/hpa-rs.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-hpa-rs-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>autoscaling/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>HorizontalPodAutoscaler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend-scaler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleTargetRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ReplicaSet<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">maxReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">targetCPUUtilizationPercentage</span>:<span style="color:#bbb"> </span><span style="color:#666">50</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Saving this manifest into `hpa-rs.yaml` and submitting it to a Kubernetes cluster should
create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage
of the replicated pods.
-->
<p>将这个列表保存到 <code>hpa-rs.yaml</code> 并提交到 Kubernetes 集群，就能创建它所定义的
HPA，进而就能根据复制的 Pod 的 CPU 利用率对目标 ReplicaSet进行自动缩放。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
</code></pre></div><!--
Alternatively, you can use the `kubectl autoscale` command to accomplish the same
(and it's easier!)
-->
<p>或者，可以使用 <code>kubectl autoscale</code> 命令完成相同的操作。 (而且它更简单！)</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl autoscale rs frontend --max<span style="color:#666">=</span><span style="color:#666">10</span> --min<span style="color:#666">=</span><span style="color:#666">3</span> --cpu-percent<span style="color:#666">=</span><span style="color:#666">50</span>
</code></pre></div><!--
## Alternatives to ReplicaSet

### Deployment (Recommended)

[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is an object which can own ReplicaSets and update
them and their Pods via declarative, server-side rolling updates.
While ReplicaSets can be used independently, today they're  mainly used by Deployments as a mechanism to orchestrate Pod
creation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that
they create. Deployments own and manage their ReplicaSets.
As such, it is recommended to use Deployments when you want ReplicaSets.
-->
<h2 id="replicaset-的替代方案">ReplicaSet 的替代方案</h2>
<h3 id="deployment-推荐">Deployment （推荐）</h3>
<p><a href="/zh/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> 是一个
可以拥有 ReplicaSet 并使用声明式方式在服务器端完成对 Pods 滚动更新的对象。
尽管 ReplicaSet 可以独立使用，目前它们的主要用途是提供给 Deployment 作为
编排 Pod 创建、删除和更新的一种机制。当使用 Deployment 时，你不必关心
如何管理它所创建的 ReplicaSet，Deployment 拥有并管理其 ReplicaSet。
因此，建议你在需要 ReplicaSet 时使用 Deployment。</p>
<!--
### Bare Pods

Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your application requires only a single Pod. Think of it similarly to a process supervisor, only it supervises multiple Pods across multiple nodes instead of individual processes on a single node. A ReplicaSet delegates local container restarts to some agent on the node (for example, Kubelet or Docker).
-->
<h3 id="裸-pod">裸 Pod</h3>
<p>与用户直接创建 Pod 的情况不同，ReplicaSet 会替换那些由于某些原因被删除或被终止的
Pod，例如在节点故障或破坏性的节点维护（如内核升级）的情况下。
因为这个原因，我们建议你使用 ReplicaSet，即使应用程序只需要一个 Pod。
想像一下，ReplicaSet 类似于进程监视器，只不过它在多个节点上监视多个 Pod，
而不是在单个节点上监视单个进程。
ReplicaSet 将本地容器重启的任务委托给了节点上的某个代理（例如，Kubelet 或 Docker）去完成。</p>
<!--
### Job

Use a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicaSet for Pods that are expected to terminate on their own
(that is, batch jobs).
-->
<h3 id="job">Job</h3>
<p>使用<a href="/zh/docs/concepts/workloads/controllers/job/"><code>Job</code></a> 代替ReplicaSet，
可以用于那些期望自行终止的 Pod。</p>
<!--
### DaemonSet

Use a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicaSet for pods that provide a
machine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.
-->
<h3 id="daemonset">DaemonSet</h3>
<p>对于管理那些提供主机级别功能（如主机监控和主机日志）的容器，
就要用 <a href="/zh/docs/concepts/workloads/controllers/daemonset/"><code>DaemonSet</code></a>
而不用 ReplicaSet。
这些 Pod 的寿命与主机寿命有关：这些 Pod 需要先于主机上的其他 Pod 运行，
并且在机器准备重新启动/关闭时安全地终止。</p>
<h3 id="replicationcontroller">ReplicationController</h3>
<!--
ReplicaSets are the successors to [_ReplicationControllers_](/docs/concepts/workloads/controllers/replicationcontroller/).
The two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based
selector requirements as described in the [labels user guide](/docs/concepts/overview/working-with-objects/labels/#label-selectors).
As such, ReplicaSets are preferred over ReplicationControllers
-->
<p>ReplicaSet 是 <a href="/zh/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>
的后继者。二者目的相同且行为类似，只是 ReplicationController 不支持
<a href="/zh/docs/concepts/overview/working-with-objects/labels/#label-selectors">标签用户指南</a>
中讨论的基于集合的选择算符需求。
因此，相比于 ReplicationController，应优先考虑 ReplicaSet。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods).
* Learn about [Deployments](/docs/concepts/workloads/controllers/deployment/).
* [Run a Stateless Application Using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/),
  which relies on ReplicaSets to work.
* `ReplicaSet` is a top-level resource in the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for replica sets.
* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how
  you can use it to manage application availability during disruptions.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。</li>
<li>了解 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployments</a>。</li>
<li><a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">使用 Deployment 运行一个无状态应用</a>，它依赖于 ReplicaSet。</li>
<li><code>ReplicaSet</code> 是 Kubernetes REST API 中的顶级资源。阅读 





<a href=""></a>
对象定义理解关于该资源的 API。</li>
<li>阅读<a href="/zh/docs/concepts/workloads/pods/disruptions/">Pod 干扰预算（Disruption Budget）</a>，了解如何在干扰下运行高度可用的应用。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-6d72299952c37ca8cc61b416e5bdbcd4">4.2.3 - StatefulSets</h1>
    
	<!--
title: StatefulSets
content_type: concept
weight: 40
-->
<!-- overview -->
<!--
StatefulSet is the workload API object used to manage stateful applications.
-->
<p>StatefulSet 是用来管理有状态应用的工作负载 API 对象。</p>
<!--
---
title: StatefulSet
id: statefulset
date: 2018-04-12
full_link: /docs/concepts/workloads/controllers/statefulset/
short_description: >
  Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.

aka: 
tags:
- fundamental
- core-object
- workload
- storage
---
-->
<!--
 Manages the deployment and scaling of a set of <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>, *and provides guarantees about the ordering and uniqueness* of these Pods.
-->
<p>StatefulSet 用来管理某 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 集合的部署和扩缩，
并为这些 Pod 提供持久存储和持久标识符。</p>
<!--
Like a <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable&#58; each has a persistent identifier that it maintains across any rescheduling.
-->
<p>和 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a> 类似，
StatefulSet 管理基于相同容器规约的一组 Pod。但和 Deployment 不同的是，
StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID。这些 Pod 是基于相同的规约来创建的，
但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。</p>
<!--
If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.
-->
<p>如果希望使用存储卷为工作负载提供持久存储，可以使用 StatefulSet 作为解决方案的一部分。
尽管 StatefulSet 中的单个 Pod 仍可能出现故障，
但持久的 Pod 标识符使得将现有卷与替换已失败 Pod 的新 Pod 相匹配变得更加容易。</p>
<!-- body -->
<!--
## Using StatefulSets

StatefulSets are valuable for applications that require one or more of the
following.
-->
<h2 id="使用-statefulsets">使用 StatefulSets</h2>
<p>StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值：</p>
<!--
* Stable, unique network identifiers.
* Stable, persistent storage.
* Ordered, graceful deployment and scaling.
* Ordered, automated rolling updates.
-->
<ul>
<li>稳定的、唯一的网络标识符。</li>
<li>稳定的、持久的存储。</li>
<li>有序的、优雅的部署和缩放。</li>
<li>有序的、自动的滚动更新。</li>
</ul>
<!--
In the above, stable is synonymous with persistence across Pod (re)scheduling.
If an application doesn't require any stable identifiers or ordered deployment,
deletion, or scaling, you should deploy your application using a workload object
that provides a set of stateless replicas.
[Deployment](/docs/concepts/workloads/controllers/deployment/) or
[ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) may be better suited to your stateless needs.
-->
<p>在上面描述中，“稳定的”意味着 Pod 调度或重调度的整个过程是有持久性的。
如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用
由一组无状态的副本控制器提供的工作负载来部署应用程序，比如
<a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a> 或者
<a href="/zh/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>
可能更适用于你的无状态应用部署需要。</p>
<!--
## Limitations
-->
<h2 id="limitations">限制 </h2>
<!--
* The storage for a given Pod must either be provisioned by a [PersistentVolume Provisioner](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md) based on the requested `storage class`, or pre-provisioned by an admin.
* Deleting and/or scaling a StatefulSet down will *not* delete the volumes associated with the StatefulSet. This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources.
* StatefulSets currently require a [Headless Service](/docs/concepts/services-networking/service/#headless-services) to be responsible for the network identity of the Pods. You are responsible for creating this Service.
* StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.
* When using [Rolling Updates](#rolling-updates) with the default
  [Pod Management Policy](#pod-management-policies) (`OrderedReady`),
  it's possible to get into a broken state that requires
  [manual intervention to repair](#forced-rollback).
-->
<ul>
<li>给定 Pod 的存储必须由
<a href="https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md">PersistentVolume 驱动</a>
基于所请求的 <code>storage class</code> 来提供，或者由管理员预先提供。</li>
<li>删除或者收缩 StatefulSet 并<em>不会</em>删除它关联的存储卷。
这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。</li>
<li>StatefulSet 当前需要<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>
来负责 Pod 的网络标识。你需要负责创建此服务。</li>
<li>当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。
为了实现 StatefulSet 中的 Pod 可以有序地且体面地终止，可以在删除之前将 StatefulSet
缩放为 0。</li>
<li>在默认 <a href="#pod-management-policies">Pod 管理策略</a>(<code>OrderedReady</code>) 时使用
<a href="#rolling-updates">滚动更新</a>，可能进入需要<a href="#forced-rollback">人工干预</a>
才能修复的损坏状态。</li>
</ul>
<!--
## Components
The example below demonstrates the components of a StatefulSet.
-->
<h2 id="components">组件 </h2>
<p>下面的示例演示了 StatefulSet 的组件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># has to match .spec.template.metadata.labels</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;nginx&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># by default is 1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># has to match .spec.selector.matchLabels</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/nginx-slim:0.8<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>www<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;my-storage-class&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></code></pre></div><!--
In the above example:

* A Headless Service, named `nginx`, is used to control the network domain.
* The StatefulSet, named `web`, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.
* The `volumeClaimTemplates` will provide stable storage using [PersistentVolumes](/docs/concepts/storage/persistent-volumes/) provisioned by a PersistentVolume Provisioner.

The name of a StatefulSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

-->
<p>上述例子中：</p>
<ul>
<li>名为 <code>nginx</code> 的 Headless Service 用来控制网络域名。</li>
<li>名为 <code>web</code> 的 StatefulSet 有一个 Spec，它表明将在独立的 3 个 Pod 副本中启动 nginx 容器。</li>
<li><code>volumeClaimTemplates</code> 将通过 PersistentVolumes 驱动提供的
<a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a> 来提供稳定的存储。</li>
</ul>
<p>StatefulSet 的命名需要遵循<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>规范。</p>
<!--
## Pod Selector
-->
<h2 id="pod-selector">Pod 选择算符    </h2>
<!--
You must set the `.spec.selector` field of a StatefulSet to match the labels of its `.spec.template.metadata.labels`. Prior to Kubernetes 1.8, the `.spec.selector` field was defaulted when omitted. In 1.8 and later versions, failing to specify a matching Pod Selector will result in a validation error during StatefulSet creation.
-->
<p>你必须设置 StatefulSet 的 <code>.spec.selector</code> 字段，使之匹配其在
<code>.spec.template.metadata.labels</code> 中设置的标签。在 Kubernetes 1.8 版本之前，
被忽略 <code>.spec.selector</code> 字段会获得默认设置值。
在 1.8 和以后的版本中，未指定匹配的 Pod 选择器将在创建 StatefulSet 期间导致验证错误。</p>
<!--
## Pod Identity

StatefulSet Pods have a unique identity that is comprised of an ordinal, a
stable network identity, and stable storage. The identity sticks to the Pod,
regardless of which node it's (re)scheduled on.
-->
<h2 id="pod-identity">Pod 标识  </h2>
<p>StatefulSet Pod 具有唯一的标识，该标识包括顺序标识、稳定的网络标识和稳定的存储。
该标识和 Pod 是绑定的，不管它被调度在哪个节点上。</p>
<!--
### Ordinal Index

For a StatefulSet with N replicas, each Pod in the StatefulSet will be
assigned an integer ordinal, from 0 up through N-1, that is unique over the Set.
-->
<h3 id="ordinal-index">有序索引  </h3>
<p>对于具有 N 个副本的 StatefulSet，StatefulSet 中的每个 Pod 将被分配一个整数序号，
从 0 到 N-1，该序号在 StatefulSet 上是唯一的。</p>
<!--
### Stable Network ID

Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet
and the ordinal of the Pod. The pattern for the constructed hostname
is `$(statefulset name)-$(ordinal)`. The example above will create three Pods
named `web-0,web-1,web-2`.
A StatefulSet can use a [Headless Service](/docs/concepts/services-networking/service/#headless-services)
to control the domain of its Pods. The domain managed by this Service takes the form:
`$(service name).$(namespace).svc.cluster.local`, where "cluster.local" is the
cluster domain.
As each Pod is created, it gets a matching DNS subdomain, taking the form:
`$(podname).$(governing service domain)`, where the governing service is defined
by the `serviceName` field on the StatefulSet.
-->
<h3 id="stable-network-id">稳定的网络 ID  </h3>
<p>StatefulSet 中的每个 Pod 根据 StatefulSet 的名称和 Pod 的序号派生出它的主机名。
组合主机名的格式为<code>$(StatefulSet 名称)-$(序号)</code>。
上例将会创建三个名称分别为 <code>web-0、web-1、web-2</code> 的 Pod。
StatefulSet 可以使用 <a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>
控制它的 Pod 的网络域。管理域的这个服务的格式为：
<code>$(服务名称).$(命名空间).svc.cluster.local</code>，其中 <code>cluster.local</code> 是集群域。
一旦每个 Pod 创建成功，就会得到一个匹配的 DNS 子域，格式为：
<code>$(pod 名称).$(所属服务的 DNS 域名)</code>，其中所属服务由 StatefulSet 的 <code>serviceName</code> 域来设定。</p>
<!--
Depending on how DNS is configured in your cluster, you may not be able to look up the DNS
name for a newly-run Pod immediately. This behavior can occur when other clients in the
cluster have already sent queries for the hostname of the Pod before it was created.
Negative caching (normal in DNS) means that the results of previous failed lookups are
remembered and reused, even after the Pod is running, for at least a few seconds.

If you need to discover Pods promptly after they are created, you have a few options:

- Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.
- Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the config map for CoreDNS, which currently caches for 30 seconds).


As mentioned in the [limitations](#limitations) section, you are responsible for
creating the [Headless Service](/docs/concepts/services-networking/service/#headless-services)
responsible for the network identity of the pods.

-->
<p>取决于集群域内部 DNS 的配置，有可能无法查询一个刚刚启动的 Pod 的 DNS 命名。
当集群内其他客户端在 Pod 创建完成前发出 Pod 主机名查询时，就会发生这种情况。
负缓存 (在 DNS 中较为常见) 意味着之前失败的查询结果会被记录和重用至少若干秒钟，
即使 Pod 已经正常运行了也是如此。</p>
<p>如果需要在 Pod 被创建之后及时发现它们，有以下选项：</p>
<ul>
<li>直接查询 Kubernetes API（比如，利用 watch 机制）而不是依赖于 DNS 查询</li>
<li>缩短 Kubernetes DNS 驱动的缓存时长（通常这意味着修改 CoreDNS 的 ConfigMap，目前缓存时长为 30 秒）</li>
</ul>
<p>正如<a href="#limitations">限制</a>中所述，你需要负责创建<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>
以便为 Pod 提供网络标识。</p>
<!--
Here are some examples of choices for Cluster Domain, Service name,
StatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.


Cluster Domain | Service (ns/name) | StatefulSet (ns/name)  | StatefulSet Domain  | Pod DNS | Pod Hostname |
-------------- | ----------------- | ----------------- | -------------- | ------- | ------------ |
 cluster.local | default/nginx     | default/web       | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |
 cluster.local | foo/nginx         | foo/web           | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |
 kube.local    | foo/nginx         | foo/web           | nginx.foo.svc.kube.local        | web-{0..N-1}.nginx.foo.svc.kube.local        | web-{0..N-1} |

-->
<p>下面给出一些选择集群域、服务名、StatefulSet 名、及其怎样影响 StatefulSet 的 Pod 上的 DNS 名称的示例：</p>
<table>
<thead>
<tr>
<th>集群域名</th>
<th>服务（名字空间/名字）</th>
<th>StatefulSet（名字空间/名字）</th>
<th>StatefulSet 域名</th>
<th>Pod DNS</th>
<th>Pod 主机名</th>
</tr>
</thead>
<tbody>
<tr>
<td>cluster.local</td>
<td>default/nginx</td>
<td>default/web</td>
<td>nginx.default.svc.cluster.local</td>
<td>web-{0..N-1}.nginx.default.svc.cluster.local</td>
<td>web-{0..N-1}</td>
</tr>
<tr>
<td>cluster.local</td>
<td>foo/nginx</td>
<td>foo/web</td>
<td>nginx.foo.svc.cluster.local</td>
<td>web-{0..N-1}.nginx.foo.svc.cluster.local</td>
<td>web-{0..N-1}</td>
</tr>
<tr>
<td>kube.local</td>
<td>foo/nginx</td>
<td>foo/web</td>
<td>nginx.foo.svc.kube.local</td>
<td>web-{0..N-1}.nginx.foo.svc.kube.local</td>
<td>web-{0..N-1}</td>
</tr>
</tbody>
</table>
<!--
Cluster Domain will be set to `cluster.local` unless
[otherwise configured](/docs/concepts/services-networking/dns-pod-service/#how-it-works).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 集群域会被设置为 <code>cluster.local</code>，除非有<a href="/zh/docs/concepts/services-networking/dns-pod-service/">其他配置</a>。
</div>
<!--
### Stable Storage

For each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one PersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume with a StorageClass of `my-storage-class` and 1 Gib of provisioned storage. If no StorageClass
is specified, then the default StorageClass will be used. When a Pod is (re)scheduled
onto a node, its `volumeMounts` mount the PersistentVolumes associated with its
PersistentVolume Claims. Note that, the PersistentVolumes associated with the
Pods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.
This must be done manually.
-->
<h3 id="stable-storage">稳定的存储 </h3>
<p>对于 StatefulSet 中定义的每个 VolumeClaimTemplate，每个 Pod 接收到一个 PersistentVolumeClaim。在上面的 nginx 示例中，每个 Pod 将会得到基于 StorageClass <code>my-storage-class</code> 提供的
1 Gib 的 PersistentVolume。
如果没有声明 StorageClass，就会使用默认的 StorageClass。
当一个 Pod 被调度（重新调度）到节点上时，它的 <code>volumeMounts</code> 会挂载与其
PersistentVolumeClaims 相关联的 PersistentVolume。
请注意，当 Pod 或者 StatefulSet 被删除时，与 PersistentVolumeClaims 相关联的
PersistentVolume 并不会被删除。要删除它必须通过手动方式来完成。</p>
<!--
### Pod Name Label

When the StatefulSet <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> creates a Pod,
it adds a label, `statefulset.kubernetes.io/pod-name`, that is set to the name of
the Pod. This label allows you to attach a Service to a specific Pod in
the StatefulSet.
-->
<h3 id="pod-name-label">Pod 名称标签  </h3>
<p>当 StatefulSet <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> 创建 Pod 时，
它会添加一个标签 <code>statefulset.kubernetes.io/pod-name</code>，该标签值设置为 Pod 名称。
这个标签允许你给 StatefulSet 中的特定 Pod 绑定一个 Service。</p>
<!--
## Deployment and Scaling Guarantees

* For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.
* When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.
* Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.
* Before a Pod is terminated, all of its successors must be completely shutdown.
-->
<h2 id="deployment-and-scaling-guarantees">部署和扩缩保证  </h2>
<ul>
<li>对于包含 N 个 副本的 StatefulSet，当部署 Pod 时，它们是依次创建的，顺序为 <code>0..N-1</code>。</li>
<li>当删除 Pod 时，它们是逆序终止的，顺序为 <code>N-1..0</code>。</li>
<li>在将缩放操作应用到 Pod 之前，它前面的所有 Pod 必须是 Running 和 Ready 状态。</li>
<li>在 Pod 终止之前，所有的继任者必须完全关闭。</li>
</ul>
<!--
The StatefulSet should not specify a `pod.Spec.TerminationGracePeriodSeconds` of 0. This practice is unsafe and strongly discouraged. For further explanation, please refer to [force deleting StatefulSet Pods](/docs/tasks/run-application/force-delete-stateful-set-pod/).
-->
<p>StatefulSet 不应将 <code>pod.Spec.TerminationGracePeriodSeconds</code> 设置为 0。
这种做法是不安全的，要强烈阻止。更多的解释请参考
<a href="/zh/docs/tasks/run-application/force-delete-stateful-set-pod/">强制删除 StatefulSet Pod</a>。</p>
<!--
When the nginx example above is created, three Pods will be deployed in the order
web-0, web-1, web-2. web-1 will not be deployed before web-0 is
[Running and Ready](/docs/user-guide/pod-states/), and web-2 will not be deployed until
web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before
web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and
becomes Running and Ready.
-->
<p>在上面的 nginx 示例被创建后，会按照 web-0、web-1、web-2 的顺序部署三个 Pod。
在 web-0 进入 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">Running 和 Ready</a>
状态前不会部署 web-1。在 web-1 进入 Running 和 Ready 状态前不会部署 web-2。
如果 web-1 已经处于 Running 和 Ready 状态，而 web-2 尚未部署，在此期间发生了
web-0 运行失败，那么 web-2 将不会被部署，要等到 web-0 部署完成并进入 Running 和
Ready 状态后，才会部署 web-2。</p>
<!--
If a user were to scale the deployed example by patching the StatefulSet such that
`replicas=1`, web-2 would be terminated first. web-1 would not be terminated until web-2
is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and
is completely shutdown, but prior to web-1's termination, web-1 would not be terminated
until web-0 is Running and Ready.
-->
<p>如果用户想将示例中的 StatefulSet 收缩为 <code>replicas=1</code>，首先被终止的是 web-2。
在 web-2 没有被完全停止和删除前，web-1 不会被终止。
当 web-2 已被终止和删除、web-1 尚未被终止，如果在此期间发生 web-0 运行失败，
那么就不会终止 web-1，必须等到 web-0 进入 Running 和 Ready 状态后才会终止 web-1。</p>
<!--
### Pod Management Policies

In Kubernetes 1.7 and later, StatefulSet allows you to relax its ordering guarantees while
preserving its uniqueness and identity guarantees via its `.spec.podManagementPolicy` field.
-->
<h3 id="pod-management-policies">Pod 管理策略</h3>
<p>在 Kubernetes 1.7 及以后的版本中，StatefulSet 允许你放宽其排序保证，
同时通过它的 <code>.spec.podManagementPolicy</code> 域保持其唯一性和身份保证。</p>
<!--
#### OrderedReady Pod Management

`OrderedReady` pod management is the default for StatefulSets. It implements the behavior
described [above](#deployment-and-scaling-guarantees).
-->
<h4 id="orderedready-pod-管理">OrderedReady Pod 管理</h4>
<p><code>OrderedReady</code> Pod 管理是 StatefulSet 的默认设置。它实现了
<a href="#deployment-and-scaling-guarantees">上面</a>描述的功能。</p>
<!--
#### Parallel Pod Management

`Parallel` pod management tells the StatefulSet controller to launch or
terminate all Pods in parallel, and to not wait for Pods to become Running
and Ready or completely terminated prior to launching or terminating another
Pod. This option only affects the behavior for scaling operations. Updates are not affected.

-->
<h4 id="parallel-pod-management">并行 Pod 管理  </h4>
<p><code>Parallel</code> Pod 管理让 StatefulSet 控制器并行的启动或终止所有的 Pod，
启动或者终止其他 Pod 前，无需等待 Pod 进入 Running 和 ready 或者完全停止状态。
这个选项只会影响伸缩操作的行为，更新则不会被影响。</p>
<!--
## Update Strategies

A StatefulSet's `.spec.updateStrategy` field allows you to configure
and disable automated rolling updates for containers, labels, resource request/limits, and
annotations for the Pods in a StatefulSet. There are two possible values:
-->
<h2 id="update-strategies">更新策略 </h2>
<p>StatefulSet 的 <code>.spec.updateStrategy</code> 字段让
你可以配置和禁用掉自动滚动更新 Pod 的容器、标签、资源请求或限制、以及注解。
有两个允许的值：</p>
<!--
`OnDelete`
: When a StatefulSet's `.spec.updateStrategy.type` is set to `OnDelete`,
  the StatefulSet controller will not automatically update the Pods in a
  StatefulSet. Users must manually delete Pods to cause the controller to
  create new Pods that reflect modifications made to a StatefulSet's `.spec.template`.

`RollingUpdate`
: The `RollingUpdate` update strategy implements automated, rolling update for the Pods in a StatefulSet. This is the default update strategy.
-->
<dl>
<dt><code>OnDelete</code></dt>
<dd>当 StatefulSet 的 <code>.spec.updateStrategy.type</code> 设置为 <code>OnDelete</code> 时，
它的控制器将不会自动更新 StatefulSet 中的 Pod。
用户必须手动删除 Pod 以便让控制器创建新的 Pod，以此来对 StatefulSet 的
<code>.spec.template</code> 的变动作出反应。</dd>
<dt><code>RollingUpdate</code></dt>
<dd><code>RollingUpdate</code> 更新策略对 StatefulSet 中的 Pod 执行自动的滚动更新。这是默认的更新策略。</dd>
</dl>
<!--
## Rolling Updates

When a StatefulSet's `.spec.updateStrategy.type` is set to `RollingUpdate`, the
StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed
in the same order as Pod termination (from the largest ordinal to the smallest), updating
each Pod one at a time.

The Kubernetes control plane waits until an updated Pod is Running and Ready prior
to updating its predecessor. If you have set `.spec.minReadySeconds` (see [Minimum Ready Seconds](#minimum-ready-seconds)), the control plane additionally waits that amount of time after the Pod turns ready, before moving on.
-->
<h2 id="rolling-updates">滚动更新</h2>
<p>当 StatefulSet 的 <code>.spec.updateStrategy.type</code> 被设置为 <code>RollingUpdate</code> 时，
StatefulSet 控制器会删除和重建 StatefulSet 中的每个 Pod。
它将按照与 Pod 终止相同的顺序（从最大序号到最小序号）进行，每次更新一个 Pod。</p>
<p>Kubernetes 控制面会等到被更新的 Pod 进入 Running 和 Ready 状态，然后再更新其前身。
如果你设置了 <code>.spec.minReadySeconds</code>（查看<a href="#minimum-ready-seconds">最短就绪秒数</a>），控制面在 Pod 就绪后会额外等待一定的时间再执行下一步。</p>
<!--
### Partitioned rolling updates {#partitions}

The `RollingUpdate` update strategy can be partitioned, by specifying a
`.spec.updateStrategy.rollingUpdate.partition`. If a partition is specified, all Pods with an
ordinal that is greater than or equal to the partition will be updated when the StatefulSet's
`.spec.template` is updated. All Pods with an ordinal that is less than the partition will not
be updated, and, even if they are deleted, they will be recreated at the previous version. If a
StatefulSet's `.spec.updateStrategy.rollingUpdate.partition` is greater than its `.spec.replicas`,
updates to its `.spec.template` will not be propagated to its Pods.
In most cases you will not need to use a partition, but they are useful if you want to stage an
update, roll out a canary, or perform a phased roll out.
-->
<h3 id="partitions">分区滚动更新  </h3>
<p>通过声明 <code>.spec.updateStrategy.rollingUpdate.partition</code> 的方式，<code>RollingUpdate</code>
更新策略可以实现分区。
如果声明了一个分区，当 StatefulSet 的 <code>.spec.template</code> 被更新时，
所有序号大于等于该分区序号的 Pod 都会被更新。
所有序号小于该分区序号的 Pod 都不会被更新，并且，即使他们被删除也会依据之前的版本进行重建。
如果 StatefulSet 的 <code>.spec.updateStrategy.rollingUpdate.partition</code> 大于它的
<code>.spec.replicas</code>，对它的 <code>.spec.template</code> 的更新将不会传递到它的 Pod。
在大多数情况下，你不需要使用分区，但如果你希望进行阶段更新、执行金丝雀或执行
分阶段上线，则这些分区会非常有用。</p>
<!--
### Forced Rollback

When using [Rolling Updates](#rolling-updates) with the default
[Pod Management Policy](#pod-management-policies) (`OrderedReady`),
it's possible to get into a broken state that requires manual intervention to repair.

If you update the Pod template to a configuration that never becomes Running and
Ready (for example, due to a bad binary or application-level configuration error),
StatefulSet will stop the rollout and wait.
-->
<h3 id="forced-rollback">强制回滚</h3>
<p>在默认 <a href="#pod-management-policies">Pod 管理策略</a>(<code>OrderedReady</code>) 下使用
<a href="#rolling-updates">滚动更新</a> ，可能进入需要人工干预才能修复的损坏状态。</p>
<p>如果更新后 Pod 模板配置进入无法运行或就绪的状态（例如，由于错误的二进制文件
或应用程序级配置错误），StatefulSet 将停止回滚并等待。</p>
<!--
In this state, it's not enough to revert the Pod template to a good configuration.
Due to a [known issue](https://github.com/kubernetes/kubernetes/issues/67250),
StatefulSet will continue to wait for the broken Pod to become Ready
(which never happens) before it will attempt to revert it back to the working
configuration.

After reverting the template, you must also delete any Pods that StatefulSet had
already attempted to run with the bad configuration.
StatefulSet will then begin to recreate the Pods using the reverted template.
-->
<p>在这种状态下，仅将 Pod 模板还原为正确的配置是不够的。由于
<a href="https://github.com/kubernetes/kubernetes/issues/67250">已知问题</a>，StatefulSet
将继续等待损坏状态的 Pod 准备就绪（永远不会发生），然后再尝试将其恢复为正常工作配置。</p>
<p>恢复模板后，还必须删除 StatefulSet 尝试使用错误的配置来运行的 Pod。这样，
StatefulSet 才会开始使用被还原的模板来重新创建 Pod。</p>
<!--
### Minimum ready seconds

`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be ready without any of its containers crashing, for it to be considered available.
This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).

Please note that this field only works if you enable the `StatefulSetMinReadySeconds` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).
-->
<h3 id="minimum-ready-seconds">最短就绪秒数  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>


<p><code>.spec.minReadySeconds</code> 是一个可选字段，用于指定新创建的 Pod 就绪（没有任何容器崩溃）后被认为可用的最小秒数。
默认值是 0（Pod 就绪时就被认为可用）。要了解 Pod 何时被认为已就绪，请参阅<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">容器探针</a>。</p>
<p>请注意只有当你启用 <code>StatefulSetMinReadySeconds</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>时，该字段才会生效。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods).
* Find out how to use StatefulSets
  * Follow an example of [deploying a stateful application](/docs/tutorials/stateful-application/basic-stateful-set/).
  * Follow an example of [deploying Cassandra with Stateful Sets](/docs/tutorials/stateful-application/cassandra/).
  * Follow an example of [running a replicated stateful application](/docs/tasks/run-application/run-replicated-stateful-application/).
  * Learn how to [scale a StatefulSet](/docs/tasks/run-application/scale-stateful-set/).
  * Learn what's involved when you [delete a StatefulSet](/docs/tasks/run-application/delete-stateful-set/).
  * Learn how to [configure a Pod to use a volume for storage](/docs/tasks/configure-pod-container/configure-volume-storage/).
  * Learn how to [configure a Pod to use a PersistentVolume for storage](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).
* `StatefulSet` is a top-level resource in the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for stateful sets.
* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how
  you can use it to manage application availability during disruptions.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。</li>
<li>了解如何使用 StatefulSet
<ul>
<li>跟随示例<a href="/zh/docs/tutorials/stateful-application/basic-stateful-set/">部署有状态应用</a>。</li>
<li>跟随示例<a href="/zh/docs/tutorials/stateful-application/cassandra/">使用 StatefulSet 部署 Cassandra</a>。</li>
<li>跟随示例<a href="/zh/docs/tasks/run-application/run-replicated-stateful-application/">运行多副本的有状态应用程序</a>。</li>
<li>了解如何<a href="/zh/docs/tasks/run-application/scale-stateful-set/">扩缩 StatefulSet</a>。</li>
<li>了解<a href="/zh/docs/tasks/run-application/delete-stateful-set/">删除 StatefulSet</a>涉及到的操作。</li>
<li>了解如何<a href="/zh/docs/tasks/configure-pod-container/configure-volume-storage/">配置 Pod 以使用卷进行存储</a>。</li>
<li>了解如何<a href="/zh/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">配置 Pod 以使用 PersistentVolume 作为存储</a>。</li>
</ul>
</li>
<li><code>StatefulSet</code> 是 Kubernetes REST API 中的顶级资源。阅读 





<a href=""></a>
对象定义理解关于该资源的 API。</li>
<li>阅读<a href="/zh/docs/concepts/workloads/pods/disruptions/">Pod 干扰预算（Disruption Budget）</a>，了解如何在干扰下运行高度可用的应用。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-41600eb8b6631c88848156f381e9d588">4.2.4 - DaemonSet</h1>
    
	<!--
title: DaemonSet
content_type: concept
weight: 50
--->
<!-- overview -->
<!--
A _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the
cluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage
collected.  Deleting a DaemonSet will clean up the Pods it created.
--->
<p><em>DaemonSet</em> 确保全部（或者某些）节点上运行一个 Pod 的副本。
当有节点加入集群时， 也会为他们新增一个 Pod 。
当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。</p>
<!--
Some typical uses of a DaemonSet are:

- running a cluster storage daemon on every node
- running a logs collection daemon on every node
- running a node monitoring daemon on every node
-->
<p>DaemonSet 的一些典型用法：</p>
<ul>
<li>在每个节点上运行集群守护进程</li>
<li>在每个节点上运行日志收集守护进程</li>
<li>在每个节点上运行监控守护进程</li>
</ul>
<!--
In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.
A more complex setup might use multiple DaemonSets for a single type of daemon, but with
different flags and/or different memory and cpu requests for different hardware types.
-->
<p>一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个 DaemonSet。
一个稍微复杂的用法是为同一种守护进程部署多个 DaemonSet；每个具有不同的标志，
并且对不同硬件类型具有不同的内存、CPU 要求。</p>
<!-- body -->
<!--
## Writing a DaemonSet Spec

### Create a DaemonSet
-->
<h2 id="writing-a-daemon-set-spec">编写 DaemonSet Spec  </h2>
<h3 id="create-a-daemon-set">创建 DaemonSet  </h3>
<!--
You can describe a DaemonSet in a YAML file. For example, the `daemonset.yaml` file below
describes a DaemonSet that runs the fluentd-elasticsearch Docker image:
-->
<p>你可以在 YAML 文件中描述 DaemonSet。
例如，下面的 daemonset.yaml 文件描述了一个运行 fluentd-elasticsearch Docker 镜像的 DaemonSet：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/daemonset.yaml" download="controllers/daemonset.yaml"><code>controllers/daemonset.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-daemonset-yaml')" title="Copy controllers/daemonset.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-daemonset-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>fluentd-logging<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this toleration is to have the daemonset runnable on master nodes</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># remove it if your masters can&#39;t run pods</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>node-role.kubernetes.io/master<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>Exists<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>quay.io/fluentd_elasticsearch/fluentd:v2.5.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>100m<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlibdockercontainers<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/docker/containers<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlibdockercontainers<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/lib/docker/containers<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create a DaemonSet based on the YAML file:
-->
<p>基于 YAML 文件创建 DaemonSet：</p>
<pre><code>kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
</code></pre><!--
### Required Fields

As with all other Kubernetes config, a DaemonSet needs `apiVersion`, `kind`, and `metadata` fields.  For
general information about working with config files, see
[running stateless applications](/docs/tasks/run-application/run-stateless-application-deployment/)
and [object management using kubectl](/docs/concepts/overview/working-with-objects/object-management/).

The name of a DaemonSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A DaemonSet also needs a
[`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)
section.
-->
<h3 id="required-fields">必需字段  </h3>
<p>和所有其他 Kubernetes 配置一样，DaemonSet 需要 <code>apiVersion</code>、<code>kind</code> 和 <code>metadata</code> 字段。
有关配置文件的基本信息，参见
<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">部署应用</a>、
<a href="/zh/docs/tasks/">配置容器</a>和
<a href="/zh/docs/concepts/overview/working-with-objects/object-management/">使用 kubectl 进行对象管理</a>
文档。</p>
<p>DaemonSet 对象的名称必须是一个合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<p>DaemonSet 也需要一个 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code></a> 配置段。</p>
<!--
### Pod Template

The `.spec.template` is one of the required fields in `.spec`.

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates).
It has exactly the same schema as a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>,
except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate
labels (see [pod selector](#pod-selector)).

A Pod Template in a DaemonSet must have a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)
 equal to `Always`, or be unspecified, which defaults to `Always`.
-->
<h3 id="pod-template">Pod 模板  </h3>
<p><code>.spec</code> 中唯一必需的字段是 <code>.spec.template</code>。</p>
<p><code>.spec.template</code> 是一个 <a href="/zh/docs/concepts/workloads/pods/#pod-templates">Pod 模板</a>。
除了它是嵌套的，因而不具有 <code>apiVersion</code> 或 <code>kind</code> 字段之外，它与
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 具有相同的 schema。</p>
<p>除了 Pod 必需字段外，在 DaemonSet 中的 Pod 模板必须指定合理的标签（查看 <a href="#pod-selector">Pod 选择算符</a>）。</p>
<p>在 DaemonSet 中的 Pod 模板必须具有一个值为 <code>Always</code> 的
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>RestartPolicy</code></a>。
当该值未指定时，默认是 <code>Always</code>。</p>
<!--
### Pod Selector

The `.spec.selector` field is a pod selector.  It works the same as the `.spec.selector` of
a [Job](/docs/concepts/jobs/run-to-completion-finite-workloads/).

You must specify a pod selector that matches the labels of the
`.spec.template`.
Also, once a DaemonSet is created,
its `.spec.selector` can not be mutated. Mutating the pod selector can lead to the
unintentional orphaning of Pods, and it was found to be confusing to users.
-->
<h3 id="pod-selector">Pod 选择算符    </h3>
<p><code>.spec.selector</code> 字段表示 Pod 选择算符，它与
<a href="/zh/docs/concepts/workloads/controllers/job/">Job</a> 的 <code>.spec.selector</code> 的作用是相同的。</p>
<p>你必须指定与 <code>.spec.template</code> 的标签匹配的 Pod 选择算符。
此外，一旦创建了 DaemonSet，它的 <code>.spec.selector</code> 就不能修改。
修改 Pod 选择算符可能导致 Pod 意外悬浮，并且这对用户来说是费解的。</p>
<!--
The `.spec.selector` is an object consisting of two fields:
-->
<p><code>spec.selector</code> 是一个对象，如下两个字段组成：</p>
<!--
* `matchLabels` - works the same as the `.spec.selector` of a
  [ReplicationController](/docs/concepts/workloads/controllers/replicationcontroller/).
* `matchExpressions` - allows to build more sophisticated selectors by specifying key,
  list of values and an operator that relates the key and values.
-->
<ul>
<li><code>matchLabels</code> - 与 <a href="/zh/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>
的 <code>.spec.selector</code> 的作用相同。</li>
<li><code>matchExpressions</code> - 允许构建更加复杂的选择器，可以通过指定 key、value
列表以及将 key 和 value 列表关联起来的 operator。</li>
</ul>
<!--
When the two are specified the result is ANDed.
-->
<p>当上述两个字段都指定时，结果会按逻辑与（AND）操作处理。</p>
<!--
The `.spec.selector` must match the `.spec.template.metadata.labels`.
Config with these two not matching will be rejected by the API.
-->
<p><code>.spec.selector</code> 必须与 <code>.spec.template.metadata.labels</code> 相匹配。
如果配置中这两个字段不匹配，则会被 API 拒绝。</p>
<!--
### Running Pods on Only Some Nodes

If you specify a `.spec.template.spec.nodeSelector`, then the DaemonSet controller will
create Pods on nodes which match that [node selector](/docs/concepts/scheduling-eviction/assign-pod-node/).
Likewise if you specify a `.spec.template.spec.affinity`,
then DaemonSet controller will create Pods on nodes which match that
[node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/).
If you do not specify either, then the DaemonSet controller will create Pods on all nodes.
-->
<h3 id="running-pods-on-only-some-nodes">仅在某些节点上运行 Pod  </h3>
<p>如果指定了 <code>.spec.template.spec.nodeSelector</code>，DaemonSet 控制器将在能够与
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">Node 选择算符</a> 匹配的节点上创建 Pod。
类似这种情况，可以指定 <code>.spec.template.spec.affinity</code>，之后 DaemonSet 控制器
将在能够与<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">节点亲和性</a>
匹配的节点上创建 Pod。
如果根本就没有指定，则 DaemonSet Controller 将在所有节点上创建 Pod。</p>
<!--
## How Daemon Pods are Scheduled

### Scheduled by default scheduler
-->
<h2 id="how-daemon-pods-are-scheduled">Daemon Pods 是如何被调度的  </h2>
<h3 id="scheduled-by-default-scheduler">通过默认调度器调度  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.17 [stable]</code>
</div>


<!--
A DaemonSet ensures that all eligible nodes run a copy of a Pod. Normally, the
node that a Pod runs on is selected by the Kubernetes scheduler. However,
DaemonSet pods are created and scheduled by the DaemonSet controller instead.
That introduces the following issues:

* Inconsistent Pod behavior: Normal Pods waiting to be scheduled are created
  and in `Pending` state, but DaemonSet pods are not created in `Pending`
  state. This is confusing to the user.
* [Pod preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/)
  is handled by default scheduler. When preemption is enabled, the DaemonSet controller
  will make scheduling decisions without considering pod priority and preemption.
-->
<p>DaemonSet 确保所有符合条件的节点都运行该 Pod 的一个副本。
通常，运行 Pod 的节点由 Kubernetes 调度器选择。
不过，DaemonSet Pods 由 DaemonSet 控制器创建和调度。这就带来了以下问题：</p>
<ul>
<li>Pod 行为的不一致性：正常 Pod 在被创建后等待调度时处于 <code>Pending</code> 状态，
DaemonSet Pods 创建后不会处于 <code>Pending</code> 状态下。这使用户感到困惑。</li>
<li><a href="/zh/docs/concepts/configuration/pod-priority-preemption/">Pod 抢占</a>
由默认调度器处理。启用抢占后，DaemonSet 控制器将在不考虑 Pod 优先级和抢占
的情况下制定调度决策。</li>
</ul>
<!--
`ScheduleDaemonSetPods` allows you to schedule DaemonSets using the default
scheduler instead of the DaemonSet controller, by adding the `NodeAffinity` term
to the DaemonSet pods, instead of the `.spec.nodeName` term. The default
scheduler is then used to bind the pod to the target host. If node affinity of
the DaemonSet pod already exists, it is replaced (the original node affinity was
taken into account before selecting the target host). The DaemonSet controller only
performs these operations when creating or modifying DaemonSet pods, and no
changes are made to the `spec.template` of the DaemonSet.
-->
<p><code>ScheduleDaemonSetPods</code> 允许您使用默认调度器而不是 DaemonSet 控制器来调度 DaemonSets，
方法是将 <code>NodeAffinity</code> 条件而不是 <code>.spec.nodeName</code> 条件添加到 DaemonSet Pods。
默认调度器接下来将 Pod 绑定到目标主机。
如果 DaemonSet Pod 的节点亲和性配置已存在，则被替换
（原始的节点亲和性配置在选择目标主机之前被考虑）。
DaemonSet 控制器仅在创建或修改 DaemonSet Pod 时执行这些操作，
并且不会更改 DaemonSet 的 <code>spec.template</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">matchFields</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>metadata.name<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- target-host-name<span style="color:#bbb">
</span></code></pre></div><!--
In addition, `node.kubernetes.io/unschedulable:NoSchedule` toleration is added
automatically to DaemonSet Pods. The default scheduler ignores
`unschedulable` Nodes when scheduling DaemonSet Pods.
-->
<p>此外，系统会自动添加 <code>node.kubernetes.io/unschedulable：NoSchedule</code> 容忍度到
DaemonSet Pods。在调度 DaemonSet Pod 时，默认调度器会忽略 <code>unschedulable</code> 节点。</p>
<!--
### Taints and Tolerations

Although Daemon Pods respect
[taints and tolerations](/docs/concepts/configuration/taint-and-toleration),
the following tolerations are added to DaemonSet Pods automatically according to
the related features.
-->
<h3 id="taint-and-toleration">污点和容忍度  </h3>
<p>尽管 Daemon Pods 遵循<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration">污点和容忍度</a>
规则，根据相关特性，控制器会自动将以下容忍度添加到 DaemonSet Pod：</p>
<table>
<thead>
<tr>
<th>容忍度键名</th>
<th>效果</th>
<th>版本</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>node.kubernetes.io/not-ready</code></td>
<td>NoExecute</td>
<td>1.13+</td>
<td>当出现类似网络断开的情况导致节点问题时，DaemonSet Pod 不会被逐出。</td>
</tr>
<tr>
<td><code>node.kubernetes.io/unreachable</code></td>
<td>NoExecute</td>
<td>1.13+</td>
<td>当出现类似于网络断开的情况导致节点问题时，DaemonSet Pod 不会被逐出。</td>
</tr>
<tr>
<td><code>node.kubernetes.io/disk-pressure</code></td>
<td>NoSchedule</td>
<td>1.8+</td>
<td>DaemonSet Pod 被默认调度器调度时能够容忍磁盘压力属性。</td>
</tr>
<tr>
<td><code>node.kubernetes.io/memory-pressure</code></td>
<td>NoSchedule</td>
<td>1.8+</td>
<td>DaemonSet Pod 被默认调度器调度时能够容忍内存压力属性。</td>
</tr>
<tr>
<td><code>node.kubernetes.io/unschedulable</code></td>
<td>NoSchedule</td>
<td>1.12+</td>
<td>DaemonSet Pod 能够容忍默认调度器所设置的 <code>unschedulable</code> 属性.</td>
</tr>
<tr>
<td><code>node.kubernetes.io/network-unavailable</code></td>
<td>NoSchedule</td>
<td>1.12+</td>
<td>DaemonSet 在使用宿主网络时，能够容忍默认调度器所设置的 <code>network-unavailable</code> 属性。</td>
</tr>
</tbody>
</table>
<!--
## Communicating with Daemon Pods
-->
<!--
Some possible patterns for communicating with Pods in a DaemonSet are:

- **Push**: Pods in the DaemonSet are configured to send updates to another service, such
  as a stats database.  They do not have clients.
- **NodeIP and Known Port**: Pods in the DaemonSet can use a `hostPort`, so that the pods
  are reachable via the node IPs.
  Clients know the list of node IPs somehow, and know the port by convention.
- **DNS**: Create a [headless service](/docs/concepts/services-networking/service/#headless-services)
  with the same pod selector, and then discover DaemonSets using the `endpoints`
  resource or retrieve multiple A records from DNS.
- **Service**: Create a service with the same Pod selector, and use the service to reach a
  daemon on a random node. (No way to reach specific node.)
-->
<h2 id="communicating-with-daemon-pods">与 Daemon Pods 通信  </h2>
<p>与 DaemonSet 中的 Pod 进行通信的几种可能模式如下：</p>
<ul>
<li>
<p><strong>推送（Push）</strong>：配置 DaemonSet 中的 Pod，将更新发送到另一个服务，例如统计数据库。
这些服务没有客户端。</p>
</li>
<li>
<p><strong>NodeIP 和已知端口</strong>：DaemonSet 中的 Pod 可以使用 <code>hostPort</code>，从而可以通过节点 IP
访问到 Pod。客户端能通过某种方法获取节点 IP 列表，并且基于此也可以获取到相应的端口。</p>
</li>
<li>
<p><strong>DNS</strong>：创建具有相同 Pod 选择算符的
<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>，
通过使用 <code>endpoints</code> 资源或从 DNS 中检索到多个 A 记录来发现 DaemonSet。</p>
</li>
<li>
<p><strong>Service</strong>：创建具有相同 Pod 选择算符的服务，并使用该服务随机访问到某个节点上的
守护进程（没有办法访问到特定节点）。</p>
</li>
</ul>
<!--
## Updating a DaemonSet

If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete
Pods from newly not-matching nodes.

You can modify the Pods that a DaemonSet creates.  However, Pods do not allow all
fields to be updated.  Also, the DaemonSet controller will use the original template the next
time a node (even with the same name) is created.
-->
<h2 id="updating-a-daemon-set">更新 DaemonSet  </h2>
<p>如果节点的标签被修改，DaemonSet 将立刻向新匹配上的节点添加 Pod，
同时删除不匹配的节点上的 Pod。</p>
<p>你可以修改 DaemonSet 创建的 Pod。不过并非 Pod 的所有字段都可更新。
下次当某节点（即使具有相同的名称）被创建时，DaemonSet 控制器还会使用最初的模板。</p>
<!--
You can delete a DaemonSet.  If you specify `--cascade=orphan` with `kubectl`, then the Pods
will be left on the nodes.  If you subsequently create a new DaemonSet with the same selector,
the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces
them according to its `updateStrategy`.

You can [perform a rolling update](/docs/tasks/manage-daemon/update-daemon-set/) on a DaemonSet.
-->
<p>您可以删除一个 DaemonSet。如果使用 <code>kubectl</code> 并指定 <code>--cascade=orphan</code> 选项，
则 Pod 将被保留在节点上。接下来如果创建使用相同选择算符的新 DaemonSet，
新的 DaemonSet 会收养已有的 Pod。
如果有 Pod 需要被替换，DaemonSet 会根据其 <code>updateStrategy</code> 来替换。</p>
<p>你可以对 DaemonSet <a href="/zh/docs/tasks/manage-daemon/update-daemon-set/">执行滚动更新</a>操作。</p>
<!--
## Alternatives to DaemonSet

### Init Scripts
-->
<h2 id="alternatives-to-daemon-set">DaemonSet 的替代方案  </h2>
<h3 id="init-scripts">init 脚本  </h3>
<!--
It is certainly possible to run daemon processes by directly starting them on a node (e.g. using
`init`, `upstartd`, or `systemd`).  This is perfectly fine.  However, there are several advantages to
running such processes via a DaemonSet:

- Ability to monitor and manage logs for daemons in the same way as applications.
- Same config language and tools (e.g. Pod templates, `kubectl`) for daemons and applications.
- Running daemons in containers with resource limits increases isolation between daemons from app
  containers.  However, this can also be accomplished by running the daemons in a container but not in a Pod
  (e.g. start directly via Docker).
-->
<p>直接在节点上启动守护进程（例如使用 <code>init</code>、<code>upstartd</code> 或 <code>systemd</code>）的做法当然是可行的。
不过，基于 DaemonSet 来运行这些进程有如下一些好处：</p>
<ul>
<li>
<p>像所运行的其他应用一样，DaemonSet 具备为守护进程提供监控和日志管理的能力。</p>
</li>
<li>
<p>为守护进程和应用所使用的配置语言和工具（如 Pod 模板、<code>kubectl</code>）是相同的。</p>
</li>
<li>
<p>在资源受限的容器中运行守护进程能够增加守护进程和应用容器的隔离性。
然而，这一点也可以通过在容器中运行守护进程但却不在 Pod 中运行之来实现。
例如，直接基于 Docker 启动。</p>
</li>
</ul>
<!--
### Bare Pods

It is possible to create Pods directly which specify a particular node to run on.  However,
a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of
node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should
use a DaemonSet rather than creating individual Pods.
-->
<h3 id="bare-pods">裸 Pod  </h3>
<p>直接创建 Pod并指定其运行在特定的节点上也是可以的。
然而，DaemonSet 能够替换由于任何原因（例如节点失败、例行节点维护、内核升级）
而被删除或终止的 Pod。
由于这个原因，你应该使用 DaemonSet 而不是单独创建 Pod。</p>
<!--
### Static Pods

It is possible to create Pods by writing a file to a certain directory watched by Kubelet.  These
are called [static pods](/docs/tasks/configure-pod-container/static-pod/).
Unlike DaemonSet, static Pods cannot be managed with kubectl
or other Kubernetes API clients.  Static Pods do not depend on the apiserver, making them useful
in cluster bootstrapping cases.  Also, static Pods may be deprecated in the future.
-->
<h3 id="static-pods">静态 Pod  </h3>
<p>通过在一个指定的、受 <code>kubelet</code> 监视的目录下编写文件来创建 Pod 也是可行的。
这类 Pod 被称为<a href="/zh/docs/tasks/configure-pod-container/static-pod/">静态 Pod</a>。
不像 DaemonSet，静态 Pod 不受 <code>kubectl</code> 和其它 Kubernetes API 客户端管理。
静态 Pod 不依赖于 API 服务器，这使得它们在启动引导新集群的情况下非常有用。
此外，静态 Pod 在将来可能会被废弃。</p>
<!--
### Deployments

DaemonSets are similar to [Deployments](/docs/concepts/workloads/controllers/deployment/) in that
they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,
storage servers).

Use a Deployment for stateless services, like frontends, where scaling up and down the
number of replicas and rolling out updates are more important than controlling exactly which host
the Pod runs on.  Use a DaemonSet when it is important that a copy of a Pod always run on
all or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.

For example, [network plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.
-->
<h3 id="deployments">Deployments</h3>
<p>DaemonSet 与 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployments</a> 非常类似，
它们都能创建 Pod，并且 Pod 中的进程都不希望被终止（例如，Web 服务器、存储服务器）。</p>
<p>建议为无状态的服务使用 Deployments，比如前端服务。
对这些服务而言，对副本的数量进行扩缩容、平滑升级，比精确控制 Pod 运行在某个主机上要重要得多。
当需要 Pod 副本总是运行在全部或特定主机上，并且当该 DaemonSet 提供了节点级别的功能（允许其他 Pod 在该特定节点上正确运行）时，
应该使用 DaemonSet。</p>
<p>例如，<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件</a>通常包含一个以 DaemonSet 运行的组件。
这个 DaemonSet 组件确保它所在的节点的集群网络正常工作。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods).
  * Learn about [static Pods](#static-pods), which are useful for running Kubernetes
    <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> components.
* Find out how to use DaemonSets
  * [Perform a rolling update on a DaemonSet](/docs/tasks/manage-daemon/update-daemon-set/)
  * [Perform a rollback on a DaemonSet](/docs/tasks/manage-daemon/rollback-daemon-set/)
    (for example, if a roll out didn't work how you expected).
* Understand [how Kubernetes assigns Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/).
* Learn about [device plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) and
  [add ons](/docs/concepts/cluster-administration/addons/), which often run as DaemonSets.
* `DaemonSet` is a top-level resource in the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for daemon sets.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。
<ul>
<li>了解<a href="#static-pods">静态 Pod</a>，这对运行 Kubernetes <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a>组件有帮助。</li>
</ul>
</li>
<li>了解如何使用 DaemonSet
<ul>
<li><a href="/zh/docs/tasks/manage-daemon/update-daemon-set/">对 DaemonSet 执行滚动更新</a></li>
<li><a href="/zh/docs/tasks/manage-daemon/rollback-daemon-set/">对 DaemonSet 执行回滚</a>（例如：新的版本没有达到你的预期）</li>
</ul>
</li>
<li>理解<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">Kubernetes 如何将 Pod 分配给节点</a>。</li>
<li>了解<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">设备插件</a>和
<a href="/zh/docs/concepts/cluster-administration/addons/">扩展（Addons）</a>，它们常以 DaemonSet 运行。</li>
<li><code>DaemonSet</code> 是 Kubernetes REST API 中的顶级资源。阅读 





<a href=""></a>
对象定义理解关于该资源的 API。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-cc7cc3c4907039d9f863162e20bfbbef">4.2.5 - Jobs</h1>
    
	<!--
reviewers:
- erictune
- soltysh
title: Jobs
content_type: concept
feature:
  title: Batch execution
  description: >
    In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.
weight: 50
-->
<!-- overview -->
<!--
A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.
As pods successfully complete, the Job tracks the successful completions.  When a specified number
of successful completions is reached, the task (ie, Job) is complete.  Deleting a Job will clean up
the Pods it created. Suspending a Job will delete its active Pods until the Job
is resumed again.

A simple case is to create one Job object in order to reliably run one Pod to completion.
The Job object will start a new Pod if the first Pod fails or is deleted (for example
due to a node hardware failure or a node reboot).

You can also use a Job to run multiple Pods in parallel.
-->
<p>Job 会创建一个或者多个 Pods，并将继续重试 Pods 的执行，直到指定数量的 Pods 成功终止。
随着 Pods 成功结束，Job 跟踪记录成功完成的 Pods 个数。
当数量达到指定的成功个数阈值时，任务（即 Job）结束。
删除 Job 的操作会清除所创建的全部 Pods。
挂起 Job 的操作会删除 Job 的所有活跃 Pod，直到 Job 被再次恢复执行。</p>
<p>一种简单的使用场景下，你会创建一个 Job 对象以便以一种可靠的方式运行某 Pod 直到完成。
当第一个 Pod 失败或者被删除（比如因为节点硬件失效或者重启）时，Job
对象会启动一个新的 Pod。</p>
<p>你也可以使用 Job 以并行的方式运行多个 Pod。</p>
<!-- body -->
<!--
## Running an example Job

Here is an example Job config.  It computes π to 2000 places and prints it out.
It takes around 10s to complete.
-->
<h2 id="running-an-example-job">运行示例 Job    </h2>
<p>下面是一个 Job 配置示例。它负责计算 π 到小数点后 2000 位，并将结果打印出来。
此计算大约需要 10 秒钟完成。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/job.yaml" download="controllers/job.yaml"><code>controllers/job.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-job-yaml')" title="Copy controllers/job.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-job-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>perl<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;perl&#34;</span>,<span style="color:#bbb">  </span><span style="color:#b44">&#34;-Mbignum=bpi&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-wle&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;print bpi(2000)&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">backoffLimit</span>:<span style="color:#bbb"> </span><span style="color:#666">4</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--You can run the example with this command:-->
<p>你可以使用下面的命令来运行此示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
</code></pre></div><p>输出类似于：</p>
<pre><code>job.batch/pi created
</code></pre><!-- Check on the status of the Job with `kubectl`: -->
<p>使用 <code>kubectl</code> 来检查 Job 的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe jobs/pi
</code></pre></div><p>输出类似于：</p>
<pre><code>Name:           pi
Namespace:      default
Selector:       controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                job-name=pi
Annotations:    kubectl.kubernetes.io/last-applied-configuration:
                  {&quot;apiVersion&quot;:&quot;batch/v1&quot;,&quot;kind&quot;:&quot;Job&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;pi&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;backoffLimit&quot;:4,&quot;template&quot;:...
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           job-name=pi
  Containers:
   pi:
    Image:      perl
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7
</code></pre><!--
To view completed Pods of a Job, use `kubectl get pods`.

To list all the Pods that belong to a Job in a machine readable form, you can use a command like this:
-->
<p>要查看 Job 对应的已完成的 Pods，可以执行 <code>kubectl get pods</code>。</p>
<p>要以机器可读的方式列举隶属于某 Job 的全部 Pods，你可以使用类似下面这条命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">pods</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get pods --selector<span style="color:#666">=</span>job-name<span style="color:#666">=</span>pi --output<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.items[*].metadata.name}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>
<span style="color:#a2f">echo</span> <span style="color:#b8860b">$pods</span>
</code></pre></div><p>输出类似于：</p>
<pre><code>pi-5rwd7
</code></pre><!--
Here, the selector is the same as the selector for the Job.  The `-output=jsonpath` option specifies an expression
with the name from each Pod in the returned list.

View the standard output of one of the pods:
-->
<p>这里，选择算符与 Job 的选择算符相同。<code>--output=jsonpath</code> 选项给出了一个表达式，
用来从返回的列表中提取每个 Pod 的 name 字段。</p>
<p>查看其中一个 Pod 的标准输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs <span style="color:#b8860b">$pods</span>
</code></pre></div><!--The output is similar to this:-->
<p>输出类似于：</p>
<pre><code>3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
</code></pre><!--
## Writing a Job spec

As with all other Kubernetes config, a Job needs `apiVersion`, `kind`, and `metadata` fields.
Its name must be a valid [DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A Job also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
-->
<h2 id="编写-job-规约">编写 Job 规约</h2>
<p>与 Kubernetes 中其他资源的配置类似，Job 也需要 <code>apiVersion</code>、<code>kind</code> 和 <code>metadata</code> 字段。
Job 的名字必须是合法的 <a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<p>Job 配置还需要一个<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> 节</a>。</p>
<!--
### Pod Template

The `.spec.template` is the only required field of the `.spec`.

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>, except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a pod template in a Job must specify appropriate
labels (see [pod selector](#pod-selector)) and an appropriate restart policy.

Only a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Never` or `OnFailure` is allowed.
-->
<h3 id="pod-模版">Pod 模版</h3>
<p>Job 的 <code>.spec</code> 中只有 <code>.spec.template</code> 是必需的字段。</p>
<p>字段 <code>.spec.template</code> 的值是一个 <a href="/zh/docs/concepts/workloads/pods/#pod-templates">Pod 模版</a>。
其定义规范与 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
完全相同，只是其中不再需要 <code>apiVersion</code> 或 <code>kind</code> 字段。</p>
<p>除了作为 Pod 所必需的字段之外，Job 中的 Pod 模版必需设置合适的标签
（参见<a href="#pod-selector">Pod 选择算符</a>）和合适的重启策略。</p>
<p>Job 中 Pod 的 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>RestartPolicy</code></a>
只能设置为 <code>Never</code> 或 <code>OnFailure</code> 之一。</p>
<!--
### Pod selector

The `.spec.selector` field is optional.  In almost all cases you should not specify it.
See section [specifying your own pod selector](#specifying-your-own-pod-selector).
-->
<h3 id="pod-selector">Pod 选择算符  </h3>
<p>字段 <code>.spec.selector</code> 是可选的。在绝大多数场合，你都不需要为其赋值。
参阅<a href="#specifying-your-own-pod-selector">设置自己的 Pod 选择算符</a>.</p>
<!--
### Parallel execution for Jobs {#parallel-jobs}

There are three main types of task suitable to run as a Job:
-->
<h3 id="parallel-jobs">Job 的并行执行</h3>
<p>适合以 Job 形式来运行的任务主要有三种：</p>
<!--
1. Non-parallel Jobs
   - normally, only one Pod is started, unless the Pod fails.
   - the Job is complete as soon as its Pod terminates successfully.
1. Parallel Jobs with a *fixed completion count*:
   - specify a non-zero positive value for `.spec.completions`.
   - the Job represents the overall task, and is complete when there are `.spec.completions` successful Pods.
   - when using `.spec.completionMode="Indexed"`, each Pod gets a different index in the range 0 to `.spec.completions-1`.
1. Parallel Jobs with a *work queue*:
   - do not specify `.spec.completions`, default to `.spec.parallelism`.
   - the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.
   - each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done.
   - when _any_ Pod from the Job terminates with success, no new Pods are created.
   - once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success.
   - once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output.  They should all be in the process of exiting.
-->
<ol>
<li>非并行 Job：
<ul>
<li>通常只启动一个 Pod，除非该 Pod 失败。</li>
<li>当 Pod 成功终止时，立即视 Job 为完成状态。</li>
</ul>
</li>
<li>具有 <em>确定完成计数</em> 的并行 Job：
<ul>
<li><code>.spec.completions</code> 字段设置为非 0 的正数值。</li>
<li>Job 用来代表整个任务，当成功的 Pod 个数达到 <code>.spec.completions</code> 时，Job 被视为完成。</li>
<li>当使用 <code>.spec.completionMode=&quot;Indexed&quot;</code> 时，每个 Pod 都会获得一个不同的
索引值，介于 0 和 <code>.spec.completions-1</code> 之间。</li>
</ul>
</li>
<li>带 <em>工作队列</em> 的并行 Job：
<ul>
<li>不设置 <code>spec.completions</code>，默认值为 <code>.spec.parallelism</code>。</li>
<li>多个 Pod 之间必须相互协调，或者借助外部服务确定每个 Pod 要处理哪个工作条目。
例如，任一 Pod 都可以从工作队列中取走最多 N 个工作条目。</li>
<li>每个 Pod 都可以独立确定是否其它 Pod 都已完成，进而确定 Job 是否完成。</li>
<li>当 Job 中 <em>任何</em> Pod 成功终止，不再创建新 Pod。</li>
<li>一旦至少 1 个 Pod 成功完成，并且所有 Pod 都已终止，即可宣告 Job 成功完成。</li>
<li>一旦任何 Pod 成功退出，任何其它 Pod 都不应再对此任务执行任何操作或生成任何输出。
所有 Pod 都应启动退出过程。</li>
</ul>
</li>
</ol>
<!--
For a _non-parallel_ Job, you can leave both `.spec.completions` and `.spec.parallelism` unset.  When both are
unset, both are defaulted to 1.

For a _fixed completion count_ Job, you should set `.spec.completions` to the number of completions needed.
You can set `.spec.parallelism`, or leave it unset and it will default to 1.

For a _work queue_ Job, you must leave `.spec.completions` unset, and set `.spec.parallelism` to
a non-negative integer.

For more information about how to make use of the different types of job, see the [job patterns](#job-patterns) section.
-->
<p>对于 <em>非并行</em> 的 Job，你可以不设置 <code>spec.completions</code> 和 <code>spec.parallelism</code>。
这两个属性都不设置时，均取默认值 1。</p>
<p>对于 <em>确定完成计数</em> 类型的 Job，你应该设置 <code>.spec.completions</code> 为所需要的完成个数。
你可以设置 <code>.spec.parallelism</code>，也可以不设置。其默认值为 1。</p>
<p>对于一个 <em>工作队列</em> Job，你不可以设置 <code>.spec.completions</code>，但要将<code>.spec.parallelism</code>
设置为一个非负整数。</p>
<p>关于如何利用不同类型的 Job 的更多信息，请参见 <a href="#job-patterns">Job 模式</a>一节。</p>
<!--
#### Controlling parallelism

The requested parallelism (`.spec.parallelism`) can be set to any non-negative value.
If it is unspecified, it defaults to 1.
If it is specified as 0, then the Job is effectively paused until it is increased.

Actual parallelism (number of pods running at any instant) may be more or less than requested
parallelism, for a variety of reasons:
-->
<h4 id="controlling-parallelism">控制并行性  </h4>
<p>并行性请求（<code>.spec.parallelism</code>）可以设置为任何非负整数。
如果未设置，则默认为 1。
如果设置为 0，则 Job 相当于启动之后便被暂停，直到此值被增加。</p>
<p>实际并行性（在任意时刻运行状态的 Pods 个数）可能比并行性请求略大或略小，
原因如下：</p>
<!--
- For _fixed completion count_ Jobs, the actual number of pods running in parallel will not exceed the number of
  remaining completions.   Higher values of `.spec.parallelism` are effectively ignored.
- For _work queue_ Jobs, no new Pods are started after any Pod has succeeded - remaining Pods are allowed to complete, however.
- If the Job <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> has not had time to react.
- If the Job controller failed to create Pods for any reason (lack of `ResourceQuota`, lack of permission, etc.),
  then there may be fewer pods than requested.
- The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.
- When a Pod is gracefully shut down, it takes time to stop.
-->
<ul>
<li>对于 <em>确定完成计数</em> Job，实际上并行执行的 Pods 个数不会超出剩余的完成数。
如果 <code>.spec.parallelism</code> 值较高，会被忽略。</li>
<li>对于 <em>工作队列</em> Job，有任何 Job 成功结束之后，不会有新的 Pod 启动。
不过，剩下的 Pods 允许执行完毕。</li>
<li>如果 Job <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a> 没有来得及作出响应，或者</li>
<li>如果 Job 控制器因为任何原因（例如，缺少 <code>ResourceQuota</code> 或者没有权限）无法创建 Pods。
Pods 个数可能比请求的数目小。</li>
<li>Job 控制器可能会因为之前同一 Job 中 Pod 失效次数过多而压制新 Pod 的创建。</li>
<li>当 Pod 处于体面终止进程中，需要一定时间才能停止。</li>
</ul>
<!--
### Completion mode
-->
<h3 id="completion-mode">完成模式  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
Jobs with _fixed completion count_ - that is, jobs that have non null
`.spec.completions` - can have a completion mode that is specified in `.spec.completionMode`:
-->
<p>带有 <em>确定完成计数</em> 的 Job，即 <code>.spec.completions</code> 不为 null 的 Job，
都可以在其 <code>.spec.completionMode</code> 中设置完成模式：</p>
<!--
- `NonIndexed` (default): the Job is considered complete when there have been
  `.spec.completions` successfully completed Pods. In other words, each Pod
  completion is homologous to each other. Note that Jobs that have null
  `.spec.completions` are implicitly `NonIndexed`.
- `Indexed`: the Pods of a Job get an associated completion index from 0 to
  `.spec.completions-1`. The index is available through three mechanisms:
  - The Pod annotation `batch.kubernetes.io/job-completion-index`.
  - As part of the Pod hostname, following the pattern `$(job-name)-$(index)`.
    When you use an Indexed Job in combination with a
    <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a>, Pods within the Job can use
    the deterministic hostnames to address each other via DNS.
  - From the containarized task, in the environment variable `JOB_COMPLETION_INDEX`.

  The Job is considered complete when there is one successfully completed Pod
  for each index. For more information about how to use this mode, see
  [Indexed Job for Parallel Processing with Static Work Assignment](/docs/tasks/job/indexed-parallel-processing-static/).
  Note that, although rare, more than one Pod could be started for the same
  index, but only one of them will count towards the completion count.
-->
<ul>
<li>
<p><code>NonIndexed</code>（默认值）：当成功完成的 Pod 个数达到 <code>.spec.completions</code> 所
设值时认为 Job 已经完成。换言之，每个 Job 完成事件都是独立无关且同质的。
要注意的是，当 <code>.spec.completions</code> 取值为 null 时，Job 被隐式处理为 <code>NonIndexed</code>。</p>
</li>
<li>
<p><code>Indexed</code>：Job 的 Pod 会获得对应的完成索引，取值为 0 到 <code>.spec.completions-1</code>。
该索引可以通过三种方式获取：</p>
<ul>
<li>Pod 注解 <code>batch.kubernetes.io/job-completion-index</code>。</li>
<li>作为 Pod 主机名的一部分，遵循模式 <code>$(job-name)-$(index)</code>。
当你同时使用带索引的 Job（Indexed Job）与 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a>，
Job 中的 Pods 可以通过 DNS 使用确切的主机名互相寻址。</li>
<li>对于容器化的任务，在环境变量 <code>JOB_COMPLETION_INDEX</code> 中。</li>
</ul>
<p>当每个索引都对应一个完成完成的 Pod 时，Job 被认为是已完成的。
关于如何使用这种模式的更多信息，可参阅
<a href="/zh/docs/tasks/job/indexed-parallel-processing-static/">用带索引的 Job 执行基于静态任务分配的并行处理</a>。
需要注意的是，对同一索引值可能被启动的 Pod 不止一个，尽管这种情况很少发生。
这时，只有一个会被记入完成计数中。</p>
</li>
</ul>
<!--
## Handling Pod and container failures

A container in a Pod may fail for a number of reasons, such as because the process in it exited with
a non-zero exit code, or the container was killed for exceeding a memory limit, etc.  If this
happens, and the `.spec.template.spec.restartPolicy = "OnFailure"`, then the Pod stays
on the node, but the container is re-run.  Therefore, your program needs to handle the case when it is
restarted locally, or else specify `.spec.template.spec.restartPolicy = "Never"`.
See [pod lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/#example-states) for more information on `restartPolicy`.
-->
<h2 id="处理-pod-和容器失效">处理 Pod 和容器失效</h2>
<p>Pod 中的容器可能因为多种不同原因失效，例如因为其中的进程退出时返回值非零，
或者容器因为超出内存约束而被杀死等等。
如果发生这类事件，并且 <code>.spec.template.spec.restartPolicy = &quot;OnFailure&quot;</code>，
Pod 则继续留在当前节点，但容器会被重新运行。
因此，你的程序需要能够处理在本地被重启的情况，或者要设置
<code>.spec.template.spec.restartPolicy = &quot;Never&quot;</code>。
关于 <code>restartPolicy</code> 的更多信息，可参阅
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#example-states">Pod 生命周期</a>。</p>
<!--
An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node
(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the
`.spec.template.spec.restartPolicy = "Never"`.  When a Pod fails, then the Job controller
starts a new Pod.  This means that your application needs to handle the case when it is restarted in a new
pod.  In particular, it needs to handle temporary files, locks, incomplete output and the like
caused by previous runs.
-->
<p>整个 Pod 也可能会失败，且原因各不相同。
例如，当 Pod 启动时，节点失效（被升级、被重启、被删除等）或者其中的容器失败而
<code>.spec.template.spec.restartPolicy = &quot;Never&quot;</code>。
当 Pod 失败时，Job 控制器会启动一个新的 Pod。
这意味着，你的应用需要处理在一个新 Pod 中被重启的情况。
尤其是应用需要处理之前运行所产生的临时文件、锁、不完整的输出等问题。</p>
<!--
Note that even if you specify `.spec.parallelism = 1` and `.spec.completions = 1` and
`.spec.template.spec.restartPolicy = "Never"`, the same program may
sometimes be started twice.

If you do specify `.spec.parallelism` and `.spec.completions` both greater than 1, then there may be
multiple pods running at once.  Therefore, your pods must also be tolerant of concurrency.
-->
<p>注意，即使你将 <code>.spec.parallelism</code> 设置为 1，且将 <code>.spec.completions</code> 设置为
1，并且 <code>.spec.template.spec.restartPolicy</code> 设置为 &quot;Never&quot;，同一程序仍然有可能被启动两次。</p>
<p>如果你确实将 <code>.spec.parallelism</code> 和 <code>.spec.completions</code> 都设置为比 1 大的值，
那就有可能同时出现多个 Pod 运行的情况。
为此，你的 Pod 也必须能够处理并发性问题。</p>
<!--
### Pod backoff failure policy

There are situations where you want to fail a Job after some amount of retries
due to a logical error in configuration etc.
To do so, set `.spec.backoffLimit` to specify the number of retries before
considering a Job as failed. The back-off limit is set by default to 6. Failed
Pods associated with the Job are recreated by the Job controller with an
exponential back-off delay (10s, 20s, 40s ...) capped at six minutes. The
back-off count is reset when a Job's Pod is deleted or successful without any
other Pods for the Job failing around that time.
-->
<h3 id="pod-回退失效策略">Pod 回退失效策略</h3>
<p>在有些情形下，你可能希望 Job 在经历若干次重试之后直接进入失败状态，因为这很
可能意味着遇到了配置错误。
为了实现这点，可以将 <code>.spec.backoffLimit</code> 设置为视 Job 为失败之前的重试次数。
失效回退的限制值默认为 6。
与 Job 相关的失效的 Pod 会被 Job 控制器重建，回退重试时间将会按指数增长
（从 10 秒、20 秒到 40 秒）最多至 6 分钟。
当 Job 的 Pod 被删除时，或者 Pod 成功时没有其它 Pod 处于失败状态，失效回退的次数也会被重置（为 0）。</p>
<!--
If your job has `restartPolicy = "OnFailure"`, keep in mind that your Pod running the Job
will be terminated once the job backoff limit has been reached. This can make debugging the Job's executable more difficult. We suggest setting
`restartPolicy = "Never"` when debugging the Job or using a logging system to ensure output
from failed Jobs is not lost inadvertently.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果你的 Job 的 <code>restartPolicy</code> 被设置为 &quot;OnFailure&quot;，就要注意运行该 Job 的 Pod
会在 Job 到达失效回退次数上限时自动被终止。
这会使得调试 Job 中可执行文件的工作变得非常棘手。
我们建议在调试 Job 时将 <code>restartPolicy</code> 设置为 &quot;Never&quot;，
或者使用日志系统来确保失效 Jobs 的输出不会意外遗失。
</div>
<!--
## Job termination and cleanup

When a Job completes, no more Pods are created, but the Pods are [usually](#pod-backoff-failure-policy) not deleted either.
Keeping them around
allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.
The job object also remains after it is completed so that you can view its status.  It is up to the user to delete
old jobs after noting their status.  Delete the job with `kubectl` (e.g. `kubectl delete jobs/pi` or `kubectl delete -f ./job.yaml`). When you delete the job using `kubectl`, all the pods it created are deleted too.
-->
<h2 id="job-终止与清理">Job 终止与清理</h2>
<p>Job 完成时不会再创建新的 Pod，不过已有的 Pod <a href="#pod-backoff-failure-policy">通常</a>也不会被删除。
保留这些 Pod 使得你可以查看已完成的 Pod 的日志输出，以便检查错误、警告
或者其它诊断性输出。
Job 完成时 Job 对象也一样被保留下来，这样你就可以查看它的状态。
在查看了 Job 状态之后删除老的 Job 的操作留给了用户自己。
你可以使用 <code>kubectl</code> 来删除 Job（例如，<code>kubectl delete jobs/pi</code>
或者 <code>kubectl delete -f ./job.yaml</code>）。
当使用 <code>kubectl</code> 来删除 Job 时，该 Job 所创建的 Pods 也会被删除。</p>
<!--
By default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`) or a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the
`.spec.backoffLimit` described above. Once `.spec.backoffLimit` has been reached the Job will be marked as failed and any running Pods will be terminated.

Another way to terminate a Job is by setting an active deadline.
Do this by setting the `.spec.activeDeadlineSeconds` field of the Job to a number of seconds.
The `activeDeadlineSeconds` applies to the duration of the job, no matter how many Pods are created.
Once a Job reaches `activeDeadlineSeconds`, all of its running Pods are terminated and the Job status will become `type: Failed` with `reason: DeadlineExceeded`.
-->
<p>默认情况下，Job 会持续运行，除非某个 Pod 失败（<code>restartPolicy=Never</code>）
或者某个容器出错退出（<code>restartPolicy=OnFailure</code>）。
这时，Job 基于前述的 <code>spec.backoffLimit</code> 来决定是否以及如何重试。
一旦重试次数到达 <code>.spec.backoffLimit</code> 所设的上限，Job 会被标记为失败，
其中运行的 Pods 都会被终止。</p>
<p>终止 Job 的另一种方式是设置一个活跃期限。
你可以为 Job 的 <code>.spec.activeDeadlineSeconds</code> 设置一个秒数值。
该值适用于 Job 的整个生命期，无论 Job 创建了多少个 Pod。
一旦 Job 运行时间达到 <code>activeDeadlineSeconds</code> 秒，其所有运行中的 Pod
都会被终止，并且 Job 的状态更新为 <code>type: Failed</code>
及 <code>reason: DeadlineExceeded</code>。</p>
<!--
Note that a Job's `.spec.activeDeadlineSeconds` takes precedence over its `.spec.backoffLimit`. Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it reaches the time limit specified by `activeDeadlineSeconds`, even if the `backoffLimit` is not yet reached.

Example:
-->
<p>注意 Job 的 <code>.spec.activeDeadlineSeconds</code> 优先级高于其 <code>.spec.backoffLimit</code> 设置。
因此，如果一个 Job 正在重试一个或多个失效的 Pod，该 Job 一旦到达
<code>activeDeadlineSeconds</code> 所设的时限即不再部署额外的 Pod，即使其重试次数还未
达到 <code>backoffLimit</code> 所设的限制。</p>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi-with-timeout<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">backoffLimit</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">activeDeadlineSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>perl<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;perl&#34;</span>,<span style="color:#bbb">  </span><span style="color:#b44">&#34;-Mbignum=bpi&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-wle&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;print bpi(2000)&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div><!--
Note that both the Job spec and the [Pod template spec](/docs/concepts/workloads/pods/init-containers/#detailed-behavior) within the Job have an `activeDeadlineSeconds` field. Ensure that you set this field at the proper level.

Keep in mind that the `restartPolicy` applies to the Pod, and not to the Job itself: there is no automatic Job restart once the Job status is `type: Failed`.
That is, the Job termination mechanisms activated with `.spec.activeDeadlineSeconds` and `.spec.backoffLimit` result in a permanent Job failure that requires manual intervention to resolve.
-->
<p>注意 Job 规约和 Job 中的
<a href="/zh/docs/concepts/workloads/pods/init-containers/#detailed-behavior">Pod 模版规约</a>
都有 <code>activeDeadlineSeconds</code> 字段。
请确保你在合适的层次设置正确的字段。</p>
<p>还要注意的是，<code>restartPolicy</code> 对应的是 Pod，而不是 Job 本身：
一旦 Job 状态变为 <code>type: Failed</code>，就不会再发生 Job 重启的动作。
换言之，由 <code>.spec.activeDeadlineSeconds</code> 和 <code>.spec.backoffLimit</code> 所触发的 Job 终结机制
都会导致 Job 永久性的失败，而这类状态都需要手工干预才能解决。</p>
<!--
## Clean up finished jobs automatically

Finished Jobs are usually no longer needed in the system. Keeping them around in
the system will put pressure on the API server. If the Jobs are managed directly
by a higher level controller, such as
[CronJobs](/docs/concepts/workloads/controllers/cron-jobs/), the Jobs can be
cleaned up by CronJobs based on the specified capacity-based cleanup policy.

### TTL mechanism for finished Jobs
-->
<h2 id="clean-up-finished-jobs-automatically">自动清理完成的 Job  </h2>
<p>完成的 Job 通常不需要留存在系统中。在系统中一直保留它们会给 API
服务器带来额外的压力。
如果 Job 由某种更高级别的控制器来管理，例如
<a href="/zh/docs/concepts/workloads/controllers/cron-jobs/">CronJobs</a>，
则 Job 可以被 CronJob 基于特定的根据容量裁定的清理策略清理掉。</p>
<h3 id="ttl-mechanisms-for-finished-jobs">已完成 Job 的 TTL 机制 </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code>
</div>


<!--
Another way to clean up finished Jobs (either `Complete` or `Failed`)
automatically is to use a TTL mechanism provided by a
[TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) for
finished resources, by specifying the `.spec.ttlSecondsAfterFinished` field of
the Job.

When the TTL controller cleans up the Job, it will delete the Job cascadingly,
i.e. delete its dependent objects, such as Pods, together with the Job. Note
that when the Job is deleted, its lifecycle guarantees, such as finalizers, will
be honored.

For example:
-->
<p>自动清理已完成 Job （状态为 <code>Complete</code> 或 <code>Failed</code>）的另一种方式是使用由
<a href="/zh/docs/concepts/workloads/controllers/ttlafterfinished/">TTL 控制器</a>所提供
的 TTL 机制。
通过设置 Job 的 <code>.spec.ttlSecondsAfterFinished</code> 字段，可以让该控制器清理掉
已结束的资源。</p>
<p>TTL 控制器清理 Job 时，会级联式地删除 Job 对象。
换言之，它会删除所有依赖的对象，包括 Pod 及 Job 本身。
注意，当 Job 被删除时，系统会考虑其生命周期保障，例如其 Finalizers。</p>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi-with-ttl<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ttlSecondsAfterFinished</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>perl<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;perl&#34;</span>,<span style="color:#bbb">  </span><span style="color:#b44">&#34;-Mbignum=bpi&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-wle&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;print bpi(2000)&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div><!--
The Job `pi-with-ttl` will be eligible to be automatically deleted, `100`
seconds after it finishes.

If the field is set to `0`, the Job will be eligible to be automatically deleted
immediately after it finishes. If the field is unset, this Job won't be cleaned
up by the TTL controller after it finishes.
-->
<p>Job <code>pi-with-ttl</code> 在结束 100 秒之后，可以成为被自动删除的对象。</p>
<p>如果该字段设置为 <code>0</code>，Job 在结束之后立即成为可被自动删除的对象。
如果该字段没有设置，Job 不会在结束之后被 TTL 控制器自动清除。</p>
<!--
## Job patterns

The Job object can be used to support reliable parallel execution of Pods.  The Job object is not
designed to support closely-communicating parallel processes, as commonly found in scientific
computing.  It does support parallel processing of a set of independent but related *work items*.
These might be emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in a
NoSQL database to scan, and so on.
-->
<h2 id="job-patterns">Job 模式 </h2>
<p>Job 对象可以用来支持多个 Pod 的可靠的并发执行。
Job 对象不是设计用来支持相互通信的并行进程的，后者一般在科学计算中应用较多。
Job 的确能够支持对一组相互独立而又有所关联的 <em>工作条目</em> 的并行处理。
这类工作条目可能是要发送的电子邮件、要渲染的视频帧、要编解码的文件、NoSQL
数据库中要扫描的主键范围等等。</p>
<!--
In a complex system, there may be multiple different sets of work items.  Here we are just
considering one set of work items that the user wants to manage together &mdash; a *batch job*.

There are several different patterns for parallel computation, each with strengths and weaknesses.
The tradeoffs are:
-->
<p>在一个复杂系统中，可能存在多个不同的工作条目集合。这里我们仅考虑用户希望一起管理的
工作条目集合之一 — <em>批处理作业</em>。</p>
<p>并行计算的模式有好多种，每种都有自己的强项和弱点。这里要权衡的因素有：</p>
<!--
- One Job object for each work item, vs. a single Job object for all work items.  The latter is
  better for large numbers of work items.  The former creates some overhead for the user and for the
  system to manage large numbers of Job objects.
- Number of pods created equals number of work items, vs. each Pod can process multiple work items.
  The former typically requires less modification to existing code and containers.  The latter
  is better for large numbers of work items, for similar reasons to the previous bullet.
- Several approaches use a work queue.  This requires running a queue service,
  and modifications to the existing program or container to make it use the work queue.
  Other approaches are easier to adapt to an existing containerised application.
-->
<ul>
<li>每个工作条目对应一个 Job 或者所有工作条目对应同一 Job 对象。
后者更适合处理大量工作条目的场景；
前者会给用户带来一些额外的负担，而且需要系统管理大量的 Job 对象。</li>
<li>创建与工作条目相等的 Pod 或者令每个 Pod 可以处理多个工作条目。
前者通常不需要对现有代码和容器做较大改动；
后者则更适合工作条目数量较大的场合，原因同上。</li>
<li>有几种技术都会用到工作队列。这意味着需要运行一个队列服务，并修改现有程序或容器
使之能够利用该工作队列。
与之比较，其他方案在修改现有容器化应用以适应需求方面可能更容易一些。</li>
</ul>
<!--
The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.
The pattern names are also links to examples and more detailed description.
-->
<p>下面是对这些权衡的汇总，列 2 到 4 对应上面的权衡比较。
模式的名称对应了相关示例和更详细描述的链接。</p>
<table>
<thead>
<tr>
<th>模式</th>
<th style="text-align:center">单个 Job 对象</th>
<th style="text-align:center">Pods 数少于工作条目数？</th>
<th style="text-align:center">直接使用应用无需修改?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="/zh/docs/tasks/job/coarse-parallel-processing-work-queue/">每工作条目一 Pod 的队列</a></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center">有时</td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/fine-parallel-processing-work-queue/">Pod 数量可变的队列</a></td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/indexed-parallel-processing-static">静态任务分派的带索引的 Job</a></td>
<td style="text-align:center">✓</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/parallel-processing-expansion/">Job 模版扩展</a></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
</tbody>
</table>
<!--
When you specify completions with `.spec.completions`, each Pod created by the Job controller
has an identical [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).  This means that
all pods for a task will have the same command line and the same
image, the same volumes, and (almost) the same environment variables.  These patterns
are different ways to arrange for pods to work on different things.

This table shows the required settings for `.spec.parallelism` and `.spec.completions` for each of the patterns.
Here, `W` is the number of work items.
-->
<p>当你使用 <code>.spec.completions</code> 来设置完成数时，Job 控制器所创建的每个 Pod
使用完全相同的 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>spec</code></a>。
这意味着任务的所有 Pod 都有相同的命令行，都使用相同的镜像和数据卷，甚至连
环境变量都（几乎）相同。
这些模式是让每个 Pod 执行不同工作的几种不同形式。</p>
<p>下表显示的是每种模式下 <code>.spec.parallelism</code> 和 <code>.spec.completions</code> 所需要的设置。
其中，<code>W</code> 表示的是工作条目的个数。</p>
<table>
<thead>
<tr>
<th>模式</th>
<th style="text-align:center"><code>.spec.completions</code></th>
<th style="text-align:center"><code>.spec.parallelism</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="/zh/docs/tasks/job/coarse-parallel-processing-work-queue/">每工作条目一 Pod 的队列</a></td>
<td style="text-align:center">W</td>
<td style="text-align:center">任意值</td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/fine-parallel-processing-work-queue/">Pod 个数可变的队列</a></td>
<td style="text-align:center">1</td>
<td style="text-align:center">任意值</td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/indexed-parallel-processing-static">静态任务分派的带索引的 Job</a></td>
<td style="text-align:center">W</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td><a href="/zh/docs/tasks/job/parallel-processing-expansion/">Job 模版扩展</a></td>
<td style="text-align:center">1</td>
<td style="text-align:center">应该为 1</td>
</tr>
</tbody>
</table>
<!--
## Advanced usage

### Suspending a Job
-->
<h2 id="advanced-usage">高级用法  </h2>
<h3 id="suspending-a-job">挂起 Job  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In Kubernetes version 1.21, this feature was in alpha, which required additional
steps to enable this feature; make sure to read the [right documentation for the
version of Kubernetes you're using](/docs/home/supported-doc-versions/).
-->
<p>该特性在 Kubernetes 1.21 版本中是 Alpha 阶段，启用该特性需要额外的步骤；
请确保你正在阅读<a href="/zh/docs/home/supported-doc-versions/">与集群版本一致的文档</a>。
</div>
<!--
When a Job is created, the Job controller will immediately begin creating Pods
to satisfy the Job's requirements and will continue to do so until the Job is
complete. However, you may want to temporarily suspend a Job's execution and
resume it later, or start Jobs in suspended state and have a custom controller
decide later when to start them.
-->
<p>Job 被创建时，Job 控制器会马上开始执行 Pod 创建操作以满足 Job 的需求，
并持续执行此操作直到 Job 完成为止。
不过你可能想要暂时挂起 Job 执行，或启动处于挂起状态的job，
并拥有一个自定义控制器以后再决定什么时候开始。</p>
<!-- 
To suspend a Job, you can update the `.spec.suspend` field of
the Job to true; later, when you want to resume it again, update it to false.
Creating a Job with `.spec.suspend` set to true will create it in the suspended
state.
-->
<p>要挂起一个 Job，你可以更新 <code>.spec.suspend</code> 字段为 true，
之后，当你希望恢复其执行时，将其更新为 false。
创建一个 <code>.spec.suspend</code> 被设置为 true 的 Job 本质上会将其创建为被挂起状态。</p>
<!--
When a Job is resumed from suspension, its `.status.startTime` field will be
reset to the current time. This means that the `.spec.activeDeadlineSeconds`
timer will be stopped and reset when a Job is suspended and resumed.
-->
<p>当 Job 被从挂起状态恢复执行时，其 <code>.status.startTime</code> 字段会被重置为
当前的时间。这意味着 <code>.spec.activeDeadlineSeconds</code> 计时器会在 Job 挂起时
被停止，并在 Job 恢复执行时复位。</p>
<!--
Remember that suspending a Job will delete all active Pods. When the Job is
suspended, your [Pods will be terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)
with a SIGTERM signal. The Pod's graceful termination period will be honored and
your Pod must handle this signal in this period. This may involve saving
progress for later or undoing changes. Pods terminated this way will not count
towards the Job's `completions` count.
-->
<p>要记住的是，挂起 Job 会删除其所有活跃的 Pod。当 Job 被挂起时，你的 Pod 会
收到 SIGTERM 信号而被<a href="/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">终止</a>。
Pod 的体面终止期限会被考虑，不过 Pod 自身也必须在此期限之内处理完信号。
处理逻辑可能包括保存进度以便将来恢复，或者取消已经做出的变更等等。
Pod 以这种形式终止时，不会被记入 Job 的 <code>completions</code> 计数。</p>
<!--
An example Job definition in the suspended state can be like so:
-->
<p>处于被挂起状态的 Job 的定义示例可能是这样子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get job myjob -o yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myjob<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">suspend</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">parallelism</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">completions</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>...<span style="color:#bbb">
</span></code></pre></div><!--
The Job's status can be used to determine if a Job is suspended or has been
suspended in the past:
-->
<p>Job 的 <code>status</code> 可以用来确定 Job 是否被挂起，或者曾经被挂起。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get jobs/myjob -o yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="">apiVersion:</span> <span style="">batch/v</span><span style="color:#666">1</span>
<span style="">kind:</span> <span style="">Job</span>
<span style="">#</span> <span style="">.metadata</span> <span style="">and</span> <span style="">.spec</span> <span style="">omitted</span>
<span style="">status:</span>
  <span style="">conditions:</span>
  <span style="">-</span> <span style="">lastProbeTime:</span> <span style="color:#b44">&#34;2021-02-05T13:14:33Z&#34;</span>
    <span style="">lastTransitionTime:</span> <span style="color:#b44">&#34;2021-02-05T13:14:33Z&#34;</span>
    <span style="">status:</span> <span style="color:#b44">&#34;True&#34;</span>
    <span style="">type:</span> <span style="">Suspended</span>
  <span style="">startTime:</span> <span style="color:#b44">&#34;2021-02-05T13:13:48Z&#34;</span>
</code></pre></div><!--
The Job condition of type "Suspended" with status "True" means the Job is
suspended; the `lastTransitionTime` field can be used to determine how long the
Job has been suspended for. If the status of that condition is "False", then the
Job was previously suspended and is now running. If such a condition does not
exist in the Job's status, the Job has never been stopped.

Events are also created when the Job is suspended and resumed:
-->
<p>Job 的 &quot;Suspended&quot; 类型的状况在状态值为 &quot;True&quot; 时意味着 Job 正被
挂起；<code>lastTransitionTime</code> 字段可被用来确定 Job 被挂起的时长。
如果此状况字段的取值为 &quot;False&quot;，则 Job 之前被挂起且现在在运行。
如果 &quot;Suspended&quot; 状况在 <code>status</code> 字段中不存在，则意味着 Job 从未
被停止执行。</p>
<p>当 Job 被挂起和恢复执行时，也会生成事件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe jobs/myjob
</code></pre></div><pre><code>Name:           myjob
...
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl
  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl
  Normal  Suspended         11m   job-controller  Job suspended
  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44
  Normal  Resumed           3s    job-controller  Job resumed
</code></pre><!--
The last four events, particularly the "Suspended" and "Resumed" events, are
directly a result of toggling the `.spec.suspend` field. In the time between
these two events, we see that no Pods were created, but Pod creation restarted
as soon as the Job was resumed.
-->
<p>最后四个事件，特别是 &quot;Suspended&quot; 和 &quot;Resumed&quot; 事件，都是因为 <code>.spec.suspend</code>
字段值被改来改去造成的。在这两个事件之间，我们看到没有 Pod 被创建，不过当
Job 被恢复执行时，Pod 创建操作立即被重启执行。</p>
<!--
### Mutable Scheduling Directives
-->
<h3 id="mutable-scheduling-directives">可变调度指令</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!--
In order to use this behavior, you must enable the `JobMutableNodeSchedulingDirectives`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
on the [API server](/docs/reference/command-line-tools-reference/kube-apiserver/).
It is enabled by default.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 为了使用此功能，你必须在 <a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">API 服务器</a>上启用
<code>JobMutableNodeSchedulingDirectives</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
默认情况下启用。
</div>
<!--
In most cases a parallel job will want the pods to run with constraints, 
like all in the same zone, or all either on GPU model x or y but not a mix of both.
-->
<p>在大多数情况下，并行作业会希望 Pod 在一定约束条件下运行，
比如所有的 Pod 都在同一个区域，或者所有的 Pod 都在 GPU 型号 x 或 y 上，而不是两者的混合。</p>
<!--
The [suspend](#suspending-a-job) field is the first step towards achieving those semantics. Suspend allows a 
custom queue controller to decide when a job should start; However, once a job is unsuspended,
a custom queue controller has no influence on where the pods of a job will actually land.
-->
<p><a href="#suspend-a-job">suspend</a> 字段是实现这些语义的第一步。
suspend 允许自定义队列控制器，以决定工作何时开始；然而，一旦工作被取消暂停，
自定义队列控制器对 Job 中 Pods 的实际放置位置没有影响。</p>
<!--
This feature allows updating a Job's scheduling directives before it starts, which gives custom queue
controllers the ability to influence pod placement while at the same time offloading actual 
pod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never 
been unsuspended before.
-->
<p>此特性允许在 Job 开始之前更新调度指令，从而为定制队列提供影响 Pod
放置的能力，同时将 Pod 与节点间的分配关系留给 kube-scheduler 决定。
这一特性仅适用于之前从未被暂停过的、已暂停的 Job。
控制器能够影响 Pod 放置，同时参考实际
pod-to-node 分配给 kube-scheduler。这仅适用于从未暂停的 Jobs。</p>
<!--
The fields in a Job's pod template that can be updated are node affinity, node selector, 
tolerations, labels and annotations.
-->
<p>Job 的 Pod 模板中可以更新的字段是节点亲和性、节点选择器、容忍、标签和注解。</p>
<!--
### Specifying your own Pod selector {#specifying-your-own-pod-selector}

Normally, when you create a Job object, you do not specify `.spec.selector`.
The system defaulting logic adds this field when the Job is created.
It picks a selector value that will not overlap with any other jobs.

However, in some cases, you might need to override this automatically set selector.
To do this, you can specify the `.spec.selector` of the Job.
-->
<h3 id="specifying-your-own-pod-selector">指定你自己的 Pod 选择算符</h3>
<p>通常，当你创建一个 Job 对象时，你不会设置 <code>.spec.selector</code>。
系统的默认值填充逻辑会在创建 Job 时添加此字段。
它会选择一个不会与任何其他 Job 重叠的选择算符设置。</p>
<p>不过，有些场合下，你可能需要重载这个自动设置的选择算符。
为了实现这点，你可以手动设置 Job 的 <code>spec.selector</code> 字段。</p>
<!--
Be very careful when doing this.  If you specify a label selector which is not
unique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated
job may be deleted, or this Job may count other Pods as completing it, or one or both
Jobs may refuse to create Pods or run to completion.  If a non-unique selector is
chosen, then other controllers (e.g. ReplicationController) and their Pods may behave
in unpredictable ways too.  Kubernetes will not stop you from making a mistake when
specifying `.spec.selector`.
-->
<p>做这个操作时请务必小心。
如果你所设定的标签选择算符并不唯一针对 Job 对应的 Pod 集合，甚或该算符还能匹配
其他无关的 Pod，这些无关的 Job 的 Pod 可能会被删除。
或者当前 Job 会将另外一些 Pod 当作是完成自身工作的 Pods，
又或者两个 Job 之一或者二者同时都拒绝创建 Pod，无法运行至完成状态。
如果所设置的算符不具有唯一性，其他控制器（如 RC 副本控制器）及其所管理的 Pod
集合可能会变得行为不可预测。
Kubernetes 不会在你设置 <code>.spec.selector</code> 时尝试阻止你犯这类错误。</p>
<!--
Here is an example of a case when you might want to use this feature.

Say Job `old` is already running.  You want existing Pods
to keep running, but you want the rest of the Pods it creates
to use a different pod template and for the Job to have a new name.
You cannot update the Job because these fields are not updatable.
Therefore, you delete Job `old` but _leave its pods
running_, using `kubectl delete jobs/old --cascade=orphan`.
Before deleting it, you make a note of what selector it uses:
-->
<p>下面是一个示例场景，在这种场景下你可能会使用刚刚讲述的特性。</p>
<p>假定名为 <code>old</code> 的 Job 已经处于运行状态。
你希望已有的 Pod 继续运行，但你希望 Job 接下来要创建的其他 Pod
使用一个不同的 Pod 模版，甚至希望 Job 的名字也发生变化。
你无法更新现有的 Job，因为这些字段都是不可更新的。
因此，你会删除 <code>old</code> Job，但 <em>允许该 Job 的 Pod 集合继续运行</em>。
这是通过 <code>kubectl delete jobs/old --cascade=orphan</code> 实现的。
在删除之前，我们先记下该 Job 所使用的选择算符。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get job old -o yaml
</code></pre></div><p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>old<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">controller-uid</span>:<span style="color:#bbb"> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!--
Then you create a new Job with name `new` and you explicitly specify the same selector.
Since the existing Pods have label `controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`,
they are controlled by Job `new` as well.

You need to specify `manualSelector: true` in the new Job since you are not using
the selector that the system normally generates for you automatically.
-->
<p>接下来你会创建名为 <code>new</code> 的新 Job，并显式地为其设置相同的选择算符。
由于现有 Pod 都具有标签 <code>controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002</code>，
它们也会被名为 <code>new</code> 的 Job 所控制。</p>
<p>你需要在新 Job 中设置 <code>manualSelector: true</code>，因为你并未使用系统通常自动为你
生成的选择算符。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>new<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">manualSelector</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">controller-uid</span>:<span style="color:#bbb"> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!--
The new Job itself will have a different uid from `a8f3d00d-c6d2-11e5-9f87-42010af00002`.  Setting
`manualSelector: true` tells the system that you know what you are doing and to allow this
mismatch.
-->
<p>新的 Job 自身会有一个不同于 <code>a8f3d00d-c6d2-11e5-9f87-42010af00002</code> 的唯一 ID。
设置 <code>manualSelector: true</code> 是在告诉系统你知道自己在干什么并要求系统允许这种不匹配
的存在。</p>
<!--
### Job tracking with finalizers

In order to use this behavior, you must enable the `JobTrackingWithFinalizers`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
on the [API server](/docs/reference/command-line-tools-reference/kube-apiserver/)
and the [controller manager](/docs/reference/command-line-tools-reference/kube-controller-manager/).
It is enabled by default.

When enabled, the control plane tracks new Jobs using the behavior described
below. Jobs created before the feature was enabled are unaffected. As a user,
the only difference you would see is that the control plane tracking of Job
completion is more accurate.
-->
<h3 id="job-tracking-with-finalizers">使用 Finalizer 追踪 Job  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>要使用该行为，你必须为 <a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">API 服务器</a>
和<a href="/zh/docs/reference/command-line-tools-reference/kube-controller-manager/">控制器管理器</a>
启用 <code>JobTrackingWithFinalizers</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
默认是启用的。</p>
<p>启用后，控制面基于下述行为追踪新的 Job。在启用该特性之前创建的 Job 不受影响。
作为用户，你会看到的唯一区别是控制面对 Job 完成情况的跟踪更加准确。</p>

</div>
<!--
When this feature isn't enabled, the Job <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a>
relies on counting the Pods that exist in the cluster to track the Job status,
that is, to keep the counters for `succeeded` and `failed` Pods.
However, Pods can be removed for a number of reasons, including:
- The garbage collector that removes orphan Pods when a Node goes down.
- The garbage collector that removes finished Pods (in `Succeeded` or `Failed`
  phase) after a threshold.
- Human intervention to delete Pods belonging to a Job.
- An external controller (not provided as part of Kubernetes) that removes or
  replaces Pods.
-->
<p>该功能未启用时，Job <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> 依靠计算集群中存在的 Pod 来跟踪作业状态。
也就是说，维持一个统计 <code>succeeded</code> 和 <code>failed</code> 的 Pod 的计数器。
然而，Pod 可以因为一些原因被移除，包括：</p>
<ul>
<li>当一个节点宕机时，垃圾收集器会删除孤立（Orphan）Pod。</li>
<li>垃圾收集器在某个阈值后删除已完成的 Pod（处于 <code>Succeeded</code> 或 <code>Failed</code> 阶段）。</li>
<li>人工干预删除 Job 的 Pod。</li>
<li>一个外部控制器（不包含于 Kubernetes）来删除或取代 Pod。</li>
</ul>
<!--
If you enable the `JobTrackingWithFinalizers` feature for your cluster, the
control plane keeps track of the Pods that belong to any Job and notices if any
such Pod is removed from the API server. To do that, the Job controller creates Pods with
the finalizer `batch.kubernetes.io/job-tracking`. The controller removes the
finalizer only after the Pod has been accounted for in the Job status, allowing
the Pod to be removed by other controllers or users.

The Job controller uses the new algorithm for new Jobs only. Jobs created
before the feature is enabled are unaffected. You can determine if the Job
controller is tracking a Job using Pod finalizers by checking if the Job has the
annotation `batch.kubernetes.io/job-tracking`. You should **not** manually add
or remove this annotation from Jobs.
-->
<p>如果你为你的集群启用了 <code>JobTrackingWithFinalizers</code> 特性，控制面会跟踪属于任何 Job 的 Pod。
并注意是否有任何这样的 Pod 被从 API 服务器上删除。
为了实现这一点，Job 控制器创建的 Pod 带有 Finalizer <code>batch.kubernetes.io/job-tracking</code>。
控制器只有在 Pod 被记入 Job 状态后才会移除 Finalizer，允许 Pod 可以被其他控制器或用户删除。</p>
<p>Job 控制器只对新的 Job 使用新的算法。在启用该特性之前创建的 Job 不受影响。
你可以根据检查 Job 是否含有 <code>batch.kubernetes.io/job-tracking</code> 注解，来确定 Job 控制器是否正在使用 Pod Finalizer 追踪 Job。
你<strong>不</strong>应该给 Job 手动添加或删除该注解。</p>
<!--
## Alternatives

### Bare Pods

When the node that a Pod is running on reboots or fails, the pod is terminated
and will not be restarted.  However, a Job will create new Pods to replace terminated ones.
For this reason, we recommend that you use a Job rather than a bare Pod, even if your application
requires only a single Pod.
-->
<h2 id="alternatives">替代方案 </h2>
<h3 id="bare-pods">裸 Pod </h3>
<p>当 Pod 运行所在的节点重启或者失败，Pod 会被终止并且不会被重启。
Job 会重新创建新的 Pod 来替代已终止的 Pod。
因为这个原因，我们建议你使用 Job 而不是独立的裸 Pod，
即使你的应用仅需要一个 Pod。</p>
<!--
### Replication Controller

Jobs are complementary to [Replication Controllers](/docs/user-guide/replication-controller).
A Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job
manages Pods that are expected to terminate (e.g. batch tasks).

As discussed in [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/), `Job` is *only* appropriate
for pods with `RestartPolicy` equal to `OnFailure` or `Never`.
(Note: If `RestartPolicy` is not set, the default value is `Always`.)
-->
<h3 id="replication-controller">副本控制器   </h3>
<p>Job 与<a href="/zh/docs/concepts/workloads/controllers/replicationcontroller/">副本控制器</a>是彼此互补的。
副本控制器管理的是那些不希望被终止的 Pod （例如，Web 服务器），
Job 管理的是那些希望被终止的 Pod（例如，批处理作业）。</p>
<p>正如在 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">Pod 生命期</a> 中讨论的，
<code>Job</code> 仅适合于 <code>restartPolicy</code> 设置为 <code>OnFailure</code> 或 <code>Never</code> 的 Pod。
注意：如果 <code>restartPolicy</code> 未设置，其默认值是 <code>Always</code>。</p>
<!--
### Single Job starts controller Pod

Another pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort
of custom controller for those Pods.  This allows the most flexibility, but may be somewhat
complicated to get started with and offers less integration with Kubernetes.
-->
<h3 id="单个-job-启动控制器-pod">单个 Job 启动控制器 Pod</h3>
<p>另一种模式是用唯一的 Job 来创建 Pod，而该 Pod 负责启动其他 Pod，因此扮演了一种
后启动 Pod 的控制器的角色。
这种模式的灵活性更高，但是有时候可能会把事情搞得很复杂，很难入门，
并且与 Kubernetes 的集成度很低。</p>
<!--
One example of this pattern would be a Job which starts a Pod which runs a script that in turn
starts a Spark master controller (see [spark example](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md)), runs a spark
driver, and then cleans up.

An advantage of this approach is that the overall process gets the completion guarantee of a Job
object, but maintains complete control over what Pods are created and how work is assigned to them.
-->
<p>这种模式的实例之一是用 Job 来启动一个运行脚本的 Pod，脚本负责启动 Spark
主控制器（参见 <a href="https://github.com/kubernetes/examples/tree/master/staging/spark/README.md">Spark 示例</a>），
运行 Spark 驱动，之后完成清理工作。</p>
<p>这种方法的优点之一是整个过程得到了 Job 对象的完成保障，
同时维持了对创建哪些 Pod、如何向其分派工作的完全控制能力，</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods).
* Read about different ways of running Jobs:
   * [Coarse Parallel Processing Using a Work Queue](/docs/tasks/job/coarse-parallel-processing-work-queue/)
   * [Fine Parallel Processing Using a Work Queue](/docs/tasks/job/fine-parallel-processing-work-queue/)
   * Use an [indexed Job for parallel processing with static work assignment](/docs/tasks/job/indexed-parallel-processing-static/) (beta)
   * Create multiple Jobs based on a template: [Parallel Processing using Expansions](/docs/tasks/job/parallel-processing-expansion/)
* Follow the links within [Clean up finished jobs automatically](#clean-up-finished-jobs-automatically)
  to learn more about how your cluster can clean up completed and / or failed tasks.
* `Job` is part of the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for jobs.
* Read about [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/), which you
  can use to define a series of Jobs that will run based on a schedule, similar to
  the Unix tool `cron`.
-->
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。</li>
<li>了解运行 Job 的不同的方式：
<ul>
<li><a href="/zh/docs/tasks/job/coarse-parallel-processing-work-queue/">使用工作队列进行粗粒度并行处理</a></li>
<li><a href="/zh/docs/tasks/job/fine-parallel-processing-work-queue/">使用工作队列进行精细的并行处理</a></li>
<li><a href="/zh/docs/tasks/job/indexed-parallel-processing-static/">使用索引作业完成静态工作分配下的并行处理</a>（Beta 阶段）</li>
<li>基于一个模板运行多个 Job：<a href="/zh/docs/tasks/job/parallel-processing-expansion/">使用展开的方式进行并行处理</a></li>
</ul>
</li>
<li>跟随<a href="#clean-up-finished-jobs-automatically">自动清理完成的 Job</a> 文中的链接，了解你的集群如何清理完成和失败的任务。</li>
<li><code>Job</code> 是 Kubernetes REST API 的一部分。阅读 





<a href=""></a>
对象定义理解关于该资源的 API。</li>
<li>阅读 <a href="/zh/docs/concepts/workloads/controllers/cron-jobs/"><code>CronJob</code></a>，它允许你定义一系列定期运行的 Job，类似于 Unix 工具 <code>cron</code>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4de50a37ebb6f2340484192126cb7a04">4.2.6 - 已完成 Job 的自动清理</h1>
    
	<!--
title: Automatic Clean-up for Finished Jobs
content_type: concept
weight: 70
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code>
</div>


<!--
TTL-after-finished <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> provides a 
TTL (time to live) mechanism to limit the lifetime of resource objects that 
have finished execution. TTL controller only handles 
<a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Jobs'>Jobs</a>.
-->
<p>TTL-after-finished <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a> 提供了一种 TTL 机制来限制已完成执行的资源对象的生命周期。
TTL 控制器目前只处理 <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a>。</p>
<!-- body -->
<!--
## TTL-after-finished Controller

The TTL-after-finished controller is only supported for Jobs. A cluster operator can use this feature to clean
up finished Jobs (either `Complete` or `Failed`) automatically by specifying the
`.spec.ttlSecondsAfterFinished` field of a Job, as in this
[example](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).
-->
<h2 id="ttl-after-finished-控制器">TTL-after-finished 控制器</h2>
<p>TTL-after-finished 控制器只支持 Job。集群操作员可以通过指定 Job 的 <code>.spec.ttlSecondsAfterFinished</code>
字段来自动清理已结束的作业（<code>Complete</code> 或 <code>Failed</code>），如
<a href="/zh/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">示例</a>
所示。</p>
<!--
The TTL-after-finished controller will assume that a job is eligible to be cleaned up
TTL seconds after the job has finished, in other words, when the TTL has expired. When the
TTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete
its dependent objects together with it. Note that when the job is deleted,
its lifecycle guarantees, such as finalizers, will be honored.
-->
<p>TTL-after-finished 控制器假设作业能在执行完成后的 TTL 秒内被清理，也就是当 TTL 过期后。
当 TTL 控制器清理作业时，它将做级联删除操作，即删除资源对象的同时也删除其依赖对象。
注意，当资源被删除时，由该资源的生命周期保证其终结器（Finalizers）等被执行。</p>
<!--
The TTL seconds can be set at any time. Here are some examples for setting the
`.spec.ttlSecondsAfterFinished` field of a Job:
-->
<p>可以随时设置 TTL 秒。以下是设置 Job 的 <code>.spec.ttlSecondsAfterFinished</code> 字段的一些示例：</p>
<!--
* Specify this field in the job manifest, so that a Job can be cleaned up
  automatically some time after it finishes.
* Set this field of existing, already finished jobs, to adopt this new feature.
* Use a
  [mutating admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)
  to set this field dynamically at job creation time. Cluster administrators can
  use this to enforce a TTL policy for finished jobs.
* Use a
  [mutating admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)
  to set this field dynamically after the job has finished, and choose
  different TTL values based on job status, labels, etc.
-->
<ul>
<li>在作业清单（manifest）中指定此字段，以便 Job 在完成后的某个时间被自动清除。</li>
<li>将此字段设置为现有的、已完成的作业，以采用此新功能。</li>
<li>在创建作业时使用 <a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">mutating admission webhook</a>
动态设置该字段。集群管理员可以使用它对完成的作业强制执行 TTL 策略。</li>
<li>使用 <a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">mutating admission webhook</a>
在作业完成后动态设置该字段，并根据作业状态、标签等选择不同的 TTL 值。</li>
</ul>
<!--
## Caveat

### Updating TTL Seconds

Note that the TTL period, e.g. `.spec.ttlSecondsAfterFinished` field of Jobs,
can be modified after the job is created or has finished. However, once the
Job becomes eligible to be deleted (when the TTL has expired), the system won't
guarantee that the Jobs will be kept, even if an update to extend the TTL
returns a successful API response.
-->
<h2 id="警告">警告</h2>
<h3 id="更新-ttl-秒数">更新 TTL 秒数</h3>
<p>请注意，在创建 Job 或已经执行结束后，仍可以修改其 TTL 周期，例如 Job 的
<code>.spec.ttlSecondsAfterFinished</code> 字段。
但是一旦 Job 变为可被删除状态（当其 TTL 已过期时），即使您通过 API 增加其 TTL
时长得到了成功的响应，系统也不保证 Job 将被保留。</p>
<!--
### Time Skew

Because TTL-after-finished controller uses timestamps stored in the Kubernetes resources to
determine whether the TTL has expired or not, this feature is sensitive to time
skew in the cluster, which may cause TTL-after-finished controller to clean up resource objects
at the wrong time.
-->
<h3 id="time-skew">时间偏差 </h3>
<p>由于 TTL-after-finished 控制器使用存储在 Kubernetes 资源中的时间戳来确定 TTL 是否已过期，
因此该功能对集群中的时间偏差很敏感，这可能导致 TTL-after-finished 控制器在错误的时间清理资源对象。</p>
<!--
Clocks aren't always correct, but the difference should be
very small. Please be aware of this risk when setting a non-zero TTL.
-->
<p>时钟并不总是如此正确，但差异应该很小。
设置非零 TTL 时请注意避免这种风险。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* [Clean up Jobs automatically](/docs/concepts/workloads/controllers/jobs-run-to-completion/#clean-up-finished-jobs-automatically)
* [Design doc](https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md)
-->
<ul>
<li><a href="/zh/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">自动清理 Job</a></li>
<li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md">设计文档</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2e4cec01c525b45eccd6010e21cc76d9">4.2.7 - CronJob</h1>
    
	<!--
title: CronJob
content_type: concept
weight: 80
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>


<!--
A _CronJob_ creates <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Jobs'>Jobs</a> on a repeating schedule.

One CronJob object is like one line of a _crontab_ (cron table) file. It runs a job periodically
on a given schedule, written in [Cron](https://en.wikipedia.org/wiki/Cron) format.
-->
<p><em>CronJob</em> 创建基于时隔重复调度的 <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Jobs'>Jobs</a>。</p>
<p>一个 CronJob 对象就像 <em>crontab</em> (cron table) 文件中的一行。
它用 <a href="https://en.wikipedia.org/wiki/Cron">Cron</a> 格式进行编写，
并周期性地在给定的调度时间执行 Job。</p>
<!--
All **CronJob** `schedule:` times are based on the timezone of the

If your control plane runs the kube-controller-manager in Pods or bare
containers, the timezone set for the kube-controller-manager container determines the timezone
that the cron job controller uses.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p>所有 <strong>CronJob</strong> 的 <code>schedule:</code> 时间都是基于
<a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>.
的时区。</p>
<p>如果你的控制平面在 Pod 或是裸容器中运行了 kube-controller-manager，
那么为该容器所设置的时区将会决定 Cron Job 的控制器所使用的时区。</p>

</div>

<!--
The [v1 CronJob API](/docs/reference/kubernetes-api/workload-resources/cron-job-v1/)
does not officially support setting timezone as explained above.

Setting variables such as `CRON_TZ` or `TZ` is not officially supported by the Kubernetes project.
`CRON_TZ` or `TZ` is an implementation detail of the internal library being used
for parsing and calculating the next Job creation time. Any usage of it is not
recommended in a production cluster.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p>如 <a href="/zh/docs/reference/kubernetes-api/workload-resources/cron-job-v1/">v1 CronJob API</a> 所述，官方并不支持设置时区。</p>
<p>Kubernetes 项目官方并不支持设置如 <code>CRON_TZ</code> 或者 <code>TZ</code> 等变量。
<code>CRON_TZ</code> 或者 <code>TZ</code> 是用于解析和计算下一个 Job 创建时间所使用的内部库中一个实现细节。
不建议在生产集群中使用它。</p>

</div>

<!--
When creating the manifest for a CronJob resource, make sure the name you provide
is a valid [DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
The name must be no longer than 52 characters. This is because the CronJob controller will automatically
append 11 characters to the job name provided and there is a constraint that the
maximum length of a Job name is no more than 63 characters.
-->
<p>为 CronJob 资源创建清单时，请确保所提供的名称是一个合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>.
名称不能超过 52 个字符。
这是因为 CronJob 控制器将自动在提供的 Job 名称后附加 11 个字符，并且存在一个限制，
即 Job 名称的最大长度不能超过 63 个字符。</p>
<!-- body -->
<!--
## CronJob

CronJobs are meant for performing regular scheduled actions such as backups,
report generation, and so on. Each of those tasks should be configured to recur
indefinitely (for example: once a day / week / month); you can define the point
in time within that interval when the job should start.
-->
<h2 id="cronjob">CronJob</h2>
<p>CronJob 用于执行周期性的动作，例如备份、报告生成等。
这些任务中的每一个都应该配置为周期性重复的（例如：每天/每周/每月一次）；
你可以定义任务开始执行的时间间隔。</p>
<!--
### Example

This example CronJob manifest prints the current time and a hello message every minute:
-->
<h3 id="示例">示例</h3>
<p>下面的 CronJob 示例清单会在每分钟打印出当前时间和问候消息：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/job/cronjob.yaml" download="application/job/cronjob.yaml"><code>application/job/cronjob.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-job-cronjob-yaml')" title="Copy application/job/cronjob.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-job-cronjob-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronJob<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">schedule</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;* * * * *&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">jobTemplate</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- /bin/sh<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- date; echo Hello from the Kubernetes cluster<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<p><a href="/zh/docs/tasks/job/automated-tasks-with-cron-jobs/">使用 CronJob 运行自动化任务</a>
一文会为你详细讲解此例。</p>
<!--
### Cron schedule syntax
-->
<h3 id="cron-时间表语法">Cron 时间表语法</h3>
<pre><code># ┌───────────── 分钟 (0 - 59)
# │ ┌───────────── 小时 (0 - 23)
# │ │ ┌───────────── 月的某天 (1 - 31)
# │ │ │ ┌───────────── 月份 (1 - 12)
# │ │ │ │ ┌───────────── 周的某天 (0 - 6)（周日到周一；在某些系统上，7 也是星期日）
# │ │ │ │ │                          或者是 sun，mon，tue，web，thu，fri，sat
# │ │ │ │ │
# │ │ │ │ │
# * * * * *
</code></pre><!-- 
| Entry 	| Description   | Equivalent to |
| ------------- | ------------- |-------------  |
| @yearly (or @annually) | Run once a year at midnight of 1 January | 0 0 1 1 * |
| @monthly               | Run once a month at midnight of the first day of the month | 0 0 1 * * |
| @weekly                | Run once a week at midnight on Sunday morning | 0 0 * * 0 |
| @daily (or @midnight)  | Run once a day at midnight | 0 0 * * * |
| @hourly                | Run once an hour at the beginning of the hour | 0 * * * * |
-->
<table>
<thead>
<tr>
<th>输入</th>
<th>描述</th>
<th>相当于</th>
</tr>
</thead>
<tbody>
<tr>
<td>@yearly (or @annually)</td>
<td>每年 1 月 1 日的午夜运行一次</td>
<td>0 0 1 1 *</td>
</tr>
<tr>
<td>@monthly</td>
<td>每月第一天的午夜运行一次</td>
<td>0 0 1 * *</td>
</tr>
<tr>
<td>@weekly</td>
<td>每周的周日午夜运行一次</td>
<td>0 0 * * 0</td>
</tr>
<tr>
<td>@daily (or @midnight)</td>
<td>每天午夜运行一次</td>
<td>0 0 * * *</td>
</tr>
<tr>
<td>@hourly</td>
<td>每小时的开始一次</td>
<td>0 * * * *</td>
</tr>
</tbody>
</table>
<!--  
For example, the line below states that the task must be started every Friday at midnight, as well as on the 13th of each month at midnight:
-->
<p>例如，下面这行指出必须在每个星期五的午夜以及每个月 13 号的午夜开始任务：</p>
<p><code>0 0 13 * 5</code></p>
<!--  
To generate CronJob schedule expressions, you can also use web tools like [crontab.guru](https://crontab.guru/).
-->
<p>要生成 CronJob 时间表表达式，你还可以使用 <a href="https://crontab.guru/">crontab.guru</a> 之类的 Web 工具。</p>
<!--
## CronJob Limitations

A cron job creates a job object _about_ once per execution time of its schedule. We say "about" because there
are certain circumstances where two jobs might be created, or no job might be created. We attempt to make these rare,
but do not completely prevent them. Therefore, jobs should be _idempotent_.
-->
<h2 id="cron-job-limitations">CronJob 限制 </h2>
<p>CronJob 根据其计划编排，在每次该执行任务的时候大约会创建一个 Job。
我们之所以说 &quot;大约&quot;，是因为在某些情况下，可能会创建两个 Job，或者不会创建任何 Job。
我们试图使这些情况尽量少发生，但不能完全杜绝。因此，Job 应该是 <em>幂等的</em>。</p>
<!--
If `startingDeadlineSeconds` is set to a large value or left unset (the default)
and if `concurrencyPolicy` is set to `Allow`, the jobs will always run
at least once.
-->
<p>如果 <code>startingDeadlineSeconds</code> 设置为很大的数值或未设置（默认），并且
<code>concurrencyPolicy</code> 设置为 <code>Allow</code>，则作业将始终至少运行一次。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
If `startingDeadlineSeconds` is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.
-->
<p>如果 <code>startingDeadlineSeconds</code> 的设置值低于 10 秒钟，CronJob 可能无法被调度。
这是因为 CronJob 控制器每 10 秒钟执行一次检查。
</div>

<!--
For every CronJob, the CronJob <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a> checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs the error
-->
<p>对于每个 CronJob，CronJob <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器（Controller）'>控制器（Controller）</a>
检查从上一次调度的时间点到现在所错过了调度次数。如果错过的调度次数超过 100 次，
那么它就不会启动这个任务，并记录这个错误:</p>
<pre><code>Cannot determine if job needs to be started. Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
</code></pre><!--
It is important to note that if the `startingDeadlineSeconds` field is set (not `nil`), the controller counts how many missed jobs occurred from the value of `startingDeadlineSeconds` until now rather than from the last scheduled time until now. For example, if `startingDeadlineSeconds` is `200`, the controller counts how many missed jobs occurred in the last 200 seconds.
-->
<p>需要注意的是，如果 <code>startingDeadlineSeconds</code> 字段非空，则控制器会统计从
<code>startingDeadlineSeconds</code> 设置的值到现在而不是从上一个计划时间到现在错过了多少次 Job。
例如，如果 <code>startingDeadlineSeconds</code> 是 <code>200</code>，则控制器会统计在过去 200 秒中错过了多少次 Job。</p>
<!--
A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, If `concurrencyPolicy` is set to `Forbid` and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.
-->
<p>如果未能在调度时间内创建 CronJob，则计为错过。
例如，如果 <code>concurrencyPolicy</code> 被设置为 <code>Forbid</code>，并且当前有一个调度仍在运行的情况下，
试图调度的 CronJob 将被计算为错过。</p>
<!--
For example, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its
`startingDeadlineSeconds` field is not set. If the CronJob controller happens to
be down from `08:29:00` to `10:21:00`, the job will not start as the number of missed jobs which missed their schedule is greater than 100.
-->
<p>例如，假设一个 CronJob 被设置为从 <code>08:30:00</code> 开始每隔一分钟创建一个新的 Job，
并且它的 <code>startingDeadlineSeconds</code> 字段未被设置。如果 CronJob 控制器从
<code>08:29:00</code> 到 <code>10:21:00</code> 终止运行，则该 Job 将不会启动，因为其错过的调度
次数超过了 100。</p>
<!--
To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its
`startingDeadlineSeconds` is set to 200 seconds. If the CronJob controller happens to
be down for the same period as the previous example (`08:29:00` to `10:21:00`,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (ie, 3 missed schedules), rather than from the last scheduled time until now.
-->
<p>为了进一步阐述这个概念，假设将 CronJob 设置为从 <code>08:30:00</code> 开始每隔一分钟创建一个新的 Job，
并将其 <code>startingDeadlineSeconds</code> 字段设置为 200 秒。
如果 CronJob 控制器恰好在与上一个示例相同的时间段（<code>08:29:00</code> 到 <code>10:21:00</code>）终止运行，
则 Job 仍将从 <code>10:22:00</code> 开始。
造成这种情况的原因是控制器现在检查在最近 200 秒（即 3 个错过的调度）中发生了多少次错过的
Job 调度，而不是从现在为止的最后一个调度时间开始。</p>
<!--
The CronJob is only responsible for creating Jobs that match its schedule, and
the Job in turn is responsible for the management of the Pods it represents.
-->
<p>CronJob 仅负责创建与其调度时间相匹配的 Job，而 Job 又负责管理其代表的 Pod。</p>
<!--
## Controller version {#new-controller}

Starting with Kubernetes v1.21 the second version of the CronJob controller
is the default implementation. To disable the default CronJob controller
and use the original CronJob controller instead, one pass the `CronJobControllerV2`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
flag to the <a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>,
and set this flag to `false`. For example:
-->
<h2 id="new-controller">控制器版本  </h2>
<p>从 Kubernetes v1.21 版本开始，CronJob 控制器的第二个版本被用作默认实现。
要禁用此默认 CronJob 控制器而使用原来的 CronJob 控制器，请在
<a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>
中设置<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>CronJobControllerV2</code>，将此标志设置为 <code>false</code>。例如：</p>
<pre><code>--feature-gates=&quot;CronJobControllerV2=false&quot;
</code></pre><h2 id="what-s-next">What's next</h2>
<!--
* Learn about [Pods](/docs/concepts/workloads/pods/) and
  [Jobs](/docs/concepts/workloads/controllers/job/), two concepts
  that CronJobs rely upon.
* Read about the [format](https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format)
  of CronJob `.spec.schedule` fields.
* For instructions on creating and working with CronJobs, and for an example
  of a CronJob manifest,
  see [Running automated tasks with CronJobs](/docs/tasks/job/automated-tasks-with-cron-jobs/).
* For instructions to clean up failed or completed jobs automatically,
  see [Clean up Jobs automatically](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)
* `CronJob` is part of the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for Kubernetes cron jobs.
-->
<ul>
<li>了解 CronJob 所依赖的 <a href="/zh/docs/concepts/workloads/pods/">Pods</a> 与 <a href="/zh/docs/concepts/workloads/controllers/job/">Job</a> 的概念。</li>
<li>阅读 CronJob <code>.spec.schedule</code> 字段的<a href="https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format">格式</a>。</li>
<li>有关创建和使用 CronJob 的说明及示例规约文件，请参见
<a href="/zh/docs/tasks/job/automated-tasks-with-cron-jobs/">使用 CronJob 运行自动化任务</a>。</li>
<li>有关自动清理失败或完成作业的说明，请参阅<a href="/zh/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically">自动清理作业</a></li>
<li><code>CronJob</code> 是 Kubernetes REST API 的一部分，
阅读 





<a href=""></a>
对象定义以了解关于该资源的 API。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-27f1331d515d95f76aa1156088b4ad91">4.2.8 - ReplicationController</h1>
    
	<!--
reviewers:
- bprashanth
- janetkuo
title: ReplicationController
feature:
  title: Self-healing
  anchor: How a ReplicationController Works
  description: >
    Restarts containers that fail, replaces and reschedules containers when nodes die, kills containers that don't respond to your user-defined health check, and doesn't advertise them to clients until they are ready to serve.

content_type: concept
weight: 90
-->
<!-- overview -->
<!--
A [`Deployment`](/docs/concepts/workloads/controllers/deployment/) that configures a [`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is now the recommended way to set up replication.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 现在推荐使用配置 <a href="/zh/docs/concepts/workloads/controllers/replicaset/"><code>ReplicaSet</code></a> 的
<a href="/zh/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> 来建立副本管理机制。
</div>
<!--
A _ReplicationController_ ensures that a specified number of pod replicas are running at any one
time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is
always up and available.
-->
<p><em>ReplicationController</em> 确保在任何时候都有特定数量的 Pod 副本处于运行状态。
换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。</p>
<!-- body -->
<!--
## How a ReplicationController Works

If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the
ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a
ReplicationController are automatically replaced if they fail, are deleted, or are terminated.
For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.
For this reason, you should use a ReplicationController even if your application requires
only a single pod. A ReplicationController is similar to a process supervisor,
but instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods
across multiple nodes.
-->
<h2 id="replicationcontroller-如何工作">ReplicationController 如何工作</h2>
<p>当 Pod 数量过多时，ReplicationController 会终止多余的 Pod。当 Pod 数量太少时，ReplicationController 将会启动新的 Pod。
与手动创建的 Pod 不同，由 ReplicationController 创建的 Pod 在失败、被删除或被终止时会被自动替换。
例如，在中断性维护（如内核升级）之后，你的 Pod 会在节点上重新创建。
因此，即使你的应用程序只需要一个 Pod，你也应该使用 ReplicationController 创建 Pod。
ReplicationController 类似于进程管理器，但是 ReplicationController 不是监控单个节点上的单个进程，而是监控跨多个节点的多个 Pod。</p>
<!--
ReplicationController is often abbreviated to "rc" in discussion, and as a shortcut in
kubectl commands.

A simple case is to create one ReplicationController object to reliably run one instance of
a Pod indefinitely.  A more complex use case is to run several identical replicas of a replicated
service, such as web servers.
-->
<p>在讨论中，ReplicationController 通常缩写为 &quot;rc&quot;，并作为 kubectl 命令的快捷方式。</p>
<p>一个简单的示例是创建一个 ReplicationController 对象来可靠地无限期地运行 Pod 的一个实例。
更复杂的用例是运行一个多副本服务（如 web 服务器）的若干相同副本。</p>
<!--
## Running an example ReplicationController

This example ReplicationController config runs three copies of the nginx web server.
-->
<h2 id="运行一个示例-replicationcontroller">运行一个示例 ReplicationController</h2>
<p>这个示例 ReplicationController 配置运行 nginx Web 服务器的三个副本。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/replication.yaml" download="controllers/replication.yaml"><code>controllers/replication.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-replication-yaml')" title="Copy controllers/replication.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-replication-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ReplicationController<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Run the example job by downloading the example file and then running this command:
-->
<p>通过下载示例文件并运行以下命令来运行示例任务:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/controllers/replication.yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>replicationcontroller/nginx created
</code></pre><!--
Check on the status of the ReplicationController using this command:
-->
<p>使用以下命令检查 ReplicationController 的状态:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe replicationcontrollers/nginx
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &lt;none&gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
</code></pre><!--
Here, three pods are created, but none is running yet, perhaps because the image is being pulled.
A little later, the same command may show:
-->
<p>在这里，创建了三个 Pod，但没有一个 Pod 正在运行，这可能是因为正在拉取镜像。
稍后，相同的命令可能会显示：</p>
<pre><code>Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
</code></pre><!--
To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:
-->
<p>要以机器可读的形式列出属于 ReplicationController 的所有 Pod，可以使用如下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">pods</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get pods --selector<span style="color:#666">=</span><span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx --output<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">={</span>.items..metadata.name<span style="color:#666">}</span><span style="color:#a2f;font-weight:bold">)</span>
<span style="color:#a2f">echo</span> <span style="color:#b8860b">$pods</span>
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre><code>nginx-3ntk0 nginx-4ok8v nginx-qrm3m
</code></pre><!--
Here, the selector is the same as the selector for the ReplicationController (seen in the
`kubectl describe` output), and in a different form in `replication.yaml`.  The `--output=jsonpath` option
specifies an expression with the name from each pod in the returned list.
-->
<p>这里，选择算符与 ReplicationController 的选择算符相同（参见 <code>kubectl describe</code> 输出），并以不同的形式出现在 <code>replication.yaml</code> 中。
<code>--output=jsonpath</code> 选项指定了一个表达式，仅从返回列表中的每个 Pod 中获取名称。</p>
<!--
## Writing a ReplicationController Spec

As with all other Kubernetes config, a ReplicationController needs `apiVersion`, `kind`, and `metadata` fields.
For general information about working with configuration files, see [object management](/docs/concepts/overview/working-with-objects/object-management/).

A ReplicationController also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
-->
<h2 id="编写一个-replicationcontroller-规约">编写一个 ReplicationController 规约</h2>
<p>与所有其它 Kubernetes 配置一样，ReplicationController 需要 <code>apiVersion</code>、
<code>kind</code> 和 <code>metadata</code> 字段。
有关使用配置文件的常规信息，参考
<a href="/zh/docs/concepts/overview/working-with-objects/object-management/">对象管理</a>。</p>
<p>ReplicationController 也需要一个 <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code> 部分</a>。</p>
<!--
### Pod Template

The `.spec.template` is the only required field of the `.spec`.

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/pod-overview/#pod-templates). It has exactly the same schema as a [pod](/docs/concepts/workloads/pods/pod/), except it is nested and does not have an `apiVersion` or `kind`.
-->
<h3 id="pod-template">Pod 模板 </h3>
<p><code>.spec.template</code> 是 <code>.spec</code> 的唯一必需字段。</p>
<p><code>.spec.template</code> 是一个 <a href="/zh/docs/concepts/workloads/pods/#pod-templates">Pod 模板</a>。
它的模式与 <a href="/zh/docs/concepts/workloads/pods/">Pod</a> 完全相同，只是它是嵌套的，没有 <code>apiVersion</code> 或 <code>kind</code> 属性。</p>
<!--
In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [pod selector](#pod-selector).

Only a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is allowed, which is the default if not specified.

For local container restarts, ReplicationControllers delegate to an agent on the node,
for example the [Kubelet](/docs/admin/kubelet/) or Docker.
-->
<p>除了 Pod 所需的字段外，ReplicationController 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。
对于标签，请确保不与其他控制器重叠。参考 <a href="#pod-selector">Pod 选择算符</a>。</p>
<p>只允许 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>.spec.template.spec.restartPolicy</code></a> 等于 <code>Always</code>，如果没有指定，这是默认值。</p>
<p>对于本地容器重启，ReplicationController 委托给节点上的代理，
例如 <a href="/zh/docs/reference/command-line-tools-reference/kubelet/">Kubelet</a> 或 Docker。</p>
<!--
### Labels on the ReplicationController

The ReplicationController can itself have labels (`.metadata.labels`).  Typically, you
would set these the same as the `.spec.template.metadata.labels`; if `.metadata.labels` is not specified
then it defaults to  `.spec.template.metadata.labels`. However, they are allowed to be
different, and the `.metadata.labels` do not affect the behavior of the ReplicationController.
-->
<h3 id="replicationcontroller-上的标签">ReplicationController 上的标签</h3>
<p>ReplicationController 本身可以有标签 （<code>.metadata.labels</code>）。
通常，你可以将这些设置为 <code>.spec.template.metadata.labels</code>；
如果没有指定 <code>.metadata.labels</code> 那么它默认为 <code>.spec.template.metadata.labels</code>。<br>
但是，Kubernetes 允许它们是不同的，<code>.metadata.labels</code> 不会影响 ReplicationController 的行为。</p>
<!--
### Pod Selector

The `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors). A ReplicationController
manages all the pods with labels that match the selector. It does not distinguish
between pods that it created or deleted and pods that another person or process created or
deleted. This allows the ReplicationController to be replaced without affecting the running pods.
-->
<h3 id="pod-selector">Pod 选择算符</h3>
<p><code>.spec.selector</code> 字段是一个<a href="/zh/docs/concepts/overview/working-with-objects/labels/#label-selectors">标签选择算符</a>。
ReplicationController 管理标签与选择算符匹配的所有 Pod。
它不区分它创建或删除的 Pod 和其他人或进程创建或删除的 Pod。
这允许在不影响正在运行的 Pod 的情况下替换 ReplicationController。</p>
<!--
If specified, the `.spec.template.metadata.labels` must be equal to the `.spec.selector`, or it will
be rejected by the API.  If `.spec.selector` is unspecified, it will be defaulted to
`.spec.template.metadata.labels`.
-->
<p>如果指定了 <code>.spec.template.metadata.labels</code>，它必须和 <code>.spec.selector</code> 相同，否则它将被 API 拒绝。
如果没有指定 <code>.spec.selector</code>，它将默认为 <code>.spec.template.metadata.labels</code>。</p>
<!--
Also you should not normally create any pods whose labels match this selector, either directly, with
another ReplicationController, or with another controller such as Job. If you do so, the
ReplicationController thinks that it created the other pods.  Kubernetes does not stop you
from doing this.

If you do end up with multiple controllers that have overlapping selectors, you
will have to manage the deletion yourself (see [below](#working-with-replicationcontrollers)).
-->
<p>另外，通常不应直接使用另一个 ReplicationController 或另一个控制器（例如 Job）
来创建其标签与该选择算符匹配的任何 Pod。如果这样做，ReplicationController 会认为它创建了这些 Pod。
Kubernetes 并没有阻止你这样做。</p>
<p>如果你的确创建了多个控制器并且其选择算符之间存在重叠，那么你将不得不自己管理删除操作（参考<a href="#working-with-replicationcontrollers">后文</a>）。</p>
<!--
### Multiple Replicas

You can specify how many pods should run concurrently by setting `.spec.replicas` to the number
of pods you would like to have running concurrently.  The number running at any time may be higher
or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully
shutdown, and a replacement starts early.

If you do not specify `.spec.replicas`, then it defaults to 1.
-->
<h3 id="多个副本">多个副本</h3>
<p>你可以通过设置 <code>.spec.replicas</code> 来指定应该同时运行多少个 Pod。
在任何时候，处于运行状态的 Pod 个数都可能高于或者低于设定值。例如，副本个数刚刚被增加或减少时，或者一个 Pod 处于优雅终止过程中而其替代副本已经提前开始创建时。</p>
<p>如果你没有指定 <code>.spec.replicas</code> ，那么它默认是 1。</p>
<!--
## Working with ReplicationControllers

### Deleting a ReplicationController and its Pods

To delete a ReplicationController and all its pods, use [`kubectl
delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  Kubectl will scale the ReplicationController to zero and wait
for it to delete each pod before deleting the ReplicationController itself.  If this kubectl
command is interrupted, it can be restarted.

When using the REST API or [client library](/docs/reference/using-api/client-libraries), you need to do the steps explicitly (scale replicas to
0, wait for pod deletions, then delete the ReplicationController).
-->
<h2 id="working-with-replicationcontrollers">使用 ReplicationController</h2>
<h3 id="删除一个-replicationcontroller-以及它的-pod">删除一个 ReplicationController 以及它的 Pod</h3>
<p>要删除一个 ReplicationController 以及它的 Pod，使用
<a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a>。
kubectl 将 ReplicationController 缩放为 0 并等待以便在删除 ReplicationController 本身之前删除每个 Pod。
如果这个 kubectl 命令被中断，可以重新启动它。</p>
<p>当使用 REST API 或<a href="/zh/docs/reference/using-api/client-libraries">客户端库</a>时，你需要明确地执行这些步骤（缩放副本为 0、
等待 Pod 删除，之后删除 ReplicationController 资源）。</p>
<!--
### Deleting only a ReplicationController

You can delete a ReplicationController without affecting any of its pods.

Using kubectl, specify the `--cascade=orphan` option to [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).

When using the REST API or [client library](/docs/reference/using-api/client-libraries), you can delete the ReplicationController object.
-->
<h3 id="只删除-replicationcontroller">只删除 ReplicationController</h3>
<p>你可以删除一个 ReplicationController 而不影响它的任何 Pod。</p>
<p>使用 kubectl，为 <a href="/docs/reference/generated/kubectl/kubectl-commands#delete"><code>kubectl delete</code></a> 指定 <code>--cascade=orphan</code> 选项。</p>
<p>当使用 REST API 或客户端库(/zh/docs/reference/using-api/client-libraries)时，只需删除 ReplicationController 对象。</p>
<!--
Once the original is deleted, you can create a new ReplicationController to replace it.  As long
as the old and new `.spec.selector` are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update pods to a new spec in a controlled way, use a [rolling update](#rolling-updates).
-->
<p>一旦原始对象被删除，你可以创建一个新的 ReplicationController 来替换它。
只要新的和旧的 <code>.spec.selector</code> 相同，那么新的控制器将领养旧的 Pod。
但是，它不会做出任何努力使现有的 Pod 匹配新的、不同的 Pod 模板。
如果希望以受控方式更新 Pod 以使用新的 spec，请执行<a href="#rolling-updates">滚动更新</a>操作。</p>
<!--
### Isolating pods from a ReplicationController

Pods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).
-->
<h3 id="从-replicationcontroller-中隔离-pod">从 ReplicationController 中隔离 Pod</h3>
<p>通过更改 Pod 的标签，可以从 ReplicationController 的目标中删除 Pod。
此技术可用于从服务中删除 Pod 以进行调试、数据恢复等。以这种方式删除的 Pod
将被自动替换（假设复制副本的数量也没有更改）。</p>
<!--
## Common usage patterns
-->
<h2 id="常见的使用模式">常见的使用模式</h2>
<!--
### Rescheduling

As mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).
-->
<h3 id="rescheduling">重新调度  </h3>
<p>如上所述，无论你想要继续运行 1 个 Pod 还是 1000 个 Pod，一个 ReplicationController 都将确保存在指定数量的 Pod，即使在节点故障或 Pod 终止(例如，由于另一个控制代理的操作)的情况下也是如此。</p>
<!--
### Scaling

The ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by updating the `replicas` field.
-->
<h3 id="scaling">扩缩容  </h3>
<p>通过设置 <code>replicas</code> 字段，ReplicationController 可以允许扩容或缩容副本的数量。
你可以手动或通过自动缩放控制代理来控制 ReplicationController 执行此操作。</p>
<!--
### Rolling updates

The ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.

As explained in [#1353](http://issue.k8s.io/1353), the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.
-->
<h3 id="rolling-updates">滚动更新</h3>
<p>ReplicationController 的设计目的是通过逐个替换 Pod 以方便滚动更新服务。</p>
<p>如 <a href="https://issue.k8s.io/1353">#1353</a> PR 中所述，建议的方法是使用 1 个副本创建一个新的 ReplicationController，
逐个扩容新的（+1）和缩容旧的（-1）控制器，然后在旧的控制器达到 0 个副本后将其删除。
这一方法能够实现可控的 Pod 集合更新，即使存在意外失效的状况。</p>
<!--
Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.

The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.

Rolling update is implemented in the client tool
[`kubectl rolling-update`](/docs/reference/generated/kubectl/kubectl-commands#rolling-update). Visit [`kubectl rolling-update` task](/docs/tasks/run-application/rolling-update-replication-controller/) for more concrete examples.
-->
<p>理想情况下，滚动更新控制器将考虑应用程序的就绪情况，并确保在任何给定时间都有足够数量的 Pod 有效地提供服务。</p>
<p>这两个 ReplicationController 将需要创建至少具有一个不同标签的 Pod，比如 Pod 主要容器的镜像标签，因为通常是镜像更新触发滚动更新。</p>
<p>滚动更新是在客户端工具 <a href="/docs/reference/generated/kubectl/kubectl-commands#rolling-update"><code>kubectl rolling-update</code></a>
中实现的。访问 <a href="/zh/docs/tasks/run-application/rolling-update-replication-controller/"><code>kubectl rolling-update</code> 任务</a>以获得更多的具体示例。</p>
<!--
### Multiple release tracks

In addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.

For instance, a service might target all pods with `tier in (frontend), environment in (prod)`.  Now say you have 10 replicated pods that make up this tier.  But you want to be able to 'canary' a new version of this component.  You could set up a ReplicationController with `replicas` set to 9 for the bulk of the replicas, with labels `tier=frontend, environment=prod, track=stable`, and another ReplicationController with `replicas` set to 1 for the canary, with labels `tier=frontend, environment=prod, track=canary`.  Now the service is covering both the canary and non-canary pods.  But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.
-->
<h3 id="多个版本跟踪">多个版本跟踪</h3>
<p>除了在滚动更新过程中运行应用程序的多个版本之外，通常还会使用多个版本跟踪来长时间，
甚至持续运行多个版本。这些跟踪将根据标签加以区分。</p>
<p>例如，一个服务可能把具有 <code>tier in (frontend), environment in (prod)</code> 的所有 Pod 作为目标。
现在假设你有 10 个副本的 Pod 组成了这个层。但是你希望能够 <code>canary</code> （<code>金丝雀</code>）发布这个组件的新版本。
你可以为大部分副本设置一个 ReplicationController，其中 <code>replicas</code> 设置为 9，
标签为 <code>tier=frontend, environment=prod, track=stable</code> 而为 <code>canary</code>
设置另一个 ReplicationController，其中 <code>replicas</code> 设置为 1，
标签为 <code>tier=frontend, environment=prod, track=canary</code>。
现在这个服务覆盖了 <code>canary</code> 和非 <code>canary</code> Pod。但你可以单独处理
ReplicationController，以测试、监控结果等。</p>
<!--
### Using ReplicationControllers with Services

Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic
goes to the old version, and some goes to the new version.

A ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.
-->
<h3 id="和服务一起使用-replicationcontroller">和服务一起使用 ReplicationController</h3>
<p>多个 ReplicationController 可以位于一个服务的后面，例如，一部分流量流向旧版本，
一部分流量流向新版本。</p>
<p>一个 ReplicationController 永远不会自行终止，但它不会像服务那样长时间存活。
服务可以由多个 ReplicationController 控制的 Pod 组成，并且在服务的生命周期内
（例如，为了执行 Pod 更新而运行服务），可以创建和销毁许多 ReplicationController。
服务本身和它们的客户端都应该忽略负责维护服务 Pod 的 ReplicationController 的存在。</p>
<!--
## Writing programs for Replication

Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the [RabbitMQ work queues](https://www.rabbitmq.com/tutorials/tutorial-two-python.html), as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.
-->
<h2 id="编写多副本的应用">编写多副本的应用</h2>
<p>由 ReplicationController 创建的 Pod 是可替换的，语义上是相同的，
尽管随着时间的推移，它们的配置可能会变得异构。
这显然适合于多副本的无状态服务器，但是 ReplicationController 也可以用于维护主选、
分片和工作池应用程序的可用性。
这样的应用程序应该使用动态的工作分配机制，例如
<a href="https://www.rabbitmq.com/tutorials/tutorial-two-python.html">RabbitMQ 工作队列</a>，
而不是静态的或者一次性定制每个 Pod 的配置，这被认为是一种反模式。
执行的任何 Pod 定制，例如资源的垂直自动调整大小（例如，CPU 或内存），
都应该由另一个在线控制器进程执行，这与 ReplicationController 本身没什么不同。</p>
<!--
## Responsibilities of the ReplicationController

The ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, [readiness](http://issue.k8s.io/620) and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.
-->
<h2 id="replicationcontroller-的职责">ReplicationController 的职责</h2>
<p>ReplicationController 仅确保所需的 Pod 数量与其标签选择算符匹配，并且是可操作的。
目前，它的计数中只排除终止的 Pod。
未来，可能会考虑系统提供的<a href="https://issue.k8s.io/620">就绪状态</a>和其他信息，
我们可能会对替换策略添加更多控制，
我们计划发出事件，这些事件可以被外部客户端用来实现任意复杂的替换和/或缩减策略。</p>
<!--
The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in [#492](http://issue.k8s.io/492)), which would change its `replicas` field. We will not add scheduling policies (for example, [spreading](http://issue.k8s.io/367#issuecomment-48428019)) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation ([#170](http://issue.k8s.io/170)).
-->
<p>ReplicationController 永远被限制在这个狭隘的职责范围内。
它本身既不执行就绪态探测，也不执行活跃性探测。
它不负责执行自动缩放，而是由外部自动缩放器控制（如
<a href="https://issue.k8s.io/492">#492</a> 中所述），后者负责更改其 <code>replicas</code> 字段值。
我们不会向 ReplicationController 添加调度策略(例如，
<a href="https://issue.k8s.io/367#issuecomment-48428019">spreading</a>)。
它也不应该验证所控制的 Pod 是否与当前指定的模板匹配，因为这会阻碍自动调整大小和其他自动化过程。
类似地，完成期限、整理依赖关系、配置扩展和其他特性也属于其他地方。
我们甚至计划考虑批量创建 Pod 的机制（查阅 <a href="https://issue.k8s.io/170">#170</a>）。</p>
<!--
The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The "macro" operations currently supported by kubectl (run, scale, rolling-update) are proof-of-concept examples of this. For instance, we could imagine something like [Asgard](https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1) managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.
-->
<p>ReplicationController 旨在成为可组合的构建基元。
我们希望在它和其他补充原语的基础上构建更高级别的 API 或者工具，以便于将来的用户使用。
kubectl 目前支持的 &quot;macro&quot; 操作（运行、缩放、滚动更新）就是这方面的概念示例。
例如，我们可以想象类似于 <a href="https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1">Asgard</a>
的东西管理 ReplicationController、自动定标器、服务、调度策略、金丝雀发布等。</p>
<!--
## API Object

Replication controller is a top-level resource in the Kubernetes REST API. More details about the
API object can be found at:
[ReplicationController API object](/docs/reference/generated/kubernetes-api/v1.23/#replicationcontroller-v1-core).
-->
<h2 id="api-对象">API 对象</h2>
<p>在 Kubernetes REST API 中 Replication controller 是顶级资源。
更多关于 API 对象的详细信息可以在
<a href="/docs/reference/generated/kubernetes-api/v1.23/#replicationcontroller-v1-core">ReplicationController API 对象</a>找到。</p>
<!--
## Alternatives to ReplicationController
### ReplicaSet

[`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is the next-generation ReplicationController that supports the new [set-based label selector](/docs/concepts/overview/working-with-objects/labels/#set-based-requirement).
It’s mainly used by [Deployment](/docs/concepts/workloads/controllers/deployment/) as a mechanism to orchestrate Pod creation, deletion and updates.
Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don’t require updates at all.
-->
<h2 id="replicationcontroller-的替代方案">ReplicationController 的替代方案</h2>
<h3 id="replicaset">ReplicaSet</h3>
<p><a href="/zh/docs/concepts/workloads/controllers/replicaset/"><code>ReplicaSet</code></a> 是下一代 ReplicationController，
支持新的<a href="/zh/docs/concepts/overview/working-with-objects/labels/#set-based-requirement">基于集合的标签选择算符</a>。
它主要被 <a href="/zh/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a>
用来作为一种编排 Pod 创建、删除及更新的机制。
请注意，我们推荐使用 Deployment 而不是直接使用 ReplicaSet，除非
你需要自定义更新编排或根本不需要更新。</p>
<!--
### Deployment (Recommended)

[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is a higher-level API object that updates its underlying Replica Sets and their Pods.
Deployments are recommended if you want the rolling update functionality,
because they are declarative, server-side, and have additional features.
-->
<h3 id="deployment-推荐">Deployment （推荐）</h3>
<p><a href="/zh/docs/concepts/workloads/controllers/deployment/"><code>Deployment</code></a> 是一种更高级别的 API 对象，用于更新其底层 ReplicaSet 及其 Pod。
如果你想要这种滚动更新功能，那么推荐使用 Deployment，因为它们是声明式的、服务端的，并且具有其它特性。</p>
<!--
### Bare Pods

Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node.  A ReplicationController delegates local container restarts to some agent on the node (for example, Kubelet or Docker).
-->
<h3 id="裸-pod">裸 Pod</h3>
<p>与用户直接创建 Pod 的情况不同，ReplicationController 能够替换因某些原因
被删除或被终止的 Pod ，例如在节点故障或中断节点维护的情况下，例如内核升级。
因此，我们建议你使用 ReplicationController，即使你的应用程序只需要一个 Pod。
可以将其看作类似于进程管理器，它只管理跨多个节点的多个 Pod ，而不是单个节点上的单个进程。
ReplicationController 将本地容器重启委托给节点上的某个代理(例如，Kubelet 或 Docker)。</p>
<!--
### Job

Use a [`Job`](/docs/concepts/jobs/run-to-completion-finite-workloads/) instead of a ReplicationController for Pods that are expected to terminate on their own
(that is, batch jobs).
-->
<h3 id="job">Job</h3>
<p>对于预期会自行终止的 Pod (即批处理任务)，使用
<a href="/zh/docs/concepts/workloads/controllers/job/"><code>Job</code></a> 而不是 ReplicationController。</p>
<!--
### DaemonSet

Use a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicationController for pods that provide a
machine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.
-->
<h3 id="daemonset">DaemonSet</h3>
<p>对于提供机器级功能（例如机器监控或机器日志记录）的 Pod，
使用 <a href="/zh/docs/concepts/workloads/controllers/daemonset/"><code>DaemonSet</code></a> 而不是
ReplicationController。
这些 Pod 的生命期与机器的生命期绑定：它们需要在其他 Pod 启动之前在机器上运行，
并且在机器准备重新启动或者关闭时安全地终止。</p>
<!--
## What's next

* Learn about [Pods](/docs/concepts/workloads/pods).
* Learn about [Deployment](/docs/concepts/workloads/controllers/deployment/), the replacement
  for ReplicationController.
* `ReplicationController` is part of the Kubernetes REST API.
  Read the 





<a href=""></a>
  object definition to understand the API for replication controllers.
-->
<h2 id="what-s-next">What's next</h2>
<ul>
<li>了解 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。</li>
<li>了解 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Depolyment</a>，ReplicationController 的替代品。</li>
<li><code>ReplicationController</code> 是 Kubernetes REST API 的一部分，阅读 





<a href=""></a>
对象定义以了解 replication controllers 的 API。</li>
</ul>

</div>



    
	
  

    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0a0a7eca3e302a3c08f8c85e15d337fd">5 - 服务、负载均衡和联网</h1>
    <div class="lead">Kubernetes 网络背后的概念和资源。</div>
	<!--
## The Kubernetes network model

Every [`Pod`](/docs/concepts/workloads/pods/) gets its own IP address.
This means you do not need to explicitly create links between `Pods` and you
almost never need to deal with mapping container ports to host ports.  
This creates a clean, backwards-compatible model where `Pods` can be treated
much like VMs or physical hosts from the perspectives of port allocation,
naming, service discovery, [load balancing](/docs/concepts/services-networking/ingress/#load-balancing), application configuration,
and migration.

Kubernetes imposes the following fundamental requirements on any networking
implementation (barring any intentional network segmentation policies):

* pods on a [node](/docs/concepts/architecture/nodes/) can communicate with all pods on all nodes without NAT
* agents on a node (e.g. system daemons, kubelet) can communicate with all
  pods on that node

Note: For those platforms that support `Pods` running in the host network (e.g.
Linux):

* pods in the host network of a node can communicate with all pods on all
  nodes without NAT
-->
<h2 id="the-kubernetes-network-model">Kubernetes 网络模型  </h2>
<p>每一个 <a href="/zh/docs/concepts/workloads/pods/"><code>Pod</code></a> 都有它自己的IP地址，
这就意味着你不需要显式地在 <code>Pod</code> 之间创建链接， 你几乎不需要处理容器端口到主机端口之间的映射。
这将形成一个干净的、向后兼容的模型；在这个模型里，从端口分配、命名、服务发现、
<a href="/zh/docs/concepts/services-networking/ingress/#load-balancing">负载均衡</a>、应用配置和迁移的角度来看，
<code>Pod</code> 可以被视作虚拟机或者物理主机。</p>
<p>Kubernetes 强制要求所有网络设施都满足以下基本要求（从而排除了有意隔离网络的策略）：</p>
<ul>
<li><a href="/zh/docs/concepts/architecture/nodes/">节点</a>上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信</li>
<li>节点上的代理（比如：系统守护进程、kubelet）可以和节点上的所有 Pod 通信</li>
</ul>
<p>备注：对于支持在主机网络中运行 <code>Pod</code> 的平台（比如：Linux）：</p>
<ul>
<li>运行在节点主机网络里的 Pod 可以不通过 NAT 和所有节点上的 Pod 通信</li>
</ul>
<!--
This model is not only less complex overall, but it is principally compatible
with the desire for Kubernetes to enable low-friction porting of apps from VMs
to containers.  If your job previously ran in a VM, your VM had an IP and could
talk to other VMs in your project.  This is the same basic model.

Kubernetes IP addresses exist at the `Pod` scope - containers within a `Pod`
share their network namespaces - including their IP address and MAC address.
This means that containers within a `Pod` can all reach each other's ports on
`localhost`. This also means that containers within a `Pod` must coordinate port
usage, but this is no different from processes in a VM.  This is called the
"IP-per-pod" model.
-->
<p>这个模型不仅不复杂，而且还和 Kubernetes 的实现从虚拟机向容器平滑迁移的初衷相符，
如果你的任务开始是在虚拟机中运行的，你的虚拟机有一个 IP，
可以和项目中其他虚拟机通信。这里的模型是基本相同的。</p>
<p>Kubernetes 的 IP 地址存在于 <code>Pod</code> 范围内 - 容器共享它们的网络命名空间 - 包括它们的 IP 地址和 MAC 地址。
这就意味着 <code>Pod</code> 内的容器都可以通过 <code>localhost</code> 到达对方端口。
这也意味着 <code>Pod</code> 内的容器需要相互协调端口的使用，但是这和虚拟机中的进程似乎没有什么不同，
这也被称为“一个 Pod 一个 IP”模型。</p>
<!--
How this is implemented is a detail of the particular container runtime in use.

It is possible to request ports on the `Node` itself which forward to your `Pod`
(called host ports), but this is a very niche operation. How that forwarding is
implemented is also a detail of the container runtime. The `Pod` itself is
blind to the existence or non-existence of host ports.
-->
<p>如何实现以上需求是所使用的特定容器运行时的细节。</p>
<p>也可以在 <code>Node</code> 本身请求端口，并用这类端口转发到你的 <code>Pod</code>（称之为主机端口），
但这是一个很特殊的操作。转发方式如何实现也是容器运行时的细节。
<code>Pod</code> 自己并不知道这些主机端口的存在。</p>
<!--
Kubernetes networking addresses four concerns:
- Containers within a Pod [use networking to communicate](/docs/concepts/services-networking/dns-pod-service/) via loopback.
- Cluster networking provides communication between different Pods.
- The [Service resource](/docs/concepts/services-networking/service/) lets you [expose an application running in Pods](/docs/concepts/services-networking/connect-applications-service/) to be reachable from outside your cluster.
- You can also use Services to [publish services only for consumption inside your cluster](/docs/concepts/services-networking/service-traffic-policy/).
-->
<p>Kubernetes 网络解决四方面的问题：</p>
<ul>
<li>一个 Pod 中的容器之间<a href="/zh/docs/concepts/services-networking/dns-pod-service/">通过本地回路（loopback）通信</a>。</li>
<li>集群网络在不同 pod 之间提供通信。</li>
<li><a href="/zh/docs/concepts/services-networking/service/">Service 资源</a>允许你
<a href="/zh/docs/concepts/services-networking/connect-applications-service/">对外暴露 Pods 中运行的应用程序</a>，
以支持来自于集群外部的访问。</li>
<li>可以使用 Services 来<a href="/zh/docs/concepts/services-networking/service-traffic-policy/">发布仅供集群内部使用的服务</a>。</li>
</ul>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3a38878244d862dfdb8d7adb32f77584">5.1 - 使用拓扑键实现拓扑感知的流量路由</h1>
    
	<!--
reviewers:
- johnbelamaric
- imroc
title: Topology-aware traffic routing with topology keys
content_type: concept
weight: 10
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code>
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
This feature, specifically the alpha `topologyKeys` API, is deprecated since
Kubernetes v1.21.
[Topology Aware Hints](/docs/concepts/services-networking/topology-aware-hints/),
introduced in Kubernetes v1.21, provide similar functionality.
-->
<p>此功能特性，尤其是 Alpha 阶段的 <code>topologyKeys</code> API，在 Kubernetes v1.21
版本中已被废弃。Kubernetes v1.21 版本中引入的
<a href="/zh/docs/concepts/services-networking/topology-aware-hints/">拓扑感知的提示</a>,
提供类似的功能。
</div>
<!--
_Service Topology_ enables a service to route traffic based upon the Node
topology of the cluster. For example, a service can specify that traffic be
preferentially routed to endpoints that are on the same Node as the client, or
in the same availability zone.
-->
<p>服务拓扑（Service Topology）可以让一个服务基于集群的 Node 拓扑进行流量路由。
例如，一个服务可以指定流量是被优先路由到一个和客户端在同一个 Node 或者在同一可用区域的端点。</p>
<!-- body -->
<!--
## Topology-aware traffic routing

By default, traffic sent to a `ClusterIP` or `NodePort` Service may be routed to
any backend address for the Service. Kubernetes 1.7 made it possible to
route "external" traffic to the Pods running on the same Node that received the
traffic. For `ClusterIP` Services, the equivalent same-node preference for
routing wasn't possible; nor could you configure your cluster to favor routing
to endpoints within the same zone.
By setting `topologyKeys` on a Service, you're able to define a policy for routing
traffic based upon the Node labels for the originating and destination Nodes.
-->
<h2 id="拓扑感知的流量路由">拓扑感知的流量路由</h2>
<p>默认情况下，发往 <code>ClusterIP</code> 或者 <code>NodePort</code> 服务的流量可能会被路由到
服务的任一后端的地址。Kubernetes 1.7 允许将“外部”流量路由到接收到流量的
节点上的 Pod。对于 <code>ClusterIP</code> 服务，无法完成同节点优先的路由，你也无法
配置集群优选路由到同一可用区中的端点。
通过在 Service 上配置 <code>topologyKeys</code>，你可以基于来源节点和目标节点的
标签来定义流量路由策略。</p>
<!--
The label matching between the source and destination lets you, as a cluster
operator, designate sets of Nodes that are "closer" and "farther" from one another.
You can define labels to represent whatever metric makes sense for your own
requirements.
In public clouds, for example, you might prefer to keep network traffic within the
same zone, because interzonal traffic has a cost associated with it (and intrazonal
traffic typically does not). Other common needs include being able to route traffic
to a local Pod managed by a DaemonSet, or directing traffic to Nodes connected to the
same top-of-rack switch for the lowest latency.
-->
<p>通过对源和目的之间的标签匹配，作为集群操作者的你可以根据节点间彼此“较近”和“较远”
来定义节点集合。你可以基于符合自身需求的任何度量值来定义标签。
例如，在公有云上，你可能更偏向于把流量控制在同一区内，因为区间流量是有费用成本的，
而区内流量则没有。
其它常见需求还包括把流量路由到由 <code>DaemonSet</code> 管理的本地 Pod 上，或者
把将流量转发到连接在同一机架交换机的节点上，以获得低延时。</p>
<!--
## Using Service Topology

If your cluster has the `ServiceTopology`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
enabled, you can control Service traffic
routing by specifying the `topologyKeys` field on the Service spec. This field
is a preference-order list of Node labels which will be used to sort endpoints
when accessing this Service. Traffic will be directed to a Node whose value for
the first label matches the originating Node's value for that label. If there is
no backend for the Service on a matching Node, then the second label will be
considered, and so forth, until no labels remain.

If no match is found, the traffic will be rejected, as if there were no
backends for the Service at all. That is, endpoints are chosen based on the first
topology key with available backends. If this field is specified and all entries
have no backends that match the topology of the client, the service has no
backends for that client and connections should fail. The special value `"*"` may
be used to mean "any topology". This catch-all value, if used, only makes sense
as the last value in the list.
-->
<h2 id="using-service-topology">使用服务拓扑</h2>
<p>如果集群启用了 <code>ServiceTopology</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>，
你就可以在 Service 规约中设定 <code>topologyKeys</code> 字段，从而控制其流量路由。
此字段是 <code>Node</code> 标签的优先顺序字段，将用于在访问这个 <code>Service</code> 时对端点进行排序。
流量会被定向到第一个标签值和源 <code>Node</code> 标签值相匹配的 <code>Node</code>。
如果这个 <code>Service</code> 没有匹配的后端 <code>Node</code>，那么第二个标签会被使用做匹配，
以此类推，直到没有标签。</p>
<p>如果没有匹配到，流量会被拒绝，就如同这个 <code>Service</code> 根本没有后端。
换言之，系统根据可用后端的第一个拓扑键来选择端点。
如果这个字段被配置了而没有后端可以匹配客户端拓扑，那么这个 <code>Service</code>
对那个客户端是没有后端的，链接应该是失败的。
这个字段配置为 <code>&quot;*&quot;</code> 意味着任意拓扑。
这个通配符值如果使用了，那么只有作为配置值列表中的最后一个才有用。</p>
<!--
If `topologyKeys` is not specified or empty, no topology constraints will be applied.

Consider a cluster with Nodes that are labeled with their hostname, zone name,
and region name. Then you can set the `topologyKeys` values of a service to direct
traffic as follows.

* Only to endpoints on the same node, failing if no endpoint exists on the node:
  `["kubernetes.io/hostname"]`.
* Preferentially to endpoints on the same node, falling back to endpoints in the
  same zone, followed by the same region, and failing otherwise: `["kubernetes.io/hostname",
  "topology.kubernetes.io/zone", "topology.kubernetes.io/region"]`.
  This may be useful, for example, in cases where data locality is critical.
* Preferentially to the same zone, but fallback on any available endpoint if
  none are available within this zone:
  `["topology.kubernetes.io/zone", "*"]`.
-->
<p>如果 <code>topologyKeys</code> 没有指定或者为空，就没有启用这个拓扑约束。</p>
<p>一个集群中，其 <code>Node</code> 的标签被打为其主机名，区域名和地区名。
那么就可以设置 <code>Service</code> 的 <code>topologyKeys</code> 的值，像下面的做法一样定向流量了。</p>
<ul>
<li>只定向到同一个 <code>Node</code> 上的端点，<code>Node</code> 上没有端点存在时就失败：
配置 <code>[&quot;kubernetes.io/hostname&quot;]</code>。</li>
<li>偏向定向到同一个 <code>Node</code>  上的端点，回退同一区域的端点上，然后是同一地区，
其它情况下就失败：配置 <code>[&quot;kubernetes.io/hostname&quot;, &quot;topology.kubernetes.io/zone&quot;, &quot;topology.kubernetes.io/region&quot;]</code>。
这或许很有用，例如，数据局部性很重要的情况下。</li>
<li>偏向于同一区域，但如果此区域中没有可用的终结点，则回退到任何可用的终结点：
配置 <code>[&quot;topology.kubernetes.io/zone&quot;, &quot;*&quot;]</code>。</li>
</ul>
<!--
## Constraints

* Service topology is not compatible with `externalTrafficPolicy=Local`, and
  therefore a Service cannot use both of these features. It is possible to use
  both features in the same cluster on different Services, only not on the same
  Service.

* Valid topology keys are currently limited to `kubernetes.io/hostname`,
  `topology.kubernetes.io/zone`, and `topology.kubernetes.io/region`, but will
  be generalized to other node labels in the future.

* Topology keys must be valid label keys and at most 16 keys may be specified.

* The catch-all value, `"*"`, must be the last value in the topology keys, if
  it is used.
-->
<h2 id="constraints">约束条件</h2>
<ul>
<li>
<p>服务拓扑和 <code>externalTrafficPolicy=Local</code> 是不兼容的，所以 <code>Service</code> 不能同时使用这两种特性。
但是在同一个集群的不同 <code>Service</code> 上是可以分别使用这两种特性的，只要不在同一个
<code>Service</code> 上就可以。</p>
</li>
<li>
<p>有效的拓扑键目前只有：<code>kubernetes.io/hostname</code>、<code>topology.kubernetes.io/zone</code> 和
<code>topology.kubernetes.io/region</code>，但是未来会推广到其它的 <code>Node</code> 标签。</p>
</li>
<li>
<p>拓扑键必须是有效的标签，并且最多指定16个。</p>
</li>
<li>
<p>通配符：<code>&quot;*&quot;</code>，如果要用，则必须是拓扑键值的最后一个值。</p>
</li>
</ul>
<!--
## Examples

The following are common examples of using the Service Topology feature.
-->
<h2 id="示例">示例</h2>
<p>以下是使用服务拓扑功能的常见示例。</p>
<!--
### Only Node Local Endpoints

A Service that only routes to node local endpoints. If no endpoints exist on the node, traffic is dropped:
-->
<h3 id="仅节点本地端点">仅节点本地端点</h3>
<p>仅路由到节点本地端点的一种服务。如果节点上不存在端点，流量则被丢弃：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologyKeys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
### Prefer Node Local Endpoints

A Service that prefers node local Endpoints but falls back to cluster wide endpoints if node local endpoints do not exist:
-->
<h3 id="首选节点本地端点">首选节点本地端点</h3>
<p>首选节点本地端点，如果节点本地端点不存在，则回退到集群范围端点的一种服务：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologyKeys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;*&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
### Only Zonal or Regional Endpoints

A Service that prefers zonal then regional endpoints. If no endpoints exist in either, traffic is dropped.
-->
<h3 id="仅地域或区域端点">仅地域或区域端点</h3>
<p>首选地域端点而不是区域端点的一种服务。 如果以上两种范围内均不存在端点，
流量则被丢弃。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologyKeys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;topology.kubernetes.io/zone&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;topology.kubernetes.io/region&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
### Prefer Node Local, Zonal, then Regional Endpoints

A Service that prefers node local, zonal, then regional endpoints but falls back to cluster wide endpoints.
-->
<h3 id="优先选择节点本地端点-地域端点-然后是区域端点">优先选择节点本地端点、地域端点，然后是区域端点</h3>
<p>优先选择节点本地端点，地域端点，然后是区域端点，最后才是集群范围端点的
一种服务。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">topologyKeys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;topology.kubernetes.io/zone&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;topology.kubernetes.io/region&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;*&#34;</span><span style="color:#bbb">
</span></code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
* Read about [enabling Service Topology](/docs/tasks/administer-cluster/enabling-service-topology)
* Read [Connecting Applications with Services](/docs/concepts/services-networking/connect-applications-service/)
-->
<ul>
<li>阅读关于<a href="/zh/docs/tasks/administer-cluster/enabling-service-topology/">启用服务拓扑</a></li>
<li>阅读<a href="/zh/docs/concepts/services-networking/connect-applications-service/">用服务连接应用程序</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5701136fd2ce258047b6ddc389112352">5.2 - 服务</h1>
    
	<!--
title: Service
feature:
  title: Service discovery and load balancing
  description: >
    No need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.

content_type: concept
weight: 10
-->
<!-- overview -->
将运行在一组 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> 上的应用程序公开为网络服务的抽象方法。
<!--
With Kubernetes you don't need to modify your application to use an unfamiliar service discovery mechanism.
Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods,
and can load-balance across them.
-->
<p>使用 Kubernetes，你无需修改应用程序即可使用不熟悉的服务发现机制。
Kubernetes 为 Pods 提供自己的 IP 地址，并为一组 Pod 提供相同的 DNS 名，
并且可以在它们之间进行负载均衡。</p>
<!-- body -->
<!--
## Motivation

Kubernetes <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> are created and destroyed
to match the state of your cluster. Pods are nonpermanent resources.
If you use a <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a> to run your app,
it can create and destroy Pods dynamically.

Each Pod gets its own IP address, however in a Deployment, the set of Pods
running in one moment in time could be different from
the set of Pods running that application a moment later.

This leads to a problem: if some set of Pods (call them "backends") provides
functionality to other Pods (call them "frontends") inside your cluster,
how do the frontends find out and keep track of which IP address to connect
to, so that the frontend can use the backend part of the workload?

Enter _Services_.
-->
<h2 id="动机">动机</h2>
<p>创建和销毁 Kubernetes <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 以匹配集群状态。
Pod 是非永久性资源。
如果你使用 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>
来运行你的应用程序，则它可以动态创建和销毁 Pod。</p>
<p>每个 Pod 都有自己的 IP 地址，但是在 Deployment 中，在同一时刻运行的 Pod 集合可能与稍后运行该应用程序的 Pod 集合不同。</p>
<p>这导致了一个问题： 如果一组 Pod（称为“后端”）为集群内的其他 Pod（称为“前端”）提供功能，
那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用提供工作负载的后端部分？</p>
<p>进入 <em>Services</em>。</p>
<!--
## Service resources {#service-resource}
-->
<h2 id="service-resource">Service 资源</h2>
<!--
In Kubernetes, a Service is an abstraction which defines a logical set of Pods
and a policy by which to access them (sometimes this pattern is called
a micro-service). The set of Pods targeted by a Service is usually determined
by a <a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='selector'>selector</a>.
To learn about other ways to define Service endpoints,
see [Services _without_ selectors](#services-without-selectors).
-->
<p>Kubernetes Service 定义了这样一种抽象：逻辑上的一组 Pod，一种可以访问它们的策略 —— 通常称为微服务。
Service 所针对的 Pods 集合通常是通过<a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='选择算符'>选择算符</a>来确定的。
要了解定义服务端点的其他方法，请参阅<a href="#services-without-selectors">不带选择算符的服务</a>。</p>
<!--
For example, consider a stateless image-processing backend which is running with
3 replicas.  Those replicas are fungible&mdash;frontends do not care which backend
they use.  While the actual Pods that compose the backend set may change, the
frontend clients should not need to be aware of that, nor should they need to keep
track of the set of backends themselves.

The Service abstraction enables this decoupling.
-->
<p>举个例子，考虑一个图片处理后端，它运行了 3 个副本。这些副本是可互换的 ——
前端不需要关心它们调用了哪个后端副本。
然而组成这一组后端程序的 Pod 实际上可能会发生变化，
前端客户端不应该也没必要知道，而且也不需要跟踪这一组后端的状态。</p>
<p>Service 定义的抽象能够解耦这种关联。</p>
<!--
### Cloud-native service discovery

If you're able to use Kubernetes APIs for service discovery in your application,
you can query the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a>
for Endpoints, that get updated whenever the set of Pods in a Service changes.

For non-native applications, Kubernetes offers ways to place a network port or load
balancer in between your application and the backend Pods.
-->
<h3 id="云原生服务发现">云原生服务发现</h3>
<p>如果你想要在应用程序中使用 Kubernetes API 进行服务发现，则可以查询
<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>
的 Endpoints 资源，只要服务中的 Pod 集合发生更改，Endpoints 就会被更新。</p>
<p>对于非本机应用程序，Kubernetes 提供了在应用程序和后端 Pod 之间放置网络端口或负载均衡器的方法。</p>
<!--
## Defining a Service

A Service in Kubernetes is a REST object, similar to a Pod.  Like all of the
REST objects, you can `POST` a Service definition to the API server to create
a new instance.
The name of a Service object must be a valid
[RFC 1035 label name](/docs/concepts/overview/working-with-objects/names#rfc-1035-label-names).

For example, suppose you have a set of Pods where each listens on TCP port 9376
and contains a label `app=MyApp`:
-->
<h2 id="定义-service">定义 Service</h2>
<p>Service 在 Kubernetes 中是一个 REST 对象，和 Pod 类似。
像所有的 REST 对象一样，Service 定义可以基于 <code>POST</code> 方式，请求 API server 创建新的实例。
Service 对象的名称必须是合法的
<a href="/docs/concepts/overview/working-with-objects/names#rfc-1035-label-names">RFC 1035 标签名称</a>.。</p>
<p>例如，假定有一组 Pod，它们对外暴露了 9376 端口，同时还被打上 <code>app=MyApp</code> 标签：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></code></pre></div><!--
This specification creates a new Service object named “my-service”, which
targets TCP port 9376 on any Pod with the `app=MyApp` label.

Kubernetes assigns this Service an IP address (sometimes called the "cluster IP"),
which is used by the Service proxies
(see [Virtual IPs and service proxies](#virtual-ips-and-service-proxies) below).

The controller for the Service selector continuously scans for Pods that
match its selector, and then POSTs any updates to an Endpoint object
also named "my-service".
-->
<p>上述配置创建一个名称为 &quot;my-service&quot; 的 Service 对象，它会将请求代理到使用
TCP 端口 9376，并且具有标签 <code>&quot;app=MyApp&quot;</code> 的 Pod 上。</p>
<p>Kubernetes 为该服务分配一个 IP 地址（有时称为 &quot;集群IP&quot;），该 IP 地址由服务代理使用。
(请参见下面的 <a href="#virtual-ips-and-service-proxies">VIP 和 Service 代理</a>).</p>
<p>服务选择算符的控制器不断扫描与其选择器匹配的 Pod，然后将所有更新发布到也称为
“my-service” 的 Endpoint 对象。</p>
<!--
A Service can map _any_ incoming `port` to a `targetPort`. By default and
for convenience, the `targetPort` is set to the same value as the `port`
field.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 需要注意的是，Service 能够将一个接收 <code>port</code> 映射到任意的 <code>targetPort</code>。
默认情况下，<code>targetPort</code> 将被设置为与 <code>port</code> 字段相同的值。
</div>
<!--
Port definitions in Pods have names, and you can reference these names in the
`targetPort` attribute of a Service. This works even if there is a mixture
of Pods in the Service using a single configured name, with the same network
protocol available via different port numbers.
This offers a lot of flexibility for deploying and evolving your Services.
For example, you can change the port numbers that Pods expose in the next
version of your backend software, without breaking clients.

The default protocol for Services is TCP; you can also use any other
[supported protocol](#protocol-support).

As many Services need to expose more than one port, Kubernetes supports multiple
port definitions on a Service object.
Each port definition can have the same `protocol`, or a different one.
-->
<p>Pod 中的端口定义是有名字的，你可以在服务的 <code>targetPort</code> 属性中引用这些名称。
即使服务中使用单个配置的名称混合使用 Pod，并且通过不同的端口号提供相同的网络协议，此功能也可以使用。
这为部署和发展服务提供了很大的灵活性。
例如，你可以更改 Pods 在新版本的后端软件中公开的端口号，而不会破坏客户端。</p>
<p>服务的默认协议是 TCP；你还可以使用任何其他<a href="#protocol-support">受支持的协议</a>。</p>
<p>由于许多服务需要公开多个端口，因此 Kubernetes 在服务对象上支持多个端口定义。
每个端口定义可以具有相同的 <code>protocol</code>，也可以具有不同的协议。</p>
<!--
### Services without selectors

Services most commonly abstract access to Kubernetes Pods, but they can also
abstract other kinds of backends.
For example:

  * You want to have an external database cluster in production, but in your
    test environment you use your own databases.
  * You want to point your Service to a Service in a different
    <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间（Namespace）'>名字空间（Namespace）</a> or on another cluster.
* You are migrating a workload to Kubernetes. While evaluating the approach,
    you run only a portion of your backends in Kubernetes.

In any of these scenarios you can define a Service _without_ a Pod selector.
For example:
-->
<h3 id="services-without-selectors">没有选择算符的 Service  </h3>
<p>服务最常见的是抽象化对 Kubernetes Pod 的访问，但是它们也可以抽象化其他种类的后端。
实例:</p>
<ul>
<li>希望在生产环境中使用外部的数据库集群，但测试环境使用自己的数据库。</li>
<li>希望服务指向另一个 <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间（Namespace）'>名字空间（Namespace）</a> 中或其它集群中的服务。</li>
<li>你正在将工作负载迁移到 Kubernetes。 在评估该方法时，你仅在 Kubernetes 中运行一部分后端。</li>
</ul>
<p>在任何这些场景中，都能够定义没有选择算符的 Service。
实例:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></code></pre></div><!--
Because this Service has no selector, the corresponding Endpoints object is not
created automatically. You can manually map the Service to the network address and port
where it's running, by adding an Endpoints object manually:
-->
<p>由于此服务没有选择算符，因此不会自动创建相应的 Endpoint 对象。
你可以通过手动添加 Endpoint 对象，将服务手动映射到运行该服务的网络地址和端口：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Endpoints<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">subsets</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">addresses</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">ip</span>:<span style="color:#bbb"> </span><span style="color:#666">192.0.2.42</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></code></pre></div><!--
The name of the Endpoints object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>Endpoints 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
The endpoint IPs _must not_ be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or
link-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6).

Endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services,
because <a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a> doesn't support virtual IPs
as a destination.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>端点 IPs <em>必须不可以</em> 是：本地回路（IPv4 的 127.0.0.0/8, IPv6 的 ::1/128）或
本地链接（IPv4 的 169.254.0.0/16 和 224.0.0.0/24，IPv6 的 fe80::/64)。</p>
<p>端点 IP 地址不能是其他 Kubernetes 服务的集群 IP，因为
<a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a> 不支持将虚拟 IP 作为目标。</p>

</div>
<!--
Accessing a Service without a selector works the same as if it had a selector.
In the example above, traffic is routed to the single endpoint defined in
the YAML: `192.0.2.42:9376` (TCP).
-->
<p>访问没有选择算符的 Service，与有选择算符的 Service 的原理相同。
请求将被路由到用户定义的 Endpoint，YAML 中为：<code>192.0.2.42:9376</code>（TCP）。</p>
<!--
An ExternalName Service is a special case of Service that does not have
selectors and uses DNS names instead. For more information, see the
[ExternalName](#externalname) section later in this document.
-->
<p>ExternalName Service 是 Service 的特例，它没有选择算符，但是使用 DNS 名称。
有关更多信息，请参阅本文档后面的<a href="#externalname">ExternalName</a>。</p>
<!--
### Over Capacity Endpoints

If an Endpoints resource has more than 1000 endpoints then a Kubernetes v1.22 (or later)
cluster annotates that Endpoints with `endpoints.kubernetes.io/over-capacity: truncated`.
This annotation indicates that the affected Endpoints object is over capacity and that
the endpoints controller has truncated the number of endpoints to 1000.
-->
<h3 id="over-capacity-endpoints">超出容量的 Endpoints   </h3>
<p>如果某个 Endpoints 资源中包含的端点个数超过 1000，则 Kubernetes v1.22 版本
（及更新版本）的集群会将为该 Endpoints 添加注解
<code>endpoints.kubernetes.io/over-capacity: truncated</code>。
这一注解表明所影响到的 Endpoints 对象已经超出容量，此外 Endpoints 控制器还会将 Endpoints 对象数量截断到 1000。</p>
<!--
### EndpointSlices
-->
<h3 id="endpointslices">EndpointSlices</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>


<!--
Endpoint Slices are an API resource that can provide a more scalable alternative
to Endpoints. Although conceptually quite similar to Endpoints, Endpoint Slices
allow for distributing network endpoints across multiple resources. By default,
an Endpoint Slice is considered "full" once it reaches 100 endpoints, at which
point additional Endpoint Slices will be created to store any additional
endpoints.

Endpoint Slices provide additional attributes and functionality which is
described in detail in [EndpointSlices](/docs/concepts/services-networking/endpoint-slices/).
-->
<p>EndpointSlices 是一种 API 资源，可以为 Endpoints 提供更可扩展的替代方案。
尽管从概念上讲与 Endpoints 非常相似，但 EndpointSlices 允许跨多个资源分布网络端点。
默认情况下，一旦到达 100 个 Endpoint，该 EndpointSlice 将被视为“已满”，
届时将创建其他 EndpointSlices 来存储任何其他 Endpoints。</p>
<p>EndpointSlices 提供了附加的属性和功能，这些属性和功能在
<a href="/zh/docs/concepts/services-networking/endpoint-slices/">EndpointSlices</a>
中有详细描述。</p>
<!-- 
### Application protocol






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>


The `appProtocol` field provides a way to specify an application protocol for
each Service port. The value of this field is mirrored by the corresponding
Endpoints and EndpointSlice objects.

This field follows standard Kubernetes label syntax. Values should either be
[IANA standard service names](https://www.iana.org/assignments/service-names) or
domain prefixed names such as `mycompany.com/my-custom-protocol`.
-->
<h3 id="application-protocol">应用协议  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>


<p><code>appProtocol</code> 字段提供了一种为每个 Service 端口指定应用协议的方式。
此字段的取值会被映射到对应的 Endpoints 和 EndpointSlices 对象。</p>
<p>该字段遵循标准的 Kubernetes 标签语法。
其值可以是 <a href="https://www.iana.org/assignments/service-names">IANA 标准服务名称</a>
或以域名为前缀的名称，如 <code>mycompany.com/my-custom-protocol</code>。</p>
<!--
## Virtual IPs and service proxies

Every node in a Kubernetes cluster runs a `kube-proxy`. `kube-proxy` is
responsible for implementing a form of virtual IP for `Services` of type other
than [`ExternalName`](#externalname).
-->
<h2 id="virtual-ips-and-service-proxies">虚拟 IP 和 Service 代理</h2>
<p>在 Kubernetes 集群中，每个 Node 运行一个 <code>kube-proxy</code> 进程。
<code>kube-proxy</code> 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是
<a href="#externalname"><code>ExternalName</code></a> 的形式。</p>
<!--
### Why not use round-robin DNS?

A question that pops up every now and then is why Kubernetes relies on
proxying to forward inbound traffic to backends. What about other
approaches? For example, would it be possible to configure DNS records that
have multiple A values (or AAAA for IPv6), and rely on round-robin name
resolution?

There are a few reasons for using proxying for Services:

 * There is a long history of DNS implementations not respecting record TTLs,
   and caching the results of name lookups after they should have expired.
 * Some apps do DNS lookups only once and cache the results indefinitely.
 * Even if apps and libraries did proper re-resolution, the low or zero TTLs
   on the DNS records could impose a high load on DNS that then becomes
   difficult to manage.
-->
<h3 id="为什么不使用-dns-轮询">为什么不使用 DNS 轮询？</h3>
<p>时不时会有人问到为什么 Kubernetes 依赖代理将入站流量转发到后端。那其他方法呢？
例如，是否可以配置具有多个 A 值（或 IPv6 为 AAAA）的 DNS 记录，并依靠轮询名称解析？</p>
<p>使用服务代理有以下几个原因：</p>
<ul>
<li>DNS 实现的历史由来已久，它不遵守记录 TTL，并且在名称查找结果到期后对其进行缓存。</li>
<li>有些应用程序仅执行一次 DNS 查找，并无限期地缓存结果。</li>
<li>即使应用和库进行了适当的重新解析，DNS 记录上的 TTL 值低或为零也可能会给
DNS 带来高负载，从而使管理变得困难。</li>
</ul>
<!--
### User space proxy mode {#proxy-mode-userspace}

In this mode, kube-proxy watches the Kubernetes control plane for the addition and
removal of Service and Endpoint objects. For each Service it opens a
port (randomly chosen) on the local node.  Any connections to this "proxy port"
are proxied to one of the Service's backend Pods (as reported via
Endpoints). kube-proxy takes the `SessionAffinity` setting of the Service into
account when deciding which backend Pod to use.

Lastly, the user-space proxy installs iptables rules which capture traffic to
the Service's `clusterIP` (which is virtual) and `port`. The rules
redirect that traffic to the proxy port which proxies the backend Pod.

By default, kube-proxy in userspace mode chooses a backend via a round-robin algorithm.

![Services overview diagram for userspace proxy](/images/docs/services-userspace-overview.svg)
-->
<h3 id="proxy-mode-userspace">userspace 代理模式</h3>
<p>这种模式，kube-proxy 会监视 Kubernetes 控制平面对 Service 对象和 Endpoints 对象的添加和移除操作。
对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。
任何连接到“代理端口”的请求，都会被代理到 Service 的后端 <code>Pods</code> 中的某个上面（如 <code>Endpoints</code> 所报告的一样）。
使用哪个后端 Pod，是 kube-proxy 基于 <code>SessionAffinity</code> 来确定的。</p>
<p>最后，它配置 iptables 规则，捕获到达该 Service 的 <code>clusterIP</code>（是虚拟 IP）
和 <code>Port</code> 的请求，并重定向到代理端口，代理端口再代理请求到后端Pod。</p>
<p>默认情况下，用户空间模式下的 kube-proxy 通过轮转算法选择后端。</p>
<p><img src="/images/docs/services-userspace-overview.svg" alt="userspace 代理模式下 Service 概览图"></p>
<!--
### `iptables` proxy mode {#proxy-mode-iptables}

In this mode, kube-proxy watches the Kubernetes control plane for the addition and
removal of Service and Endpoint objects. For each Service, it installs
iptables rules, which capture traffic to the Service's `clusterIP` and `port`,
and redirect that traffic to one of the Service's
backend sets.  For each Endpoint object, it installs iptables rules which
select a backend Pod.

By default, kube-proxy in iptables mode chooses a backend at random.

Using iptables to handle traffic has a lower system overhead, because traffic
is handled by Linux netfilter without the need to switch between userspace and the
kernel space. This approach is also likely to be more reliable.

If kube-proxy is running in iptables mode and the first Pod that's selected
does not respond, the connection fails. This is different from userspace
mode: in that scenario, kube-proxy would detect that the connection to the first
Pod had failed and would automatically retry with a different backend Pod.

You can use Pod [readiness probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)
to verify that backend Pods are working OK, so that kube-proxy in iptables mode
only sees backends that test out as healthy. Doing this means you avoid
having traffic sent via kube-proxy to a Pod that's known to have failed.

![Services overview diagram for iptables proxy](/images/docs/services-iptables-overview.svg)
-->
<h3 id="proxy-mode-iptables">iptables 代理模式</h3>
<p>这种模式，<code>kube-proxy</code> 会监视 Kubernetes 控制节点对 Service 对象和 Endpoints 对象的添加和移除。
对每个 Service，它会配置 iptables 规则，从而捕获到达该 Service 的 <code>clusterIP</code>
和端口的请求，进而将请求重定向到 Service 的一组后端中的某个 Pod 上面。
对于每个 Endpoints 对象，它也会配置 iptables 规则，这个规则会选择一个后端组合。</p>
<p>默认的策略是，kube-proxy 在 iptables 模式下随机选择一个后端。</p>
<p>使用 iptables 处理流量具有较低的系统开销，因为流量由 Linux netfilter 处理，
而无需在用户空间和内核空间之间切换。 这种方法也可能更可靠。</p>
<p>如果 kube-proxy 在 iptables 模式下运行，并且所选的第一个 Pod 没有响应，
则连接失败。
这与用户空间模式不同：在这种情况下，kube-proxy 将检测到与第一个 Pod 的连接已失败，
并会自动使用其他后端 Pod 重试。</p>
<p>你可以使用 Pod <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">就绪探测器</a>
验证后端 Pod 可以正常工作，以便 iptables 模式下的 kube-proxy 仅看到测试正常的后端。
这样做意味着你避免将流量通过 kube-proxy 发送到已知已失败的 Pod。</p>
<p><img src="/images/docs/services-iptables-overview.svg" alt="iptables代理模式下Service概览图"></p>
<!--
### IPVS proxy mode {#proxy-mode-ipvs}
-->
<h3 id="proxy-mode-ipvs">IPVS 代理模式</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.11 [stable]</code>
</div>


<!--
In `ipvs` mode, kube-proxy watches Kubernetes Services and Endpoints,
calls `netlink` interface to create IPVS rules accordingly and synchronizes
IPVS rules with Kubernetes Services and Endpoints periodically.
This control loop ensures that IPVS status matches the desired
state.
When accessing a Service, IPVS directs traffic to one of the backend Pods.

The IPVS proxy mode is based on netfilter hook function that is similar to
iptables mode, but uses a hash table as the underlying data structure and works
in the kernel space.
That means kube-proxy in IPVS mode redirects traffic with lower latency than
kube-proxy in iptables mode, with much better performance when synchronising
proxy rules. Compared to the other proxy modes, IPVS mode also supports a
higher throughput of network traffic.

IPVS provides more options for balancing traffic to backend Pods;
these are:
-->
<p>在 <code>ipvs</code> 模式下，kube-proxy 监视 Kubernetes 服务和端点，调用 <code>netlink</code> 接口相应地创建 IPVS 规则，
并定期将 IPVS 规则与 Kubernetes 服务和端点同步。 该控制循环可确保IPVS
状态与所需状态匹配。访问服务时，IPVS 将流量定向到后端Pod之一。</p>
<p>IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数，
但是使用哈希表作为基础数据结构，并且在内核空间中工作。
这意味着，与 iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy
重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。
与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。</p>
<p>IPVS 提供了更多选项来平衡后端 Pod 的流量。 这些是：</p>
<ul>
<li><code>rr</code>：轮替（Round-Robin）</li>
<li><code>lc</code>：最少链接（Least Connection），即打开链接数量最少者优先</li>
<li><code>dh</code>：目标地址哈希（Destination Hashing）</li>
<li><code>sh</code>：源地址哈希（Source Hashing）</li>
<li><code>sed</code>：最短预期延迟（Shortest Expected Delay）</li>
<li><code>nq</code>：从不排队（Never Queue）</li>
</ul>
<!--
To run kube-proxy in IPVS mode, you must make IPVS available on
the node before starting kube-proxy.

When kube-proxy starts in IPVS proxy mode, it verifies whether IPVS
kernel modules are available. If the IPVS kernel modules are not detected, then kube-proxy
falls back to running in iptables proxy mode.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>要在 IPVS 模式下运行 kube-proxy，必须在启动 kube-proxy 之前使 IPVS 在节点上可用。</p>
<p>当 kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。
如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。</p>

</div>
<!--
![Services overview diagram for IPVS proxy](/images/docs/services-ipvs-overview.svg)

In these proxy models, the traffic bound for the Service's IP:Port is
proxied to an appropriate backend without the clients knowing anything
about Kubernetes or Services or Pods.

If you want to make sure that connections from a particular client
are passed to the same Pod each time, you can select the session affinity based
on the client's IP addresses by setting `service.spec.sessionAffinity` to "ClientIP"
(the default is "None").
You can also set the maximum session sticky time by setting
`service.spec.sessionAffinityConfig.clientIP.timeoutSeconds` appropriately.
(the default value is 10800, which works out to be 3 hours).
-->
<p><img src="/images/docs/services-ipvs-overview.svg" alt="IPVS代理的 Services 概述图"></p>
<p>在这些代理模型中，绑定到服务 IP 的流量：
在客户端不了解 Kubernetes 或服务或 Pod 的任何信息的情况下，将 Port 代理到适当的后端。</p>
<p>如果要确保每次都将来自特定客户端的连接传递到同一 Pod，
则可以通过将 <code>service.spec.sessionAffinity</code> 设置为 &quot;ClientIP&quot;
（默认值是 &quot;None&quot;），来基于客户端的 IP 地址选择会话关联。
你还可以通过适当设置 <code>service.spec.sessionAffinityConfig.clientIP.timeoutSeconds</code>
来设置最大会话停留时间。
（默认值为 10800 秒，即 3 小时）。</p>
<!--
## Multi-Port Services

For some Services, you need to expose more than one port.
Kubernetes lets you configure multiple port definitions on a Service object.
When using multiple ports for a Service, you must give all of your ports names
so that these are unambiguous.
For example:
-->
<h2 id="multi-port-services">多端口 Service  </h2>
<p>对于某些服务，你需要公开多个端口。
Kubernetes 允许你在 Service 对象上配置多个端口定义。
为服务使用多个端口时，必须提供所有端口名称，以使它们无歧义。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>https<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">443</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9377</span><span style="color:#bbb">
</span></code></pre></div><!--
As with Kubernetes <a class='glossary-tooltip' title='客户端提供的字符串，用来指代资源 URL 中的对象，如 /api/v1/pods/some-name。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/names/' target='_blank' aria-label='names'>names</a> in general, names for ports
must only contain lowercase alphanumeric characters and `-`. Port names must
also start and end with an alphanumeric character.

For example, the names `123-abc` and `web` are valid, but `123_abc` and `-web` are not.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>与一般的Kubernetes名称一样，端口名称只能包含小写字母数字字符 和 <code>-</code>。
端口名称还必须以字母数字字符开头和结尾。</p>
<p>例如，名称 <code>123-abc</code> 和 <code>web</code> 有效，但是 <code>123_abc</code> 和 <code>-web</code> 无效。</p>

</div>
<!--
## Choosing your own IP address

You can specify your own cluster IP address as part of a `Service` creation
request.  To do this, set the `.spec.clusterIP` field. For example, if you
already have an existing DNS entry that you wish to reuse, or legacy systems
that are configured for a specific IP address and difficult to re-configure.

The IP address that you choose must be a valid IPv4 or IPv6 address from within the
`service-cluster-ip-range` CIDR range that is configured for the API server.
If you try to create a Service with an invalid clusterIP address value, the API
server will return a 422 HTTP status code to indicate that there's a problem.
-->
<h2 id="选择自己的-ip-地址">选择自己的 IP 地址</h2>
<p>在 <code>Service</code> 创建的请求中，可以通过设置 <code>spec.clusterIP</code> 字段来指定自己的集群 IP 地址。
比如，希望替换一个已经已存在的 DNS 条目，或者遗留系统已经配置了一个固定的 IP 且很难重新配置。</p>
<p>用户选择的 IP 地址必须合法，并且这个 IP 地址在 <code>service-cluster-ip-range</code> CIDR 范围内，
这对 API 服务器来说是通过一个标识来指定的。
如果 IP 地址不合法，API 服务器会返回 HTTP 状态码 422，表示值不合法。</p>
<!--
## Traffic policies
-->
<h2 id="traffic-policies">流量策略 </h2>
<!--
### External traffic policy
-->
<h3 id="external-traffic-policy">外部流量策略   </h3>
<!--
You can set the `spec.externalTrafficPolicy` field to control how traffic from external sources is routed.
Valid values are `Cluster` and `Local`. Set the field to `Cluster` to route external traffic to all ready endpoints
and `Local` to only route to ready node-local endpoints. If the traffic policy is `Local` and there are are no node-local
endpoints, the kube-proxy does not forward any traffic for the relevant Service.
-->
<p>你可以通过设置 <code>spec.externalTrafficPolicy</code> 字段来控制来自于外部的流量是如何路由的。
可选值有 <code>Cluster</code> 和 <code>Local</code>。字段设为 <code>Cluster</code> 会将外部流量路由到所有就绪的端点，
设为 <code>Local</code> 会只路由到当前节点上就绪的端点。
如果流量策略设置为 <code>Local</code>，而且当前节点上没有就绪的端点，kube-proxy 不会转发请求相关服务的任何流量。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>
<!--
If you enable the `ProxyTerminatingEndpoints`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`ProxyTerminatingEndpoints` for the kube-proxy, the kube-proxy checks if the node
has local endpoints and whether or not all the local endpoints are marked as terminating.
-->
<p>如果你启用了 kube-proxy 的 <code>ProxyTerminatingEndpoints</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>，
kube-proxy 会检查节点是否有本地的端点，以及是否所有的本地端点都被标记为终止中。</p>
<!--
If there are local endpoints and **all** of those are terminating, then the kube-proxy ignores
any external traffic policy of `Local`. Instead, whilst the node-local endpoints remain as all
terminating, the kube-proxy forwards traffic for that Service to healthy endpoints elsewhere,
as if the external traffic policy were set to `Cluster`.
-->
<p>如果本地有端点，而且所有端点处于终止中的状态，那么 kube-proxy 会忽略任何设为 <code>Local</code> 的外部流量策略。
在所有本地端点处于终止中的状态的同时，kube-proxy 将请求指定服务的流量转发到位于其它节点的
状态健康的端点，如同外部流量策略设为 <code>Cluster</code>。</p>
<!--
This forwarding behavior for terminating endpoints exists to allow external load balancers to
gracefully drain connections that are backed by `NodePort` Services, even when the health check
node port starts to fail. Otherwise, traffic can be lost between the time a node is still in the node pool of a load
balancer and traffic is being dropped during the termination period of a pod.
-->
<p>针对处于正被终止状态的端点这一转发行为使得外部负载均衡器可以优雅地排出由
<code>NodePort</code> 服务支持的连接，就算是健康检查节点端口开始失败也是如此。
否则，当节点还在负载均衡器的节点池内，在 Pod 终止过程中的流量会被丢掉，这些流量可能会丢失。</p>

</div>
<!--
### Internal traffic policy
-->
<h3 id="internal-traffic-policy">内部流量策略   </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
You can set the `spec.internalTrafficPolicy` field to control how traffic from internal sources is routed.
Valid values are `Cluster` and `Local`. Set the field to `Cluster` to route internal traffic to all ready endpoints
and `Local` to only route to ready node-local endpoints. If the traffic policy is `Local` and there are no node-local
endpoints, traffic is dropped by kube-proxy.
-->
<p>你可以设置 <code>spec.internalTrafficPolicy</code> 字段来控制内部来源的流量是如何转发的。可设置的值有 <code>Cluster</code> 和 <code>Local</code>。
将字段设置为 <code>Cluster</code> 会将内部流量路由到所有就绪端点，设置为 <code>Local</code> 只会路由到当前节点上就绪的端点。
如果流量策略是 <code>Local</code>，而且当前节点上没有就绪的端点，那么 kube-proxy 会丢弃流量。</p>
<!--
## Discovering services

Kubernetes supports 2 primary modes of finding a Service - environment
variables and DNS.
-->
<h2 id="discovering-services">服务发现 </h2>
<p>Kubernetes 支持两种基本的服务发现模式 —— 环境变量和 DNS。</p>
<!--
### Environment variables

When a Pod is run on a Node, the kubelet adds a set of environment variables
for each active Service.  It supports both [Docker links
compatible](https://docs.docker.com/userguide/dockerlinks/) variables (see
[makeLinkVariables](https://releases.k8s.io/main/pkg/kubelet/envvars/envvars.go#L49))
and simpler `{SVCNAME}_SERVICE_HOST` and `{SVCNAME}_SERVICE_PORT` variables,
where the Service name is upper-cased and dashes are converted to underscores.

For example, the Service `redis-master` which exposes TCP port 6379 and has been
allocated cluster IP address 10.0.0.11, produces the following environment
variables:
-->
<h3 id="environment-variables">环境变量  </h3>
<p>当 Pod 运行在 <code>Node</code> 上，kubelet 会为每个活跃的 Service 添加一组环境变量。
它同时支持 <a href="https://docs.docker.com/userguide/dockerlinks/">Docker links兼容</a> 变量
（查看 <a href="https://releases.k8s.io/main/pkg/kubelet/envvars/envvars.go#L49">makeLinkVariables</a>）、
简单的 <code>{SVCNAME}_SERVICE_HOST</code> 和 <code>{SVCNAME}_SERVICE_PORT</code> 变量。
这里 Service 的名称需大写，横线被转换成下划线。</p>
<p>举个例子，一个名称为 <code>redis-master</code> 的 Service 暴露了 TCP 端口 6379，
同时给它分配了 Cluster IP 地址 10.0.0.11，这个 Service 生成了如下环境变量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">REDIS_MASTER_SERVICE_HOST</span><span style="color:#666">=</span>10.0.0.11
<span style="color:#b8860b">REDIS_MASTER_SERVICE_PORT</span><span style="color:#666">=</span><span style="color:#666">6379</span>
<span style="color:#b8860b">REDIS_MASTER_PORT</span><span style="color:#666">=</span>tcp://10.0.0.11:6379
<span style="color:#b8860b">REDIS_MASTER_PORT_6379_TCP</span><span style="color:#666">=</span>tcp://10.0.0.11:6379
<span style="color:#b8860b">REDIS_MASTER_PORT_6379_TCP_PROTO</span><span style="color:#666">=</span>tcp
<span style="color:#b8860b">REDIS_MASTER_PORT_6379_TCP_PORT</span><span style="color:#666">=</span><span style="color:#666">6379</span>
<span style="color:#b8860b">REDIS_MASTER_PORT_6379_TCP_ADDR</span><span style="color:#666">=</span>10.0.0.11
</code></pre></div><!--
When you have a Pod that needs to access a Service, and you are using
the environment variable method to publish the port and cluster IP to the client
Pods, you must create the Service *before* the client Pods come into existence.
Otherwise, those client Pods won't have their environment variables populated.

If you only use DNS to discover the cluster IP for a Service, you don't need to
worry about this ordering issue.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>当你具有需要访问服务的 Pod 时，并且你正在使用环境变量方法将端口和集群 IP 发布到客户端
Pod 时，必须在客户端 Pod 出现 <em>之前</em> 创建服务。
否则，这些客户端 Pod 将不会设定其环境变量。</p>
<p>如果仅使用 DNS 查找服务的集群 IP，则无需担心此设定问题。</p>

</div>
<h3 id="dns">DNS</h3>
<!--
You can (and almost always should) set up a DNS service for your Kubernetes
cluster using an [add-on](/docs/concepts/cluster-administration/addons/).

A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new
Services and creates a set of DNS records for each one.  If DNS has been enabled
throughout your cluster then all Pods should automatically be able to resolve
Services by their DNS name.
-->
<p>你可以（几乎总是应该）使用<a href="/zh/docs/concepts/cluster-administration/addons/">附加组件</a>
为 Kubernetes 集群设置 DNS 服务。</p>
<p>支持集群的 DNS 服务器（例如 CoreDNS）监视 Kubernetes API 中的新服务，并为每个服务创建一组 DNS 记录。
如果在整个集群中都启用了 DNS，则所有 Pod 都应该能够通过其 DNS 名称自动解析服务。</p>
<!--
For example, if you have a Service called `my-service` in a Kubernetes
namespace `my-ns`, the control plane and the DNS Service acting together
create a DNS record for `my-service.my-ns`. Pods in the `my-ns` namespace
should be able to find the service by doing a name lookup for `my-service`
(`my-service.my-ns` would also work).

Pods in other Namespaces must qualify the name as `my-service.my-ns`. These names
will resolve to the cluster IP assigned for the Service.
-->
<p>例如，如果你在 Kubernetes 命名空间 <code>my-ns</code> 中有一个名为 <code>my-service</code> 的服务，
则控制平面和 DNS 服务共同为 <code>my-service.my-ns</code> 创建 DNS 记录。
<code>my-ns</code> 命名空间中的 Pod 应该能够通过按名检索 <code>my-service</code> 来找到服务
（<code>my-service.my-ns</code> 也可以工作）。</p>
<p>其他命名空间中的 Pod 必须将名称限定为 <code>my-service.my-ns</code>。
这些名称将解析为为服务分配的集群 IP。</p>
<!--
Kubernetes also supports DNS SRV (Service) records for named ports.  If the
`my-service.my-ns` Service has a port named `http` with the protocol set to
`TCP`, you can do a DNS SRV query for `_http._tcp.my-service.my-ns` to discover
the port number for `http`, as well as the IP address.

The Kubernetes DNS server is the only way to access `ExternalName` Services.
You can find more information about `ExternalName` resolution in
[DNS Pods and Services](/docs/concepts/services-networking/dns-pod-service/).
-->
<p>Kubernetes 还支持命名端口的 DNS SRV（服务）记录。
如果 <code>my-service.my-ns</code> 服务具有名为 <code>http</code>　的端口，且协议设置为 TCP，
则可以对 <code>_http._tcp.my-service.my-ns</code> 执行 DNS SRV 查询查询以发现该端口号,
<code>&quot;http&quot;</code> 以及 IP 地址。</p>
<p>Kubernetes DNS 服务器是唯一的一种能够访问 <code>ExternalName</code> 类型的 Service 的方式。
更多关于 <code>ExternalName</code> 信息可以查看
<a href="/zh/docs/concepts/services-networking/dns-pod-service/">DNS Pod 和 Service</a>。</p>
<!--
## Headless Services  {#headless-services}

Sometimes you don't need load-balancing and a single Service IP.  In
this case, you can create what are termed "headless" Services, by explicitly
specifying `"None"` for the cluster IP (`.spec.clusterIP`).

You can use a headless Service to interface with other service discovery mechanisms,
without being tied to Kubernetes' implementation.

For headless `Services`, a cluster IP is not allocated, kube-proxy does not handle
these Services, and there is no load balancing or proxying done by the platform
for them. How DNS is automatically configured depends on whether the Service has
selectors defined:
-->
<h2 id="headless-services">无头服务（Headless Services） </h2>
<p>有时不需要或不想要负载均衡，以及单独的 Service IP。
遇到这种情况，可以通过指定 Cluster IP（<code>spec.clusterIP</code>）的值为 <code>&quot;None&quot;</code>
来创建 <code>Headless</code> Service。</p>
<p>你可以使用无头 Service 与其他服务发现机制进行接口，而不必与 Kubernetes
的实现捆绑在一起。</p>
<p>对这无头 Service 并不会分配 Cluster IP，kube-proxy 不会处理它们，
而且平台也不会为它们进行负载均衡和路由。
DNS 如何实现自动配置，依赖于 Service 是否定义了选择算符。</p>
<!--
### With selectors

For headless Services that define selectors, the endpoints controller creates
`Endpoints` records in the API, and modifies the DNS configuration to return
A records (IP addresses) that point directly to the `Pods` backing the `Service`.
-->
<h3 id="with-selectors">带选择算符的服务</h3>
<p>对定义了选择算符的无头服务，Endpoint 控制器在 API 中创建了 Endpoints 记录，
并且修改 DNS 配置返回 A 记录（IP 地址），通过这个地址直接到达 <code>Service</code> 的后端 Pod 上。</p>
<!--
### Without selectors

For headless Services that do not define selectors, the endpoints controller does
not create `Endpoints` records. However, the DNS system looks for and configures
either:

* CNAME records for [`ExternalName`](#externalname)-type Services.
* A records for any `Endpoints` that share a name with the Service, for all
  other types.
-->
<h3 id="without-selectors">无选择算符的服务 </h3>
<p>对没有定义选择算符的无头服务，Endpoint 控制器不会创建 <code>Endpoints</code> 记录。
然而 DNS 系统会查找和配置，无论是：</p>
<ul>
<li>对于 <a href="#external-name"><code>ExternalName</code></a> 类型的服务，查找其 CNAME 记录</li>
<li>对所有其他类型的服务，查找与 Service 名称相同的任何 <code>Endpoints</code> 的记录</li>
</ul>
<!--
## Publishing Services (ServiceTypes) {#publishing-services-service-types}

For some parts of your application (for example, frontends) you may want to expose a
Service onto an external IP address, that's outside of your cluster.

Kubernetes `ServiceTypes` allow you to specify what kind of Service you want.
The default is `ClusterIP`.

`Type` values and their behaviors are:
-->
<h2 id="publishing-services-service-types">发布服务（服务类型)     </h2>
<p>对一些应用的某些部分（如前端），可能希望将其暴露给 Kubernetes 集群外部
的 IP 地址。</p>
<p>Kubernetes <code>ServiceTypes</code> 允许指定你所需要的 Service 类型，默认是 <code>ClusterIP</code>。</p>
<p><code>Type</code> 的取值以及行为如下：</p>
<!--
* `ClusterIP`: Exposes the Service on a cluster-internal IP. Choosing this value
  makes the Service only reachable from within the cluster. This is the
  default `ServiceType`.
* [`NodePort`](#type-nodeport): Exposes the Service on each Node's IP at a static port
  (the `NodePort`). A `ClusterIP` Service, to which the `NodePort` Service
  routes, is automatically created.  You'll be able to contact the `NodePort` Service,
  from outside the cluster,
  by requesting `<NodeIP>:<NodePort>`.
* [`LoadBalancer`](#loadbalancer): Exposes the Service externally using a cloud
  provider's load balancer. `NodePort` and `ClusterIP` Services, to which the external
  load balancer routes, are automatically created.
* [`ExternalName`](#externalname): Maps the Service to the contents of the
  `externalName` field (e.g. `foo.bar.example.com`), by returning a `CNAME` record
  with its value. No proxying of any kind is set up.
  <div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> You need either kube-dns version 1.7 or CoreDNS version 0.0.8 or higher to use the <code>ExternalName</code> type.
</div>
-->
<ul>
<li>
<p><code>ClusterIP</code>：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。
这也是默认的 <code>ServiceType</code>。</p>
</li>
<li>
<p><a href="#type-nodeport"><code>NodePort</code></a>：通过每个节点上的 IP 和静态端口（<code>NodePort</code>）暴露服务。
<code>NodePort</code> 服务会路由到自动创建的 <code>ClusterIP</code> 服务。
通过请求 <code>&lt;节点 IP&gt;:&lt;节点端口&gt;</code>，你可以从集群的外部访问一个 <code>NodePort</code> 服务。</p>
</li>
<li>
<p><a href="#loadbalancer"><code>LoadBalancer</code></a>：使用云提供商的负载均衡器向外部暴露服务。
外部负载均衡器可以将流量路由到自动创建的 <code>NodePort</code> 服务和 <code>ClusterIP</code> 服务上。</p>
</li>
<li>
<p><a href="#externalname"><code>ExternalName</code></a>：通过返回 <code>CNAME</code> 和对应值，可以将服务映射到
<code>externalName</code> 字段的内容（例如，<code>foo.bar.example.com</code>）。
无需创建任何类型代理。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你需要使用 kube-dns 1.7 及以上版本或者 CoreDNS 0.0.8 及以上版本才能使用 <code>ExternalName</code> 类型。
</div>
</li>
</ul>
<!--
You can also use [Ingress](/docs/concepts/services-networking/ingress/) to expose your Service. Ingress is not a Service type, but it acts as the entry point for your cluster. It lets you consolidate your routing rules into a single resource as it can expose multiple services under the same IP address.
-->
<p>你也可以使用 <a href="/zh/docs/concepts/services-networking/ingress/">Ingress</a> 来暴露自己的服务。
Ingress 不是一种服务类型，但它充当集群的入口点。
它可以将路由规则整合到一个资源中，因为它可以在同一IP地址下公开多个服务。</p>
<!--
### Type NodePort {#type-nodeport}

If you set the `type` field to `NodePort`, the Kubernetes control plane
allocates a port from a range specified by `--service-node-port-range` flag (default: 30000-32767).
Each node proxies that port (the same port number on every Node) into your Service.
Your Service reports the allocated port in its `.spec.ports[*].nodePort` field.

If you want to specify particular IP(s) to proxy the port, you can set the
`--nodeport-addresses` flag for kube-proxy or the equivalent `nodePortAddresses`
field of the
[kube-proxy configuration file](/docs/reference/config-api/kube-proxy-config.v1alpha1/)
to particular IP block(s).

This flag takes a comma-delimited list of IP blocks (e.g. `10.0.0.0/8`, `192.0.2.0/25`) to specify IP address ranges that kube-proxy should consider as local to this node.
-->
<h3 id="type-nodeport">NodePort 类型 </h3>
<p>如果你将 <code>type</code> 字段设置为 <code>NodePort</code>，则 Kubernetes 控制平面将在
<code>--service-node-port-range</code> 标志指定的范围内分配端口（默认值：30000-32767）。
每个节点将那个端口（每个节点上的相同端口号）代理到你的服务中。
你的服务在其 <code>.spec.ports[*].nodePort</code> 字段中要求分配的端口。</p>
<p>如果你想指定特定的 IP 代理端口，则可以设置 kube-proxy 中的 <code>--nodeport-addresses</code> 参数
或者将<a href="/docs/reference/config-api/kube-proxy-config.v1alpha1/">kube-proxy 配置文件</a>
中的等效 <code>nodePortAddresses</code> 字段设置为特定的 IP 块。
该标志采用逗号分隔的 IP 块列表（例如，<code>10.0.0.0/8</code>、<code>192.0.2.0/25</code>）来指定
kube-proxy 应该认为是此节点本地的 IP 地址范围。</p>
<!--
For example, if you start kube-proxy with the `--nodeport-addresses=127.0.0.0/8` flag, kube-proxy only selects the loopback interface for NodePort Services. The default for `--nodeport-addresses` is an empty list. This means that kube-proxy should consider all available network interfaces for NodePort. (That's also compatible with earlier Kubernetes releases).

If you want a specific port number, you can specify a value in the `nodePort`
field. The control plane will either allocate you that port or report that
the API transaction failed.
This means that you need to take care of possible port collisions yourself.
You also have to use a valid port number, one that's inside the range configured
for NodePort use.
-->
<p>例如，如果你使用 <code>--nodeport-addresses=127.0.0.0/8</code> 标志启动 kube-proxy，
则 kube-proxy 仅选择 NodePort Services 的本地回路接口。
<code>--nodeport-addresses</code> 的默认值是一个空列表。
这意味着 kube-proxy 应该考虑 NodePort 的所有可用网络接口。
（这也与早期的 Kubernetes 版本兼容）。</p>
<p>如果需要特定的端口号，你可以在 <code>nodePort</code> 字段中指定一个值。
控制平面将为你分配该端口或报告 API 事务失败。
这意味着你需要自己注意可能发生的端口冲突。
你还必须使用有效的端口号，该端口号在配置用于 NodePort 的范围内。</p>
<!--
Using a NodePort gives you the freedom to set up your own load balancing solution,
to configure environments that are not fully supported by Kubernetes, or even
to expose one or more nodes' IPs directly.

Note that this Service is visible as `<NodeIP>:spec.ports[*].nodePort`
and `.spec.clusterIP:spec.ports[*].port`.
If the `--nodeport-addresses` flag for kube-proxy or the equivalent field
in the kube-proxy configuration file is set, `<NodeIP>` would be filtered node IP(s).

For example:
-->
<p>使用 NodePort 可以让你自由设置自己的负载均衡解决方案，
配置 Kubernetes 不完全支持的环境，
甚至直接暴露一个或多个节点的 IP。</p>
<p>需要注意的是，Service 能够通过 <code>&lt;NodeIP&gt;:spec.ports[*].nodePort</code> 和
<code>spec.clusterIp:spec.ports[*].port</code> 而对外可见。
如果设置了 kube-proxy 的 <code>--nodeport-addresses</code> 参数或 kube-proxy 配置文件中的等效字段，
<code>&lt;NodeIP&gt;</code> 将被过滤 NodeIP。</p>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>NodePort<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># 默认情况下，为了方便起见，`targetPort` 被设置为与 `port` 字段相同的值。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># 可选字段</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># 默认情况下，为了方便起见，Kubernetes 控制平面会从某个范围内分配一个端口号（默认：30000-32767）</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">nodePort</span>:<span style="color:#bbb"> </span><span style="color:#666">30007</span><span style="color:#bbb">
</span></code></pre></div><!--
### Type LoadBalancer {#loadbalancer}

On cloud providers which support external load balancers, setting the `type`
field to `LoadBalancer` provisions a load balancer for your Service.
The actual creation of the load balancer happens asynchronously, and
information about the provisioned balancer is published in the Service's
`.status.loadBalancer` field.
For example:
-->
<h3 id="loadbalancer">LoadBalancer 类型 </h3>
<p>在使用支持外部负载均衡器的云提供商的服务时，设置 <code>type</code> 的值为 <code>&quot;LoadBalancer&quot;</code>，
将为 Service 提供负载均衡器。
负载均衡器是异步创建的，关于被提供的负载均衡器的信息将会通过 Service 的
<code>status.loadBalancer</code> 字段发布出去。</p>
<p>实例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span><span style="color:#666">10.0.171.239</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">loadBalancer</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ingress</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">ip</span>:<span style="color:#bbb"> </span><span style="color:#666">192.0.2.127</span><span style="color:#bbb">
</span></code></pre></div><!--
Traffic from the external load balancer is directed at the backend Pods. The cloud provider decides how it is load balanced.
-->
<p>来自外部负载均衡器的流量将直接重定向到后端 Pod 上，不过实际它们是如何工作的，这要依赖于云提供商。</p>
<!--
Some cloud providers allow you to specify the `loadBalancerIP`. In those cases, the load-balancer is created
with the user-specified `loadBalancerIP`. If the `loadBalancerIP` field is not specified,
the loadBalancer is set up with an ephemeral IP address. If you specify a `loadBalancerIP`
but your cloud provider does not support the feature, the `loadbalancerIP` field that you
set is ignored.
-->
<p>某些云提供商允许设置 <code>loadBalancerIP</code>。
在这些情况下，将根据用户设置的 <code>loadBalancerIP</code> 来创建负载均衡器。
如果没有设置 <code>loadBalancerIP</code> 字段，将会给负载均衡器指派一个临时 IP。
如果设置了 <code>loadBalancerIP</code>，但云提供商并不支持这种特性，那么设置的
<code>loadBalancerIP</code> 值将会被忽略掉。</p>
<!--
On **Azure**, if you want to use a user-specified public type `loadBalancerIP`, you first need
to create a static type public IP address resource. This public IP address resource should
be in the same resource group of the other automatically created resources of the cluster.
For example, `MC_myResourceGroup_myAKSCluster_eastus`.

Specify the assigned IP address as loadBalancerIP. Ensure that you have updated the securityGroupName in the cloud provider configuration file. For information about troubleshooting `CreatingLoadBalancerFailed` permission issues see, [Use a static IP address with the Azure Kubernetes Service (AKS) load balancer](https://docs.microsoft.com/en-us/azure/aks/static-ip) or [CreatingLoadBalancerFailed on AKS cluster with advanced networking](https://github.com/Azure/AKS/issues/357).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>在 <strong>Azure</strong> 上，如果要使用用户指定的公共类型 <code>loadBalancerIP</code>，则
首先需要创建静态类型的公共 IP 地址资源。
此公共 IP 地址资源应与集群中其他自动创建的资源位于同一资源组中。
例如，<code>MC_myResourceGroup_myAKSCluster_eastus</code>。</p>
<p>将分配的 IP 地址设置为 loadBalancerIP。确保你已更新云提供程序配置文件中的
securityGroupName。
有关对 <code>CreatingLoadBalancerFailed</code> 权限问题进行故障排除的信息，
请参阅 <a href="https://docs.microsoft.com/en-us/azure/aks/static-ip">与 Azure Kubernetes 服务（AKS）负载平衡器一起使用静态 IP 地址</a>
或<a href="https://github.com/Azure/AKS/issues/357">在 AKS 集群上使用高级联网时出现 CreatingLoadBalancerFailed</a>。</p>

</div>
<!--
#### Load balancers with mixed protocol types






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>



By default, for LoadBalancer type of Services, when there is more than one port defined, all
ports must have the same protocol, and the protocol must be one which is supported
by the cloud provider.

If the feature gate `MixedProtocolLBService` is enabled for the kube-apiserver it is allowed to use different protocols when there is more than one port defined.
-->
<h4 id="混合协议类型的负载均衡器">混合协议类型的负载均衡器</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>


<p>默认情况下，对于 LoadBalancer 类型的服务，当定义了多个端口时，所有
端口必须具有相同的协议，并且该协议必须是受云提供商支持的协议。</p>
<p>如果为 kube-apiserver 启用了 <code>MixedProtocolLBService</code> 特性门控，
则当定义了多个端口时，允许使用不同的协议。</p>
<!--
The set of protocols that can be used for LoadBalancer type of Services is still defined by the cloud provider.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 可用于 LoadBalancer 类型服务的协议集仍然由云提供商决定。
</div>
<!--
#### Disabling load balancer NodePort allocation {#load-balancer-nodeport-allocation}
-->
<h3 id="load-balancer-nodeport-allocation">禁用负载均衡器节点端口分配</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>


<!--
Starting in v1.20, you can optionally disable node port allocation for a Service Type=LoadBalancer by setting
the field `spec.allocateLoadBalancerNodePorts` to `false`. This should only be used for load balancer implementations
that route traffic directly to pods as opposed to using node ports. By default, `spec.allocateLoadBalancerNodePorts`
is `true` and type LoadBalancer Services will continue to allocate node ports. If `spec.allocateLoadBalancerNodePorts`
is set to `false` on an existing Service with allocated node ports, those node ports will NOT be de-allocated automatically.
You must explicitly remove the `nodePorts` entry in every Service port to de-allocate those node ports.
You must enable the `ServiceLBNodePortControl` feature gate to use this field.
-->
<p>从 v1.20 版本开始， 你可以通过设置 <code>spec.allocateLoadBalancerNodePorts</code> 为 <code>false</code>
对类型为 LoadBalancer 的服务禁用节点端口分配。
这仅适用于直接将流量路由到 Pod 而不是使用节点端口的负载均衡器实现。
默认情况下，<code>spec.allocateLoadBalancerNodePorts</code> 为 <code>true</code>，
LoadBalancer 类型的服务继续分配节点端口。
如果现有服务已被分配节点端口，将参数 <code>spec.allocateLoadBalancerNodePorts</code>
设置为 <code>false</code> 时，这些服务上已分配置的节点端口不会被自动释放。
你必须显式地在每个服务端口中删除 <code>nodePorts</code> 项以释放对应端口。
你必须启用 <code>ServiceLBNodePortControl</code> 特性门控才能使用该字段。</p>
<!--
#### Specifying class of load balancer implementation {#load-balancer-class}
-->
<h4 id="load-balancer-class">设置负载均衡器实现的类别</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
`spec.loadBalancerClass` enables you to use a load balancer implementation other than the cloud provider default. This feature is available from v1.21, you must enable the `ServiceLoadBalancerClass` feature gate to use this field in v1.21, and the feature gate is enabled by default from v1.22 onwards.
By default, `spec.loadBalancerClass` is `nil` and a `LoadBalancer` type of Service uses
the cloud provider's default load balancer implementation if the cluster is configured with
a cloud provider using the `--cloud-provider` component flag. 
If `spec.loadBalancerClass` is specified, it is assumed that a load balancer
implementation that matches the specified class is watching for Services.
Any default load balancer implementation (for example, the one provided by
the cloud provider) will ignore Services that have this field set.
`spec.loadBalancerClass` can be set on a Service of type `LoadBalancer` only.
Once set, it cannot be changed. 
-->
<p><code>spec.loadBalancerClass</code> 允许你不使用云提供商的默认负载均衡器实现，转而使用指定的负载均衡器实现。
这个特性从 v1.21 版本开始可以使用，你在 v1.21 版本中使用这个字段必须启用 <code>ServiceLoadBalancerClass</code>
特性门控，这个特性门控从 v1.22 版本及以后默认打开。
默认情况下，<code>.spec.loadBalancerClass</code> 的取值是 <code>nil</code>，如果集群使用 <code>--cloud-provider</code> 配置了云提供商，
<code>LoadBalancer</code> 类型服务会使用云提供商的默认负载均衡器实现。
如果设置了 <code>.spec.loadBalancerClass</code>，则假定存在某个与所指定的类相匹配的
负载均衡器实现在监视服务变化。
所有默认的负载均衡器实现（例如，由云提供商所提供的）都会忽略设置了此字段
的服务。<code>.spec.loadBalancerClass</code> 只能设置到类型为 <code>LoadBalancer</code> 的 Service
之上，而且一旦设置之后不可变更。</p>
<!--
The value of `spec.loadBalancerClass` must be a label-style identifier,
with an optional prefix such as "`internal-vip`" or "`example.com/internal-vip`".
Unprefixed names are reserved for end-users.
-->
<p><code>.spec.loadBalancerClass</code> 的值必须是一个标签风格的标识符，
可以有选择地带有类似 &quot;<code>internal-vip</code>&quot; 或 &quot;<code>example.com/internal-vip</code>&quot; 这类
前缀。没有前缀的名字是保留给最终用户的。</p>
<!--
#### Internal load balancer

In a mixed environment it is sometimes necessary to route traffic from Services inside the same
(virtual) network address block.

In a split-horizon DNS environment you would need two Services to be able to route both external and internal traffic to your endpoints.

To set an internal load balancer, add one of the following annotations to your Service
depending on the cloud Service provider you're using.
-->
<h4 id="internal-load-balancer">内部负载均衡器</h4>
<p>在混合环境中，有时有必要在同一(虚拟)网络地址块内路由来自服务的流量。</p>
<p>在水平分割 DNS 环境中，你需要两个服务才能将内部和外部流量都路由到你的端点（Endpoints）。</p>
<p>如要设置内部负载均衡器，请根据你所使用的云运营商，为服务添加以下注解之一。</p>
<ul class="nav nav-tabs" id="service-tabs" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#service-tabs-0" role="tab" aria-controls="service-tabs-0" aria-selected="true">Default</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-1" role="tab" aria-controls="service-tabs-1">GCP</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-2" role="tab" aria-controls="service-tabs-2">AWS</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-3" role="tab" aria-controls="service-tabs-3">Azure</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-4" role="tab" aria-controls="service-tabs-4">IBM Cloud</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-5" role="tab" aria-controls="service-tabs-5">OpenStack</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-6" role="tab" aria-controls="service-tabs-6">Baidu Cloud</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-7" role="tab" aria-controls="service-tabs-7">Tencent Cloud</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#service-tabs-8" role="tab" aria-controls="service-tabs-8">Alibaba Cloud</a></li></ul>
<div class="tab-content" id="service-tabs"><div id="service-tabs-0" class="tab-pane show active" role="tabpanel" aria-labelledby="service-tabs-0">

<p><!--
Select one of the tabs.
-->
<p>选择一个标签</p>
</div>
  <div id="service-tabs-1" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[...]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cloud.google.com/load-balancer-type</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Internal&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>[...]<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="service-tabs-2" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-2">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[...]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-internal</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>[...]<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="service-tabs-3" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-3">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[...]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/azure-load-balancer-internal</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>[...]<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="service-tabs-4" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-4">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[...]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;private&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>[...]<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="service-tabs-5" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-5">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[...]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/openstack-internal-load-balancer</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>[...]<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="service-tabs-6" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-6">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[...]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/cce-load-balancer-internal-vpc</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>[...]<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="service-tabs-7" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-7">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[...]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">service.kubernetes.io/qcloud-loadbalancer-internal-subnetid</span>:<span style="color:#bbb"> </span>subnet-xxxxx<span style="color:#bbb">
</span><span style="color:#bbb"></span>[...]<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="service-tabs-8" class="tab-pane" role="tabpanel" aria-labelledby="service-tabs-8">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[...]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;intranet&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>[...]<span style="color:#bbb">
</span></code></pre></div></div></div>

<!--
#### TLS support on AWS {#ssl-support-on-aws}

For partial TLS / SSL support on clusters running on AWS, you can add three
annotations to a `LoadBalancer` service:
-->
<h3 id="ssl-support-on-aws">AWS TLS 支持</h3>
<p>为了对在 AWS 上运行的集群提供 TLS/SSL 部分支持，你可以向 <code>LoadBalancer</code>
服务添加三个注解：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-ssl-cert</span>:<span style="color:#bbb"> </span>arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012<span style="color:#bbb">
</span></code></pre></div><!--
The first specifies the ARN of the certificate to use. It can be either a
certificate from a third party issuer that was uploaded to IAM or one created
within AWS Certificate Manager.
-->
<p>第一个指定要使用的证书的 ARN。 它可以是已上载到 IAM 的第三方颁发者的证书，
也可以是在 AWS Certificate Manager 中创建的证书。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-backend-protocol</span>:<span style="color:#bbb"> </span>(https|http|ssl|tcp)<span style="color:#bbb">
</span></code></pre></div><!--
The second annotation specifies which protocol a Pod speaks. For HTTPS and
SSL, the ELB expects the Pod to authenticate itself over the encrypted
connection, using a certificate.

HTTP and HTTPS selects layer 7 proxying: the ELB terminates
the connection with the user, parses headers, and injects the `X-Forwarded-For`
header with the user's IP address (Pods only see the IP address of the
ELB at the other end of its connection) when forwarding requests.

TCP and SSL selects layer 4 proxying: the ELB forwards traffic without
modifying the headers.

In a mixed-use environment where some ports are secured and others are left unencrypted,
you can use the following annotations:
-->
<p>第二个注解指定 Pod 使用哪种协议。 对于 HTTPS 和 SSL，ELB 希望 Pod 使用证书
通过加密连接对自己进行身份验证。</p>
<p>HTTP 和 HTTPS 选择第7层代理：ELB 终止与用户的连接，解析标头，并在转发请求时向
<code>X-Forwarded-For</code> 标头注入用户的 IP 地址（Pod 仅在连接的另一端看到 ELB 的 IP 地址）。</p>
<p>TCP 和 SSL 选择第4层代理：ELB 转发流量而不修改报头。</p>
<p>在某些端口处于安全状态而其他端口未加密的混合使用环境中，可以使用以下注解：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-backend-protocol</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-ssl-ports</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;443,8443&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
In the above example, if the Service contained three ports, `80`, `443`, and
`8443`, then `443` and `8443` would use the SSL certificate, but `80` would be proxied HTTP.

From Kubernetes v1.9 onwards you can use [predefined AWS SSL policies](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html) with HTTPS or SSL listeners for your Services.
To see which policies are available for use, you can use the `aws` command line tool:
-->
<p>在上例中，如果服务包含 <code>80</code>、<code>443</code> 和 <code>8443</code> 三个端口， 那么 <code>443</code> 和 <code>8443</code> 将使用 SSL 证书，
而 <code>80</code> 端口将转发 HTTP 数据包。</p>
<p>从 Kubernetes v1.9 起可以使用
<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html">预定义的 AWS SSL 策略</a>
为你的服务使用 HTTPS 或 SSL 侦听器。
要查看可以使用哪些策略，可以使用 <code>aws</code> 命令行工具：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">aws elb describe-load-balancer-policies --query <span style="color:#b44">&#39;PolicyDescriptions[].PolicyName&#39;</span>
</code></pre></div><!--
You can then specify any one of those policies using the
"`service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy`"
annotation; for example:
-->
<p>然后，你可以使用 &quot;<code>service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy</code>&quot;
注解; 例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;ELBSecurityPolicy-TLS-1-2-2017-01&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
#### PROXY protocol support on AWS

To enable [PROXY protocol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)
support for clusters running on AWS, you can use the following service
annotation:
-->
<h4 id="aws-上的-proxy-协议支持">AWS 上的 PROXY 协议支持</h4>
<p>为了支持在 AWS 上运行的集群，启用
<a href="https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">PROXY 协议</a>。
你可以使用以下服务注解：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-proxy-protocol</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;*&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
Since version 1.3.0, the use of this annotation applies to all ports proxied by the ELB
and cannot be configured otherwise.
-->
<p>从 1.3.0 版开始，此注解的使用适用于 ELB 代理的所有端口，并且不能进行其他配置。</p>
<!--
#### ELB Access Logs on AWS

There are several annotations to manage access logs for ELB Services on AWS.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-enabled`
controls whether access logs are enabled.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval`
controls the interval in minutes for publishing the access logs. You can specify
an interval of either 5 or 60 minutes.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name`
controls the name of the Amazon S3 bucket where load balancer access logs are
stored.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix`
specifies the logical hierarchy you created for your Amazon S3 bucket.
-->
<h4 id="aws-上的-elb-访问日志">AWS 上的 ELB 访问日志</h4>
<p>有几个注解可用于管理 AWS 上 ELB 服务的访问日志。</p>
<p>注解 <code>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled</code> 控制是否启用访问日志。</p>
<p>注解 <code>service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval</code>
控制发布访问日志的时间间隔（以分钟为单位）。你可以指定 5 分钟或 60 分钟的间隔。</p>
<p>注解 <code>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name</code>
控制存储负载均衡器访问日志的 Amazon S3 存储桶的名称。</p>
<p>注解 <code>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix</code>
指定为 Amazon S3 存储桶创建的逻辑层次结构。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-access-log-enabled</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 指定是否为负载均衡器启用访问日志</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;60&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 发布访问日志的时间间隔。你可以将其设置为 5 分钟或 60 分钟。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;my-bucket&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 用来存放访问日志的 Amazon S3 Bucket 名称</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;my-bucket-prefix/prod&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 你为 Amazon S3 Bucket 所创建的逻辑层次结构，例如 `my-bucket-prefix/prod`</span><span style="color:#bbb">
</span></code></pre></div><!--
#### Connection Draining on AWS

Connection draining for Classic ELBs can be managed with the annotation
`service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled` set
to the value of `"true"`. The annotation
`service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout` can
also be used to set maximum time, in seconds, to keep the existing connections open before deregistering the instances.
-->
<h4 id="aws-上的连接排空">AWS 上的连接排空</h4>
<p>可以将注解 <code>service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled</code>
设置为 <code>&quot;true&quot;</code> 来管理 ELB 的连接排空。
注解 <code>service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout</code>
也可以用于设置最大时间（以秒为单位），以保持现有连接在注销实例之前保持打开状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;60&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
#### Other ELB annotations

There are other annotations to manage Classic Elastic Load Balancers that are described below.
-->
<h4 id="其他-elb-注解">其他 ELB 注解</h4>
<p>还有其他一些注解，用于管理经典弹性负载均衡器，如下所述。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 按秒计的时间，表示负载均衡器关闭连接之前连接可以保持空闲</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># （连接上无数据传输）的时间长度</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;60&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 指定该负载均衡器上是否启用跨区的负载均衡能力</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 逗号分隔列表值，每一项都是一个键-值耦对，会作为额外的标签记录于 ELB 中</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;environment=prod,owner=devops&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 将某后端视为健康、可接收请求之前需要达到的连续成功健康检查次数。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 默认为 2，必须介于 2 和 10 之间</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 将某后端视为不健康、不可接收请求之前需要达到的连续不成功健康检查次数。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 默认为 6，必须介于 2 和 10 之间</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;3&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 对每个实例进行健康检查时，连续两次检查之间的大致间隔秒数</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 默认为 10，必须介于 5 和 300 之间</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 时长秒数，在此期间没有响应意味着健康检查失败</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 此值必须小于 service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 默认值为 5，必须介于 2 和 60 之间</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 由已有的安全组所构成的列表，可以配置到所创建的 ELB 之上。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 与注解 service.beta.kubernetes.io/aws-load-balancer-extra-security-groups 不同，</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 这一设置会替代掉之前指定给该 ELB 的所有其他安全组，也会覆盖掉为此</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ELB 所唯一创建的安全组。 </span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 此列表中的第一个安全组 ID 被用来作为决策源，以允许入站流量流入目标工作节点</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># (包括服务流量和健康检查）。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 如果多个 ELB 配置了相同的安全组 ID，为工作节点安全组添加的允许规则行只有一个，</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 这意味着如果你删除了这些 ELB 中的任何一个，都会导致该规则记录被删除，</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 以至于所有共享该安全组 ID 的其他 ELB 都无法访问该节点。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 此注解如果使用不当，会导致跨服务的不可用状况。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-security-groups</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;sg-53fae93f&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 额外的安全组列表，将被添加到所创建的 ELB 之上。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 添加时，会保留为 ELB 所专门创建的安全组。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 这样会确保每个 ELB 都有一个唯一的安全组 ID 和与之对应的允许规则记录，</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 允许请求（服务流量和健康检查）发送到目标工作节点。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 这里顶一个安全组可以被多个服务共享。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-extra-security-groups</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;sg-53fae93f,sg-42efd82e&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 用逗号分隔的一个键-值偶对列表，用来为负载均衡器选择目标节点</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-target-node-labels</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;ingress-gw,gw-name=public-api&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
#### Network Load Balancer support on AWS {#aws-nlb-support}
-->
<h4 id="aws-nlb-support">AWS 上网络负载均衡器支持</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.15 [beta]</code>
</div>


<!--
To use a Network Load Balancer on AWS, use the annotation `service.beta.kubernetes.io/aws-load-balancer-type` with the value set to `nlb`.
-->
<p>要在 AWS 上使用网络负载均衡器，可以使用注解
<code>service.beta.kubernetes.io/aws-load-balancer-type</code>，将其取值设为 <code>nlb</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.beta.kubernetes.io/aws-load-balancer-type</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;nlb&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
NLB only works with certain instance classes; see the [AWS documentation](http://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#register-deregister-targets)
on Elastic Load Balancing for a list of supported instance types.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> NLB 仅适用于某些实例类。有关受支持的实例类型的列表，
请参见
<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#register-deregister-targets">AWS文档</a>
中关于所支持的实例类型的 Elastic Load Balancing 说明。
</div>
<!--
Unlike Classic Elastic Load Balancers, Network Load Balancers (NLBs) forward the
client's IP address through to the node. If a Service's `.spec.externalTrafficPolicy`
is set to `Cluster`, the client's IP address is not propagated to the end
Pods.

By setting `.spec.externalTrafficPolicy` to `Local`, the client IP addresses is
propagated to the end Pods, but this could result in uneven distribution of
traffic. Nodes without any Pods for a particular LoadBalancer Service will fail
the NLB Target Group's health check on the auto-assigned
`.spec.healthCheckNodePort` and not receive any traffic.
-->
<p>与经典弹性负载平衡器不同，网络负载平衡器（NLB）将客户端的 IP 地址转发到该节点。
如果服务的 <code>.spec.externalTrafficPolicy</code> 设置为 <code>Cluster</code> ，则客户端的IP地址不会传达到最终的 Pod。</p>
<p>通过将 <code>.spec.externalTrafficPolicy</code> 设置为 <code>Local</code>，客户端IP地址将传播到最终的 Pod，
但这可能导致流量分配不均。
没有针对特定 LoadBalancer 服务的任何 Pod 的节点将无法通过自动分配的
<code>.spec.healthCheckNodePort</code> 进行 NLB 目标组的运行状况检查，并且不会收到任何流量。</p>
<!--
In order to achieve even traffic, either use a DaemonSet, or specify a
[pod anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
to not locate on the same node.

You can also use NLB Services with the [internal load balancer](/docs/concepts/services-networking/service/#internal-load-balancer)
annotation.

In order for client traffic to reach instances behind an NLB, the Node security
groups are modified with the following IP rules:
-->
<p>为了获得均衡流量，请使用 DaemonSet 或指定
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">Pod 反亲和性</a>
使其不在同一节点上。</p>
<p>你还可以将 NLB 服务与<a href="/zh/docs/concepts/services-networking/service/#internal-load-balancer">内部负载平衡器</a>
注解一起使用。</p>
<p>为了使客户端流量能够到达 NLB 后面的实例，使用以下 IP 规则修改了节点安全组：</p>
<table>
<thead>
<tr>
<th>Rule</th>
<th>Protocol</th>
<th>Port(s)</th>
<th>IpRange(s)</th>
<th>IpRange Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Health Check</td>
<td>TCP</td>
<td>NodePort(s) (<code>.spec.healthCheckNodePort</code> for <code>.spec.externalTrafficPolicy = Local</code>)</td>
<td>Subnet CIDR</td>
<td>kubernetes.io/rule/nlb/health=&lt;loadBalancerName&gt;</td>
</tr>
<tr>
<td>Client Traffic</td>
<td>TCP</td>
<td>NodePort(s)</td>
<td><code>.spec.loadBalancerSourceRanges</code> (defaults to <code>0.0.0.0/0</code>)</td>
<td>kubernetes.io/rule/nlb/client=&lt;loadBalancerName&gt;</td>
</tr>
<tr>
<td>MTU Discovery</td>
<td>ICMP</td>
<td>3,4</td>
<td><code>.spec.loadBalancerSourceRanges</code> (defaults to <code>0.0.0.0/0</code>)</td>
<td>kubernetes.io/rule/nlb/mtu=&lt;loadBalancerName&gt;</td>
</tr>
</tbody>
</table>
<!--
In order to limit which client IP's can access the Network Load Balancer,
specify `loadBalancerSourceRanges`.
-->
<p>为了限制哪些客户端IP可以访问网络负载平衡器，请指定 <code>loadBalancerSourceRanges</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">loadBalancerSourceRanges</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;143.231.0.0/16&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
If `.spec.loadBalancerSourceRanges` is not set, Kubernetes
allows traffic from `0.0.0.0/0` to the Node Security Group(s). If nodes have
public IP addresses, be aware that non-NLB traffic can also reach all instances
in those modified security groups.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果未设置 <code>.spec.loadBalancerSourceRanges</code> ，则 Kubernetes 允许从 <code>0.0.0.0/0</code> 到节点安全组的流量。
如果节点具有公共 IP 地址，请注意，非 NLB 流量也可以到达那些修改后的安全组中的所有实例。
</div>
<!--
Further documentation on annotations for Elastic IPs and other common use-cases may be found
in the [AWS Load Balancer Controller documentation](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/).
-->
<p>有关弹性 IP 注解和更多其他常见用例，
请参阅<a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/">AWS负载均衡控制器文档</a>。</p>
<!--
#### Other CLB annotations on Tencent Kubernetes Engine (TKE)

There are other annotations for managing Cloud Load Balancers on TKE as shown below.

```yaml
    metadata:
      name: my-service
      annotations:
        # Bind Loadbalancers with specified nodes
        service.kubernetes.io/qcloud-loadbalancer-backends-label: key in (value1, value2)

        # ID of an existing load balancer
        service.kubernetes.io/tke-existed-lbid：lb-6swtxxxx

        # Custom parameters for the load balancer (LB), does not support modification of LB type yet
        service.kubernetes.io/service.extensiveParameters: ""

        # Custom parameters for the LB listener
        service.kubernetes.io/service.listenerParameters: ""

        # Specifies the type of Load balancer;
        # valid values: classic (Classic Cloud Load Balancer) or application (Application Cloud Load Balancer)
        service.kubernetes.io/loadbalance-type: xxxxx

        # Specifies the public network bandwidth billing method;
        # valid values: TRAFFIC_POSTPAID_BY_HOUR(bill-by-traffic) and BANDWIDTH_POSTPAID_BY_HOUR (bill-by-bandwidth).
        service.kubernetes.io/qcloud-loadbalancer-internet-charge-type: xxxxxx

        # Specifies the bandwidth value (value range: [1,2000] Mbps).
        service.kubernetes.io/qcloud-loadbalancer-internet-max-bandwidth-out: "10"

        # When this annotation is set，the loadbalancers will only register nodes
        # with pod running on it, otherwise all nodes will be registered.
        service.kubernetes.io/local-svc-only-bind-node-with-pod: true
```
-->
<h4 id="腾讯-kubernetes-引擎-tke-上的-clb-注解">腾讯 Kubernetes 引擎（TKE）上的 CLB 注解</h4>
<p>以下是在 TKE 上管理云负载均衡器的注解。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 绑定负载均衡器到指定的节点。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.kubernetes.io/qcloud-loadbalancer-backends-label</span>:<span style="color:#bbb"> </span>key in (value1, value2)<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 为已有负载均衡器添加 ID。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>service.kubernetes.io/tke-existed-lbid：lb-6swtxxxx<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 负载均衡器（LB）的自定义参数尚不支持修改 LB 类型。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.kubernetes.io/service.extensiveParameters</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 自定义负载均衡监听器。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.kubernetes.io/service.listenerParameters</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 指定负载均衡类型。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 可用参数: classic (Classic Cloud Load Balancer) 或 application (Application Cloud Load Balancer)</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.kubernetes.io/loadbalance-type</span>:<span style="color:#bbb"> </span>xxxxx<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 指定公用网络带宽计费方法。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 可用参数: TRAFFIC_POSTPAID_BY_HOUR(bill-by-traffic) 和 BANDWIDTH_POSTPAID_BY_HOUR (bill-by-bandwidth).</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.kubernetes.io/qcloud-loadbalancer-internet-charge-type</span>:<span style="color:#bbb"> </span>xxxxxx<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 指定带宽参数 (取值范围： [1,2000] Mbps).</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.kubernetes.io/qcloud-loadbalancer-internet-max-bandwidth-out</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 当设置该注解时，负载平衡器将只注册正在运行 Pod 的节点，</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 否则所有节点将会被注册。</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service.kubernetes.io/local-svc-only-bind-node-with-pod</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div><!--
### Type ExternalName {#externalname}

Services of type ExternalName map a Service to a DNS name, not to a typical selector such as
`my-service` or `cassandra`. You specify these Services with the `spec.externalName` parameter.

This Service definition, for example, maps
the `my-service` Service in the `prod` namespace to `my.database.example.com`:
-->
<h3 id="externalname">ExternalName 类型        </h3>
<p>类型为 ExternalName 的服务将服务映射到 DNS 名称，而不是典型的选择器，例如 <code>my-service</code> 或者 <code>cassandra</code>。
你可以使用 <code>spec.externalName</code> 参数指定这些服务。</p>
<p>例如，以下 Service 定义将 <code>prod</code> 名称空间中的 <code>my-service</code> 服务映射到 <code>my.database.example.com</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>prod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>ExternalName<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">externalName</span>:<span style="color:#bbb"> </span>my.database.example.com<span style="color:#bbb">
</span></code></pre></div><!--
ExternalName accepts an IPv4 address string, but as a DNS name comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName
is intended to specify a canonical DNS name. To hardcode an IP address, consider using
[headless Services](#headless-services).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> ExternalName 服务接受 IPv4 地址字符串，但作为包含数字的 DNS 名称，而不是 IP 地址。
类似于 IPv4 地址的外部名称不能由 CoreDNS 或 ingress-nginx 解析，因为外部名称旨在指定规范的 DNS 名称。
要对 IP 地址进行硬编码，请考虑使用 <a href="#headless-services">headless Services</a>。
</div>
<!--
When looking up the host `my-service.prod.svc.cluster.local`, the cluster DNS Service
returns a `CNAME` record with the value `my.database.example.com`. Accessing
`my-service` works in the same way as other Services but with the crucial
difference that redirection happens at the DNS level rather than via proxying or
forwarding. Should you later decide to move your database into your cluster, you
can start its Pods, add appropriate selectors or endpoints, and change the
Service's `type`.
-->
<p>当查找主机 <code>my-service.prod.svc.cluster.local</code> 时，集群 DNS 服务返回 <code>CNAME</code> 记录，
其值为 <code>my.database.example.com</code>。
访问 <code>my-service</code> 的方式与其他服务的方式相同，但主要区别在于重定向发生在 DNS 级别，而不是通过代理或转发。
如果以后你决定将数据库移到集群中，则可以启动其 Pod，添加适当的选择器或端点以及更改服务的 <code>type</code>。</p>
<!--
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <p>You may have trouble using ExternalName for some common protocols, including HTTP and HTTPS. If you use ExternalName then the hostname used by clients inside your cluster is different from the name that the ExternalName references.</p>
<p>For protocols that use hostnames this difference may lead to errors or unexpected responses. HTTP requests will have a <code>Host:</code> header that the origin server does not recognize; TLS servers will not be able to provide a certificate matching the hostname that the client connected to.</p>

</div>


-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <p>对于一些常见的协议，包括 HTTP 和 HTTPS，
你使用 ExternalName 可能会遇到问题。
如果你使用 ExternalName，那么集群内客户端使用的主机名
与 ExternalName 引用的名称不同。</p>
<p>对于使用主机名的协议，此差异可能会导致错误或意外响应。
HTTP 请求将具有源服务器无法识别的 <code>Host:</code> 标头；TLS 服
务器将无法提供与客户端连接的主机名匹配的证书。</p>

</div>


<!--
This section is indebted to the [Kubernetes Tips - Part
1](https://akomljen.com/kubernetes-tips-part-1/) blog post from [Alen Komljen](https://akomljen.com/).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 本部分感谢 <a href="https://akomljen.com/">Alen Komljen</a>的
<a href="https://akomljen.com/kubernetes-tips-part-1/">Kubernetes Tips - Part1</a> 博客文章。
</div>
<!--
### External IPs

If there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those
`externalIPs`. Traffic that ingresses into the cluster with the external IP (as destination IP), on the Service port,
will be routed to one of the Service endpoints. `externalIPs` are not managed by Kubernetes and are the responsibility
of the cluster administrator.

In the Service spec, `externalIPs` can be specified along with any of the `ServiceTypes`.
In the example below, "`my-service`" can be accessed by clients on "`80.11.12.10:80`" (`externalIP:port`)
-->
<h3 id="external-ips">外部 IP </h3>
<p>如果外部的 IP 路由到集群中一个或多个 Node 上，Kubernetes Service 会被暴露给这些 externalIPs。
通过外部 IP（作为目的 IP 地址）进入到集群，打到 Service 的端口上的流量，
将会被路由到 Service 的 Endpoint 上。
<code>externalIPs</code> 不会被 Kubernetes 管理，它属于集群管理员的职责范畴。</p>
<p>根据 Service 的规定，<code>externalIPs</code> 可以同任意的 <code>ServiceType</code> 来一起指定。
在上面的例子中，<code>my-service</code> 可以在  &quot;<code>80.11.12.10:80</code>&quot;(<code>externalIP:port</code>) 上被客户端访问。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">externalIPs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#666">80.11.12.10</span><span style="color:#bbb">
</span></code></pre></div><!--
## Shortcomings

Using the userspace proxy for VIPs works at small to medium scale, but will
not scale to very large clusters with thousands of Services.  The
[original design proposal for portals](https://github.com/kubernetes/kubernetes/issues/1107)
has more details on this.

Using the userspace proxy obscures the source IP address of a packet accessing
a Service.
This makes some kinds of network filtering (firewalling) impossible.  The iptables
proxy mode does not
obscure in-cluster source IPs, but it does still impact clients coming through
a load balancer or node-port.

The `Type` field is designed as nested functionality - each level adds to the
previous.  This is not strictly required on all cloud providers (e.g. Google Compute Engine does
not need to allocate a `NodePort` to make `LoadBalancer` work, but AWS does)
but the current API requires it.
-->
<h2 id="不足之处">不足之处</h2>
<p>为 VIP 使用用户空间代理，将只适合小型到中型规模的集群，不能够扩展到上千 Service 的大型集群。
查看<a href="https://github.com/kubernetes/kubernetes/issues/1107">最初设计方案</a> 获取更多细节。</p>
<p>使用用户空间代理，隐藏了访问 Service 的数据包的源 IP 地址。
这使得一些类型的防火墙无法起作用。
iptables 代理不会隐藏 Kubernetes 集群内部的 IP 地址，但却要求客户端请求
必须通过一个负载均衡器或 Node 端口。</p>
<p><code>Type</code> 字段支持嵌套功能 —— 每一层需要添加到上一层里面。
不会严格要求所有云提供商（例如，GCE 就没必要为了使一个 <code>LoadBalancer</code>
能工作而分配一个 <code>NodePort</code>，但是 AWS 需要 ），但当前 API 是强制要求的。</p>
<!--
## Virtual IP implementation {#the-gory-details-of-virtual-ips}

The previous information should be sufficient for many people who want to
use Services.  However, there is a lot going on behind the scenes that may be
worth understanding.
-->
<h2 id="the-gory-details-of-virtual-ips">虚拟IP实施</h2>
<p>对很多想使用 Service 的人来说，前面的信息应该足够了。
然而，有很多内部原理性的内容，还是值去理解的。</p>
<!--
### Avoiding collisions

One of the primary philosophies of Kubernetes is that you should not be
exposed to situations that could cause your actions to fail through no fault
of your own. For the design of the Service resource, this means not making
you choose your own port number if that choice might collide with
someone else's choice.  That is an isolation failure.

In order to allow you to choose a port number for your Services, we must
ensure that no two Services can collide. Kubernetes does that by allocating each
Service its own IP address.

To ensure each Service receives a unique IP, an internal allocator atomically
updates a global allocation map in <a class='glossary-tooltip' title='etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/' target='_blank' aria-label='etcd'>etcd</a>
prior to creating each Service. The map object must exist in the registry for
Services to get IP address assignments, otherwise creations will
fail with a message indicating an IP address could not be allocated.

In the control plane, a background controller is responsible for creating that
map (needed to support migrating from older versions of Kubernetes that used
in-memory locking). Kubernetes also uses controllers to check for invalid
assignments (eg due to administrator intervention) and for cleaning up allocated
IP addresses that are no longer used by any Services.
-->
<h3 id="避免冲突">避免冲突</h3>
<p>Kubernetes 最主要的哲学之一，是用户不应该暴露那些能够导致他们操作失败、但又不是他们的过错的场景。
对于 Service 资源的设计，这意味着如果用户的选择有可能与他人冲突，那就不要让用户自行选择端口号。
这是一个隔离性的失败。</p>
<p>为了使用户能够为他们的 Service 选择一个端口号，我们必须确保不能有2个 Service 发生冲突。
Kubernetes 通过为每个 Service 分配它们自己的 IP 地址来实现。</p>
<p>为了保证每个 Service 被分配到一个唯一的 IP，需要一个内部的分配器能够原子地更新
<a class='glossary-tooltip' title='etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/' target='_blank' aria-label='etcd'>etcd</a> 中的一个全局分配映射表，
这个更新操作要先于创建每一个 Service。
为了使 Service 能够获取到 IP，这个映射表对象必须在注册中心存在，
否则创建 Service 将会失败，指示一个 IP 不能被分配。</p>
<p>在控制平面中，一个后台 Controller 的职责是创建映射表
（需要支持从使用了内存锁的 Kubernetes 的旧版本迁移过来）。
同时 Kubernetes 会通过控制器检查不合理的分配（如管理员干预导致的）
以及清理已被分配但不再被任何 Service 使用的 IP 地址。</p>
<!--
### Service IP addresses {#ips-and-vips}

Unlike Pod IP addresses, which actually route to a fixed destination,
Service IPs are not actually answered by a single host.  Instead, kube-proxy
uses iptables (packet processing logic in Linux) to define _virtual_ IP addresses
which are transparently redirected as needed.  When clients connect to the
VIP, their traffic is automatically transported to an appropriate endpoint.
The environment variables and DNS for Services are actually populated in
terms of the Service's virtual IP address (and port).

kube-proxy supports three proxy modes&mdash;userspace, iptables and IPVS&mdash;which
each operate slightly differently.
-->
<h3 id="ips-and-vips">Service IP 地址</h3>
<p>不像 Pod 的 IP 地址，它实际路由到一个固定的目的地，Service 的 IP 实际上
不能通过单个主机来进行应答。
相反，我们使用 <code>iptables</code>（Linux 中的数据包处理逻辑）来定义一个
虚拟 IP 地址（VIP），它可以根据需要透明地进行重定向。
当客户端连接到 VIP 时，它们的流量会自动地传输到一个合适的 Endpoint。
环境变量和 DNS，实际上会根据 Service 的 VIP 和端口来进行填充。</p>
<p>kube-proxy支持三种代理模式: 用户空间，iptables和IPVS；它们各自的操作略有不同。</p>
<h4 id="userspace">Userspace </h4>
<!--
As an example, consider the image processing application described above.
When the backend Service is created, the Kubernetes master assigns a virtual
IP address, for example 10.0.0.1.  Assuming the Service port is 1234, the
Service is observed by all of the kube-proxy instances in the cluster.
When a proxy sees a new Service, it opens a new random port, establishes an
iptables redirect from the virtual IP address to this new port, and starts accepting
connections on it.

When a client connects to the Service's virtual IP address, the iptables
rule kicks in, and redirects the packets to the proxy's own port.
The "Service proxy" chooses a backend, and starts proxying traffic from the client to the backend.

This means that Service owners can choose any port they want without risk of
collision.  Clients can connect to an IP and port, without being aware
of which Pods they are actually accessing.
-->
<p>作为一个例子，考虑前面提到的图片处理应用程序。
当创建后端 Service 时，Kubernetes master 会给它指派一个虚拟 IP 地址，比如 10.0.0.1。
假设 Service 的端口是 1234，该 Service 会被集群中所有的 <code>kube-proxy</code> 实例观察到。
当代理看到一个新的 Service， 它会打开一个新的端口，建立一个从该 VIP 重定向到
新端口的 iptables，并开始接收请求连接。</p>
<p>当一个客户端连接到一个 VIP，iptables 规则开始起作用，它会重定向该数据包到
&quot;服务代理&quot; 的端口。
&quot;服务代理&quot; 选择一个后端，并将客户端的流量代理到后端上。</p>
<p>这意味着 Service 的所有者能够选择任何他们想使用的端口，而不存在冲突的风险。
客户端可以连接到一个 IP 和端口，而不需要知道实际访问了哪些 Pod。</p>
<h4 id="iptables">iptables</h4>
<!--
Again, consider the image processing application described above.
When the backend Service is created, the Kubernetes control plane assigns a virtual
IP address, for example 10.0.0.1.  Assuming the Service port is 1234, the
Service is observed by all of the kube-proxy instances in the cluster.
When a proxy sees a new Service, it installs a series of iptables rules which
redirect from the virtual IP address  to per-Service rules.  The per-Service
rules link to per-Endpoint rules which redirect traffic (using destination NAT)
to the backends.

When a client connects to the Service's virtual IP address the iptables rule kicks in.
A backend is chosen (either based on session affinity or randomly) and packets are
redirected to the backend.  Unlike the userspace proxy, packets are never
copied to userspace, the kube-proxy does not have to be running for the virtual
IP address to work, and Nodes see traffic arriving from the unaltered client IP
address.

This same basic flow executes when traffic comes in through a node-port or
through a load-balancer, though in those cases the client IP does get altered.
-->
<p>再次考虑前面提到的图片处理应用程序。
当创建后端 Service 时，Kubernetes 控制面板会给它指派一个虚拟 IP 地址，比如 10.0.0.1。
假设 Service 的端口是 1234，该 Service 会被集群中所有的 <code>kube-proxy</code> 实例观察到。
当代理看到一个新的 Service， 它会配置一系列的 iptables 规则，从 VIP 重定向到每个 Service 规则。
该特定于服务的规则连接到特定于 Endpoint 的规则，而后者会重定向（目标地址转译）到后端。</p>
<p>当客户端连接到一个 VIP，iptables 规则开始起作用。一个后端会被选择（或者根据会话亲和性，或者随机），
数据包被重定向到这个后端。
不像用户空间代理，数据包从来不拷贝到用户空间，kube-proxy 不是必须为该 VIP 工作而运行，
并且客户端 IP 是不可更改的。</p>
<p>当流量打到 Node 的端口上，或通过负载均衡器，会执行相同的基本流程，
但是在那些案例中客户端 IP 是可以更改的。</p>
<h4 id="ipvs">IPVS</h4>
<!--
iptables operations slow down dramatically in large scale cluster e.g 10,000 Services.
IPVS is designed for load balancing and based on in-kernel hash tables. So you can achieve performance consistency in large number of Services from IPVS-based kube-proxy. Meanwhile, IPVS-based kube-proxy has more sophisticated load balancing algorithms (least conns, locality, weighted, persistence).
-->
<p>在大规模集群（例如 10000 个服务）中，iptables 操作会显着降低速度。 IPVS
专为负载平衡而设计，并基于内核内哈希表。
因此，你可以通过基于 IPVS 的 kube-proxy 在大量服务中实现性能一致性。
同时，基于 IPVS 的 kube-proxy 具有更复杂的负载均衡算法（最小连接、局部性、
加权、持久性）。</p>
<h2 id="api-对象">API 对象</h2>
<!--
Service is a top-level resource in the Kubernetes REST API. You can find more details
about the API object at: [Service API object](/docs/reference/generated/kubernetes-api/v1.23/#service-v1-core).
-->
<p>Service 是 Kubernetes REST API 中的顶级资源。你可以在以下位置找到有关 API 对象的更多详细信息：
<a href="/docs/reference/generated/kubernetes-api/v1.23/#service-v1-core">Service 对象 API</a>.</p>
<h2 id="protocol-support">受支持的协议</h2>
<h3 id="tcp">TCP</h3>
<!--
You can use TCP for any kind of Service, and it's the default network protocol.
-->
<p>你可以将 TCP 用于任何类型的服务，这是默认的网络协议。</p>
<h3 id="udp">UDP</h3>
<!--
You can use UDP for most Services. For type=LoadBalancer Services, UDP support
depends on the cloud provider offering this facility.
-->
<p>你可以将 UDP 用于大多数服务。 对于 type=LoadBalancer 服务，对 UDP 的支持取决于提供此功能的云提供商。</p>
<!-- 

### SCTP






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>



When using a network plugin that supports SCTP traffic, you can use SCTP for
most Services. For type=LoadBalancer Services, SCTP support depends on the cloud
provider offering this facility. (Most do not).
-->
<h3 id="sctp">SCTP</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>


<p>一旦你使用了支持 SCTP 流量的网络插件，你就可以使用 SCTP 于更多的服务。
对于 type = LoadBalancer 的服务，SCTP 的支持取决于提供此设施的云供应商（大多数不支持）。</p>
<!--
#### Warnings {#caveat-sctp-overview}

##### Support for multihomed SCTP associations {#caveat-sctp-multihomed}
-->
<h4 id="caveat-sctp-overview">警告</h4>
<h5 id="caveat-sctp-multihomed">支持多宿主 SCTP 关联</h5>
<!--
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <p>The support of multihomed SCTP associations requires that the CNI plugin can support the assignment of multiple interfaces and IP addresses to a Pod.</p>
<p>NAT for multihomed SCTP associations requires special logic in the corresponding kernel modules.</p>

</div>


-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <p>支持多宿主SCTP关联要求 CNI 插件能够支持为一个 Pod 分配多个接口和IP地址。</p>
<p>用于多宿主 SCTP 关联的 NAT 在相应的内核模块中需要特殊的逻辑。</p>

</div>


<!--
##### Windows {#caveat-sctp-windows-os}

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> SCTP is not supported on Windows based nodes.
</div>
-->
<h5 id="caveat-sctp-windows-os">Windows</h5>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 基于 Windows 的节点不支持 SCTP。
</div>
<!--
##### Userspace kube-proxy {#caveat-sctp-kube-proxy-userspace}

<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> The kube-proxy does not support the management of SCTP associations when it is in userspace mode.
</div>


-->
<h5 id="caveat-sctp-kube-proxy-userspace">用户空间 kube-proxy</h5>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 当 kube-proxy 处于用户空间模式时，它不支持 SCTP 关联的管理。
</div>


<h3 id="http">HTTP</h3>
<!--
If your cloud provider supports it, you can use a Service in LoadBalancer mode
to set up external HTTP / HTTPS reverse proxying, forwarded to the Endpoints
of the Service.
-->
<p>如果你的云提供商支持它，则可以在 LoadBalancer 模式下使用服务来设置外部
HTTP/HTTPS 反向代理，并将其转发到该服务的 Endpoints。</p>
<!--
You can also use <a class='glossary-tooltip' title='Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/ingress/' target='_blank' aria-label='Ingress'>Ingress</a> in place of Service
to expose HTTP / HTTPS Services.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你还可以使用 <a class='glossary-tooltip' title='Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/ingress/' target='_blank' aria-label='Ingress'>Ingress</a> 代替
Service 来公开 HTTP/HTTPS 服务。
</div>
<!--
### PROXY protocol

If your cloud provider supports it,
you can use a Service in LoadBalancer mode to configure a load balancer outside
of Kubernetes itself, that will forward connections prefixed with
[PROXY protocol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt).

The load balancer will send an initial series of octets describing the
incoming connection, similar to this example
-->
<h3 id="proxy-协议">PROXY 协议</h3>
<p>如果你的云提供商支持它，
则可以在 LoadBalancer 模式下使用 Service 在 Kubernetes 本身之外配置负载均衡器，
该负载均衡器将转发前缀为
<a href="https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">PROXY 协议</a>
的连接。</p>
<p>负载平衡器将发送一系列初始字节，描述传入的连接，类似于此示例</p>
<pre><code>PROXY TCP4 192.0.2.202 10.0.42.7 12345 7\r\n
</code></pre><!--
followed by the data from the client.
-->
<p>上述是来自客户端的数据。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read [Connecting Applications with Services](/docs/concepts/services-networking/connect-applications-service/)
* Read about [Ingress](/docs/concepts/services-networking/ingress/)
* Read about [Endpoint Slices](/docs/concepts/services-networking/endpoint-slices/)
-->
<ul>
<li>阅读<a href="/zh/docs/concepts/services-networking/connect-applications-service/">使用服务访问应用</a></li>
<li>阅读了解 <a href="/zh/docs/concepts/services-networking/ingress/">Ingress</a></li>
<li>阅读了解<a href="/zh/docs/concepts/services-networking/endpoint-slices/">端点切片（Endpoint Slices）</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-91cb8a4438b003df11bc1c426a81b756">5.3 - Pod 与 Service 的 DNS</h1>
    
	<!--
reviewers:
- davidopp
- thockin
title: DNS for Services and Pods
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
Kubernetes creates DNS records for services and pods. You can contact
services with consistent DNS names instead of IP addresses.
-->
<p>Kubernetes 为服务和 Pods 创建 DNS 记录。
你可以使用一致的 DNS 名称而非 IP 地址来访问服务。</p>
<!-- body -->
<!--
## Introduction

Kubernetes DNS schedules a DNS Pod and Service on the cluster, and configures
the kubelets to tell individual containers to use the DNS Service's IP to
resolve DNS names.
-->
<h2 id="介绍">介绍</h2>
<p>Kubernetes DNS 在集群上调度 DNS Pod 和服务，并配置 kubelet 以告知各个容器
使用 DNS 服务的 IP 来解析 DNS 名称。</p>
<!--
Every Service defined in the cluster (including the DNS server itself) is
assigned a DNS name. By default, a client Pod's DNS search list includes the
Pod's own namespace and the cluster's default domain.
-->
<p>集群中定义的每个 Service （包括 DNS 服务器自身）都被赋予一个 DNS 名称。
默认情况下，客户端 Pod 的 DNS 搜索列表会包含 Pod 自身的名字空间和集群
的默认域。</p>
<!--
### Namespaces of Services

A DNS query may return different results based on the namespace of the pod making
it. DNS queries that don't specify a namespace are limited to the pod's
namespace. Access services in other namespaces by specifying it in the DNS query.

For example, consider a pod in a `test` namespace. A `data` service is in
the `prod` namespace.

A query for `data` returns no results, because it uses the pod's `test` namespace.

A query for `data.prod` returns the intended result, because it specifies the
namespace.
-->
<h3 id="service-的名字空间">Service 的名字空间</h3>
<p>DNS 查询可能因为执行查询的 Pod 所在的名字空间而返回不同的结果。
不指定名字空间的 DNS 查询会被限制在 Pod 所在的名字空间内。
要访问其他名字空间中的服务，需要在 DNS 查询中给出名字空间。</p>
<p>例如，假定名字空间 <code>test</code> 中存在一个 Pod，<code>prod</code> 名字空间中存在一个服务
<code>data</code>。</p>
<p>Pod 查询 <code>data</code> 时没有返回结果，因为使用的是 Pod 的名字空间 <code>test</code>。</p>
<p>Pod 查询 <code>data.prod</code> 时则会返回预期的结果，因为查询中指定了名字空间。</p>
<!--
DNS queries may be expanded using the pod's `/etc/resolv.conf`. Kubelet
sets this file for each pod. For example, a query for just `data` may be
expanded to `data.test.cluster.local`. The values of the `search` option
are used to expand queries. To learn more about DNS queries, see
[the `resolv.conf` manual page.](https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html)
-->
<p>DNS 查询可以使用 Pod 中的 <code>/etc/resolv.conf</code> 展开。kubelet 会为每个 Pod
生成此文件。例如，对 <code>data</code> 的查询可能被展开为 <code>data.test.svc.cluster.local</code>。
<code>search</code> 选项的取值会被用来展开查询。要进一步了解 DNS 查询，可参阅
<a href="https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html"><code>resolv.conf</code> 手册页面</a>。</p>
<pre><code>nameserver 10.32.0.10
search &lt;namespace&gt;.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
</code></pre><!--
In summary, a pod in the _test_ namespace can successfully resolve either
`data.prod` or `data.prod.svc.cluster.local`.
-->
<p>概括起来，名字空间 <code>test</code> 中的 Pod 可以成功地解析 <code>data.prod</code> 或者
<code>data.prod.svc.cluster.local</code>。</p>
<!--
### DNS Records

What objects get DNS records?
-->
<h3 id="dns-records">DNS 记录 </h3>
<p>哪些对象会获得 DNS 记录呢？</p>
<ol>
<li>Services</li>
<li>Pods</li>
</ol>
<!--
The following sections detail the supported DNS record types and layout that is
supported.  Any other layout or names or queries that happen to work are
considered implementation details and are subject to change without warning.
For more up-to-date specification, see
[Kubernetes DNS-Based Service Discovery](https://github.com/kubernetes/dns/blob/master/docs/specification.md).
-->
<p>以下各节详细介绍了被支持的 DNS 记录类型和被支持的布局。
其它布局、名称或者查询即使碰巧可以工作，也应视为实现细节，
将来很可能被更改而且不会因此发出警告。
有关最新规范请查看
<a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes 基于 DNS 的服务发现</a>。</p>
<!--
## Services

### A/AAAA records

"Normal" (not headless) Services are assigned a DNS A or AAAA record for a name of the
form `my-svc.my-namespace.svc.cluster-domain.example`.  This resolves to the cluster IP
of the Service.

"Headless" (without a cluster IP) Services are also assigned a DNS A record for
a name of the form `my-svc.my-namespace.svc.cluster-domain.example`.  Unlike normal
Services, this resolves to the set of IPs of the pods selected by the Service.
Clients are expected to consume the set or else use standard round-robin
selection from the set.
-->
<h3 id="services">服务 </h3>
<h4 id="a-aaaa-记录">A/AAAA 记录</h4>
<p>“普通” 服务（除了无头服务）会以 <code>my-svc.my-namespace.svc.cluster-domain.example</code>
这种名字的形式被分配一个 DNS A 或 AAAA 记录，取决于服务的 IP 协议族。
该名称会解析成对应服务的集群 IP。</p>
<p>“无头（Headless）” 服务（没有集群 IP）也会以
<code>my-svc.my-namespace.svc.cluster-domain.example</code> 这种名字的形式被指派一个 DNS A 或 AAAA 记录，
具体取决于服务的 IP 协议族。
与普通服务不同，这一记录会被解析成对应服务所选择的 Pod 集合的 IP。
客户端要能够使用这组 IP，或者使用标准的轮转策略从这组 IP 中进行选择。</p>
<!--
### SRV records

SRV Records are created for named ports that are part of normal or [Headless
Services](/docs/concepts/services-networking/service/#headless-services).
For each named port, the SRV record would have the form
`_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example`.
For a regular service, this resolves to the port number and the domain name:
`my-svc.my-namespace.svc.cluster-domain.example`.
For a headless service, this resolves to multiple answers, one for each pod
that is backing the service, and contains the port number and the domain name of the pod
of the form `auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example`.
-->
<h4 id="srv-records">SRV 记录 </h4>
<p>Kubernetes 会为命名端口创建 SRV 记录，这些端口是普通服务或
<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>的一部分。
对每个命名端口，SRV 记录具有 <code>_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example</code> 这种形式。
对普通服务，该记录会被解析成端口号和域名：<code>my-svc.my-namespace.svc.cluster-domain.example</code>。
对无头服务，该记录会被解析成多个结果，服务对应的每个后端 Pod 各一个；
其中包含 Pod 端口号和形为 <code>auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example</code>
的域名。</p>
<h2 id="pods">Pods</h2>
<!--
### A/AAAA records

In general a pod has the following DNS resolution:

`pod-ip-address.my-namespace.pod.cluster-domain.example`.

For example, if a pod in the `default` namespace has the IP address 172.17.0.3,
and the domain name for your cluster is `cluster.local`, then the Pod has a DNS name:

`172-17-0-3.default.pod.cluster.local`.

Any pods exposed by a Service have the following DNS resolution available:

`pod-ip-address.service-name.my-namespace.svc.cluster-domain.example`.
-->
<h3 id="a-aaaa-记录-1">A/AAAA 记录</h3>
<p>一般而言，Pod 会对应如下 DNS 名字解析：</p>
<p><code>pod-ip-address.my-namespace.pod.cluster-domain.example</code></p>
<p>例如，对于一个位于 <code>default</code> 名字空间，IP 地址为 172.17.0.3 的 Pod，
如果集群的域名为 <code>cluster.local</code>，则 Pod 会对应 DNS 名称：</p>
<p><code>172-17-0-3.default.pod.cluster.local</code>.</p>
<p>通过 Service 暴露出来的所有 Pod 都会有如下 DNS 解析名称可用：</p>
<p><code>pod-ip-address.service-name.my-namespace.svc.cluster-domain.example</code>.</p>
<!--
### Pod's hostname and subdomain fields

Currently when a pod is created, its hostname is the Pod's `metadata.name` value.

The Pod spec has an optional `hostname` field, which can be used to specify the
Pod's hostname. When specified, it takes precedence over the Pod's name to be
the hostname of the pod. For example, given a Pod with `hostname` set to
"`my-host`", the Pod will have its hostname set to "`my-host`".

The Pod spec also has an optional `subdomain` field which can be used to specify
its subdomain. For example, a Pod with `hostname` set to "`foo`", and `subdomain`
set to "`bar`", in namespace "`my-namespace`", will have the fully qualified
domain name (FQDN) "`foo.bar.my-namespace.svc.cluster-domain.example`".

Example:
-->
<h3 id="pod-的-hostname-和-subdomain-字段">Pod 的 hostname 和 subdomain 字段</h3>
<p>当前，创建 Pod 时其主机名取自 Pod 的 <code>metadata.name</code> 值。</p>
<p>Pod 规约中包含一个可选的 <code>hostname</code> 字段，可以用来指定 Pod 的主机名。
当这个字段被设置时，它将优先于 Pod 的名字成为该 Pod 的主机名。
举个例子，给定一个 <code>hostname</code> 设置为 &quot;<code>my-host</code>&quot; 的 Pod，
该 Pod 的主机名将被设置为 &quot;<code>my-host</code>&quot;。</p>
<p>Pod 规约还有一个可选的 <code>subdomain</code> 字段，可以用来指定 Pod 的子域名。
举个例子，某 Pod 的 <code>hostname</code> 设置为 “<code>foo</code>”，<code>subdomain</code> 设置为 “<code>bar</code>”，
在名字空间 “<code>my-namespace</code>” 中对应的完全限定域名（FQDN）为
“<code>foo.bar.my-namespace.svc.cluster-domain.example</code>”。</p>
<p>示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-subdomain<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 实际上不需要指定端口号</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">1234</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">1234</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>busybox1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostname</span>:<span style="color:#bbb"> </span>busybox-1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">subdomain</span>:<span style="color:#bbb"> </span>default-subdomain<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;3600&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>busybox2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostname</span>:<span style="color:#bbb"> </span>busybox-2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">subdomain</span>:<span style="color:#bbb"> </span>default-subdomain<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;3600&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span></code></pre></div><!--
If there exists a headless service in the same namespace as the pod and with
the same name as the subdomain, the cluster's DNS Server also returns an A or AAAA
record for the Pod's fully qualified hostname.
For example, given a Pod with the hostname set to "`busybox-1`" and the subdomain set to
"`default-subdomain`", and a headless Service named "`default-subdomain`" in
the same namespace, the pod will see its own FQDN as
"`busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example`". DNS serves an
A or AAAA record at that name, pointing to the Pod's IP. Both pods "`busybox1`" and
"`busybox2`" can have their distinct A or AAAA records.
-->
<p>如果某无头服务与某 Pod 在同一个名字空间中，且它们具有相同的子域名，
集群的 DNS 服务器也会为该 Pod 的全限定主机名返回 A 记录或 AAAA 记录。
例如，在同一个名字空间中，给定一个主机名为 “busybox-1”、
子域名设置为 “default-subdomain” 的 Pod，和一个名称为 “<code>default-subdomain</code>”
的无头服务，Pod 将看到自己的 FQDN 为
&quot;<code>busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example</code>&quot;。
DNS 会为此名字提供一个 A 记录或 AAAA 记录，指向该 Pod 的 IP。
“<code>busybox1</code>” 和 “<code>busybox2</code>” 这两个 Pod 分别具有它们自己的 A 或 AAAA 记录。</p>
<!--
The Endpoints object can specify the `hostname` for any endpoint addresses,
along with its IP.
-->
<p>Endpoints 对象可以为任何端点地址及其 IP 指定 <code>hostname</code>。</p>
<!--
Because A records are not created for Pod names, `hostname` is required for the Pod's A
record to be created. A Pod with no `hostname` but with `subdomain` will only create the
A record for the headless service (`default-subdomain.my-namespace.svc.cluster-domain.example`),
pointing to the Pod's IP address. Also, Pod needs to become ready in order to have a
record unless `publishNotReadyAddresses=True` is set on the Service.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>因为没有为 Pod 名称创建 A 记录或 AAAA 记录，所以要创建 Pod 的 A 记录
或 AAAA 记录需要 <code>hostname</code>。</p>
<p>没有设置 <code>hostname</code> 但设置了 <code>subdomain</code> 的 Pod 只会为
无头服务创建 A 或 AAAA 记录（<code>default-subdomain.my-namespace.svc.cluster-domain.example</code>）
指向 Pod 的 IP 地址。
另外，除非在服务上设置了 <code>publishNotReadyAddresses=True</code>，否则只有 Pod 进入就绪状态
才会有与之对应的记录。</p>

</div>
<!--
### Pod's setHostnameAsFQDN field {#pod-sethostnameasfqdn-field}






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [stable]</code>
</div>


-->
<h3 id="pod-sethostnameasfqdn-field">Pod 的 setHostnameAsFQDN 字段 </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [stable]</code>
</div>


<!--
When a Pod is configured to have fully qualified domain name (FQDN), its hostname is the short hostname. For example, if you have a Pod with the fully qualified domain name `busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example`, then by default the `hostname` command inside that Pod returns `busybox-1` and  the `hostname -fqdn` command returns the FQDN.
-->
<p><strong>前置条件</strong>：<code>SetHostnameAsFQDN</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
必须在 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>
上启用。</p>
<p>当你在 Pod 规约中设置了 <code>setHostnameAsFQDN: true</code> 时，kubelet 会将 Pod
的全限定域名（FQDN）作为该 Pod 的主机名记录到 Pod 所在名字空间。
在这种情况下，<code>hostname</code> 和 <code>hostname --fqdn</code> 都会返回 Pod 的全限定域名。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In Linux, the hostname field of the kernel (the `nodename` field of `struct utsname`) is limited to 64 characters.

If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in `Pending` status (`ContainerCreating` as seen by `kubectl`) generating error events, such as Failed to construct FQDN from pod hostname and cluster domain, FQDN `long-FQDN` is too long (64 characters is the max, 70 characters requested). One way of improving user experience for this scenario is to create an [admission webhook controller](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks) to control FQDN size when users create top level objects, for example, Deployment.
-->
<p>在 Linux 中，内核的主机名字段（<code>struct utsname</code> 的 <code>nodename</code> 字段）限定
最多 64 个字符。</p>
<p>如果 Pod 启用这一特性，而其 FQDN 超出 64 字符，Pod 的启动会失败。
Pod 会一直出于 <code>Pending</code> 状态（通过 <code>kubectl</code> 所看到的 <code>ContainerCreating</code>），
并产生错误事件，例如
&quot;Failed to construct FQDN from pod hostname and cluster domain, FQDN
<code>long-FQDN</code> is too long (64 characters is the max, 70 characters requested).&quot;
（无法基于 Pod 主机名和集群域名构造 FQDN，FQDN <code>long-FQDN</code> 过长，至多 64
字符，请求字符数为 70）。
对于这种场景而言，改善用户体验的一种方式是创建一个
<a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">准入 Webhook 控制器</a>，
在用户创建顶层对象（如 Deployment）的时候控制 FQDN 的长度。</p>

</div>
<!--
### Pod's DNS Policy

DNS policies can be set on a per-pod basis. Currently Kubernetes supports the
following pod-specific DNS policies. These policies are specified in the
`dnsPolicy` field of a Pod Spec.

- "`Default`": The Pod inherits the name resolution configuration from the node
  that the pods run on.
  See [related discussion](/docs/tasks/administer-cluster/dns-custom-nameservers)
  for more details.
- "`ClusterFirst`": Any DNS query that does not match the configured cluster
  domain suffix, such as "`www.kubernetes.io`", is forwarded to the upstream
  nameserver inherited from the node. Cluster administrators may have extra
  stub-domain and upstream DNS servers configured.
  See [related discussion](/docs/tasks/administer-cluster/dns-custom-nameservers)
  for details on how DNS queries are handled in those cases.
- "`ClusterFirstWithHostNet`": For Pods running with hostNetwork, you should
  explicitly set its DNS policy "`ClusterFirstWithHostNet`".
- "`None`": It allows a Pod to ignore DNS settings from the Kubernetes
  environment. All DNS settings are supposed to be provided using the
  `dnsConfig` field in the Pod Spec.
  See [Pod's DNS config](#pod-s-dns-config) subsection below.
-->
<h3 id="pod-s-dns-policy">Pod 的 DNS 策略   </h3>
<p>DNS 策略可以逐个 Pod 来设定。目前 Kubernetes 支持以下特定 Pod 的 DNS 策略。
这些策略可以在 Pod 规约中的 <code>dnsPolicy</code> 字段设置：</p>
<ul>
<li>&quot;<code>Default</code>&quot;: Pod 从运行所在的节点继承名称解析配置。参考
<a href="/zh/docs/tasks/administer-cluster/dns-custom-nameservers">相关讨论</a>
获取更多信息。</li>
<li>&quot;<code>ClusterFirst</code>&quot;: 与配置的集群域后缀不匹配的任何 DNS 查询（例如 &quot;www.kubernetes.io&quot;）
都将转发到从节点继承的上游名称服务器。集群管理员可能配置了额外的存根域和上游 DNS 服务器。
参阅<a href="/zh/docs/tasks/administer-cluster/dns-custom-nameservers">相关讨论</a>
了解在这些场景中如何处理 DNS 查询的信息。</li>
<li>&quot;<code>ClusterFirstWithHostNet</code>&quot;：对于以 hostNetwork 方式运行的 Pod，应显式设置其 DNS 策略
&quot;<code>ClusterFirstWithHostNet</code>&quot;。</li>
<li>&quot;<code>None</code>&quot;: 此设置允许 Pod 忽略 Kubernetes 环境中的 DNS 设置。Pod 会使用其 <code>dnsConfig</code> 字段
所提供的 DNS 设置。
参见 <a href="#pod-dns-config">Pod 的 DNS 配置</a>节。</li>
</ul>
<!--
"Default" is not the default DNS policy. If `dnsPolicy` is not
explicitly specified, then "ClusterFirst" is used.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> &quot;Default&quot; 不是默认的 DNS 策略。如果未明确指定 <code>dnsPolicy</code>，则使用 &quot;ClusterFirst&quot;。
</div>
<!--
The example below shows a Pod with its DNS policy set to
"`ClusterFirstWithHostNet`" because it has `hostNetwork` set to `true`.
-->
<p>下面的示例显示了一个 Pod，其 DNS 策略设置为 &quot;<code>ClusterFirstWithHostNet</code>&quot;，
因为它已将 <code>hostNetwork</code> 设置为 <code>true</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;3600&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostNetwork</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">dnsPolicy</span>:<span style="color:#bbb"> </span>ClusterFirstWithHostNet<span style="color:#bbb">
</span></code></pre></div><!--
### Pod's DNS Config






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code>
</div>



Pod's DNS Config allows users more control on the DNS settings for a Pod.

The `dnsConfig` field is optional and it can work with any `dnsPolicy` settings.
However, when a Pod's `dnsPolicy` is set to "`None`", the `dnsConfig` field has
to be specified.

Below are the properties a user can specify in the `dnsConfig` field:
-->
<h3 id="pod-dns-config">Pod 的 DNS 配置 </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code>
</div>


<p>Pod 的 DNS 配置可让用户对 Pod 的 DNS 设置进行更多控制。</p>
<p><code>dnsConfig</code> 字段是可选的，它可以与任何 <code>dnsPolicy</code> 设置一起使用。
但是，当 Pod 的 <code>dnsPolicy</code> 设置为 &quot;<code>None</code>&quot; 时，必须指定 <code>dnsConfig</code> 字段。</p>
<p>用户可以在 <code>dnsConfig</code> 字段中指定以下属性：</p>
<!--
- `nameservers`: a list of IP addresses that will be used as DNS servers for the
  Pod. There can be at most 3 IP addresses specified. When the Pod's `dnsPolicy`
  is set to "`None`", the list must contain at least one IP address, otherwise
  this property is optional.
  The servers listed will be combined to the base nameservers generated from the
  specified DNS policy with duplicate addresses removed.
- `searches`: a list of DNS search domains for hostname lookup in the Pod.
  This property is optional. When specified, the provided list will be merged
  into the base search domain names generated from the chosen DNS policy.
  Duplicate domain names are removed.
  Kubernetes allows for at most 6 search domains.
- `options`: an optional list of objects where each object may have a `name`
  property (required) and a `value` property (optional). The contents in this
  property will be merged to the options generated from the specified DNS policy.
  Duplicate entries are removed.
-->
<ul>
<li>
<p><code>nameservers</code>：将用作于 Pod 的 DNS 服务器的 IP 地址列表。
最多可以指定 3 个 IP 地址。当 Pod 的 <code>dnsPolicy</code> 设置为 &quot;<code>None</code>&quot; 时，
列表必须至少包含一个 IP 地址，否则此属性是可选的。
所列出的服务器将合并到从指定的 DNS 策略生成的基本名称服务器，并删除重复的地址。</p>
</li>
<li>
<p><code>searches</code>：用于在 Pod 中查找主机名的 DNS 搜索域的列表。此属性是可选的。
指定此属性时，所提供的列表将合并到根据所选 DNS 策略生成的基本搜索域名中。
重复的域名将被删除。Kubernetes 最多允许 6 个搜索域。</p>
</li>
<li>
<p><code>options</code>：可选的对象列表，其中每个对象可能具有 <code>name</code> 属性（必需）和 <code>value</code> 属性（可选）。
此属性中的内容将合并到从指定的 DNS 策略生成的选项。
重复的条目将被删除。</p>
</li>
</ul>
<!--
The following is an example Pod with custom DNS settings:
-->
<p>以下是具有自定义 DNS 设置的 Pod 示例：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/custom-dns.yaml" download="service/networking/custom-dns.yaml"><code>service/networking/custom-dns.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-custom-dns-yaml')" title="Copy service/networking/custom-dns.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-custom-dns-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dns-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">dnsPolicy</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;None&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">dnsConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nameservers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#666">1.2.3.4</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">searches</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- ns1.svc.cluster-domain.example<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- my.dns.search.suffix<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">options</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ndots<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>edns0<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
When the Pod above is created, the container `test` gets the following contents
in its `/etc/resolv.conf` file:
-->
<p>创建上面的 Pod 后，容器 <code>test</code> 会在其 <code>/etc/resolv.conf</code> 文件中获取以下内容：</p>
<pre><code>nameserver 1.2.3.4
search ns1.svc.cluster-domain.example my.dns.search.suffix
options ndots:2 edns0
</code></pre><!--
For IPv6 setup, search path and name server should be setup like this:
-->
<p>对于 IPv6 设置，搜索路径和名称服务器应按以下方式设置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it dns-example -- cat /etc/resolv.conf
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于</p>
<pre><code>nameserver fd00:79:30::a
search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example
options ndots:5
</code></pre><!--
#### Expanded DNS Configuration






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.22 [alpha]</code>
</div>



By default, for Pod's DNS Config, Kubernetes allows at most 6 search domains and
a list of search domains of up to 256 characters.

If the feature gate `ExpandedDNSConfig` is enabled for the kube-apiserver and
the kubelet, it is allowed for Kubernetes to have at most 32 search domains and
a list of search domains of up to 2048 characters.
-->
<h4 id="expanded-dns-configuration">扩展 DNS 配置 </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.22 [alpha]</code>
</div>


<p>对于 Pod DNS 配置，Kubernetes 默认允许最多 6 个 搜索域（ Search Domain）
以及一个最多 256 个字符的搜索域列表。</p>
<p>如果启用 kube-apiserver 和 kubelet 的特性门控 <code>ExpandedDNSConfig</code>，Kubernetes 将可以有最多 32 个
搜索域以及一个最多 2048 个字符的搜索域列表。</p>
<h2 id="what-s-next">What's next</h2>
<!--
For guidance on administering DNS configurations, check
[Configure DNS Service](/docs/tasks/administer-cluster/dns-custom-nameservers/)
-->
<p>有关管理 DNS 配置的指导，请查看
<a href="/zh/docs/tasks/administer-cluster/dns-custom-nameservers/">配置 DNS 服务</a></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f804ac0532fcade3966ea2e3769ca031">5.4 - 使用 Service 连接到应用</h1>
    
	<!-- overview -->
<!--
## The Kubernetes model for connecting containers

Now that you have a continuously running, replicated application you can expose it on a network.

Kubernetes assumes that pods can communicate with other pods, regardless of which host they land on. Kubernetes gives every pod its own cluster-private IP address, so you do not need to explicitly create links between pods or map container ports to host ports. This means that containers within a Pod can all reach each other's ports on localhost, and all pods in a cluster can see each other without NAT. The rest of this document elaborates on how you can run reliable services on such a networking model.

This guide uses a simple nginx server to demonstrate proof of concept.
-->
<h2 id="kubernetes-连接容器的模型">Kubernetes 连接容器的模型</h2>
<p>既然有了一个持续运行、可复制的应用，我们就能够将它暴露到网络上。</p>
<p>Kubernetes 假设 Pod 可与其它 Pod 通信，不管它们在哪个主机上。
Kubernetes 给每一个 Pod 分配一个集群私有 IP 地址，所以没必要在
Pod 与 Pod 之间创建连接或将容器的端口映射到主机端口。
这意味着同一个 Pod 内的所有容器能通过 localhost 上的端口互相连通，集群中的所有 Pod
也不需要通过 NAT 转换就能够互相看到。
本文档的剩余部分详述如何在上述网络模型之上运行可靠的服务。</p>
<p>本指南使用一个简单的 Nginx 服务器来演示概念验证原型。</p>
<!-- body -->
<!--
## Exposing pods to the cluster

We did this in a previous example, but let's do it once again and focus on the networking perspective.
Create an nginx Pod, and note that it has a container port specification:
-->
<h2 id="在集群中暴露-pod">在集群中暴露 Pod</h2>
<p>我们在之前的示例中已经做过，然而让我们以网络连接的视角再重做一遍。
创建一个 Nginx Pod，注意其中包含一个容器端口的规约：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/run-my-nginx.yaml" download="service/networking/run-my-nginx.yaml"><code>service/networking/run-my-nginx.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-run-my-nginx-yaml')" title="Copy service/networking/run-my-nginx.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-run-my-nginx-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
This makes it accessible from any node in your cluster. Check the nodes the Pod is running on:
-->
<p>这使得可以从集群中任何一个节点来访问它。检查节点，该 Pod 正在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f ./run-my-nginx.yaml
kubectl get pods -l <span style="color:#b8860b">run</span><span style="color:#666">=</span>my-nginx -o wide
</code></pre></div><pre><code>NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE
my-nginx-3800858182-jr4a2   1/1       Running   0          13s       10.244.3.4    kubernetes-minion-905m
my-nginx-3800858182-kna2y   1/1       Running   0          13s       10.244.2.5    kubernetes-minion-ljyd
</code></pre><!--
Check your pods' IPs:
-->
<p>检查 Pod 的 IP 地址：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">run</span><span style="color:#666">=</span>my-nginx -o yaml | grep podIP
    podIP: 10.244.3.4
    podIP: 10.244.2.5
</code></pre></div><!--
You should be able to ssh into any node in your cluster and use a tool such as `curl` to make queries against both IPs. Note that the containers are *not* using port 80 on the node, nor are there any special NAT rules to route traffic to the pod. This means you can run multiple nginx pods on the same node all using the same `containerPort`, and access them from any other pod or node in your cluster using the assigned IP address for the Service. If you want to arrange for a specific port on the host Node to be forwarded to backing Pods, you can - but the networking model should mean that you do not need to do so.

You can read more about the [Kubernetes Networking Model](/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model) if you're curious.
-->
<p>你应该能够通过 ssh 登录到集群中的任何一个节点上，并使用诸如 <code>curl</code> 之类的工具向这两个 IP 地址发出查询请求。
需要注意的是，容器不会使用该节点上的 80 端口，也不会使用任何特定的 NAT 规则去路由流量到 Pod 上。
这意味着可以在同一个节点上运行多个 Nginx Pod，使用相同的 <code>containerPort</code>，并且可以从集群中任何其他的
Pod 或节点上使用 IP 的方式访问到它们。
如果你想的话，你依然可以将宿主节点的某个端口的流量转发到 Pod 中，但是出于网络模型的原因，你不必这么做。</p>
<p>如果对此好奇，请参考 <a href="/zh/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model">Kubernetes 网络模型</a>。</p>
<!--
## Creating a Service

So we have pods running nginx in a flat, cluster wide, address space. In theory, you could talk to these pods directly, but what happens when a node dies? The pods die with it, and the Deployment will create new ones, with different IPs. This is the problem a Service solves.

A Kubernetes Service is an abstraction which defines a logical set of Pods running somewhere in your cluster, that all provide the same functionality. When created, each Service is assigned a unique IP address (also called clusterIP). This address is tied to the lifespan of the Service, and will not change while the Service is alive. Pods can be configured to talk to the Service, and know that communication to the Service will be automatically load-balanced out to some pod that is a member of the Service.

You can create a Service for your 2 nginx replicas with `kubectl expose`:
-->
<h2 id="创建-service">创建 Service</h2>
<p>我们有一组在一个扁平的、集群范围的地址空间中运行 Nginx 服务的 Pod。
理论上，你可以直接连接到这些 Pod，但如果某个节点死掉了会发生什么呢？
Pod 会终止，Deployment 将创建新的 Pod，且使用不同的 IP。这正是 Service 要解决的问题。</p>
<p>Kubernetes Service 是集群中提供相同功能的一组 Pod 的抽象表达。
当每个 Service 创建时，会被分配一个唯一的 IP 地址（也称为 clusterIP）。
这个 IP 地址与 Service 的生命周期绑定在一起，只要 Service 存在，它就不会改变。
可以配置 Pod 使它与 Service 进行通信，Pod 知道与 Service 通信将被自动地负载均衡到该 Service 中的某些 Pod 上。</p>
<p>可以使用 <code>kubectl expose</code> 命令为 2个 Nginx 副本创建一个 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose deployment/my-nginx
</code></pre></div><pre><code>service/my-nginx exposed
</code></pre><!--
This is equivalent to `kubectl apply -f` the following yaml:
-->
<p>这等价于使用 <code>kubectl create -f</code> 命令及如下的 yaml 文件创建：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/nginx-svc.yaml" download="service/networking/nginx-svc.yaml"><code>service/networking/nginx-svc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-nginx-svc-yaml')" title="Copy service/networking/nginx-svc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-nginx-svc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
This specification will create a Service which targets TCP port 80 on any Pod
with the `run: my-nginx` label, and expose it on an abstracted Service port
(`targetPort`: is the port the container accepts traffic on, `port`: is the
abstracted Service port, which can be any port other pods use to access the
Service).
View [Service](/docs/reference/generated/kubernetes-api/v1.23/#service-v1-core)
API object to see the list of supported fields in service definition.
Check your Service:
-->
<p>上述规约将创建一个 Service，该 Service 会将所有具有标签 <code>run: my-nginx</code> 的 Pod 的 TCP
80 端口暴露到一个抽象的 Service 端口上（<code>targetPort</code>：容器接收流量的端口；<code>port</code>：可任意取值的抽象的 Service
端口，其他 Pod 通过该端口访问 Service）。
查看 <a href="/docs/reference/generated/kubernetes-api/v1.23/#service-v1-core">Service</a>
API 对象以了解 Service 所能接受的字段列表。
查看你的 Service 资源:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc my-nginx
</code></pre></div><pre><code>NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
my-nginx   ClusterIP   10.0.162.149   &lt;none&gt;        80/TCP    21s
</code></pre><!--
As mentioned previously, a Service is backed by a group of Pods. These Pods are
exposed through `endpoints`. The Service's selector will be evaluated continuously
and the results will be POSTed to an Endpoints object also named `my-nginx`.
When a Pod dies, it is automatically removed from the endpoints, and new Pods
matching the Service's selector will automatically get added to the endpoints.
Check the endpoints, and note that the IPs are the same as the Pods created in
the first step:
-->
<p>正如前面所提到的，一个 Service 由一组 Pod 提供支撑。这些 Pod 通过 <code>endpoints</code> 暴露出来。
Service Selector 将持续评估，结果被 POST 到一个名称为 <code>my-nginx</code> 的 Endpoint 对象上。
当 Pod 终止后，它会自动从 Endpoint 中移除，新的能够匹配上 Service Selector 的 Pod 将自动地被添加到 Endpoint 中。
检查该 Endpoint，注意到 IP 地址与在第一步创建的 Pod 是相同的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe svc my-nginx
</code></pre></div><pre><code>Name:                my-nginx
Namespace:           default
Labels:              run=my-nginx
Annotations:         &lt;none&gt;
Selector:            run=my-nginx
Type:                ClusterIP
IP:                  10.0.162.149
Port:                &lt;unset&gt; 80/TCP
Endpoints:           10.244.2.5:80,10.244.3.4:80
Session Affinity:    None
Events:              &lt;none&gt;
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get ep my-nginx
</code></pre></div><pre><code>NAME       ENDPOINTS                     AGE
my-nginx   10.244.2.5:80,10.244.3.4:80   1m
</code></pre><!--
You should now be able to curl the nginx Service on `<CLUSTER-IP>:<PORT>` from
any node in your cluster. Note that the Service IP is completely virtual, it
never hits the wire. If you're curious about how this works you can read more
about the [service proxy](/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies).
-->
<p>现在，你应该能够从集群中任意节点上使用 curl 命令向 <code>&lt;CLUSTER-IP&gt;:&lt;PORT&gt;</code> 发送请求以访问 Nginx Service。
注意 Service IP 完全是虚拟的，它从来没有走过网络，如果对它如何工作的原理感到好奇，
可以进一步阅读<a href="/zh/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">服务代理</a>
的内容。</p>
<!--
## Accessing the Service

Kubernetes supports 2 primary modes of finding a Service - environment variables
and DNS. The former works out of the box while the latter requires the
[CoreDNS cluster addon](https://releases.k8s.io/v1.23.0/cluster/addons/dns/coredns).
-->
<h2 id="访问-service">访问 Service</h2>
<p>Kubernetes支持两种查找服务的主要模式: 环境变量和 DNS。前者开箱即用，而后者则需要
<a href="https://releases.k8s.io/v1.23.0/cluster/addons/dns/coredns">CoreDNS 集群插件</a>.</p>
<!--
If the service environment variables are not desired (because possible clashing with expected program ones,
too many variables to process, only using DNS, etc) you can disable this mode by setting the `enableServiceLinks`
flag to `false` on the [pod spec](/docs/reference/generated/kubernetes-api/v1.23/#pod-v1-core).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果不需要服务环境变量（因为可能与预期的程序冲突，可能要处理的变量太多，或者仅使用DNS等），则可以通过在
<a href="/docs/reference/generated/kubernetes-api/v1.23/#pod-v1-core">pod spec</a>
上将 <code>enableServiceLinks</code> 标志设置为 <code>false</code> 来禁用此模式。
</div>
<!--
### Environment Variables

When a Pod runs on a Node, the kubelet adds a set of environment variables for
each active Service. This introduces an ordering problem. To see why, inspect
the environment of your running nginx Pods (your Pod name will be different):
-->
<h3 id="环境变量">环境变量</h3>
<p>当 Pod 在节点上运行时，kubelet 会针对每个活跃的 Service 为 Pod 添加一组环境变量。
这就引入了一个顺序的问题。为解释这个问题，让我们先检查正在运行的 Nginx Pod
的环境变量（你的环境中的 Pod 名称将会与下面示例命令中的不同）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE
</code></pre></div><pre><code>KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
</code></pre><!--
Note there's no mention of your Service. This is because you created the replicas
before the Service. Another disadvantage of doing this is that the scheduler might
put both Pods on the same machine, which will take your entire Service down if
it dies. We can do this the right way by killing the 2 Pods and waiting for the
Deployment to recreate them. This time around the Service exists *before* the
replicas. This will give you scheduler-level Service spreading of your Pods
(provided all your nodes have equal capacity), as well as the right environment
variables:
-->
<p>能看到环境变量中并没有你创建的 Service 相关的值。这是因为副本的创建先于 Service。
这样做的另一个缺点是，调度器可能会将所有 Pod 部署到同一台机器上，如果该机器宕机则整个 Service 都会离线。
要改正的话，我们可以先终止这 2 个 Pod，然后等待 Deployment 去重新创建它们。
这次 Service 会<em>先于</em>副本存在。这将实现调度器级别的 Pod 按 Service
分布（假定所有的节点都具有同样的容量），并提供正确的环境变量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale deployment my-nginx --replicas<span style="color:#666">=</span>0; kubectl scale deployment my-nginx --replicas<span style="color:#666">=</span>2;

kubectl get pods -l <span style="color:#b8860b">run</span><span style="color:#666">=</span>my-nginx -o wide
</code></pre></div><pre><code>NAME                        READY     STATUS    RESTARTS   AGE     IP            NODE
my-nginx-3800858182-e9ihh   1/1       Running   0          5s      10.244.2.7    kubernetes-minion-ljyd
my-nginx-3800858182-j4rm4   1/1       Running   0          5s      10.244.3.8    kubernetes-minion-905m
</code></pre><!--
You may notice that the pods have different names, since they are killed and recreated.
-->
<p>你可能注意到，Pod 具有不同的名称，这是因为它们是被重新创建的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> my-nginx-3800858182-e9ihh -- printenv | grep SERVICE
</code></pre></div><pre><code>KUBERNETES_SERVICE_PORT=443
MY_NGINX_SERVICE_HOST=10.0.162.149
KUBERNETES_SERVICE_HOST=10.0.0.1
MY_NGINX_SERVICE_PORT=80
KUBERNETES_SERVICE_PORT_HTTPS=443
</code></pre><h3 id="dns">DNS</h3>
<!--
Kubernetes offers a DNS cluster addon Service that automatically assigns dns names to other Services. You can check if it's running on your cluster:
-->
<p>Kubernetes 提供了一个自动为其它 Service 分配 DNS 名字的 DNS 插件 Service。
你可以通过如下命令检查它是否在工作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get services kube-dns --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><pre><code>NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.0.0.10    &lt;none&gt;        53/UDP,53/TCP   8m
</code></pre><!--
The rest of this section will assume you have a Service with a long lived IP
(my-nginx), and a DNS server that has assigned a name to that IP. Here we use the CoreDNS cluster addon (application name `kube-dns`), so you can talk to the Service from any pod in your cluster using standard methods (e.g. `gethostbyname()`). If CoreDNS isn't running, you can enable it referring to the [CoreDNS README](https://github.com/coredns/deployment/tree/master/kubernetes) or [Installing CoreDNS](/docs/tasks/administer-cluster/coredns/#installing-coredns). Let's run another curl application to test this:
-->
<p>本段剩余的内容假设你已经有一个拥有持久 IP 地址的 Service（my-nginx），以及一个为其
IP 分配名称的 DNS 服务器。 这里我们使用 CoreDNS 集群插件（应用名为 <code>kube-dns</code>），
所以在集群中的任何 Pod 中，你都可以使用标准方法（例如：<code>gethostbyname()</code>）与该 Service 通信。
如果 CoreDNS 没有在运行，你可以参照
<a href="https://github.com/coredns/deployment/tree/master/kubernetes">CoreDNS README</a>
或者<a href="/zh/docs/tasks/administer-cluster/coredns/#installing-coredns">安装 CoreDNS</a> 来启用它。
让我们运行另一个 curl 应用来进行测试：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run curl --image<span style="color:#666">=</span>radial/busyboxplus:curl -i --tty
</code></pre></div><pre><code>Waiting for pod default/curl-131556218-9fnch to be running, status is Pending, pod ready: false
Hit enter for command prompt
</code></pre><!--
Then, hit enter and run `nslookup my-nginx`:
-->
<p>然后，按回车并执行命令 <code>nslookup my-nginx</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#666">[</span> root@curl-131556218-9fnch:/ <span style="color:#666">]</span>$ nslookup my-nginx
Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      my-nginx
Address 1: 10.0.162.149
</code></pre></div><!--
## Securing the Service

Till now we have only accessed the nginx server from within the cluster. Before exposing the Service to the internet, you want to make sure the communication channel is secure. For this, you will need:

* Self signed certificates for https (unless you already have an identity certificate)
* An nginx server configured to use the certificates
* A [secret](/docs/concepts/configuration/secret/) that makes the certificates accessible to pods

You can acquire all these from the [nginx https example](https://github.com/kubernetes/examples/tree/master/staging/https-nginx/). This requires having go and make tools installed. If you don't want to install those, then follow the manual steps later. In short:
-->
<h2 id="securing-the-service">保护 Service</h2>
<p>到现在为止，我们只在集群内部访问了 Nginx 服务器。在将 Service 暴露到因特网之前，我们希望确保通信信道是安全的。
为实现这一目的，需要：</p>
<ul>
<li>用于 HTTPS 的自签名证书（除非已经有了一个身份证书）</li>
<li>使用证书配置的 Nginx 服务器</li>
<li>使 Pod 可以访问证书的 <a href="/zh/docs/concepts/configuration/secret/">Secret</a></li>
</ul>
<p>你可以从
<a href="https://github.com/kubernetes/examples/tree/master/staging/https-nginx/">Nginx https 示例</a>获取所有上述内容。
你需要安装 go 和 make 工具。如果你不想安装这些软件，可以按照后文所述的手动执行步骤执行操作。简要过程如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">make keys <span style="color:#b8860b">KEY</span><span style="color:#666">=</span>/tmp/nginx.key <span style="color:#b8860b">CERT</span><span style="color:#666">=</span>/tmp/nginx.crt
kubectl create secret tls nginxsecret --key /tmp/nginx.key --cert /tmp/nginx.crt
</code></pre></div><pre><code>secret/nginxsecret created
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secrets
</code></pre></div><pre><code>NAME                  TYPE                                  DATA      AGE
default-token-il9rc   kubernetes.io/service-account-token   1         1d
nginxsecret           kubernetes.io/tls                     2         1m
</code></pre><!--
And also the configmap:
-->
<p>以下是 configmap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create configmap nginxconfigmap --from-file<span style="color:#666">=</span>default.conf
</code></pre></div><pre><code>configmap/nginxconfigmap created
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get configmaps
</code></pre></div><pre><code>NAME             DATA   AGE
nginxconfigmap   1      114s
</code></pre><!--
Following are the manual steps to follow in case you run into problems running make (on windows for example):
-->
<p>以下是你在运行 make 时遇到问题时要遵循的手动步骤（例如，在 Windows 上）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建公钥和相对应的私钥</span>
openssl req -x509 -nodes -days <span style="color:#666">365</span> -newkey rsa:2048 -keyout /d/tmp/nginx.key -out /d/tmp/nginx.crt -subj <span style="color:#b44">&#34;/CN=my-nginx/O=my-nginx&#34;</span>
<span style="color:#080;font-style:italic"># 对密钥实施 base64 编码</span>
cat /d/tmp/nginx.crt | base64
cat /d/tmp/nginx.key | base64
</code></pre></div><!--
Use the output from the previous commands to create a yaml file as follows. The base64 encoded value should all be on a single line.
-->
<p>使用前面命令的输出来创建 yaml 文件，如下所示。 base64 编码的值应全部放在一行上。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Secret&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;nginxsecret&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;default&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>kubernetes.io/tls  <span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tls.crt</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURIekNDQWdlZ0F3SUJBZ0lKQUp5M3lQK0pzMlpJTUEwR0NTcUdTSWIzRFFFQkJRVUFNQ1l4RVRBUEJnTlYKQkFNVENHNW5hVzU0YzNaak1SRXdEd1lEVlFRS0V3aHVaMmx1ZUhOMll6QWVGdzB4TnpFd01qWXdOekEzTVRKYQpGdzB4T0RFd01qWXdOekEzTVRKYU1DWXhFVEFQQmdOVkJBTVRDRzVuYVc1NGMzWmpNUkV3RHdZRFZRUUtFd2h1CloybHVlSE4yWXpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSjFxSU1SOVdWM0IKMlZIQlRMRmtobDRONXljMEJxYUhIQktMSnJMcy8vdzZhU3hRS29GbHlJSU94NGUrMlN5ajBFcndCLzlYTnBwbQppeW1CL3JkRldkOXg5UWhBQUxCZkVaTmNiV3NsTVFVcnhBZW50VWt1dk1vLzgvMHRpbGhjc3paenJEYVJ4NEo5Ci82UVRtVVI3a0ZTWUpOWTVQZkR3cGc3dlVvaDZmZ1Voam92VG42eHNVR0M2QURVODBpNXFlZWhNeVI1N2lmU2YKNHZpaXdIY3hnL3lZR1JBRS9mRTRqakxCdmdONjc2SU90S01rZXV3R0ljNDFhd05tNnNTSzRqYUNGeGpYSnZaZQp2by9kTlEybHhHWCtKT2l3SEhXbXNhdGp4WTRaNVk3R1ZoK0QrWnYvcW1mMFgvbVY0Rmo1NzV3ajFMWVBocWtsCmdhSXZYRyt4U1FVQ0F3RUFBYU5RTUU0d0hRWURWUjBPQkJZRUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjcKTUI4R0ExVWRJd1FZTUJhQUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjdNQXdHQTFVZEV3UUZNQU1CQWY4dwpEUVlKS29aSWh2Y05BUUVGQlFBRGdnRUJBRVhTMW9FU0lFaXdyMDhWcVA0K2NwTHI3TW5FMTducDBvMm14alFvCjRGb0RvRjdRZnZqeE04Tzd2TjB0clcxb2pGSW0vWDE4ZnZaL3k4ZzVaWG40Vm8zc3hKVmRBcStNZC9jTStzUGEKNmJjTkNUekZqeFpUV0UrKzE5NS9zb2dmOUZ3VDVDK3U2Q3B5N0M3MTZvUXRUakViV05VdEt4cXI0Nk1OZWNCMApwRFhWZmdWQTRadkR4NFo3S2RiZDY5eXM3OVFHYmg5ZW1PZ05NZFlsSUswSGt0ejF5WU4vbVpmK3FqTkJqbWZjCkNnMnlwbGQ0Wi8rUUNQZjl3SkoybFIrY2FnT0R4elBWcGxNSEcybzgvTHFDdnh6elZPUDUxeXdLZEtxaUMwSVEKQ0I5T2wwWW5scE9UNEh1b2hSUzBPOStlMm9KdFZsNUIyczRpbDlhZ3RTVXFxUlU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tls.key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUV2UUlCQURBTkJna3Foa2lHOXcwQkFRRUZBQVNDQktjd2dnU2pBZ0VBQW9JQkFRQ2RhaURFZlZsZHdkbFIKd1V5eFpJWmVEZWNuTkFhbWh4d1NpeWF5N1AvOE9ta3NVQ3FCWmNpQ0RzZUh2dGtzbzlCSzhBZi9WemFhWm9zcApnZjYzUlZuZmNmVUlRQUN3WHhHVFhHMXJKVEVGSzhRSHA3VkpMcnpLUC9QOUxZcFlYTE0yYzZ3MmtjZUNmZitrCkU1bEVlNUJVbUNUV09UM3c4S1lPNzFLSWVuNEZJWTZMMDUrc2JGQmd1Z0ExUE5JdWFubm9UTWtlZTRuMG4rTDQKb3NCM01ZUDhtQmtRQlAzeE9JNHl3YjREZXUraURyU2pKSHJzQmlIT05Xc0RadXJFaXVJMmdoY1kxeWIyWHI2UAozVFVOcGNSbC9pVG9zQngxcHJHclk4V09HZVdPeGxZZmcvbWIvNnBuOUYvNWxlQlkrZStjSTlTMkQ0YXBKWUdpCkwxeHZzVWtGQWdNQkFBRUNnZ0VBZFhCK0xkbk8ySElOTGo5bWRsb25IUGlHWWVzZ294RGQwci9hQ1Zkank4dlEKTjIwL3FQWkUxek1yall6Ry9kVGhTMmMwc0QxaTBXSjdwR1lGb0xtdXlWTjltY0FXUTM5SjM0VHZaU2FFSWZWNgo5TE1jUHhNTmFsNjRLMFRVbUFQZytGam9QSFlhUUxLOERLOUtnNXNrSE5pOWNzMlY5ckd6VWlVZWtBL0RBUlBTClI3L2ZjUFBacDRuRWVBZmI3WTk1R1llb1p5V21SU3VKdlNyblBESGtUdW1vVlVWdkxMRHRzaG9reUxiTWVtN3oKMmJzVmpwSW1GTHJqbGtmQXlpNHg0WjJrV3YyMFRrdWtsZU1jaVlMbjk4QWxiRi9DSmRLM3QraTRoMTVlR2ZQegpoTnh3bk9QdlVTaDR2Q0o3c2Q5TmtEUGJvS2JneVVHOXBYamZhRGR2UVFLQmdRRFFLM01nUkhkQ1pKNVFqZWFKClFGdXF4cHdnNzhZTjQyL1NwenlUYmtGcVFoQWtyczJxWGx1MDZBRzhrZzIzQkswaHkzaE9zSGgxcXRVK3NHZVAKOWRERHBsUWV0ODZsY2FlR3hoc0V0L1R6cEdtNGFKSm5oNzVVaTVGZk9QTDhPTm1FZ3MxMVRhUldhNzZxelRyMgphRlpjQ2pWV1g0YnRSTHVwSkgrMjZnY0FhUUtCZ1FEQmxVSUUzTnNVOFBBZEYvL25sQVB5VWs1T3lDdWc3dmVyClUycXlrdXFzYnBkSi9hODViT1JhM05IVmpVM25uRGpHVHBWaE9JeXg5TEFrc2RwZEFjVmxvcG9HODhXYk9lMTAKMUdqbnkySmdDK3JVWUZiRGtpUGx1K09IYnRnOXFYcGJMSHBzUVpsMGhucDBYSFNYVm9CMUliQndnMGEyOFVadApCbFBtWmc2d1BRS0JnRHVIUVV2SDZHYTNDVUsxNFdmOFhIcFFnMU16M2VvWTBPQm5iSDRvZUZKZmcraEppSXlnCm9RN3hqWldVR3BIc3AyblRtcHErQWlSNzdyRVhsdlhtOElVU2FsbkNiRGlKY01Pc29RdFBZNS9NczJMRm5LQTQKaENmL0pWb2FtZm1nZEN0ZGtFMXNINE9MR2lJVHdEbTRpb0dWZGIwMllnbzFyb2htNUpLMUI3MkpBb0dBUW01UQpHNDhXOTVhL0w1eSt5dCsyZ3YvUHM2VnBvMjZlTzRNQ3lJazJVem9ZWE9IYnNkODJkaC8xT2sybGdHZlI2K3VuCnc1YytZUXRSTHlhQmd3MUtpbGhFZDBKTWU3cGpUSVpnQWJ0LzVPbnlDak9OVXN2aDJjS2lrQ1Z2dTZsZlBjNkQKckliT2ZIaHhxV0RZK2Q1TGN1YSt2NzJ0RkxhenJsSlBsRzlOZHhrQ2dZRUF5elIzT3UyMDNRVVV6bUlCRkwzZAp4Wm5XZ0JLSEo3TnNxcGFWb2RjL0d5aGVycjFDZzE2MmJaSjJDV2RsZkI0VEdtUjZZdmxTZEFOOFRwUWhFbUtKCnFBLzVzdHdxNWd0WGVLOVJmMWxXK29xNThRNTBxMmk1NVdUTThoSDZhTjlaMTltZ0FGdE5VdGNqQUx2dFYxdEYKWSs4WFJkSHJaRnBIWll2NWkwVW1VbGc9Ci0tLS0tRU5EIFBSSVZBVEUgS0VZLS0tLS0K&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
Now create the secrets using the file:
-->
<p>现在使用文件创建 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f nginxsecrets.yaml
kubectl get secrets
</code></pre></div><pre><code>NAME                  TYPE                                  DATA      AGE
default-token-il9rc   kubernetes.io/service-account-token   1         1d
nginxsecret           kubernetes.io/tls                     2         1m
</code></pre><!--
Now modify your nginx replicas to start an https server using the certificate in the secret, and the Service, to expose both ports (80 and 443):
-->
<p>现在修改 nginx 副本以启动一个使用 Secret 中的证书的 HTTPS 服务器以及相应的用于暴露其端口（80 和 443）的 Service：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/nginx-secure-app.yaml" download="service/networking/nginx-secure-app.yaml"><code>service/networking/nginx-secure-app.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-nginx-secure-app-yaml')" title="Copy service/networking/nginx-secure-app.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-nginx-secure-app-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>NodePort<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">443</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>https<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>nginxsecret<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>configmap-volume<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginxconfigmap<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginxhttps<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>bprashanth/nginxhttps:1.0<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">443</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/nginx/ssl<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/nginx/conf.d<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>configmap-volume<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Noteworthy points about the nginx-secure-app manifest:

- It contains both Deployment and Service specification in the same file.
- The [nginx server](https://github.com/kubernetes/examples/tree/master/staging/https-nginx/default.conf)
  serves HTTP traffic on port 80 and HTTPS traffic on 443, and nginx Service
  exposes both ports.
- Each container has access to the keys through a volume mounted at `/etc/nginx/ssl`.
  This is setup *before* the nginx server is started.
-->
<p>关于 nginx-secure-app 清单，值得注意的几点如下：</p>
<ul>
<li>它将 Deployment 和 Service 的规约放在了同一个文件中。</li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/https-nginx/default.conf">Nginx 服务器</a>通过
80 端口处理 HTTP 流量，通过 443 端口处理 HTTPS 流量，而 Nginx Service 则暴露了这两个端口。</li>
<li>每个容器能通过挂载在 <code>/etc/nginx/ssl</code> 的卷访问秘钥。卷和密钥需要在 Nginx 服务器启动<em>之前</em>配置好。</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yaml
</code></pre></div><!--
At this point you can reach the nginx server from any node.
-->
<p>这时，你可以从任何节点访问到 Nginx 服务器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -o yaml | grep -i podip
    podIP: 10.244.3.5
node $ curl -k https://10.244.3.5
...
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
</code></pre></div><!--
Note how we supplied the `-k` parameter to curl in the last step, this is because we don't know anything about the pods running nginx at certificate generation time,
so we have to tell curl to ignore the CName mismatch. By creating a Service we linked the CName used in the certificate with the actual DNS name used by pods during Service lookup.
Let's test this from a pod (the same secret is being reused for simplicity, the pod only needs nginx.crt to access the Service):
-->
<p>注意最后一步我们是如何提供 <code>-k</code> 参数执行 curl 命令的，这是因为在证书生成时，
我们不知道任何关于运行 nginx 的 Pod 的信息，所以不得不在执行 curl 命令时忽略 CName 不匹配的情况。
通过创建 Service，我们连接了在证书中的 CName 与在 Service 查询时被 Pod 使用的实际 DNS 名字。
让我们从一个 Pod 来测试（为了方便，这里使用同一个 Secret，Pod 仅需要使用 nginx.crt 去访问 Service）：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/curlpod.yaml" download="service/networking/curlpod.yaml"><code>service/networking/curlpod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-curlpod-yaml')" title="Copy service/networking/curlpod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-curlpod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>curl-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>curlpod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>curlpod<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>nginxsecret<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>curlpod<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- while true; do sleep 1; done<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>radial/busyboxplus:curl<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/nginx/ssl<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f ./curlpod.yaml
kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>curlpod
</code></pre></div><pre><code>NAME                               READY     STATUS    RESTARTS   AGE
curl-deployment-1515033274-1410r   1/1       Running   0          1m
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> curl-deployment-1515033274-1410r -- curl https://my-nginx --cacert /etc/nginx/ssl/tls.crt
...
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...
</code></pre></div><!--
## Exposing the Service

For some parts of your applications you may want to expose a Service onto an
external IP address. Kubernetes supports two ways of doing this: NodePorts and
LoadBalancers. The Service created in the last section already used `NodePort`,
so your nginx HTTPS replica is ready to serve traffic on the internet if your
node has a public IP.
-->
<h2 id="暴露-service">暴露 Service</h2>
<p>对应用的某些部分，你可能希望将 Service 暴露在一个外部 IP 地址上。
Kubernetes 支持两种实现方式：NodePort 和 LoadBalancer。
在上一段创建的 Service 使用了 <code>NodePort</code>，因此，如果你的节点有一个公网
IP，那么 Nginx HTTPS 副本已经能够处理因特网上的流量。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc my-nginx -o yaml | grep nodePort -C <span style="color:#666">5</span>
</code></pre></div><pre><code>  uid: 07191fb3-f61a-11e5-8ae5-42010af00002
spec:
  clusterIP: 10.0.162.149
  ports:
  - name: http
    nodePort: 31704
    port: 8080
    protocol: TCP
    targetPort: 80
  - name: https
    nodePort: 32453
    port: 443
    protocol: TCP
    targetPort: 443
  selector:
    run: my-nginx
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes -o yaml | grep ExternalIP -C <span style="color:#666">1</span>
</code></pre></div><pre><code>    - address: 104.197.41.11
      type: ExternalIP
    allocatable:
--
    - address: 23.251.152.56
      type: ExternalIP
    allocatable:
...

$ curl https://&lt;EXTERNAL-IP&gt;:&lt;NODE-PORT&gt; -k
...
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
</code></pre><!--
Let's now recreate the Service to use a cloud load balancer. Change the `Type` of `my-nginx` Service from `NodePort` to `LoadBalancer`:
-->
<p>让我们重新创建一个 Service 以使用云负载均衡器。
将 <code>my-nginx</code> Service 的 <code>Type</code> 由 <code>NodePort</code> 改成 <code>LoadBalancer</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit svc my-nginx
kubectl get svc my-nginx
</code></pre></div><pre><code>NAME       TYPE           CLUSTER-IP     EXTERNAL-IP        PORT(S)               AGE
my-nginx   LoadBalancer   10.0.162.149   xx.xxx.xxx.xxx     8080:30163/TCP        21s
</code></pre><pre><code>curl https://&lt;EXTERNAL-IP&gt; -k
...
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</code></pre><!--
The IP address in the `EXTERNAL-IP` column is the one that is available on the public internet.  The `CLUSTER-IP` is only available inside your
cluster/private cloud network.

Note that on AWS, type `LoadBalancer` creates an ELB, which uses a (long)
hostname, not an IP.  It's too long to fit in the standard `kubectl get svc`
output, in fact, so you'll need to do `kubectl describe service my-nginx` to
see it.  You'll see something like this:
-->
<p>在 <code>EXTERNAL-IP</code> 列中的 IP 地址能在公网上被访问到。<code>CLUSTER-IP</code> 只能从集群/私有云网络中访问。</p>
<p>注意，在 AWS 上，类型 <code>LoadBalancer</code> 的服务会创建一个 ELB，且 ELB 使用主机名（比较长），而不是 IP。
ELB 的主机名太长以至于不能适配标准 <code>kubectl get svc</code> 的输出，所以需要通过执行
<code>kubectl describe service my-nginx</code> 命令来查看它。
可以看到类似如下内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe service my-nginx
...
LoadBalancer Ingress:   a320587ffd19711e5a37606cf4a74574-1142138393.us-east-1.elb.amazonaws.com
...
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [Using a Service to Access an Application in a Cluster](/docs/tasks/access-application-cluster/service-access-application-cluster/)
* Learn more about [Connecting a Front End to a Back End Using a Service](/docs/tasks/access-application-cluster/connecting-frontend-backend/)
* Learn more about [Creating an External Load Balancer](/docs/tasks/access-application-cluster/create-external-load-balancer/)
-->
<ul>
<li>进一步了解如何<a href="/zh/docs/tasks/access-application-cluster/service-access-application-cluster/">使用 Service 访问集群中的应用</a></li>
<li>进一步了解如何<a href="/zh/docs/tasks/access-application-cluster/connecting-frontend-backend/">使用 Service 将前端连接到后端</a></li>
<li>进一步了解如何<a href="/zh/docs/tasks/access-application-cluster/create-external-load-balancer/">创建外部负载均衡器</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-199bcc92443dbc9bed44819467d7eb75">5.5 - Ingress</h1>
    
	<!--
title: Ingress
content_type: concept
weight: 40
-->
<!-- overview -->
<p>




<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code>
</div>


<!--
---
title: Ingress
id: ingress
date: 2018-04-12
full_link: /docs/concepts/services-networking/ingress/
short_description: >
  An API object that manages external access to the services in a cluster, typically HTTP.

aka: 
tags:
- networking
- architecture
- extension
---
-->
<!--
 An API object that manages external access to the services in a cluster, typically HTTP.
-->
<p>Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。</p>
<!--
Ingress may provide load balancing, SSL termination and name-based virtual hosting.
-->
<p>Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。</p></p>
<!-- body -->
<!--
## Terminology

For clarity, this guide defines the following terms:
-->
<h2 id="术语">术语</h2>
<p>为了表达更加清晰，本指南定义了以下术语：</p>
<!-- 
* Node: A worker machine in Kubernetes, part of a cluster.
* Cluster: A set of Nodes that run containerized applications managed by Kubernetes. For this example, and in most common Kubernetes deployments, nodes in the cluster are not part of the public internet.
* Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware.
* Cluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the Kubernetes [networking model](/docs/concepts/cluster-administration/networking/).
* Service: A Kubernetes <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a> that identifies a set of Pods using <a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='label'>label</a> selectors. Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.
 -->
<ul>
<li>节点（Node）: Kubernetes 集群中的一台工作机器，是集群的一部分。</li>
<li>集群（Cluster）: 一组运行由 Kubernetes 管理的容器化应用程序的节点。
在此示例和在大多数常见的 Kubernetes 部署环境中，集群中的节点都不在公共网络中。</li>
<li>边缘路由器（Edge Router）: 在集群中强制执行防火墙策略的路由器。可以是由云提供商管理的网关，也可以是物理硬件。</li>
<li>集群网络（Cluster Network）: 一组逻辑的或物理的连接，根据 Kubernetes
<a href="/zh/docs/concepts/cluster-administration/networking/">网络模型</a>在集群内实现通信。</li>
<li>服务（Service）：Kubernetes <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a>，
使用<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>选择器（selectors）辨认一组 Pod。
除非另有说明，否则假定服务只具有在集群网络中可路由的虚拟 IP。</li>
</ul>
<!--
## What is Ingress?

[Ingress](/docs/reference/generated/kubernetes-api/v1.23/#ingress-v1beta1-networking-k8s-io) exposes HTTP and HTTPS routes from outside the cluster to
<a href="/docs/concepts/services-networking/service/" target="_blank">services</a> within the cluster.
Traffic routing is controlled by rules defined on the Ingress resource.
-->
<h2 id="ingress-是什么">Ingress 是什么？</h2>
<p><a href="/docs/reference/generated/kubernetes-api/v1.23/#ingress-v1beta1-networking-k8s-io">Ingress</a>
公开了从集群外部到集群内<a href="/zh/docs/concepts/services-networking/service/">服务</a>的 HTTP 和 HTTPS 路由。
流量路由由 Ingress 资源上定义的规则控制。</p>
<!--
Here is a simple example where an Ingress sends all its traffic to one Service:
-->
<p>下面是一个将所有流量都发送到同一 Service 的简单 Ingress 示例：</p>
<figure>
<div class="mermaid">
    
graph LR;
  client([客户端])-. Ingress-管理的 <br> 负载均衡器 .->ingress[Ingress];
  ingress-->|路由规则|service[Service];
  subgraph cluster
  ingress;
  service-->pod1[Pod];
  service-->pod2[Pod];
  end
  classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
  classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
  classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
  class ingress,service,pod1,pod2 k8s;
  class client plain;
  class cluster cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!-- 
An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name based virtual hosting. An [Ingress controller](/docs/concepts/services-networking/ingress-controllers) is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.
-->
<p>Ingress 可为 Service 提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS，以及基于名称的虚拟托管。
<a href="/zh/docs/concepts/services-networking/ingress-controllers">Ingress 控制器</a>
通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。</p>
<!-- 
An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically
uses a service of type [Service.Type=NodePort](/docs/concepts/services-networking/service/#type-nodeport) or
[Service.Type=LoadBalancer](/docs/concepts/services-networking/service/#loadbalancer).
-->
<p>Ingress 不会公开任意端口或协议。
将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用
<a href="/zh/docs/concepts/services-networking/service/#type-nodeport">Service.Type=NodePort</a>
或 <a href="/zh/docs/concepts/services-networking/service/#loadbalancer">Service.Type=LoadBalancer</a>
类型的 Service。</p>
<!--
## Prerequisites

You must have an [ingress controller](/docs/concepts/services-networking/ingress-controllers) to satisfy an Ingress. Only creating an Ingress resource has no effect.
-->
<h2 id="环境准备">环境准备</h2>
<p>你必须拥有一个 <a href="/zh/docs/concepts/services-networking/ingress-controllers">Ingress 控制器</a> 才能满足 Ingress 的要求。
仅创建 Ingress 资源本身没有任何效果。</p>
<!-- 
You may need to deploy an Ingress controller such as [ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/). You can choose from a number of
[Ingress controllers](/docs/concepts/services-networking/ingress-controllers).
-->
<p>你可能需要部署 Ingress 控制器，例如 <a href="https://kubernetes.github.io/ingress-nginx/deploy/">ingress-nginx</a>。
你可以从许多 <a href="/zh/docs/concepts/services-networking/ingress-controllers">Ingress 控制器</a> 中进行选择。</p>
<!-- 
Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress
controllers operate slightly differently.
 -->
<p>理想情况下，所有 Ingress 控制器都应符合参考规范。但实际上，不同的 Ingress 控制器操作略有不同。</p>
<!--
Make sure you review your Ingress controller's documentation to understand the caveats of choosing it.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 确保你查看了 Ingress 控制器的文档，以了解选择它的注意事项。
</div>
<!--
## The Ingress Resource

A minimal Ingress resource example:
-->
<h2 id="the-ingress-resource">Ingress 资源 </h2>
<p>一个最小的 Ingress 资源示例：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/minimal-ingress.yaml" download="service/networking/minimal-ingress.yaml"><code>service/networking/minimal-ingress.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-minimal-ingress-yaml')" title="Copy service/networking/minimal-ingress.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-minimal-ingress-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>minimal-ingress<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nginx.ingress.kubernetes.io/rewrite-target</span>:<span style="color:#bbb"> </span>/<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ingressClassName</span>:<span style="color:#bbb"> </span>nginx-example<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/testpath<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 
An Ingress needs `apiVersion`, `kind`, `metadata` and `spec` fields.
The name of an Ingress object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
For general information about working with config files, see [deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/), [configuring containers](/docs/tasks/configure-pod-container/configure-pod-configmap/), [managing resources](/docs/concepts/cluster-administration/manage-deployment/).
 Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which
 is the [rewrite-target annotation](https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md).
Different [Ingress controllers](/docs/concepts/services-networking/ingress-controllers) support different annotations. Review the documentation for
 your choice of Ingress controller to learn which annotations are supported.
-->
<p>Ingress 需要指定 <code>apiVersion</code>、<code>kind</code>、 <code>metadata</code>和 <code>spec</code> 字段。
Ingress 对象的命名必须是合法的 <a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名名称</a>。
关于如何使用配置文件，请参见<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">部署应用</a>、
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">配置容器</a>、
<a href="/zh/docs/concepts/cluster-administration/manage-deployment/">管理资源</a>。
Ingress 经常使用注解（annotations）来配置一些选项，具体取决于 Ingress
控制器，例如<a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md">重写目标注解</a>。
不同的 <a href="/zh/docs/concepts/services-networking/ingress-controllers">Ingress 控制器</a>支持不同的注解。
查看你所选的 Ingress 控制器的文档，以了解其支持哪些注解。</p>
<!-- 
The Ingress [spec](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)
has all the information needed to configure a load balancer or proxy server. Most importantly, it
contains a list of rules matched against all incoming requests. Ingress resource only supports rules
for directing HTTP(S) traffic.
-->
<p>Ingress <a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status">规约</a>
提供了配置负载均衡器或者代理服务器所需的所有信息。
最重要的是，其中包含与所有传入请求匹配的规则列表。
Ingress 资源仅支持用于转发 HTTP(S) 流量的规则。</p>
<!--
If the `ingressClassName` is omitted, a [default Ingress class](#default-ingress-class)
should be defined.

There are some ingress controllers, that work without the definition of a
default `IngressClass`. For example, the Ingress-NGINX controller can be
configured with a [flag](https://kubernetes.github.io/ingress-nginx/#what-is-the-flag-watch-ingress-without-class)
`--watch-ingress-without-class`. It is [recommended](https://kubernetes.github.io/ingress-nginx/#i-have-only-one-instance-of-the-ingresss-nginx-controller-in-my-cluster-what-should-i-do)  though, to specify the
default `IngressClass` as shown [below](#default-ingress-class).
-->
<p>如果 <code>ingressClassName</code> 被省略，那么你应该定义一个<a href="#default-ingress-class">默认 Ingress 类</a>。</p>
<p>有一些 Ingress 控制器不需要定义默认的 <code>IngressClass</code>。比如：Ingress-NGINX
控制器可以通过<a href="https://kubernetes.github.io/ingress-nginx/#what-is-the-flag-watch-ingress-without-class">参数</a>
<code>--watch-ingress-without-class</code> 来配置。
不过仍然<a href="https://kubernetes.github.io/ingress-nginx/#i-have-only-one-instance-of-the-ingresss-nginx-controller-in-my-cluster-what-should-i-do">推荐</a>
按<a href="#default-ingress-class">下文</a>所示来设置默认的 <code>IngressClass</code>。</p>
<!-- 
### Ingress rules

Each HTTP rule contains the following information:
-->
<h3 id="ingress-rules">Ingress 规则 </h3>
<p>每个 HTTP 规则都包含以下信息：</p>
<!-- 
* An optional host. In this example, no host is specified, so the rule applies to all inbound
  HTTP traffic through the IP address specified. If a host is provided (for example,
  foo.bar.com), the rules apply to that host.
* A list of paths (for example, `/testpath`), each of which has an associated backend defined with a `serviceName`
  and `servicePort`. Both the host and path must match the content of an incoming request before the
  load balancer directs traffic to the referenced Service.
* A backend is a combination of Service and port names as described in the
  [Service doc](/docs/concepts/services-networking/service/). HTTP (and HTTPS) requests to the
  Ingress that matches the host and path of the rule are sent to the listed backend.
-->
<ul>
<li>可选的 <code>host</code>。在此示例中，未指定 <code>host</code>，因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信。
如果提供了 <code>host</code>（例如 foo.bar.com），则 <code>rules</code> 适用于该 <code>host</code>。</li>
<li>路径列表 paths（例如，<code>/testpath</code>）,每个路径都有一个由 <code>serviceName</code> 和 <code>servicePort</code> 定义的关联后端。
在负载均衡器将流量定向到引用的服务之前，主机和路径都必须匹配传入请求的内容。</li>
<li><code>backend</code>（后端）是 <a href="/zh/docs/concepts/services-networking/service/">Service 文档</a>中所述的服务和端口名称的组合。
与规则的 <code>host</code> 和 <code>path</code> 匹配的对 Ingress 的 HTTP（和 HTTPS ）请求将发送到列出的 <code>backend</code>。</li>
</ul>
<!-- 
A `defaultBackend` is often configured in an Ingress controller to service any requests that do not
match a path in the spec. 
-->
<p>通常在 Ingress 控制器中会配置 <code>defaultBackend</code>（默认后端），以服务于无法与规约中 <code>path</code> 匹配的所有请求。</p>
<!-- 
### DefaultBackend {#default-backend}

An Ingress with no rules sends all traffic to a single default backend and `.spec.defaultBackend`
is the backend that should handle requests in that case.
The `defaultBackend` is conventionally a configuration option of the
[Ingress controller](/docs/concepts/services-networking/ingress-controllers) and
is not specified in your Ingress resources.
If no `.spec.rules` are specified, `.spec.defaultBackend` must be specified.
If `defaultBackend` is not set, the handling of requests that do not match any of the rules will be up to the
ingress controller (consult the documentation for your ingress controller to find out how it handles this case). 

If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is
routed to your default backend.
-->
<h3 id="default-backend">默认后端 </h3>
<p>没有设置规则的 Ingress 将所有流量发送到同一个默认后端，而
<code>.spec.defaultBackend</code> 则是在这种情况下处理请求的那个默认后端。
<code>defaultBackend</code> 通常是
<a href="/zh/docs/concepts/services-networking/ingress-controllers">Ingress 控制器</a>的配置选项，而非在
Ingress 资源中指定。
如果未设置任何的 <code>.spec.rules</code>，那么必须指定 <code>.spec.defaultBackend</code>。
如果未设置 <code>defaultBackend</code>，那么如何处理所有与规则不匹配的流量将交由
Ingress 控制器决定（请参考你的 Ingress 控制器的文档以了解它是如何处理那些流量的）。</p>
<p>如果没有 <code>hosts</code> 或 <code>paths</code> 与 Ingress 对象中的 HTTP 请求匹配，则流量将被路由到默认后端。</p>
<!--
### Resource backends {#resource-backend}

A `Resource` backend is an ObjectRef to another Kubernetes resource within the
same namespace as the Ingress object. A `Resource` is a mutually exclusive
setting with Service, and will fail validation if both are specified. A common
usage for a `Resource` backend is to ingress data to an object storage backend
with static assets.
-->
<h3 id="resource-backend">资源后端 </h3>
<p><code>Resource</code> 后端是一个引用，指向同一命名空间中的另一个 Kubernetes 资源，将其作为 Ingress 对象。
<code>Resource</code> 后端与 Service 后端是互斥的，在二者均被设置时会无法通过合法性检查。
<code>Resource</code> 后端的一种常见用法是将所有入站数据导向带有静态资产的对象存储后端。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/ingress-resource-backend.yaml" download="service/networking/ingress-resource-backend.yaml"><code>service/networking/ingress-resource-backend.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-ingress-resource-backend-yaml')" title="Copy service/networking/ingress-resource-backend.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-ingress-resource-backend-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ingress-resource-backend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">defaultBackend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>k8s.example.com<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageBucket<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>static-assets<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/icons<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>ImplementationSpecific<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>k8s.example.com<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageBucket<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>icon-assets<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
After creating the Ingress above, you can view it with the following command:
-->
<p>创建了如上的 Ingress 之后，你可以使用下面的命令查看它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl describe ingress ingress-resource-backend
</code></pre></div><pre><code>Name:             ingress-resource-backend
Namespace:        default
Address:
Default backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets
Annotations:  &lt;none&gt;
Events:       &lt;none&gt;
</code></pre><!-- 
### Path Types

Each path in an Ingress is required to have a corresponding path type. Paths
that do not include an explicit `pathType` will fail validation. There are three
supported path types:
-->
<h3 id="path-types">路径类型 </h3>
<p>Ingress 中的每个路径都需要有对应的路径类型（Path Type）。未明确设置 <code>pathType</code>
的路径无法通过合法性检查。当前支持的路径类型有三种：</p>
<!-- 
* `ImplementationSpecific`: With this path type, matching is up to
  the IngressClass. Implementations can treat this as a separate `pathType` or
  treat it identically to `Prefix` or `Exact` path types.

* `Exact`: Matches the URL path exactly and with case sensitivity.

* `Prefix`: Matches based on a URL path prefix split by `/`. Matching is case
  sensitive and done on a path element by element basis. A path element refers
  to the list of labels in the path split by the `/` separator. A request is a
  match for path _p_ if every _p_ is an element-wise prefix of _p_ of the
  request path.

  If the last element of the path is a substring of the last
  element in request path, it is not a match (for example: `/foo/bar`
  matches`/foo/bar/baz`, but does not match `/foo/barbaz`).
 -->
<ul>
<li>
<p><code>ImplementationSpecific</code>：对于这种路径类型，匹配方法取决于 IngressClass。
具体实现可以将其作为单独的 <code>pathType</code> 处理或者与 <code>Prefix</code> 或 <code>Exact</code> 类型作相同处理。</p>
</li>
<li>
<p><code>Exact</code>：精确匹配 URL 路径，且区分大小写。</p>
</li>
<li>
<p><code>Prefix</code>：基于以 <code>/</code> 分隔的 URL 路径前缀匹配。匹配区分大小写，并且对路径中的元素逐个完成。
路径元素指的是由 <code>/</code> 分隔符分隔的路径中的标签列表。
如果每个 <em>p</em> 都是请求路径 <em>p</em> 的元素前缀，则请求与路径 <em>p</em> 匹配。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果路径的最后一个元素是请求路径中最后一个元素的子字符串，则不会匹配
（例如：<code>/foo/bar</code> 匹配 <code>/foo/bar/baz</code>, 但不匹配 <code>/foo/barbaz</code>）。
</div>
</li>
</ul>
<!--
### Examples

| Kind   | Path(s)                         | Request path(s)             | Matches?                           |
|--------|---------------------------------|-----------------------------|------------------------------------|
| Prefix | `/`                             | (all paths)                 | Yes                                |
| Exact  | `/foo`                          | `/foo`                      | Yes                                |
| Exact  | `/foo`                          | `/bar`                      | No                                 |
| Exact  | `/foo`                          | `/foo/`                     | No                                 |
| Exact  | `/foo/`                         | `/foo`                      | No                                 |
| Prefix | `/foo`                          | `/foo`, `/foo/`             | Yes                                |
| Prefix | `/foo/`                         | `/foo`, `/foo/`             | Yes                                |
| Prefix | `/aaa/bb`                       | `/aaa/bbb`                  | No                                 |
| Prefix | `/aaa/bbb`                      | `/aaa/bbb`                  | Yes                                |
| Prefix | `/aaa/bbb/`                     | `/aaa/bbb`                  | Yes, ignores trailing slash        |
| Prefix | `/aaa/bbb`                      | `/aaa/bbb/`                 | Yes,  matches trailing slash       |
| Prefix | `/aaa/bbb`                      | `/aaa/bbb/ccc`              | Yes, matches subpath               |
| Prefix | `/aaa/bbb`                      | `/aaa/bbbxyz`               | No, does not match string prefix   |
| Prefix | `/`, `/aaa`                     | `/aaa/ccc`                  | Yes, matches `/aaa` prefix         |
| Prefix | `/`, `/aaa`, `/aaa/bbb`         | `/aaa/bbb`                  | Yes, matches `/aaa/bbb` prefix     |
| Prefix | `/`, `/aaa`, `/aaa/bbb`         | `/ccc`                      | Yes, matches `/` prefix            |
| Prefix | `/aaa`                          | `/ccc`                      | No, uses default backend           |
| Mixed  | `/foo` (Prefix), `/foo` (Exact) | `/foo`                      | Yes, prefers Exact                 |
-->
<h3 id="示例">示例</h3>
<table>
<thead>
<tr>
<th>类型</th>
<th>路径</th>
<th>请求路径</th>
<th>匹配与否？</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prefix</td>
<td><code>/</code></td>
<td>（所有路径）</td>
<td>是</td>
</tr>
<tr>
<td>Exact</td>
<td><code>/foo</code></td>
<td><code>/foo</code></td>
<td>是</td>
</tr>
<tr>
<td>Exact</td>
<td><code>/foo</code></td>
<td><code>/bar</code></td>
<td>否</td>
</tr>
<tr>
<td>Exact</td>
<td><code>/foo</code></td>
<td><code>/foo/</code></td>
<td>否</td>
</tr>
<tr>
<td>Exact</td>
<td><code>/foo/</code></td>
<td><code>/foo</code></td>
<td>否</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/foo</code></td>
<td><code>/foo</code>, <code>/foo/</code></td>
<td>是</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/foo/</code></td>
<td><code>/foo</code>, <code>/foo/</code></td>
<td>是</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/aaa/bb</code></td>
<td><code>/aaa/bbb</code></td>
<td>否</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/aaa/bbb</code></td>
<td><code>/aaa/bbb</code></td>
<td>是</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/aaa/bbb/</code></td>
<td><code>/aaa/bbb</code></td>
<td>是，忽略尾部斜线</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/aaa/bbb</code></td>
<td><code>/aaa/bbb/</code></td>
<td>是，匹配尾部斜线</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/aaa/bbb</code></td>
<td><code>/aaa/bbb/ccc</code></td>
<td>是，匹配子路径</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/aaa/bbb</code></td>
<td><code>/aaa/bbbxyz</code></td>
<td>否，字符串前缀不匹配</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/</code>, <code>/aaa</code></td>
<td><code>/aaa/ccc</code></td>
<td>是，匹配 <code>/aaa</code> 前缀</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/</code>, <code>/aaa</code>, <code>/aaa/bbb</code></td>
<td><code>/aaa/bbb</code></td>
<td>是，匹配 <code>/aaa/bbb</code> 前缀</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/</code>, <code>/aaa</code>, <code>/aaa/bbb</code></td>
<td><code>/ccc</code></td>
<td>是，匹配 <code>/</code> 前缀</td>
</tr>
<tr>
<td>Prefix</td>
<td><code>/aaa</code></td>
<td><code>/ccc</code></td>
<td>否，使用默认后端</td>
</tr>
<tr>
<td>混合</td>
<td><code>/foo</code> (Prefix), <code>/foo</code> (Exact)</td>
<td><code>/foo</code></td>
<td>是，优选 Exact 类型</td>
</tr>
</tbody>
</table>
<!-- 
#### Multiple Matches

In some cases, multiple paths within an Ingress will match a request. In those
cases precedence will be given first to the longest matching path. If two paths
are still equally matched, precedence will be given to paths with an exact path
type over prefix path type.
 -->
<h4 id="multiple-matches">多重匹配 </h4>
<p>在某些情况下，Ingress 中的多条路径会匹配同一个请求。
这种情况下最长的匹配路径优先。
如果仍然有两条同等的匹配路径，则精确路径类型优先于前缀路径类型。</p>
<!--
## Hostname wildcards

Hosts can be precise matches (for example “`foo.bar.com`”) or a wildcard (for
example “`*.foo.com`”). Precise matches require that the HTTP `host` header
matches the `host` field. Wildcard matches require the HTTP `host` header is
equal to the suffix of the wildcard rule.
-->
<h2 id="hostname-wildcards">主机名通配符  </h2>
<p>主机名可以是精确匹配（例如“<code>foo.bar.com</code>”）或者使用通配符来匹配
（例如“<code>*.foo.com</code>”）。
精确匹配要求 HTTP <code>host</code> 头部字段与 <code>host</code> 字段值完全匹配。
通配符匹配则要求 HTTP <code>host</code> 头部字段与通配符规则中的后缀部分相同。</p>
<!--
| Host         | Host header        | Match?                                              |
| ------------ |--------------------| ----------------------------------------------------|
| `*.foo.com`  | `bar.foo.com`      | Matches based on shared suffix                      |
| `*.foo.com`  | `baz.bar.foo.com`  | No match, wildcard only covers a single DNS label   |
| `*.foo.com`  | `foo.com`          | No match, wildcard only covers a single DNS label   |
-->
<table>
<thead>
<tr>
<th>主机</th>
<th>host 头部</th>
<th>匹配与否？</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>*.foo.com</code></td>
<td><code>bar.foo.com</code></td>
<td>基于相同的后缀匹配</td>
</tr>
<tr>
<td><code>*.foo.com</code></td>
<td><code>baz.bar.foo.com</code></td>
<td>不匹配，通配符仅覆盖了一个 DNS 标签</td>
</tr>
<tr>
<td><code>*.foo.com</code></td>
<td><code>foo.com</code></td>
<td>不匹配，通配符仅覆盖了一个 DNS 标签</td>
</tr>
</tbody>
</table>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/ingress-wildcard-host.yaml" download="service/networking/ingress-wildcard-host.yaml"><code>service/networking/ingress-wildcard-host.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-ingress-wildcard-host-yaml')" title="Copy service/networking/ingress-wildcard-host.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-ingress-wildcard-host-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ingress-wildcard-host<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;foo.bar.com&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/bar&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;*.foo.com&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/foo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service2<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 
## Ingress class

Ingresses can be implemented by different controllers, often with different
configuration. Each Ingress should specify a class, a reference to an
IngressClass resource that contains additional configuration including the name
of the controller that should implement the class.
-->
<h2 id="ingress-class">Ingress 类 </h2>
<p>Ingress 可以由不同的控制器实现，通常使用不同的配置。
每个 Ingress 应当指定一个类，也就是一个对 IngressClass 资源的引用。
IngressClass 资源包含额外的配置，其中包括应当实现该类的控制器名称。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/external-lb.yaml" download="service/networking/external-lb.yaml"><code>service/networking/external-lb.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-external-lb-yaml')" title="Copy service/networking/external-lb.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-external-lb-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>IngressClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>external-lb<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">controller</span>:<span style="color:#bbb"> </span>example.com/ingress-controller<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>k8s.example.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>IngressParameters<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>external-lb<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 
The `.spec.parameters` field of an IngressClass lets you reference another
resource that provides configuration related to that IngressClass.

The specific type of parameters to use depends on the ingress controller
that you specify in the `.spec.controller` field of the IngressClass.
 -->
<p>IngressClass 中的 <code>.spec.parameters</code> 字段可用于引用其他资源以提供额外的相关配置。</p>
<p>参数（<code>parameters</code>）的具体类型取决于你在 <code>.spec.controller</code> 字段中指定的 Ingress 控制器。</p>
<!--
### IngressClass scope

Depending on your ingress controller, you may be able to use parameters
that you set cluster-wide, or just for one namespace.
-->
<h3 id="ingressclass-的作用域">IngressClass 的作用域</h3>
<p>取决于你的 Ingress 控制器，你可能可以使用集群范围设置的参数或某个名字空间范围的参数。</p>
<ul class="nav nav-tabs" id="tabs-ingressclass-parameter-scope" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tabs-ingressclass-parameter-scope-0" role="tab" aria-controls="tabs-ingressclass-parameter-scope-0" aria-selected="true">集群作用域</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tabs-ingressclass-parameter-scope-1" role="tab" aria-controls="tabs-ingressclass-parameter-scope-1">命名空间作用域</a></li></ul>
<div class="tab-content" id="tabs-ingressclass-parameter-scope"><div id="tabs-ingressclass-parameter-scope-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tabs-ingressclass-parameter-scope-0">

<p><!--
The default scope for IngressClass parameters is cluster-wide.

If you set the `.spec.parameters` field and don't set
`.spec.parameters.scope`, or if you set `.spec.parameters.scope` to
`Cluster`, then the IngressClass refers to a cluster-scoped resource.
The `kind` (in combination the `apiGroup`) of the parameters
refers to a cluster-scoped API (possibly a custom resource), and
the `name` of the parameters identifies a specific cluster scoped
resource for that API.

For example:
-->
<p>IngressClass 的参数默认是集群范围的。</p>
<p>如果你设置了 <code>.spec.parameters</code> 字段且未设置 <code>.spec.parameters.scope</code>
字段，或是将 <code>.spec.parameters.scope</code> 字段设为了 <code>Cluster</code>，那么该
IngressClass 所指代的即是一个集群作用域的资源。
参数的 <code>kind</code>（和 <code>apiGroup</code> 一起）指向一个集群作用域的
API（可能是一个定制资源（Custom Resource）），而它的
<code>name</code> 则为此 API 确定了一个具体的集群作用域的资源。</p>
<p>示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>IngressClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>external-lb-1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">controller</span>:<span style="color:#bbb"> </span>example.com/ingress-controller<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此 IngressClass 的配置定义在一个名为 “external-config-1” 的</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ClusterIngressParameter（API 组为 k8s.example.net）资源中。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 这项定义告诉 Kubernetes 去寻找一个集群作用域的参数资源。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Cluster<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>k8s.example.net<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterIngressParameter<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>external-config-1<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="tabs-ingressclass-parameter-scope-1" class="tab-pane" role="tabpanel" aria-labelledby="tabs-ingressclass-parameter-scope-1">

<p><div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code>
</div>
<!--
If you set the `.spec.parameters` field and set
`.spec.parameters.scope` to `Namespace`, then the IngressClass refers
to a namespaced-scoped resource. You must also set the `namespace`
field within `.spec.parameters` to the namespace that contains
the parameters you want to use.

The `kind` (in combination the `apiGroup`) of the parameters
refers to a namespaced API (for example: ConfigMap), and
the `name` of the parameters identifies a specific resource
in the namespace you specified in `namespace`.
-->
<p>如果你设置了 <code>.spec.parameters</code> 字段且将 <code>.spec.parameters.scope</code>
字段设为了 <code>Namespace</code>，那么该 IngressClass 将会引用一个命名空间作用域的资源。
<code>.spec.parameters.namespace</code> 必须和此资源所处的命名空间相同。</p>
<p>参数的 <code>kind</code>（和 <code>apiGroup</code>
一起）指向一个命名空间作用域的 API（例如：ConfigMap），而它的
<code>name</code> 则确定了一个位于你指定的命名空间中的具体的资源。</p>
<!--
Namespace-scoped parameters help the cluster operator delegate control over the
configuration (for example: load balancer settings, API gateway definition)
that is used for a workload. If you used a cluster-scoped parameter then either:

- the cluster operator team needs to approve a different team's changes every
  time there's a new configuration change being applied.
- the cluster operator must define specific access controls, such as
  [RBAC](/docs/reference/access-authn-authz/rbac/) roles and bindings, that let
  the application team make changes to the cluster-scoped parameters resource.
-->
<p>命名空间作用域的参数帮助集群操作者将控制细分到用于工作负载的各种配置中（比如：负载均衡设置、API
网关定义）。如果你使用集群作用域的参数，那么你必须从以下两项中选择一项执行：</p>
<ul>
<li>每次修改配置，集群操作团队需要批准其他团队的修改。</li>
<li>集群操作团队定义具体的准入控制，比如 <a href="/zh/docs/reference/access-authn-authz/rbac/">RBAC</a>
角色与角色绑定，以使得应用程序团队可以修改集群作用域的配置参数资源。</li>
</ul>
<!--
The IngressClass API itself is always cluster-scoped.

Here is an example of an IngressClass that refers to parameters that are
namespaced:
-->
<p>IngressClass API 本身是集群作用域的。</p>
<p>这里是一个引用命名空间作用域的配置参数的 IngressClass 的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>IngressClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>external-lb-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">controller</span>:<span style="color:#bbb"> </span>example.com/ingress-controller<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此 IngressClass 的配置定义在一个名为 “external-config” 的</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># IngressParameter（API 组为 k8s.example.com）资源中，</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 该资源位于 “external-configuration” 命名空间中。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespace<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>k8s.example.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>IngressParameter<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>external-configuration<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>external-config<span style="color:#bbb">
</span></code></pre></div></div></div>

<!-- 
### Deprecated Annotation

Before the IngressClass resource and `ingressClassName` field were added in
Kubernetes 1.18, Ingress classes were specified with a
`kubernetes.io/ingress.class` annotation on the Ingress. This annotation was
never formally defined, but was widely supported by Ingress controllers.
-->
<h3 id="deprecated-annotation">废弃的注解 </h3>
<p>在 Kubernetes 1.18 版本引入 IngressClass 资源和 <code>ingressClassName</code> 字段之前，Ingress
类是通过 Ingress 中的一个 <code>kubernetes.io/ingress.class</code> 注解来指定的。
这个注解从未被正式定义过，但是得到了 Ingress 控制器的广泛支持。</p>
<!-- 
The newer `ingressClassName` field on Ingresses is a replacement for that
annotation, but is not a direct equivalent. While the annotation was generally
used to reference the name of the Ingress controller that should implement the
Ingress, the field is a reference to an IngressClass resource that contains
additional Ingress configuration, including the name of the Ingress controller.
-->
<p>Ingress 中新的 <code>ingressClassName</code> 字段是该注解的替代品，但并非完全等价。
该注解通常用于引用实现该 Ingress 的控制器的名称，而这个新的字段则是对一个包含额外
Ingress 配置的 IngressClass 资源的引用，包括 Ingress 控制器的名称。</p>
<!-- 
### Default IngressClass {#default-ingress-class}

You can mark a particular IngressClass as default for your cluster. Setting the
`ingressclass.kubernetes.io/is-default-class` annotation to `true` on an
IngressClass resource will ensure that new Ingresses without an
`ingressClassName` field specified will be assigned this default IngressClass.
-->
<h3 id="default-ingress-class">默认 Ingress 类 </h3>
<p>你可以将一个特定的 IngressClass 标记为集群默认 Ingress 类。
将一个 IngressClass 资源的 <code>ingressclass.kubernetes.io/is-default-class</code> 注解设置为
<code>true</code> 将确保新的未指定 <code>ingressClassName</code> 字段的 Ingress 能够分配为这个默认的
IngressClass.</p>
<!-- 
If you have more than one IngressClass marked as the default for your cluster,
the admission controller prevents creating new Ingress objects that don't have
an `ingressClassName` specified. You can resolve this by ensuring that at most 1
IngressClasess are marked as default in your cluster.
 -->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 如果集群中有多个 IngressClass 被标记为默认，准入控制器将阻止创建新的未指定
<code>ingressClassName</code> 的 Ingress 对象。
解决这个问题只需确保集群中最多只能有一个 IngressClass 被标记为默认。
</div>

<!--
There are some ingress controllers, that work without the definition of a
default `IngressClass`. For example, the Ingress-NGINX controller can be
configured with a [flag](https://kubernetes.github.io/ingress-nginx/#what-is-the-flag-watch-ingress-without-class)
`--watch-ingress-without-class`. It is [recommended](https://kubernetes.github.io/ingress-nginx/#i-have-only-one-instance-of-the-ingresss-nginx-controller-in-my-cluster-what-should-i-do)  though, to specify the
default `IngressClass`:
-->
<p>有一些 Ingress 控制器不需要定义默认的 <code>IngressClass</code>。比如：Ingress-NGINX
控制器可以通过<a href="https://kubernetes.github.io/ingress-nginx/#what-is-the-flag-watch-ingress-without-class">参数</a>
<code>--watch-ingress-without-class</code> 来配置。
不过仍然<a href="https://kubernetes.github.io/ingress-nginx/#i-have-only-one-instance-of-the-ingresss-nginx-controller-in-my-cluster-what-should-i-do">推荐</a>
设置默认的 <code>IngressClass</code>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/default-ingressclass.yaml" download="service/networking/default-ingressclass.yaml"><code>service/networking/default-ingressclass.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-default-ingressclass-yaml')" title="Copy service/networking/default-ingressclass.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-default-ingressclass-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>IngressClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app.kubernetes.io/component</span>:<span style="color:#bbb"> </span>controller<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-example<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ingressclass.kubernetes.io/is-default-class</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">controller</span>:<span style="color:#bbb"> </span>k8s.io/ingress-nginx<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
## Types of Ingress

### Ingress backed by a single Service {#single-service-ingress}

There are existing Kubernetes concepts that allow you to expose a single Service
(see [alternatives](#alternatives)). You can also do this with an Ingress by specifying a
*default backend* with no rules.
-->
<h2 id="types-of-ingress">Ingress 类型 </h2>
<h3 id="single-service-ingress">由单个 Service 来完成的 Ingress  </h3>
<p>现有的 Kubernetes 概念允许你暴露单个 Service (参见<a href="#alternatives">替代方案</a>)。
你也可以通过指定无规则的 <em>默认后端</em> 来对 Ingress 进行此操作。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/test-ingress.yaml" download="service/networking/test-ingress.yaml"><code>service/networking/test-ingress.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-test-ingress-yaml')" title="Copy service/networking/test-ingress.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-test-ingress-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">defaultBackend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 
If you create it using `kubectl apply -f` you should be able to view the state
of the Ingress you added:
-->
<p>如果使用 <code>kubectl apply -f</code> 创建此 Ingress，则应该能够查看刚刚添加的 Ingress 的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get ingress test-ingress
</code></pre></div><pre><code>NAME           CLASS         HOSTS   ADDRESS         PORTS   AGE
test-ingress   external-lb   *       203.0.113.123   80      59s
</code></pre><!-- 
Where `203.0.113.123` is the IP allocated by the Ingress controller to satisfy
this Ingress.
-->
<p>其中 <code>203.0.113.123</code> 是由 Ingress 控制器分配以满足该 Ingress 的 IP。</p>
<!--
Ingress controllers and load balancers may take a minute or two to allocate an IP address.
Until that time, you often see the address listed as `<pending>`.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 入口控制器和负载平衡器可能需要一两分钟才能分配 IP 地址。
在此之前，你通常会看到地址字段的值被设定为 <code>&lt;pending&gt;</code>。
</div>
<!--
### Simple fanout

A fanout configuration routes traffic from a single IP address to more than one Service,
based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers
down to a minimum. For example, a setup like:
-->
<h3 id="simple-fanout">简单扇出 </h3>
<p>一个扇出（fanout）配置根据请求的 HTTP URI 将来自同一 IP 地址的流量路由到多个 Service。
Ingress 允许你将负载均衡器的数量降至最低。例如，这样的设置：</p>
<figure>
<div class="mermaid">
    
graph LR;
  client([客户端])-. Ingress-管理的 <br> 负载均衡器 .->ingress[Ingress, 178.91.123.132];
  ingress-->|/foo|service1[Service service1:4200];
  ingress-->|/bar|service2[Service service2:8080];
  subgraph cluster
  ingress;
  service1-->pod1[Pod];
  service1-->pod2[Pod];
  service2-->pod3[Pod];
  service2-->pod4[Pod];
  end
  classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
  classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
  classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
  class ingress,service1,service2,pod1,pod2,pod3,pod4 k8s;
  class client plain;
  class cluster cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
would require an Ingress such as:
-->
<p>将需要一个如下所示的 Ingress：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/simple-fanout-example.yaml" download="service/networking/simple-fanout-example.yaml"><code>service/networking/simple-fanout-example.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-simple-fanout-example-yaml')" title="Copy service/networking/simple-fanout-example.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-simple-fanout-example-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>simple-fanout-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>foo.bar.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/foo<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">4200</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/bar<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service2<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
When you create the Ingress with `kubectl apply -f`:
-->
<p>当你使用 <code>kubectl apply -f</code> 创建 Ingress 时：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe ingress simple-fanout-example
</code></pre></div><pre><code>Name:             simple-fanout-example
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:4200 (10.8.0.90:4200)
               /bar   service2:8080 (10.8.0.91:8080)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     22s                loadbalancer-controller  default/test
</code></pre><!--
The Ingress controller provisions an implementation-specific load balancer
that satisfies the Ingress, as long as the Services (`service1`, `service2`) exist.
When it has done so, you can see the address of the load balancer at the
Address field.
-->
<p>Ingress 控制器将提供实现特定的负载均衡器来满足 Ingress，
只要 Service (<code>service1</code>，<code>service2</code>) 存在。
当它这样做时，你会在 Address 字段看到负载均衡器的地址。</p>
<!--
Depending on the [Ingress controller](/docs/concepts/services-networking/ingress-controllers)
you are using, you may need to create a default-http-backend
[Service](/docs/concepts/services-networking/service/).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 取决于你所使用的 <a href="/zh/docs/concepts/services-networking/ingress-controllers">Ingress 控制器</a>，
你可能需要创建默认 HTTP 后端<a href="/zh/docs/concepts/services-networking/service/">服务</a>。
</div>
<!--
### Name based virtual hosting

Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.
-->
<h3 id="name-based-virtual-hosting">基于名称的虚拟托管  </h3>
<p>基于名称的虚拟主机支持将针对多个主机名的 HTTP 流量路由到同一 IP 地址上。</p>
<figure>
<div class="mermaid">
    
graph LR;
  client([客户端])-. Ingress-管理的 <br> 负载均衡器 .->ingress[Ingress, 178.91.123.132];
  ingress-->|Host: foo.bar.com|service1[Service service1:80];
  ingress-->|Host: bar.foo.com|service2[Service service2:80];
  subgraph cluster
  ingress;
  service1-->pod1[Pod];
  service1-->pod2[Pod];
  service2-->pod3[Pod];
  service2-->pod4[Pod];
  end
  classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
  classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
  classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
  class ingress,service1,service2,pod1,pod2,pod3,pod4 k8s;
  class client plain;
  class cluster cluster;

</div>
</figure>

<noscript>
  <div class="alert alert-secondary callout" role="alert">
    <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em>
  </div>
</noscript>
<!--
The following Ingress tells the backing load balancer to route requests based on
the [Host header](https://tools.ietf.org/html/rfc7230#section-5.4).
-->
<p>以下 Ingress 让后台负载均衡器基于<a href="https://tools.ietf.org/html/rfc7230#section-5.4">host 头部字段</a>
来路由请求。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/name-virtual-host-ingress.yaml" download="service/networking/name-virtual-host-ingress.yaml"><code>service/networking/name-virtual-host-ingress.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-name-virtual-host-ingress-yaml')" title="Copy service/networking/name-virtual-host-ingress.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-name-virtual-host-ingress-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>name-virtual-host-ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>foo.bar.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>bar.foo.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service2<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 
If you create an Ingress resource without any hosts defined in the rules, then any
web traffic to the IP address of your Ingress controller can be matched without a name based
virtual host being required.
-->
<p>如果你创建的 Ingress 资源没有在 <code>rules</code> 中定义的任何 <code>hosts</code>，则可以匹配指向
Ingress 控制器 IP 地址的任何网络流量，而无需基于名称的虚拟主机。</p>
<!-- 
For example, the following Ingress routes traffic
requested for `first.bar.com` to `service1`, `second.bar.com` to `service2`, and any traffic whose request host header doesn't match `first.bar.com` and `second.bar.com` to `service3`.
-->
<p>例如，以下 Ingress 会将请求 <code>first.bar.com</code> 的流量路由到 <code>service1</code>，将请求
<code>second.bar.com</code> 的流量路由到 <code>service2</code>，而所有其他流量都会被路由到 <code>service3</code>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/name-virtual-host-ingress-no-third-host.yaml" download="service/networking/name-virtual-host-ingress-no-third-host.yaml"><code>service/networking/name-virtual-host-ingress-no-third-host.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-name-virtual-host-ingress-no-third-host-yaml')" title="Copy service/networking/name-virtual-host-ingress-no-third-host.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-name-virtual-host-ingress-no-third-host-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>name-virtual-host-ingress-no-third-host<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>first.bar.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>second.bar.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service2<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service3<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
### TLS

You can secure an Ingress by specifying a <a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>
that contains a TLS private key and certificate. The Ingress resource only
supports a single TLS port, 443, and assumes TLS termination at the ingress point
(traffic to the Service and its Pods is in plaintext).
If the TLS configuration section in an Ingress specifies different hosts, they are
multiplexed on the same port according to the hostname specified through the
SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret
must contain keys named `tls.crt` and `tls.key` that contain the certificate
and private key to use for TLS. For example:
-->
<h3 id="tls">TLS</h3>
<p>你可以通过设定包含 TLS 私钥和证书的<a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>
来保护 Ingress。
Ingress 只支持单个 TLS 端口 443，并假定 TLS 连接终止于
Ingress 节点（与 Service 及其 Pod 之间的流量都以明文传输）。
如果 Ingress 中的 TLS 配置部分指定了不同的主机，那么它们将根据通过
SNI TLS 扩展指定的主机名（如果 Ingress 控制器支持 SNI）在同一端口上进行复用。
TLS Secret 的数据中必须包含用于 TLS 的以键名 <code>tls.crt</code> 保存的证书和以键名 <code>tls.key</code> 保存的私钥。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>testsecret-tls<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tls.crt</span>:<span style="color:#bbb"> </span>base64 编码的证书<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tls.key</span>:<span style="color:#bbb"> </span>base64 编码的私钥<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>kubernetes.io/tls<span style="color:#bbb">
</span></code></pre></div><!--
Referencing this secret in an Ingress tells the Ingress controller to
secure the channel from the client to the load balancer using TLS. You need to make
sure the TLS secret you created came from a certificate that contains a Common
Name (CN), also known as a Fully Qualified Domain Name (FQDN) for `https-example.foo.com`.
-->
<p>在 Ingress 中引用此 Secret 将会告诉 Ingress 控制器使用 TLS 加密从客户端到负载均衡器的通道。
你需要确保创建的 TLS Secret 创建自包含 <code>https-example.foo.com</code> 的公用名称（CN）的证书。
这里的公共名称也被称为全限定域名（FQDN）。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Keep in mind that TLS will not work on the default rule because the
certificates would have to be issued for all the possible sub-domains. Therefore,
`hosts` in the `tls` section need to explicitly match the `host` in the `rules`
section.
-->
<p>注意，默认规则上无法使用 TLS，因为需要为所有可能的子域名发放证书。
因此，<code>tls</code> 字段中的 <code>hosts</code> 的取值需要与 <code>rules</code> 字段中的 <code>host</code> 完全匹配。
</div>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/tls-example-ingress.yaml" download="service/networking/tls-example-ingress.yaml"><code>service/networking/tls-example-ingress.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-tls-example-ingress-yaml')" title="Copy service/networking/tls-example-ingress.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-tls-example-ingress-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>tls-example-ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tls</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">hosts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- https-example.foo.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>testsecret-tls<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>https-example.foo.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 
There is a gap between TLS features supported by various Ingress
controllers. Please refer to documentation on
[nginx](https://kubernetes.github.io/ingress-nginx/user-guide/tls/),
[GCE](https://git.k8s.io/ingress-gce/README.md#frontend-https), or any other
platform specific Ingress controller to understand how TLS works in your environment.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 各种 Ingress 控制器所支持的 TLS 功能之间存在差异。请参阅有关
<a href="https://kubernetes.github.io/ingress-nginx/user-guide/tls/">nginx</a>、
<a href="https://git.k8s.io/ingress-gce/README.md#frontend-https">GCE</a>
或者任何其他平台特定的 Ingress 控制器的文档，以了解 TLS 如何在你的环境中工作。
</div>
<!--
### Load Balancing   {#load-balancing}

An Ingress controller is bootstrapped with some load balancing policy settings
that it applies to all Ingress, such as the load balancing algorithm, backend
weight scheme, and others. More advanced load balancing concepts
(e.g. persistent sessions, dynamic weights) are not yet exposed through the
Ingress. You can instead get these features through the load balancer used for
a Service.
-->
<h3 id="load-balancing">负载均衡 </h3>
<p>Ingress 控制器启动引导时使用一些适用于所有 Ingress
的负载均衡策略设置，例如负载均衡算法、后端权重方案等。
更高级的负载均衡概念（例如持久会话、动态权重）尚未通过 Ingress 公开。
你可以通过用于服务的负载均衡器来获取这些功能。</p>
<!--
It's also worth noting that even though health checks are not exposed directly
through the Ingress, there exist parallel concepts in Kubernetes such as
[readiness probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
that allow you to achieve the same end result. Please review the controller
specific documentation to see how they handle health checks (
[nginx](https://git.k8s.io/ingress-nginx/README.md),
[GCE](https://git.k8s.io/ingress-gce/README.md#health-checks)).
-->
<p>值得注意的是，尽管健康检查不是通过 Ingress 直接暴露的，在 Kubernetes
中存在并行的概念，比如
<a href="/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">就绪检查</a>，
允许你实现相同的目的。
请检查特定控制器的说明文档（<a href="https://git.k8s.io/ingress-nginx/README.md">nginx</a>、
<a href="https://git.k8s.io/ingress-gce/README.md#health-checks">GCE</a>）以了解它们是怎样处理健康检查的。</p>
<!--
## Updating an Ingress

To update an existing Ingress to add a new Host, you can update it by editing the resource:
-->
<h2 id="updating-an-ingress">更新 Ingress  </h2>
<p>要更新现有的 Ingress 以添加新的 Host，可以通过编辑资源来对其进行更新：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe ingress <span style="color:#a2f">test</span>
</code></pre></div><pre><code>Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit ingress <span style="color:#a2f">test</span>
</code></pre></div><!--
This pops up an editor with the existing configuration in YAML format.
Modify it to include the new Host:
-->
<p>这一命令将打开编辑器，允许你以 YAML 格式编辑现有配置。
修改它来增加新的主机：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>foo.bar.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span>service1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">servicePort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/foo<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>bar.baz.com<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span>service2<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">servicePort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/foo<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb"></span>..<span style="color:#bbb">
</span></code></pre></div><!--
After you save your changes, kubectl updates the resource in the API server, which tells the
Ingress controller to reconfigure the load balancer.
-->
<p>保存更改后，kubectl 将更新 API 服务器中的资源，该资源将告诉 Ingress 控制器重新配置负载均衡器。</p>
<!--
Verify this:
-->
<p>验证：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe ingress <span style="color:#a2f">test</span>
</code></pre></div><pre><code>Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test
</code></pre><!--
You can achieve the same outcome by invoking `kubectl replace -f` on a modified Ingress YAML file.
-->
<p>你也可以通过 <code>kubectl replace -f</code> 命令调用修改后的 Ingress yaml 文件来获得同样的结果。</p>
<!--
## Failing across availability zones

Techniques for spreading traffic across failure domains differs between cloud providers.
Please check the documentation of the relevant [Ingress controller](/docs/concepts/services-networking/ingress-controllers) for details.
-->
<h2 id="failing-across-availability-zones">跨可用区失败 </h2>
<p>不同的云厂商使用不同的技术来实现跨故障域的流量分布。详情请查阅相关 Ingress 控制器的文档。
请查看相关 <a href="/zh/docs/concepts/services-networking/ingress-controllers">Ingress 控制器</a>的文档以了解详细信息。</p>
<!--
## Alternatives

You can expose a Service in multiple ways that don't directly involve the Ingress resource:
-->
<h2 id="alternatives">替代方案   </h2>
<p>不直接使用 Ingress 资源，也有多种方法暴露 Service：</p>
<!--
* Use [Service.Type=LoadBalancer](/docs/concepts/services-networking/service/#loadbalancer)
* Use [Service.Type=NodePort](/docs/concepts/services-networking/service/#nodeport)
-->
<ul>
<li>使用 <a href="/zh/docs/concepts/services-networking/service/#loadbalancer">Service.Type=LoadBalancer</a></li>
<li>使用 <a href="/zh/docs/concepts/services-networking/service/#nodeport">Service.Type=NodePort</a></li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about the [Ingress](/docs/reference/kubernetes-api/service-resources/ingress-v1/) API
* Learn about [Ingress Controllers](/docs/concepts/services-networking/ingress-controllers/)
* [Set up Ingress on Minikube with the NGINX Controller](/docs/tasks/access-application-cluster/ingress-minikube/)
-->
<ul>
<li>进一步了解 <a href="/docs/reference/kubernetes-api/service-resources/ingress-v1/">Ingress</a> API</li>
<li>进一步了解 <a href="/zh/docs/concepts/services-networking/ingress-controllers/">Ingress 控制器</a></li>
<li><a href="/zh/docs/tasks/access-application-cluster/ingress-minikube/">使用 NGINX 控制器在 Minikube 上安装 Ingress</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5a8edeb1f2dc8e38cd6d561bb08b0d78">5.6 - Ingress 控制器</h1>
    
	<!--
title: Ingress Controllers
content_type: concept
weight: 40
-->
<!-- overview -->
<!--
In order for the Ingress resource to work, the cluster must have an ingress controller running. 

Unlike other types of controllers which run as part of the `kube-controller-manager` binary, Ingress controllers 
are not started automatically with a cluster. Use this page to choose the ingress controller implementation 
that best fits your cluster.

Kubernetes as a project supports and maintains [AWS](https://github.com/kubernetes-sigs/aws-load-balancer-controller#readme), [GCE](https://git.k8s.io/ingress-gce/README.md#readme), and
  [nginx](https://git.k8s.io/ingress-nginx/README.md#readme) ingress controllers.
-->
<p>为了让 Ingress 资源工作，集群必须有一个正在运行的 Ingress 控制器。</p>
<p>与作为 <code>kube-controller-manager</code> 可执行文件的一部分运行的其他类型的控制器不同，
Ingress 控制器不是随集群自动启动的。
基于此页面，你可选择最适合你的集群的 ingress 控制器实现。</p>
<p>Kubernetes 作为一个项目，目前支持和维护
<a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller#readme">AWS</a>、
<a href="https://git.k8s.io/ingress-gce/README.md">GCE</a>
和 <a href="https://git.k8s.io/ingress-nginx/README.md#readme">Nginx</a> Ingress 控制器。</p>
<!-- body -->
<!--
## Additional controllers
-->
<h2 id="其他控制器">其他控制器</h2>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<!--
* [AKS Application Gateway Ingress Controller](https://docs.microsoft.com/azure/application-gateway/tutorial-ingress-controller-add-on-existing?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Faks%2Ftoc.json&bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json) is an ingress controller that configures the [Azure Application Gateway](https://docs.microsoft.com/azure/application-gateway/overview).
* [Ambassador](https://www.getambassador.io/) API Gateway is an [Envoy](https://www.envoyproxy.io)-based ingress
  controller.
* [Apache APISIX ingress controller](https://github.com/apache/apisix-ingress-controller) is an [Apache APISIX](https://github.com/apache/apisix)-based ingress controller.
* [Avi Kubernetes Operator](https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes) provides L4-L7 load-balancing using [VMware NSX Advanced Load Balancer](https://avinetworks.com/).
-->
<ul>
<li><a href="https://docs.microsoft.com/azure/application-gateway/tutorial-ingress-controller-add-on-existing?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Faks%2Ftoc.json&amp;bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json">AKS 应用程序网关 Ingress 控制器</a>
是一个配置 <a href="https://docs.microsoft.com/azure/application-gateway/overview">Azure 应用程序网关</a>
的 Ingress 控制器。</li>
<li><a href="https://www.getambassador.io/">Ambassador</a> API 网关是一个基于
<a href="https://www.envoyproxy.io">Envoy</a> 的 Ingress 控制器。</li>
<li><a href="https://github.com/apache/apisix-ingress-controller">Apache APISIX Ingress 控制器</a>
是一个基于 <a href="https://github.com/apache/apisix">Apache APISIX 网关</a> 的 Ingress 控制器。</li>
<li><a href="https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes">Avi Kubernetes Operator</a>
使用 <a href="https://avinetworks.com/">VMware NSX Advanced Load Balancer</a>
提供第 4 到第 7 层的负载均衡。</li>
</ul>
<!--
* [BFE Ingress Controller](https://github.com/bfenetworks/ingress-bfe) is a [BFE](https://www.bfe-networks.net)-based ingress controller.
* The [Citrix ingress controller](https://github.com/citrix/citrix-k8s-ingress-controller#readme) works with
  Citrix Application Delivery Controller.
* [Contour](https://projectcontour.io/) is an [Envoy](https://www.envoyproxy.io/) based ingress controller.
* [EnRoute](https://getenroute.io/) is an [Envoy](https://www.envoyproxy.io) based API gateway that can run as an ingress controller.
* [Easegress IngressController](https://github.com/megaease/easegress/blob/main/doc/reference/ingresscontroller.md) is an [Easegress](https://megaease.com/easegress/) based API gateway that can run as an ingress controller.
-->
<ul>
<li><a href="https://github.com/bfenetworks/ingress-bfe">BFE Ingress 控制器</a>是一个基于
<a href="https://www.bfe-networks.net">BFE</a> 的 Ingress 控制器。</li>
<li><a href="https://github.com/citrix/citrix-k8s-ingress-controller#readme">Citrix Ingress 控制器</a>
可以用来与 Citrix Application Delivery Controller 一起使用。</li>
<li><a href="https://projectcontour.io/">Contour</a> 是一个基于 <a href="https://www.envoyproxy.io/">Envoy</a>
的 Ingress 控制器。</li>
<li><a href="https://getenroute.io/">EnRoute</a> 是一个基于 <a href="https://www.envoyproxy.io">Envoy</a>
的 API 网关，可以用作 Ingress 控制器。</li>
<li><a href="https://github.com/megaease/easegress/blob/main/doc/reference/ingresscontroller.md">Easegress IngressController</a>
是一个基于 <a href="https://megaease.com/easegress/">Easegress</a> 的 API 网关，可以用作 Ingress 控制器。</li>
</ul>
<!--
* F5 BIG-IP [Container Ingress Services for Kubernetes](https://clouddocs.f5.com/containers/latest/userguide/kubernetes/)
  lets you use an Ingress to configure F5 BIG-IP virtual servers.
* [Gloo](https://gloo.solo.io) is an open-source ingress controller based on [Envoy](https://www.envoyproxy.io),
  which offers API gateway functionality.
* [HAProxy Ingress](https://haproxy-ingress.github.io/) is an ingress controller for
  [HAProxy](https://www.haproxy.org/#desc).
* The [HAProxy Ingress Controller for Kubernetes](https://github.com/haproxytech/kubernetes-ingress#readme)
  is also an ingress controller for [HAProxy](https://www.haproxy.org/#desc).
* [Istio Ingress](https://istio.io/latest/docs/tasks/traffic-management/ingress/kubernetes-ingress/)
  is an [Istio](https://istio.io/) based ingress controller.
-->
<ul>
<li>F5 BIG-IP 的
<a href="https://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/latest">用于 Kubernetes 的容器 Ingress 服务</a>
让你能够使用 Ingress 来配置 F5 BIG-IP 虚拟服务器。</li>
<li><a href="https://gloo.solo.io">Gloo</a> 是一个开源的、基于 <a href="https://www.envoyproxy.io">Envoy</a> 的
Ingress 控制器，能够提供 API 网关功能。</li>
<li><a href="https://haproxy-ingress.github.io/">HAProxy Ingress</a> 是一个针对
<a href="https://www.haproxy.org/#desc">HAProxy</a> 的 Ingress 控制器。</li>
<li><a href="https://github.com/haproxytech/kubernetes-ingress#readme">用于 Kubernetes 的 HAProxy Ingress 控制器</a>
也是一个针对 <a href="https://www.haproxy.org/#desc">HAProxy</a> 的 Ingress 控制器。</li>
<li><a href="https://istio.io/latest/docs/tasks/traffic-management/ingress/kubernetes-ingress/">Istio Ingress</a>
是一个基于 <a href="https://istio.io/">Istio</a> 的 Ingress 控制器。</li>
</ul>
<!--
* The [Kong Ingress Controller for Kubernetes](https://github.com/Kong/kubernetes-ingress-controller#readme)
  is an ingress controller driving [Kong Gateway](https://konghq.com/kong/).
* The [NGINX Ingress Controller for Kubernetes](https://www.nginx.com/products/nginx-ingress-controller/)
  works with the [NGINX](https://www.nginx.com/resources/glossary/nginx/) webserver (as a proxy).
* The [Pomerium Ingress Controller](https://www.pomerium.com/docs/k8s/ingress.html) is based on [Pomerium](https://pomerium.com/), which offers context-aware access policy.
* [Skipper](https://opensource.zalando.com/skipper/kubernetes/ingress-controller/) HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress, designed as a library to build your custom proxy.
-->
<ul>
<li><a href="https://github.com/Kong/kubernetes-ingress-controller#readme">用于 Kubernetes 的 Kong Ingress 控制器</a>
是一个用来驱动 <a href="https://konghq.com/kong/">Kong Gateway</a> 的 Ingress 控制器。</li>
<li><a href="https://www.nginx.com/products/nginx-ingress-controller/">用于 Kubernetes 的 NGINX Ingress 控制器</a>
能够与 <a href="https://www.nginx.com/resources/glossary/nginx/">NGINX</a>
网页服务器（作为代理）一起使用。</li>
<li><a href="https://www.pomerium.com/docs/k8s/ingress.html">Pomerium Ingress 控制器</a>
基于 <a href="https://pomerium.com/">Pomerium</a>，能提供上下文感知的准入策略。</li>
<li><a href="https://opensource.zalando.com/skipper/kubernetes/ingress-controller/">Skipper</a> HTTP
路由器和反向代理可用于服务组装，支持包括 Kubernetes Ingress
这类使用场景，是一个用以构造你自己的定制代理的库。</li>
</ul>
<!--
* The [Traefik Kubernetes Ingress provider](https://doc.traefik.io/traefik/providers/kubernetes-ingress/) is an
  ingress controller for the [Traefik](https://traefik.io/traefik/) proxy.
* [Tyk Operator](https://github.com/TykTechnologies/tyk-operator) extends Ingress with Custom Resources to bring API Management capabilities to Ingress. Tyk Operator works with the Open Source Tyk Gateway & Tyk Cloud control plane.
* [Voyager](https://appscode.com/products/voyager) is an ingress controller for
  [HAProxy](https://www.haproxy.org/#desc).
-->
<ul>
<li><a href="https://doc.traefik.io/traefik/providers/kubernetes-ingress/">Traefik Kubernetes Ingress 提供程序</a>
是一个用于 <a href="https://traefik.io/traefik/">Traefik</a> 代理的 Ingress 控制器。</li>
<li><a href="https://github.com/TykTechnologies/tyk-operator">Tyk Operator</a>
使用自定义资源扩展 Ingress，为之带来 API 管理能力。Tyk Operator
使用开源的 Tyk Gateway &amp; Tyk Cloud 控制面。</li>
<li><a href="https://appscode.com/products/voyager">Voyager</a> 是一个针对
<a href="https://www.haproxy.org/#desc">HAProxy</a> 的 Ingress 控制器。</li>
</ul>
<!--
## Using multiple Ingress controllers
-->
<h2 id="使用多个-ingress-控制器">使用多个 Ingress 控制器</h2>
<!--
You may deploy any number of ingress controllers using [ingress class](/docs/concepts/services-networking/ingress/#ingress-class)
within a cluster. Note the `.metadata.name` of your ingress class resource. When you create an ingress you would need that name to specify the `ingressClassName` field on your Ingress object (refer to [IngressSpec v1 reference](/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec). `ingressClassName` is a replacement of the older [annotation method](/docs/concepts/services-networking/ingress/#deprecated-annotation).
-->
<p>你可以使用
<a href="/zh/docs/concepts/services-networking/ingress/#ingress-class">Ingress 类</a>在集群中部署任意数量的
Ingress 控制器。
请注意你的 Ingress 类资源的 <code>.metadata.name</code> 字段。
当你创建 Ingress 时，你需要用此字段的值来设置 Ingress 对象的 <code>ingressClassName</code> 字段（请参考
<a href="/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec">IngressSpec v1 reference</a>）。
<code>ingressClassName</code>
是之前的<a href="/zh/docs/concepts/services-networking/ingress/#deprecated-annotation">注解</a>做法的替代。</p>
<!--
If you do not specify an IngressClass for an Ingress, and your cluster has exactly one IngressClass marked as default, then Kubernetes [applies](/docs/concepts/services-networking/ingress/#default-ingress-class) the cluster's default IngressClass to the Ingress.
You mark an IngressClass as default by setting the [`ingressclass.kubernetes.io/is-default-class` annotation](/docs/reference/labels-annotations-taints/#ingressclass-kubernetes-io-is-default-class) on that IngressClass, with the string value `"true"`.

Ideally, all ingress controllers should fulfill this specification, but the various ingress
controllers operate slightly differently.
-->
<p>如果你不为 Ingress 指定一个 IngressClass，并且你的集群中只有一个 IngressClass 被标记为了集群默认，那么
Kubernetes 会<a href="/zh/docs/concepts/services-networking/ingress/#default-ingress-class">应用</a>此默认
IngressClass。
你可以通过将
<a href="/zh/docs/reference/labels-annotations-taints/#ingressclass-kubernetes-io-is-default-class"><code>ingressclass.kubernetes.io/is-default-class</code> 注解</a>
的值设置为 <code>&quot;true&quot;</code> 来将一个 IngressClass 标记为集群默认。</p>
<p>理想情况下，所有 Ingress 控制器都应满足此规范，但各种 Ingress 控制器的操作略有不同。</p>
<!--
Make sure you review your ingress controller's documentation to understand the caveats of choosing it.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 确保你查看了 ingress 控制器的文档，以了解选择它的注意事项。
</div>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [Ingress](/docs/concepts/services-networking/ingress/).
* [Set up Ingress on Minikube with the NGINX Controller](/docs/tasks/access-application-cluster/ingress-minikube).
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/services-networking/ingress/">Ingress</a>。</li>
<li><a href="/zh/docs/tasks/access-application-cluster/ingress-minikube">在 Minikube 上使用 NGINX 控制器安装 Ingress</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-374e5c954990aec58a0797adc70a5039">5.7 - 拓扑感知提示</h1>
    
	<!-- 
---
reviewers:
- robscott
title: Topology Aware Hints
content_type: concept
weight: 45
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!-- 
_Topology Aware Hints_ enable topology aware routing by including suggestions
for how clients should consume endpoints. This approach adds metadata to enable
consumers of EndpointSlice and / or Endpoints objects, so that traffic to
those network endpoints can be routed closer to where it originated.

For example, you can route traffic within a locality to reduce
costs, or to improve network performance.
-->
<p><em>拓扑感知提示</em> 包含客户怎么使用服务端点的建议，从而实现了拓扑感知的路由功能。
这种方法添加了元数据，以启用 EndpointSlice 和/或 Endpoints 对象的调用者，
这样，访问这些网络端点的请求流量就可以在它的发起点附近就近路由。</p>
<p>例如，你可以在一个地域内路由流量，以降低通信成本，或提高网络性能。</p>
<!-- 
The "topology-aware hints" feature is at Beta stage and it is **NOT** enabled
by default. To try out this feature, you have to enable the `TopologyAwareHints`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> “拓扑感知提示”特性处于 Beta 阶段，并且默认情况下<strong>未</strong>启用。
要试用此特性，你必须启用 <code>TopologyAwareHints</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
</div>
<!-- body -->
<!-- 
## Motivation
-->
<h2 id="motivation">动机</h2>
<!-- 
Kubernetes clusters are increasingly deployed in multi-zone environments.
_Topology Aware Hints_ provides a mechanism to help keep traffic within the zone
it originated from. This concept is commonly referred to as "Topology Aware
Routing". When calculating the endpoints for a <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a>,
the EndpointSlice controller considers the topology (region and zone) of each endpoint
and populates the hints field to allocate it to a zone.
Cluster components such as the <a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a>
can then consume those hints, and use them to influence how the traffic to is routed
(favoring topologically closer endpoints).
-->
<p>Kubernetes 集群越来越多的部署到多区域环境中。
<em>拓扑感知提示</em> 提供了一种把流量限制在它的发起区域之内的机制。
这个概念一般被称之为 “拓扑感知路由”。
在计算 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a> 的端点时，
EndpointSlice 控制器会评估每一个端点的拓扑（地域和区域），填充提示字段，并将其分配到某个区域。
集群组件，例如<a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a>
就可以使用这些提示信息，并用他们来影响流量的路由（倾向于拓扑上相邻的端点）。</p>
<!-- 
## Using Topology Aware Hints
-->
<h2 id="using-topology-aware-hints">使用拓扑感知提示</h2>
<!-- 
You can activate Topology Aware Hints for a Service by setting the
`service.kubernetes.io/topology-aware-hints` annotation to `auto`. This tells
the EndpointSlice controller to set topology hints if it is deemed safe.
Importantly, this does not guarantee that hints will always be set.
-->
<p>你可以通过把注解 <code>service.kubernetes.io/topology-aware-hints</code> 的值设置为 <code>auto</code>，
来激活服务的拓扑感知提示功能。
这告诉 EndpointSlice 控制器在它认为安全的时候来设置拓扑提示。
重要的是，这并不能保证总会设置提示（hints）。</p>
<!-- 
## How it works {#implementation}
-->
<h2 id="implementation">工作原理</h2>
<!-- 
The functionality enabling this feature is split into two components: The
EndpointSlice controller and the kube-proxy. This section provides a high level overview
of how each component implements this feature.
-->
<p>此特性启用的功能分为两个组件：EndpointSlice 控制器和 kube-proxy。
本节概述每个组件如何实现此特性。</p>
<!-- 
### EndpointSlice controller {#implementation-control-plane}
-->
<h3 id="implementation-control-plane">EndpointSlice 控制器</h3>
<!-- 
The EndpointSlice controller is responsible for setting hints on EndpointSlices
when this feature is enabled. The controller allocates a proportional amount of
endpoints to each zone. This proportion is based on the
[allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)
CPU cores for nodes running in that zone. For example, if one zone had 2 CPU
cores and another zone only had 1 CPU core, the controller would allocated twice
as many endpoints to the zone with 2 CPU cores.
-->
<p>此特性开启后，EndpointSlice 控制器负责在 EndpointSlice 上设置提示信息。
控制器按比例给每个区域分配一定比例数量的端点。
这个比例来源于此区域中运行节点的
<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">可分配</a>
CPU 核心数。
例如，如果一个区域拥有 2 CPU 核心，而另一个区域只有 1 CPU 核心，
那控制器将给那个有 2 CPU 的区域分配两倍数量的端点。</p>
<!-- 
The following example shows what an EndpointSlice looks like when hints have
been populated:
-->
<p>以下示例展示了提供提示信息后 EndpointSlice 的样子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>discovery.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EndpointSlice<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-hints<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/service-name</span>:<span style="color:#bbb"> </span>example-svc<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">addressType</span>:<span style="color:#bbb"> </span>IPv4<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">endpoints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">addresses</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;10.1.2.3&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">conditions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ready</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostname</span>:<span style="color:#bbb"> </span>pod-1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">zone</span>:<span style="color:#bbb"> </span>zone-a<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">forZones</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;zone-a&#34;</span><span style="color:#bbb">
</span></code></pre></div><h3 id="implementation-kube-proxy">kube-proxy</h3>
<!-- 
The kube-proxy component filters the endpoints it routes to based on the hints set by
the EndpointSlice controller. In most cases, this means that the kube-proxy is able
to route traffic to endpoints in the same zone. Sometimes the controller allocates endpoints
from a different zone to ensure more even distribution of endpoints between zones.
This would result in some traffic being routed to other zones.
-->
<p>kube-proxy 组件依据 EndpointSlice 控制器设置的提示，过滤由它负责路由的端点。
在大多数场合，这意味着 kube-proxy 可以把流量路由到同一个区域的端点。
有时，控制器从某个不同的区域分配端点，以确保在多个区域之间更平均的分配端点。
这会导致部分流量被路由到其他区域。</p>
<!-- 
## Safeguards
-->
<h2 id="safeguards">保护措施</h2>
<!-- 
The Kubernetes control plane and the kube-proxy on each node apply some
safeguard rules before using Topology Aware Hints. If these don't check out,
the kube-proxy selects endpoints from anywhere in your cluster, regardless of the
zone.
-->
<p>Kubernetes 控制平面和每个节点上的 kube-proxy，在使用拓扑感知提示功能前，会应用一些保护措施规则。
如果没有检出，kube-proxy 将无视区域限制，从集群中的任意节点上选择端点。</p>
<!-- 
1. **Insufficient number of endpoints:** If there are less endpoints than zones
   in a cluster, the controller will not assign any hints.
-->
<ol>
<li><strong>端点数量不足：</strong> 如果一个集群中，端点数量少于区域数量，控制器不创建任何提示。</li>
</ol>
<!-- 
2. **Impossible to achieve balanced allocation:** In some cases, it will be
   impossible to achieve a balanced allocation of endpoints among zones. For
   example, if zone-a is twice as large as zone-b, but there are only 2
   endpoints, an endpoint allocated to zone-a may receive twice as much traffic
   as zone-b. The controller does not assign hints if it can't get this "expected
   overload" value below an acceptable threshold for each zone. Importantly this
   is not based on real-time feedback. It is still possible for individual
   endpoints to become overloaded.
-->
<ol start="2">
<li><strong>不可能实现均衡分配：</strong> 在一些场合中，不可能实现端点在区域中的平衡分配。
例如，假设 zone-a 比 zone-b 大两倍，但只有 2 个端点，
那分配到 zone-a 的端点可能收到比 zone-b多两倍的流量。
如果控制器不能确定此“期望的过载”值低于每一个区域可接受的阈值，控制器将不指派提示信息。
重要的是，这不是基于实时反馈。所以对于单独的端点仍有可能超载。</li>
</ol>
<!-- 
3. **One or more Nodes has insufficient information:** If any node does not have
   a `topology.kubernetes.io/zone` label or is not reporting a value for
   allocatable CPU, the control plane does not set any topology-aware endpoint
   hints and so kube-proxy does not filter endpoints by zone.
-->
<ol start="3">
<li><strong>一个或多个节点信息不足：</strong> 如果任一节点没有设置标签 <code>topology.kubernetes.io/zone</code>，
或没有上报可分配的 CPU 数据，控制平面将不会设置任何拓扑感知提示，
继而 kube-proxy 也就不能通过区域过滤端点。</li>
</ol>
<!-- 
4. **One or more endpoints does not have a zone hint:** When this happens,
   the kube-proxy assumes that a transition from or to Topology Aware Hints is
   underway. Filtering endpoints for a Service in this state would be dangerous
   so the kube-proxy falls back to using all endpoints.
-->
<ol start="4">
<li><strong>一个或多个端点没有设置区域提示：</strong> 当这类事情发生时，
kube-proxy 会假设这是正在执行一个从/到拓扑感知提示的转移。
在这种场合下过滤Service 的端点是有风险的，所以 kube-proxy 回撤为使用所有的端点。</li>
</ol>
<!-- 
5. **A zone is not represented in hints:** If the kube-proxy is unable to find
   at least one endpoint with a hint targeting the zone it is running in, it falls
   to using endpoints from all zones. This is most likely to happen as you add
   a new zone into your existing cluster.
-->
<ol start="5">
<li><strong>不在提示中的区域：</strong> 如果 kube-proxy 不能根据一个指示在它所在的区域中发现一个端点，
它回撤为使用所有节点的端点。当你的集群新增一个新的区域时，这种情况发生概率很高。</li>
</ol>
<!-- 
## Constraints
-->
<h2 id="constraints">限制</h2>
<!-- 
* Topology Aware Hints are not used when either `externalTrafficPolicy` or
  `internalTrafficPolicy` is set to `Local` on a Service. It is possible to use
  both features in the same cluster on different Services, just not on the same
  Service.
-->
<ul>
<li>当 Service 的 <code>externalTrafficPolicy</code> 或 <code>internalTrafficPolicy</code> 设置值为 <code>Local</code> 时，
拓扑感知提示功能不可用。
你可以在一个集群的不同服务中使用这两个特性，但不能在同一个服务中这么做。</li>
</ul>
<!-- 
* This approach will not work well for Services that have a large proportion of
  traffic originating from a subset of zones. Instead this assumes that incoming
  traffic will be roughly proportional to the capacity of the Nodes in each
  zone.
-->
<ul>
<li>这种方法不适用于大部分流量来自于一部分区域的服务。
相反的，这里假设入站流量将根据每个区域中节点的服务能力按比例的分配。</li>
</ul>
<!-- 
* The EndpointSlice controller ignores unready nodes as it calculates the
  proportions of each zone. This could have unintended consequences if a large
  portion of nodes are unready.
-->
<ul>
<li>EndpointSlice 控制器在计算每一个区域的容量比例时，会忽略未就绪的节点。
在大量节点未就绪的场景下，这样做会带来非预期的结果。</li>
</ul>
<!-- 
* The EndpointSlice controller does not take into account <a class='glossary-tooltip' title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='tolerations'>tolerations</a> when deploying calculating the
  proportions of each zone. If the Pods backing a Service are limited to a
  subset of Nodes in the cluster, this will not be taken into account.
-->
<ul>
<li>EndpointSlice 控制器在计算每一个区域的部署比例时，并不会考虑
<a class='glossary-tooltip' title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='容忍度'>容忍度</a>。
如果服务后台的 Pod 被限制只能运行在集群节点的一个子集上，这些信息并不会被使用。</li>
</ul>
<!-- 
* This may not work well with autoscaling. For example, if a lot of traffic is
  originating from a single zone, only the endpoints allocated to that zone will
  be handling that traffic. That could result in <a class='glossary-tooltip' title='Pod 水平自动扩缩器（Horizontal Pod Autoscaler）是一种 API 资源，它根据目标 CPU 利用率或自定义度量目标扩缩 Pod 副本的数量。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/run-application/horizontal-pod-autoscale/' target='_blank' aria-label='Horizontal Pod Autoscaler'>Horizontal Pod Autoscaler</a>
  either not picking up on this event, or newly added pods starting in a
  different zone.
-->
<ul>
<li>这种方法和自动扩展机制之间不能很好的协同工作。例如，如果大量流量来源于一个区域，
那只有分配到该区域的端点才可用来处理流量。这会导致
<a class='glossary-tooltip' title='Pod 水平自动扩缩器（Horizontal Pod Autoscaler）是一种 API 资源，它根据目标 CPU 利用率或自定义度量目标扩缩 Pod 副本的数量。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/run-application/horizontal-pod-autoscale/' target='_blank' aria-label='Pod 自动水平扩展'>Pod 自动水平扩展</a>
要么不能拾取此事件，要么新增 Pod 被启动到其他区域。</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!-- 
* Read [Connecting Applications with Services](/docs/concepts/services-networking/connect-applications-service/)
-->
<ul>
<li>参阅<a href="/zh/docs/concepts/services-networking/connect-applications-service/">通过服务连通应用</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-cd7657b1056ad32451974db57a951ba5">5.8 - 服务内部流量策略</h1>
    
	<!-- 
---
reviewers:
- maplain
title: Service Internal Traffic Policy
content_type: concept
weight: 45
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!-- 
_Service Internal Traffic Policy_ enables internal traffic restrictions to only route
internal traffic to endpoints within the node the traffic originated from. The
"internal" traffic here refers to traffic originated from Pods in the current
cluster. This can help to reduce costs and improve performance.
-->
<p><em>服务内部流量策略</em> 开启了内部流量限制，只路由内部流量到和发起方处于相同节点的服务端点。
这里的”内部“流量指当前集群中的 Pod 所发起的流量。
这种机制有助于节省开销，提升效率。</p>
<!-- body -->
<!-- 
## Using Service Internal Traffic Policy
-->
<h2 id="using-service-internal-traffic-policy">使用服务内部流量策略</h2>
<!-- 
The `ServiceInternalTrafficPolicy` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
is a Beta feature and enabled by default.
When the feature is enabled, you can enable the internal-only traffic policy for a
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a>, by setting its
`.spec.internalTrafficPolicy` to `Local`.
This tells kube-proxy to only use node local endpoints for cluster internal traffic.
-->
<p><code>ServiceInternalTrafficPolicy</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a> 是 Beta 功能，默认启用。
启用该功能后，你就可以通过将 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a> 的
<code>.spec.internalTrafficPolicy</code> 项设置为 <code>Local</code>，
来为它指定一个内部专用的流量策略。
此设置就相当于告诉 kube-proxy 对于集群内部流量只能使用本地的服务端口。</p>
<!-- 
For pods on nodes with no endpoints for a given Service, the Service
behaves as if it has zero endpoints (for Pods on this node) even if the service
does have endpoints on other nodes.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果某节点上的 Pod 均不提供指定 Service 的服务端点，
即使该 Service 在其他节点上有可用的服务端点，
Service 的行为看起来也像是它只有 0 个服务端点（只针对此节点上的 Pod）。
</div>
<!-- 
The following example shows what a Service looks like when you set
`.spec.internalTrafficPolicy` to `Local`:
-->
<p>以下示例展示了把 Service 的 <code>.spec.internalTrafficPolicy</code> 项设为 <code>Local</code> 时，
Service 的样子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">internalTrafficPolicy</span>:<span style="color:#bbb"> </span>Local<span style="color:#bbb">
</span></code></pre></div><!-- 
## How it works
-->
<h2 id="how-it-works">工作原理</h2>
<!-- 
The kube-proxy filters the endpoints it routes to based on the
`spec.internalTrafficPolicy` setting. When it's set to `Local`, only node local
endpoints are considered. When it's `Cluster` or missing, all endpoints are
considered.
When the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`ServiceInternalTrafficPolicy` is enabled, `spec.internalTrafficPolicy` defaults to "Cluster".
-->
<p>kube-proxy 基于 <code>spec.internalTrafficPolicy</code> 的设置来过滤路由的目标服务端点。
当它的值设为 <code>Local</code> 时，只选择节点本地的服务端点。
当它的值设为 <code>Cluster</code> 或缺省时，则选择所有的服务端点。
启用<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>ServiceInternalTrafficPolicy</code> 后，
<code>spec.internalTrafficPolicy</code> 的值默认设为 <code>Cluster</code>。</p>
<!-- 
## Constraints
-->
<h2 id="constraints">限制</h2>
<!-- 
* Service Internal Traffic Policy is not used when `externalTrafficPolicy` is set
  to `Local` on a Service. It is possible to use both features in the same cluster
  on different Services, just not on the same Service.
-->
<ul>
<li>在一个Service上，当 <code>externalTrafficPolicy</code> 已设置为 <code>Local</code>时，服务内部流量策略无法使用。
换句话说，在一个集群的不同 Service 上可以同时使用这两个特性，但在一个 Service 上不行。</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!-- 
* Read about [Topology Aware Hints](/docs/concepts/services-networking/topology-aware-hints)
* Read about [Service External Traffic Policy](/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip)
* Read [Connecting Applications with Services](/docs/concepts/services-networking/connect-applications-service/)
-->
<ul>
<li>请阅读<a href="/zh/docs/concepts/services-networking/topology-aware-hints">拓扑感知提示</a></li>
<li>请阅读<a href="/zh/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">Service 的外部流量策略</a></li>
<li>请阅读<a href="/zh/docs/concepts/services-networking/connect-applications-service/">用 Service 连接应用</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f51db1097575de8072afe1f5b156a70c">5.9 - 端点切片（Endpoint Slices）</h1>
    
	<!--
reviewers:
- freehan
title: Endpoint Slices
content_type: concept
weight: 45
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>


<!--
_EndpointSlices_ provide a simple way to track network endpoints within a
Kubernetes cluster. They offer a more scalable and extensible alternative to
Endpoints.
-->
<p><em>端点切片（EndpointSlices）</em> 提供了一种简单的方法来跟踪 Kubernetes 集群中的网络端点
（network endpoints）。它们为 Endpoints 提供了一种可伸缩和可拓展的替代方案。</p>
<!-- body -->
<!--
## Motivation

The Endpoints API has provided a simple and straightforward way of
tracking network endpoints in Kubernetes. Unfortunately as Kubernetes clusters
and <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a> have grown to handle and
send more traffic to more backend Pods, limitations of that original API became
more visible.
Most notably, those included challenges with scaling to larger numbers of
network endpoints.
-->
<h2 id="motivation">动机   </h2>
<p>Endpoints API 提供了在 Kubernetes 跟踪网络端点的一种简单而直接的方法。
不幸的是，随着 Kubernetes 集群和 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务'>服务</a>
逐渐开始为更多的后端 Pods 处理和发送请求，原来的 API 的局限性变得越来越明显。
最重要的是那些因为要处理大量网络端点而带来的挑战。</p>
<!--
Since all network endpoints for a Service were stored in a single Endpoints
resource, those resources could get quite large. That affected the performance
of Kubernetes components (notably the master control plane) and resulted in
significant amounts of network traffic and processing when Endpoints changed.
EndpointSlices help you mitigate those issues as well as provide an extensible
platform for additional features such as topological routing.
-->
<p>由于任一服务的所有网络端点都保存在同一个 Endpoints 资源中，这类资源可能变得
非常巨大，而这一变化会影响到 Kubernetes 组件（比如主控组件）的性能，并
在 Endpoints 变化时产生大量的网络流量和额外的处理。
EndpointSlice 能够帮助你缓解这一问题，还能为一些诸如拓扑路由这类的额外
功能提供一个可扩展的平台。</p>
<!--
## Endpoint Slice resources {#endpointslice-resource}

In Kubernetes, an EndpointSlice contains references to a set of network
endpoints. The control plane automatically creates EndpointSlices
for any Kubernetes Service that has a <a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='selector'>selector</a> specified. These EndpointSlices include
references to any Pods that match the Service selector. EndpointSlices group
network endpoints together by unique combinations of protocol, port number, and
Service name.  
The name of a EndpointSlice object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

As an example, here's a sample EndpointSlice resource for the `example`
Kubernetes Service.
-->
<h2 id="endpointslice-resource">Endpoint Slice 资源</h2>
<p>在 Kubernetes 中，<code>EndpointSlice</code> 包含对一组网络端点的引用。
指定选择器后控制面会自动为设置了 <a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='选择算符'>选择算符</a>
的 Kubernetes 服务创建 EndpointSlice。
这些 EndpointSlice 将包含对与服务选择算符匹配的所有 Pod 的引用。
EndpointSlice 通过唯一的协议、端口号和服务名称将网络端点组织在一起。
EndpointSlice 的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<p>例如，下面是 Kubernetes 服务 <code>example</code> 的 EndpointSlice 资源示例。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>discovery.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EndpointSlice<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-abc<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/service-name</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">addressType</span>:<span style="color:#bbb"> </span>IPv4<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">endpoints</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">addresses</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;10.1.2.3&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">conditions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ready</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostname</span>:<span style="color:#bbb"> </span>pod-1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nodeName</span>:<span style="color:#bbb"> </span>node-1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">zone</span>:<span style="color:#bbb"> </span>us-west2-a<span style="color:#bbb">
</span></code></pre></div><!--
By default, the control plane creates and manages EndpointSlices to have no
more than 100 endpoints each. You can configure this with the
`-max-endpoints-per-slice`
<a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>
flag, up to a maximum of 1000.

EndpointSlices can act as the source of truth for
<a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a> when it comes to
how to route internal traffic. When enabled, they should provide a performance
improvement for services with large numbers of endpoints.
-->
<p>默认情况下，控制面创建和管理的 EndpointSlice 将包含不超过 100 个端点。
你可以使用 <a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>
的 <code>--max-endpoints-per-slice</code> 标志设置此值，最大值为 1000。</p>
<p>当涉及如何路由内部流量时，EndpointSlice 可以充当
<a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a>
的决策依据。
启用该功能后，在服务的端点数量庞大时会有可观的性能提升。</p>
<!--
## Address Types

EndpointSlices support three address types:

* IPv4
* IPv6
* FQDN (Fully Qualified Domain Name)
-->
<h2 id="地址类型">地址类型</h2>
<p>EndpointSlice 支持三种地址类型：</p>
<ul>
<li>IPv4</li>
<li>IPv6</li>
<li>FQDN (完全合格的域名)</li>
</ul>
<!--
### Conditions

The EndpointSlice API stores conditions about endpoints that may be useful for consumers.
The three conditions are `ready`, `serving`, and `terminating`.
-->
<h3 id="状况">状况</h3>
<p>EndpointSlice API 存储了可能对使用者有用的、有关端点的状况。
这三个状况分别是 <code>ready</code>、<code>serving</code> 和 <code>terminating</code>。</p>
<!--
#### Ready

`ready` is a condition that maps to a Pod's `Ready` condition. A running Pod with the `Ready`
condition set to `True` should have this EndpointSlice condition also set to `true`. For
compatibility reasons, `ready` is NEVER `true` when a Pod is terminating. Consumers should refer
to the `serving` condition to inspect the readiness of terminating Pods. The only exception to
this rule is for Services with `spec.publishNotReadyAddresses` set to `true`. Endpoints for these
Services will always have the `ready` condition set to `true`.
-->
<h4 id="ready-就绪">Ready（就绪）</h4>
<p><code>ready</code> 状况是映射 Pod 的 <code>Ready</code> 状况的。
处于运行中的 Pod，它的 <code>Ready</code> 状况被设置为 <code>True</code>，应该将此 EndpointSlice 状况也设置为 <code>true</code>。
出于兼容性原因，当 Pod 处于终止过程中，<code>ready</code> 永远不会为 <code>true</code>。
消费者应参考 <code>serving</code> 状况来检查处于终止中的 Pod 的就绪情况。
该规则的唯一例外是将 <code>spec.publishNotReadyAddresses</code> 设置为 <code>true</code> 的服务。
这些服务（Service）的端点将始终将 <code>ready</code> 状况设置为 <code>true</code>。</p>
<!--
#### Serving






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>



`serving` is identical to the `ready` condition, except it does not account for terminating states.
Consumers of the EndpointSlice API should check this condition if they care about pod readiness while
the pod is also terminating.
-->
<h4 id="serving-服务中">Serving（服务中）</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>


<p><code>serving</code> 状况与 <code>ready</code> 状况相同，不同之处在于它不考虑终止状态。
如果 EndpointSlice API 的使用者关心 Pod 终止时的就绪情况，就应检查此状况。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Although `serving` is almost identical to `ready`, it was added to prevent break the existing meaning
of `ready`. It may be unexpected for existing clients if `ready` could be `true` for terminating
endpoints, since historically terminating endpoints were never included in the Endpoints or
EndpointSlice API to begin with. For this reason, `ready` is _always_ `false` for terminating
endpoints, and a new condition `serving` was added in v1.20 so that clients can track readiness
for terminating pods independent of the existing semantics for `ready`.
-->
<p>尽管 <code>serving</code> 与 <code>ready</code> 几乎相同，但是它是为防止破坏 <code>ready</code> 的现有含义而增加的。
如果对于处于终止中的端点，<code>ready</code> 可能是 <code>true</code>，那么对于现有的客户端来说可能是有些意外的，
因为从始至终，Endpoints 或 EndpointSlice API 从未包含处于终止中的端点。
出于这个原因，<code>ready</code> 对于处于终止中的端点 <em>总是</em> <code>false</code>，
并且在 v1.20 中添加了新的状况 <code>serving</code>，以便客户端可以独立于 <code>ready</code>
的现有语义来跟踪处于终止中的 Pod 的就绪情况。
</div>
<!-- 
#### Terminating






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>



`Terminating` is a condition that indicates whether an endpoint is terminating.
For pods, this is any pod that has a deletion timestamp set.
-->
<h4 id="terminating-终止中">Terminating（终止中）</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>


<p><code>Terminating</code> 是表示端点是否处于终止中的状况。
对于 Pod 来说，这是设置了删除时间戳的 Pod。</p>
<!--
### Topology information {#topology}

Each endpoint within an EndpointSlice can contain relevant topology information.
The topology information includes the location of the endpoint and information
about the corresponding Node and zone. These are available in the following
per endpoint fields on EndpointSlices:
-->
<h3 id="topology">拓扑信息  </h3>
<p>EndpointSlice 中的每个端点都可以包含一定的拓扑信息。
拓扑信息包括端点的位置，对应节点、可用区的信息。
这些信息体现为 EndpointSlices 的如下端点字段：</p>
<!--
* `nodeName` - The name of the Node this endpoint is on.
* `zone` - The zone this endpoint is in.
-->
<ul>
<li><code>nodeName</code> - 端点所在的 Node 名称；</li>
<li><code>zone</code> - 端点所处的可用区。</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In the v1 API, the per endpoint `topology` was effectively removed in favor of
the dedicated fields `nodeName` and `zone`.
-->
<p>在 v1 API 中，逐个端点设置的 <code>topology</code> 实际上被去除，以鼓励使用专用
的字段 <code>nodeName</code> 和 <code>zone</code>。</p>
<!--
Setting arbitrary topology fields on the `endpoint` field of an `EndpointSlice`
resource has been deprecated and is not supported in the v1 API. 
Instead, the v1 API supports setting individual `nodeName` and `zone` fields. 
These fields are automatically translated between API versions. For example, the
value of the `"topology.kubernetes.io/zone"` key in the `topology` field in
the v1beta1 API is accessible as the `zone` field in the v1 API.
-->
<p>对 <code>EndpointSlice</code> 对象的 <code>endpoint</code> 字段设置任意的拓扑结构信息这一操作已被
废弃，不再被 v1 API 所支持。取而代之的是 v1 API 所支持的 <code>nodeName</code> 和 <code>zone</code>
这些独立的字段。这些字段可以在不同的 API 版本之间自动完成转译。
例如，v1beta1 API 中 <code>topology</code> 字段的 <code>topology.kubernetes.io/zone</code> 取值可以
在 v1 API 中通过 <code>zone</code> 字段访问。</p>

</div>
<!--
### Management

Most often, the control plane (specifically, the endpoint slice
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a>) creates and
manages EndpointSlice objects. There are a variety of other use cases for
EndpointSlices, such as service mesh implementations, that could result in other
entities or controllers managing additional sets of EndpointSlices.
-->
<h3 id="management">管理  </h3>
<p>通常，控制面（尤其是端点切片的 <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>）
会创建和管理 EndpointSlice 对象。EndpointSlice 对象还有一些其他使用场景，
例如作为服务网格（Service Mesh）的实现。这些场景都会导致有其他实体
或者控制器负责管理额外的 EndpointSlice 集合。</p>
<!--
To ensure that multiple entities can manage EndpointSlices without interfering
with each other, Kubernetes defines the
<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='label'>label</a>
`endpointslice.kubernetes.io/managed-by`, which indicates the entity managing
an EndpointSlice.
The endpoint slice controller sets `endpointslice-controller.k8s.io` as the value
for this label on all EndpointSlices it manages. Other entities managing
EndpointSlices should also set a unique value for this label.
-->
<p>为了确保多个实体可以管理 EndpointSlice 而且不会相互产生干扰，Kubernetes 定义了
<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>
<code>endpointslice.kubernetes.io/managed-by</code>，用来标明哪个实体在管理某个
EndpointSlice。端点切片控制器会在自己所管理的所有 EndpointSlice 上将该标签值设置
为 <code>endpointslice-controller.k8s.io</code>。
管理 EndpointSlice 的其他实体也应该为此标签设置一个唯一值。</p>
<!--
### Ownership

In most use cases, EndpointSlices are owned by the Service that the endpoint
slice object tracks endpoints for. This ownership is indicated by an owner
reference on each EndpointSlice as well as a `kubernetes.io/service-name`
label that enables simple lookups of all EndpointSlices belonging to a Service.
-->
<h3 id="ownership">属主关系  </h3>
<p>在大多数场合下，EndpointSlice 都由某个 Service 所有，（因为）该端点切片正是
为该服务跟踪记录其端点。这一属主关系是通过为每个 EndpointSlice 设置一个
属主（owner）引用，同时设置 <code>kubernetes.io/service-name</code> 标签来标明的，
目的是方便查找隶属于某服务的所有 EndpointSlice。</p>
<!--
### EndpointSlice mirroring

In some cases, applications create custom Endpoints resources. To ensure that
these applications do not need to concurrently write to both Endpoints and
EndpointSlice resources, the cluster's control plane mirrors most Endpoints
resources to corresponding EndpointSlices.
-->
<h3 id="endpointslice-mirroring">EndpointSlice 镜像   </h3>
<p>在某些场合，应用会创建定制的 Endpoints 资源。为了保证这些应用不需要并发
的更改 Endpoints 和 EndpointSlice 资源，集群的控制面将大多数 Endpoints
映射到对应的 EndpointSlice 之上。</p>
<!--
The control plane mirrors Endpoints resources unless:

* the Endpoints resource has a `endpointslice.kubernetes.io/skip-mirror` label
  set to `true`.
* the Endpoints resource has a `control-plane.alpha.kubernetes.io/leader`
  annotation.
* the corresponding Service resource does not exist.
* the corresponding Service resource has a non-nil selector.
-->
<p>控制面对 Endpoints 资源进行映射的例外情况有：</p>
<ul>
<li>Endpoints 资源上标签 <code>endpointslice.kubernetes.io/skip-mirror</code> 值为 <code>true</code>。</li>
<li>Endpoints 资源包含标签 <code>control-plane.alpha.kubernetes.io/leader</code>。</li>
<li>对应的 Service 资源不存在。</li>
<li>对应的 Service 的选择算符不为空。</li>
</ul>
<!--
Individual Endpoints resources may translate into multiple EndpointSlices. This
will occur if an Endpoints resource has multiple subsets or includes endpoints
with multiple IP families (IPv4 and IPv6). A maximum of 1000 addresses per
subset will be mirrored to EndpointSlices.
-->
<p>每个 Endpoints 资源可能会被翻译到多个 EndpointSlices 中去。
当 Endpoints 资源中包含多个子网或者包含多个 IP 地址族（IPv4 和 IPv6）的端点时，
就有可能发生这种状况。
每个子网最多有 1000 个地址会被镜像到 EndpointSlice 中。</p>
<!--
### Distribution of EndpointSlices

Each EndpointSlice has a set of ports that applies to all endpoints within the
resource. When named ports are used for a Service, Pods may end up with
different target port numbers for the same named port, requiring different
EndpointSlices. This is similar to the logic behind how subsets are grouped
with Endpoints.
-->
<h3 id="distribution-of-endpointslices">EndpointSlices 的分布问题 </h3>
<p>每个 EndpointSlice 都有一组端口值，适用于资源内的所有端点。
当为服务使用命名端口时，Pod 可能会就同一命名端口获得不同的端口号，因而需要
不同的 EndpointSlice。这有点像 Endpoints 用来对子网进行分组的逻辑。</p>
<!--
The control plane tries to fill EndpointSlices as full as possible, but does not
actively rebalance them. The logic is fairly straightforward:

1. Iterate through existing EndpointSlices, remove endpoints that are no longer
   desired and update matching endpoints that have changed.
2. Iterate through EndpointSlices that have been modified in the first step and
   fill them up with any new endpoints needed.
3. If there's still new endpoints left to add, try to fit them into a previously
   unchanged slice and/or create new ones.
-->
<p>控制面尝试尽量将 EndpointSlice 填满，不过不会主动地在若干 EndpointSlice 之间
执行再平衡操作。这里的逻辑也是相对直接的：</p>
<ol>
<li>列举所有现有的 EndpointSlices，移除那些不再需要的端点并更新那些已经
变化的端点。</li>
<li>列举所有在第一步中被更改过的 EndpointSlices，用新增加的端点将其填满。</li>
<li>如果还有新的端点未被添加进去，尝试将这些端点添加到之前未更改的切片中，
或者创建新切片。</li>
</ol>
<!--
Importantly, the third step prioritizes limiting EndpointSlice updates over a
perfectly full distribution of EndpointSlices. As an example, if there are 10
new endpoints to add and 2 EndpointSlices with room for 5 more endpoints each,
this approach will create a new EndpointSlice instead of filling up the 2
existing EndpointSlices. In other words, a single EndpointSlice creation is
preferrable to multiple EndpointSlice updates.
-->
<p>这里比较重要的是，与在 EndpointSlice 之间完成最佳的分布相比，第三步中更看重
限制 EndpointSlice 更新的操作次数。例如，如果有 10 个端点待添加，有两个
EndpointSlice 中各有 5 个空位，上述方法会创建一个新的 EndpointSlice 而不是
将现有的两个 EndpointSlice 都填满。换言之，与执行多个 EndpointSlice 更新操作
相比较，方法会优先考虑执行一个 EndpointSlice 创建操作。</p>
<!--
With kube-proxy running on each Node and watching EndpointSlices, every change
to an EndpointSlice becomes relatively expensive since it will be transmitted to
every Node in the cluster. This approach is intended to limit the number of
changes that need to be sent to every Node, even if it may result with multiple
EndpointSlices that are not full.
-->
<p>由于 kube-proxy 在每个节点上运行并监视 EndpointSlice 状态，EndpointSlice 的
每次变更都变得相对代价较高，因为这些状态变化要传递到集群中每个节点上。
这一方法尝试限制要发送到所有节点上的变更消息个数，即使这样做可能会导致有
多个 EndpointSlice 没有被填满。</p>
<!--
In practice, this less than ideal distribution should be rare. Most changes
processed by the EndpointSlice controller will be small enough to fit in an
existing EndpointSlice, and if not, a new EndpointSlice is likely going to be
necessary soon anyway. Rolling updates of Deployments also provide a natural
repacking of EndpointSlices with all Pods and their corresponding endpoints
getting replaced.
-->
<p>在实践中，上面这种并非最理想的分布是很少出现的。大多数被 EndpointSlice 控制器
处理的变更都是足够小的，可以添加到某已有 EndpointSlice 中去的。并且，假使无法
添加到已有的切片中，不管怎样都会快就会需要一个新的 EndpointSlice 对象。
Deployment 的滚动更新为重新为 EndpointSlice 打包提供了一个自然的机会，所有
Pod 及其对应的端点在这一期间都会被替换掉。</p>
<!--
### Duplicate endpoints

Due to the nature of EndpointSlice changes, endpoints may be represented in more
than one EndpointSlice at the same time. This naturally occurs as changes to
different EndpointSlice objects can arrive at the Kubernetes client watch/cache
at different times. Implementations using EndpointSlice must be able to have the
endpoint appear in more than one slice. A reference implementation of how to
perform endpoint deduplication can be found in the `EndpointSliceCache`
implementation in `kube-proxy`.
-->
<h3 id="duplicate-endpoints">重复的端点  </h3>
<p>由于 EndpointSlice 变化的自身特点，端点可能会同时出现在不止一个 EndpointSlice
中。鉴于不同的 EndpointSlice 对象在不同时刻到达 Kubernetes 的监视/缓存中，
这种情况的出现是很自然的。
使用 EndpointSlice 的实现必须能够处理端点出现在多个切片中的状况。
关于如何执行端点去重（deduplication）的参考实现，你可以在 <code>kube-proxy</code> 的
<code>EndpointSlice</code> 实现中找到。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read [Connecting Applications with Services](/docs/concepts/services-networking/connect-applications-service/)
-->
<ul>
<li>阅读<a href="/zh/docs/concepts/services-networking/connect-applications-service/">使用服务连接应用</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ded1daafdcd293023ee333728007ca61">5.10 - 网络策略</h1>
    
	<!--
title: Network Policies
content_type: concept
weight: 50
-->
<!-- overview -->
<!--
If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster.  NetworkPolicies are an application-centric construct which allow you to specify how a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='pod'>pod</a> is allowed to communicate with various network "entities" (we use the word "entity" here to avoid overloading the more common terms such as "endpoints" and "services", which have specific Kubernetes connotations) over the network.
-->
<p>如果你希望在 IP 地址或端口层面（OSI 第 3 层或第 4 层）控制网络流量，
则你可以考虑为集群中特定应用使用 Kubernetes 网络策略（NetworkPolicy）。
NetworkPolicy 是一种以应用为中心的结构，允许你设置如何允许
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 与网络上的各类网络“实体”
（我们这里使用实体以避免过度使用诸如“端点”和“服务”这类常用术语，
这些术语在 Kubernetes 中有特定含义）通信。</p>
<!--
The entities that a Pod can communicate with are identified through a combination of the following 3 identifiers:

1. Other pods that are allowed (exception: a pod cannot block access to itself)
2. Namespaces that are allowed
3. IP blocks (exception: traffic to and from the node where a Pod is running is always allowed, regardless of the IP address of the Pod or the node)
-->
<p>Pod 可以通信的 Pod 是通过如下三个标识符的组合来辩识的：</p>
<ol>
<li>其他被允许的 Pods（例外：Pod 无法阻塞对自身的访问）</li>
<li>被允许的名字空间</li>
<li>IP 组块（例外：与 Pod 运行所在的节点的通信总是被允许的，
无论 Pod 或节点的 IP 地址）</li>
</ol>
<!--
When defining a pod- or namespace- based NetworkPolicy, you use a <a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='selector'>selector</a> to specify what traffic is allowed to and from the Pod(s) that match the selector.

Meanwhile, when IP based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).
-->
<p>在定义基于 Pod 或名字空间的 NetworkPolicy 时，你会使用
<a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='选择算符'>选择算符</a> 来设定哪些流量
可以进入或离开与该算符匹配的 Pod。</p>
<p>同时，当基于 IP 的 NetworkPolicy 被创建时，我们基于 IP 组块（CIDR 范围）
来定义策略。</p>
<!-- body -->
<!--
## Prerequisites

Network policies are implemented by the [network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/). To use network policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.
-->
<h2 id="prerequisites">前置条件  </h2>
<p>网络策略通过<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件</a>
来实现。要使用网络策略，你必须使用支持 NetworkPolicy 的网络解决方案。
创建一个 NetworkPolicy 资源对象而没有控制器来使它生效的话，是没有任何作用的。</p>
<!--
## The Two Sorts of Pod Isolation

There are two sorts of isolation for a pod: isolation for egress, and isolation for ingress.  They concern what connections may be established.  "Isolation" here is not absolute, rather it means "some restrictions apply".  The alternative, "non-isolated for $direction", means that no restrictions apply in the stated direction.  The two sorts of isolation (or not) are declared independently, and are both relevant for a connection from one pod to another.
-->
<h2 id="pod-隔离的两种类型">Pod 隔离的两种类型</h2>
<p>Pod 有两种隔离: 出口的隔离和入口的隔离。它们涉及到可以建立哪些连接。
这里的“隔离”不是绝对的，而是意味着“有一些限制”。
另外的，“非隔离方向”意味着在所述方向上没有限制。这两种隔离（或不隔离）是独立声明的，
并且都与从一个 Pod 到另一个 Pod 的连接有关。</p>
<!--
By default, a pod is non-isolated for egress; all outbound connections are allowed.  A pod is isolated for egress if there is any NetworkPolicy that both selects the pod and has "Egress" in its `policyTypes`; we say that such a policy applies to the pod for egress.  When a pod is isolated for egress, the only allowed connections from the pod are those allowed by the `egress` list of some NetworkPolicy that applies to the pod for egress.  The effects of those `egress` lists combine additively.
-->
<p>默认情况下，一个 Pod 的出口是非隔离的，即所有外向连接都是被允许的。如果有任何的 NetworkPolicy
选择该 Pod 并在其 <code>policyTypes</code> 中包含 “Egress”，则该 Pod 是出口隔离的，
我们称这样的策略适用于该 Pod 的出口。当一个 Pod 的出口被隔离时，
唯一允许的来自 Pod 的连接是适用于出口的 Pod 的某个 NetworkPolicy 的 <code>egress</code> 列表所允许的连接。
这些 <code>egress</code> 列表的效果是相加的。</p>
<!--
By default, a pod is non-isolated for ingress; all inbound connections are allowed.  A pod is isolated for ingress if there is any NetworkPolicy that both selects the pod and has "Ingress" in its `policyTypes`; we say that such a policy applies to the pod for ingress.  When a pod is isolated for ingress, the only allowed connections into the pod are those from the pod's node and those allowed by the `ingress` list of some NetworkPolicy that applies to the pod for ingress.  The effects of those `ingress` lists combine additively.
-->
<p>默认情况下，一个 Pod 对入口是非隔离的，即所有入站连接都是被允许的。如果有任何的 NetworkPolicy
选择该 Pod 并在其 <code>policyTypes</code> 中包含 “Ingress”，则该 Pod 被隔离入口，
我们称这种策略适用于该 Pod 的入口。 当一个 Pod 的入口被隔离时，唯一允许进入该 Pod
的连接是来自该 Pod 节点的连接和适用于入口的 Pod 的某个 NetworkPolicy 的 <code>ingress</code>
列表所允许的连接。这些 <code>ingress</code> 列表的效果是相加的。</p>
<!--
Network policies do not conflict; they are additive. If any policy or policies apply to a given pod for a given direction, the connections allowed in that direction from that pod is the union of what the applicable policies allow. Thus, order of evaluation does not affect the policy result.

For a connection from a source pod to a destination pod to be allowed, both the egress policy on the source pod and the ingress policy on the destination pod need to allow the connection. If either side does not allow the connection, it will not happen.
-->
<p>网络策略是相加的，所以不会产生冲突。如果策略适用于 Pod 某一特定方向的流量，
Pod 在对应方向所允许的连接是适用的网络策略所允许的集合。
因此，评估的顺序不影响策略的结果。</p>
<p>要允许从源 Pod 到目的 Pod 的连接，源 Pod 的出口策略和目的 Pod 的入口策略都需要允许连接。
如果任何一方不允许连接，建立连接将会失败。</p>
<!--
## The NetworkPolicy resource {#networkpolicy-resource}

See the [NetworkPolicy](/docs/reference/generated/kubernetes-api/v1.23/#networkpolicy-v1-networking-k8s-io) reference for a full definition of the resource.

An example NetworkPolicy might look like this:
-->
<h2 id="networkpolicy-resource">NetworkPolicy 资源</h2>
<p>参阅 <a href="/docs/reference/generated/kubernetes-api/v1.23/#networkpolicy-v1-networking-k8s-io">NetworkPolicy</a>
来了解资源的完整定义。</p>
<p>下面是一个 NetworkPolicy 的示例:</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/networkpolicy.yaml" download="service/networking/networkpolicy.yaml"><code>service/networking/networkpolicy.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-networkpolicy-yaml')" title="Copy service/networking/networkpolicy.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-networkpolicy-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-network-policy<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>db<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">policyTypes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- Ingress<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- Egress<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ingress</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">from</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">ipBlock</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cidr</span>:<span style="color:#bbb"> </span><span style="color:#666">172.17.0.0</span>/16<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">except</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- <span style="color:#666">172.17.1.0</span>/24<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">namespaceSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">project</span>:<span style="color:#bbb"> </span>myproject<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">6379</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">egress</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">to</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">ipBlock</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cidr</span>:<span style="color:#bbb"> </span><span style="color:#666">10.0.0.0</span>/24<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">5978</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
POSTing this to the API server for your cluster will have no effect unless your chosen networking solution supports network policy.
 -->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 除非选择支持网络策略的网络解决方案，否则将上述示例发送到API服务器没有任何效果。
</div>
<!--
__Mandatory Fields__: As with all other Kubernetes config, a NetworkPolicy
needs `apiVersion`, `kind`, and `metadata` fields.  For general information
about working with config files, see
[Configure Containers Using a ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/),
and [Object Management](/docs/concepts/overview/working-with-objects/object-management).

__spec__: NetworkPolicy [spec](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status) has all the information needed to define a particular network policy in the given namespace.

__podSelector__: Each NetworkPolicy includes a `podSelector` which selects the grouping of pods to which the policy applies. The example policy selects pods with the label "role=db". An empty `podSelector` selects all pods in the namespace.
-->
<p><strong>必需字段</strong>：与所有其他的 Kubernetes 配置一样，NetworkPolicy 需要 <code>apiVersion</code>、
<code>kind</code> 和 <code>metadata</code> 字段。关于配置文件操作的一般信息，请参考
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">使用 ConfigMap 配置容器</a>,
和<a href="/zh/docs/concepts/overview/working-with-objects/object-management">对象管理</a>。</p>
<p><strong>spec</strong>：NetworkPolicy <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status">规约</a>
中包含了在一个名字空间中定义特定网络策略所需的所有信息。</p>
<p><strong>podSelector</strong>：每个 NetworkPolicy 都包括一个 <code>podSelector</code>，它对该策略所
适用的一组 Pod 进行选择。示例中的策略选择带有 &quot;role=db&quot; 标签的 Pod。
空的 <code>podSelector</code> 选择名字空间下的所有 Pod。</p>
<!--
__policyTypes__: Each NetworkPolicy includes a `policyTypes` list which may include either `Ingress`, `Egress`, or both. The `policyTypes` field indicates whether or not the given policy applies to ingress traffic to selected pod, egress traffic from selected pods, or both. If no `policyTypes` are specified on a NetworkPolicy then by default `Ingress` will always be set and `Egress` will be set if the NetworkPolicy has any egress rules.

__ingress__: Each NetworkPolicy may include a list of allowed `ingress` rules.  Each rule allows traffic which matches both the `from` and `ports` sections. The example policy contains a single rule, which matches traffic on a single port, from one of three sources, the first specified via an `ipBlock`, the second via a `namespaceSelector` and the third via a `podSelector`.

__egress__: Each NetworkPolicy may include a list of allowed `egress` rules.  Each rule allows traffic which matches both the `to` and `ports` sections. The example policy contains a single rule, which matches traffic on a single port to any destination in `10.0.0.0/24`.
-->
<p><strong>policyTypes</strong>: 每个 NetworkPolicy 都包含一个 <code>policyTypes</code> 列表，其中包含
<code>Ingress</code> 或 <code>Egress</code> 或两者兼具。<code>policyTypes</code> 字段表示给定的策略是应用于
进入所选 Pod 的入站流量还是来自所选 Pod 的出站流量，或两者兼有。
如果 NetworkPolicy 未指定 <code>policyTypes</code> 则默认情况下始终设置 <code>Ingress</code>；
如果 NetworkPolicy 有任何出口规则的话则设置 <code>Egress</code>。</p>
<p><strong>ingress</strong>: 每个 NetworkPolicy 可包含一个 <code>ingress</code> 规则的白名单列表。
每个规则都允许同时匹配 <code>from</code> 和 <code>ports</code> 部分的流量。示例策略中包含一条
简单的规则： 它匹配某个特定端口，来自三个来源中的一个，第一个通过 <code>ipBlock</code>
指定，第二个通过 <code>namespaceSelector</code> 指定，第三个通过 <code>podSelector</code> 指定。</p>
<p><strong>egress</strong>: 每个 NetworkPolicy 可包含一个 <code>egress</code> 规则的白名单列表。
每个规则都允许匹配 <code>to</code> 和 <code>port</code> 部分的流量。该示例策略包含一条规则，
该规则将指定端口上的流量匹配到 <code>10.0.0.0/24</code> 中的任何目的地。</p>
<!--
So, the example NetworkPolicy:

1. isolates "role=db" pods in the "default" namespace for both ingress and egress traffic (if they weren't already isolated)
2. (Ingress rules) allows connections to all pods in the “default” namespace with the label “role=db” on TCP port 6379 from:

   * any pod in the "default" namespace with the label "role=frontend"
   * any pod in a namespace with the label "project=myproject"
   * IP addresses in the ranges 172.17.0.0–172.17.0.255 and 172.17.2.0–172.17.255.255 (ie, all of 172.17.0.0/16 except 172.17.1.0/24)
3. (Egress rules) allows connections from any pod in the "default" namespace with the label "role=db" to CIDR 10.0.0.0/24 on TCP port 5978

See the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) walkthrough for further examples.
-->
<p>所以，该网络策略示例:</p>
<ol>
<li>
<p>隔离 &quot;default&quot; 名字空间下 &quot;role=db&quot; 的 Pod （如果它们不是已经被隔离的话）。</p>
</li>
<li>
<p>（Ingress 规则）允许以下 Pod 连接到 &quot;default&quot; 名字空间下的带有 &quot;role=db&quot;
标签的所有 Pod 的 6379 TCP 端口：</p>
<ul>
<li>&quot;default&quot; 名字空间下带有 &quot;role=frontend&quot; 标签的所有 Pod</li>
<li>带有 &quot;project=myproject&quot; 标签的所有名字空间中的 Pod</li>
<li>IP 地址范围为 172.17.0.0–172.17.0.255 和 172.17.2.0–172.17.255.255
（即，除了 172.17.1.0/24 之外的所有 172.17.0.0/16）</li>
</ul>
</li>
<li>
<p>（Egress 规则）允许从带有 &quot;role=db&quot; 标签的名字空间下的任何 Pod 到 CIDR
10.0.0.0/24 下 5978 TCP 端口的连接。</p>
</li>
</ol>
<p>参阅<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>演练
了解更多示例。</p>
<!--
## Behavior of `to` and `from` selectors

There are four kinds of selectors that can be specified in an `ingress` `from` section or `egress` `to` section:

__podSelector__: This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations.

__namespaceSelector__: This selects particular namespaces for which all Pods should be allowed as ingress sources or egress destinations.

__namespaceSelector__ *and* __podSelector__: A single `to`/`from` entry that specifies both `namespaceSelector` and `podSelector` selects particular Pods within particular namespaces. Be careful to use correct YAML syntax; this policy:
-->
<h2 id="behavior-of-to-and-from-selectors">选择器 <code>to</code> 和 <code>from</code> 的行为  </h2>
<p>可以在 <code>ingress</code> 的 <code>from</code> 部分或 <code>egress</code> 的 <code>to</code> 部分中指定四种选择器：</p>
<p><strong>podSelector</strong>: 此选择器将在与 NetworkPolicy 相同的名字空间中选择特定的
Pod，应将其允许作为入站流量来源或出站流量目的地。</p>
<p><strong>namespaceSelector</strong>：此选择器将选择特定的名字空间，应将所有 Pod 用作其
入站流量来源或出站流量目的地。</p>
<p><strong>namespaceSelector</strong> <em>和</em> <strong>podSelector</strong>： 一个指定 <code>namespaceSelector</code>
和 <code>podSelector</code> 的 <code>to</code>/<code>from</code> 条目选择特定名字空间中的特定 Pod。
注意使用正确的 YAML 语法；下面的策略：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ingress</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">from</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">namespaceSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>alice<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>client<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!--
contains a single `from` element allowing connections from Pods with the label `role=client` in namespaces with the label `user=alice`. But *this* policy:
 -->
<p>在 <code>from</code> 数组中仅包含一个元素，只允许来自标有 <code>role=client</code> 的 Pod 且
该 Pod 所在的名字空间中标有 <code>user=alice</code> 的连接。但是 <em>这项</em> 策略：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ingress</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">from</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">namespaceSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>alice<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>client<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!--
contains two elements in the `from` array, and allows connections from Pods in the local Namespace with the label `role=client`, *or* from any Pod in any namespace with the label `user=alice`.
-->
<p>在 <code>from</code> 数组中包含两个元素，允许来自本地名字空间中标有 <code>role=client</code> 的
Pod 的连接，<em>或</em> 来自任何名字空间中标有 <code>user=alice</code> 的任何 Pod 的连接。</p>
<!--
When in doubt, use `kubectl describe` to see how Kubernetes has interpreted the policy.

<a name="behavior-of-ipblock-selectors"></a>
__ipBlock__: This selects particular IP CIDR ranges to allow as ingress sources or egress destinations. These should be cluster-external IPs, since Pod IPs are ephemeral and unpredictable.

Cluster ingress and egress mechanisms often require rewriting the source or destination IP
of packets. In cases where this happens, it is not defined whether this happens before or
after NetworkPolicy processing, and the behavior may be different for different
combinations of network plugin, cloud provider, `Service` implementation, etc.

In the case of ingress, this means that in some cases you may be able to filter incoming
packets based on the actual original source IP, while in other cases, the "source IP" that
the NetworkPolicy acts on may be the IP of a `LoadBalancer` or of the Pod's node, etc.

For egress, this means that connections from pods to `Service` IPs that get rewritten to
cluster-external IPs may or may not be subject to `ipBlock`-based policies.
-->
<p>如有疑问，请使用 <code>kubectl describe</code> 查看 Kubernetes 如何解释该策略。</p>
<p><strong>ipBlock</strong>: 此选择器将选择特定的 IP CIDR 范围以用作入站流量来源或出站流量目的地。
这些应该是集群外部 IP，因为 Pod IP 存在时间短暂的且随机产生。</p>
<p>集群的入站和出站机制通常需要重写数据包的源 IP 或目标 IP。
在发生这种情况时，不确定在 NetworkPolicy 处理之前还是之后发生，
并且对于网络插件、云提供商、<code>Service</code> 实现等的不同组合，其行为可能会有所不同。</p>
<p>对入站流量而言，这意味着在某些情况下，你可以根据实际的原始源 IP 过滤传入的数据包，
而在其他情况下，NetworkPolicy 所作用的 <code>源IP</code> 则可能是 <code>LoadBalancer</code> 或
Pod 的节点等。</p>
<p>对于出站流量而言，这意味着从 Pod 到被重写为集群外部 IP 的 <code>Service</code> IP
的连接可能会或可能不会受到基于 <code>ipBlock</code> 的策略的约束。</p>
<!--
## Default policies

By default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. The following examples let you change the default behavior
in that namespace.
-->
<h2 id="default-policies">默认策略  </h2>
<p>默认情况下，如果名字空间中不存在任何策略，则所有进出该名字空间中 Pod 的流量都被允许。
以下示例使你可以更改该名字空间中的默认行为。</p>
<!--
### Default deny all ingress traffic

You can create a "default" isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any ingress traffic to those pods.
-->
<h3 id="默认拒绝所有入站流量">默认拒绝所有入站流量</h3>
<p>你可以通过创建选择所有容器但不允许任何进入这些容器的入站流量的 NetworkPolicy
来为名字空间创建 “default” 隔离策略。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/network-policy-default-deny-ingress.yaml" download="service/networking/network-policy-default-deny-ingress.yaml"><code>service/networking/network-policy-default-deny-ingress.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-network-policy-default-deny-ingress-yaml')" title="Copy service/networking/network-policy-default-deny-ingress.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-network-policy-default-deny-ingress-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-deny-ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">policyTypes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- Ingress<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
This ensures that even pods that aren't selected by any other NetworkPolicy will still be isolated. This policy does not change the default egress isolation behavior.
-->
<p>这样可以确保即使容器没有选择其他任何 NetworkPolicy，也仍然可以被隔离。
此策略不会更改默认的出口隔离行为。</p>
<!--
### Default allow all ingress traffic

If you want to allow all traffic to all pods in a namespace (even if policies are added that cause some pods to be treated as "isolated"), you can create a policy that explicitly allows all traffic in that namespace.
-->
<h3 id="默认允许所有入站流量">默认允许所有入站流量</h3>
<p>如果要允许所有流量进入某个名字空间中的所有 Pod（即使添加了导致某些 Pod 被视为
“隔离”的策略），则可以创建一个策略来明确允许该名字空间中的所有流量。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/network-policy-allow-all-ingress.yaml" download="service/networking/network-policy-allow-all-ingress.yaml"><code>service/networking/network-policy-allow-all-ingress.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-network-policy-allow-all-ingress-yaml')" title="Copy service/networking/network-policy-allow-all-ingress.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-network-policy-allow-all-ingress-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>allow-all-ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ingress</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- {}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">policyTypes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- Ingress<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
### Default deny all egress traffic

You can create a "default" egress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any egress traffic from those pods.
-->
<h3 id="默认拒绝所有出站流量">默认拒绝所有出站流量</h3>
<p>你可以通过创建选择所有容器但不允许来自这些容器的任何出站流量的 NetworkPolicy
来为名字空间创建 “default” 隔离策略。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/network-policy-default-deny-egress.yaml" download="service/networking/network-policy-default-deny-egress.yaml"><code>service/networking/network-policy-default-deny-egress.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-network-policy-default-deny-egress-yaml')" title="Copy service/networking/network-policy-default-deny-egress.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-network-policy-default-deny-egress-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-deny-egress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">policyTypes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- Egress<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed egress traffic. This policy does not
change the default ingress isolation behavior.
-->
<p>此策略可以确保即使没有被其他任何 NetworkPolicy 选择的 Pod 也不会被允许流出流量。
此策略不会更改默认的入站流量隔离行为。</p>
<!--
### Default allow all egress traffic

If you want to allow all traffic from all pods in a namespace (even if policies are added that cause some pods to be treated as "isolated"), you can create a policy that explicitly allows all egress traffic in that namespace.
-->
<h3 id="默认允许所有出站流量">默认允许所有出站流量</h3>
<p>如果要允许来自名字空间中所有 Pod 的所有流量（即使添加了导致某些 Pod 被视为“隔离”的策略），
则可以创建一个策略，该策略明确允许该名字空间中的所有出站流量。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/network-policy-allow-all-egress.yaml" download="service/networking/network-policy-allow-all-egress.yaml"><code>service/networking/network-policy-allow-all-egress.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-network-policy-allow-all-egress-yaml')" title="Copy service/networking/network-policy-allow-all-egress.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-network-policy-allow-all-egress-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>allow-all-egress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">egress</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- {}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">policyTypes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- Egress<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
### Default deny all ingress and all egress traffic

You can create a "default" policy for a namespace which prevents all ingress AND egress traffic by creating the following NetworkPolicy in that namespace.
-->
<h3 id="默认拒绝所有入口和所有出站流量">默认拒绝所有入口和所有出站流量</h3>
<p>你可以为名字空间创建“默认”策略，以通过在该名字空间中创建以下 NetworkPolicy
来阻止所有入站和出站流量。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/network-policy-default-deny-all.yaml" download="service/networking/network-policy-default-deny-all.yaml"><code>service/networking/network-policy-default-deny-all.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-network-policy-default-deny-all-yaml')" title="Copy service/networking/network-policy-default-deny-all.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-network-policy-default-deny-all-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-deny-all<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">policyTypes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- Ingress<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- Egress<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed ingress or egress traffic.
-->
<p>此策略可以确保即使没有被其他任何 NetworkPolicy 选择的 Pod 也不会被
允许入站或出站流量。</p>
<!--
## SCTP support
-->
<h2 id="sctp-支持">SCTP 支持</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>


<!--
As a stable feature, this is enabled by default. To disable SCTP at a cluster level, you (or your cluster administrator) will need to disable the `SCTPSupport` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for the API server with `--feature-gates=SCTPSupport=false,…`.
When the feature gate is enabled, you can set the `protocol` field of a NetworkPolicy to `SCTP`.
-->
<p>作为一个稳定特性，SCTP 支持默认是被启用的。
要在集群层面禁用 SCTP，你（或你的集群管理员）需要为 API 服务器指定
<code>--feature-gates=SCTPSupport=false,...</code>
来禁用 <code>SCTPSupport</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
启用该特性门控后，用户可以将 NetworkPolicy 的 <code>protocol</code> 字段设置为 <code>SCTP</code>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
You must be using a <a class='glossary-tooltip' title='容器网络接口 (CNI) 插件是遵循 appc/CNI 协议的一类网络插件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni' target='_blank' aria-label='CNI'>CNI</a> plugin that supports SCTP protocol NetworkPolicies.
 -->
<p>你必须使用支持 SCTP 协议网络策略的 <a class='glossary-tooltip' title='容器网络接口 (CNI) 插件是遵循 appc/CNI 协议的一类网络插件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni' target='_blank' aria-label='CNI'>CNI</a> 插件。
</div>
<!--
## Targeting a range of Ports
-->
<h2 id="targeting-a-range-of-ports">针对某个端口范围  </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
When writing a NetworkPolicy, you can target a range of ports instead of a single port.

This is achievable with the usage of the `endPort` field, as the following example:
-->
<p>在编写 NetworkPolicy 时，你可以针对一个端口范围而不是某个固定端口。</p>
<p>这一目的可以通过使用 <code>endPort</code> 字段来实现，如下例所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>multi-port-egress<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>db<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">policyTypes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- Egress<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">egress</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">to</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">ipBlock</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cidr</span>:<span style="color:#bbb"> </span><span style="color:#666">10.0.0.0</span>/24<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">32000</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">endPort</span>:<span style="color:#bbb"> </span><span style="color:#666">32768</span><span style="color:#bbb">
</span></code></pre></div><!--
The above rule allows any Pod with label `role=db` on the namespace `default` to communicate
with any IP within the range `10.0.0.0/24` over TCP, provided that the target
port is between the range 32000 and 32768.
-->
<p>上面的规则允许名字空间 <code>default</code> 中所有带有标签 <code>role=db</code> 的 Pod 使用 TCP 协议
与 <code>10.0.0.0/24</code> 范围内的 IP 通信，只要目标端口介于 32000 和 32768 之间就可以。</p>
<!--
The following restrictions apply when using this field:
* As a beta feature, this is enabled by default. To disable the `endPort` field at a cluster level, you (or your cluster administrator) need to disable the `NetworkPolicyEndPort` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for the API server with `-feature-gates=NetworkPolicyEndPort=false,…`.
* The `endPort` field must be equal to or greater than the `port` field.
* `endPort` can only be defined if `port` is also defined.
* Both ports must be numeric.
-->
<p>使用此字段时存在以下限制：</p>
<ul>
<li>作为一种 Beta 阶段的特性，端口范围设定默认是被启用的。要在整个集群
范围内禁止使用 <code>endPort</code> 字段，你（或者你的集群管理员）需要为 API
服务器设置 <code>-feature-gates=NetworkPolicyEndPort=false,...</code> 以禁用
<code>NetworkPolicyEndPort</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。</li>
<li><code>endPort</code> 字段必须等于或者大于 <code>port</code> 字段的值。</li>
<li>两个字段的设置值都只能是数字。</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Your cluster must be using a <a class='glossary-tooltip' title='容器网络接口 (CNI) 插件是遵循 appc/CNI 协议的一类网络插件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni' target='_blank' aria-label='CNI'>CNI</a> plugin that
supports the `endPort` field in NetworkPolicy specifications.
If your [network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)
does not support the `endPort` field and you specify a NetworkPolicy with that,
the policy will be applied only for the single `port` field.
-->
<p>你的集群所使用的 <a class='glossary-tooltip' title='容器网络接口 (CNI) 插件是遵循 appc/CNI 协议的一类网络插件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni' target='_blank' aria-label='CNI'>CNI</a> 插件
必须支持在 NetworkPolicy 规约中使用 <code>endPort</code> 字段。
如果你的<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件</a>
不支持 <code>endPort</code> 字段，而你指定了一个包含 <code>endPort</code> 字段的 NetworkPolicy，
策略只对单个 <code>port</code> 字段生效。
</div>
<!--
## Targeting a Namespace by its name
-->
<h2 id="targeting-a-namespace-by-its-name">基于名字指向某名字空间  </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.22 [stable]</code>
</div>


<!--
The Kubernetes control plane sets an immutable label `kubernetes.io/metadata.name` on all
namespaces, provided that the `NamespaceDefaultLabelName`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.
The value of the label is the namespace name.

While NetworkPolicy cannot target a namespace by its name with some object field, you can use the
standardized label to target a specific namespace.
-->
<p>只要 <code>NamespaceDefaultLabelName</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
被启用，Kubernetes 控制面会在所有名字空间上设置一个不可变更的标签
<code>kubernetes.io/metadata.name</code>。该标签的值是名字空间的名称。</p>
<p>如果 NetworkPolicy 无法在某些对象字段中指向某名字空间，你可以使用标准的
标签方式来指向特定名字空间。</p>
<!--
## What you can't do with network policies (at least, not yet)

As of Kubernetes 1.23, the following functionality does not exist in the NetworkPolicy API, but you might be able to implement workarounds using Operating System components (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress controllers, Service Mesh implementations) or admission controllers.  In case you are new to network security in Kubernetes, its worth noting that the following User Stories cannot (yet) be implemented using the NetworkPolicy API.
-->
<h2 id="通过网络策略-至少目前还-无法完成的工作">通过网络策略（至少目前还）无法完成的工作</h2>
<p>到 Kubernetes 1.23 为止，NetworkPolicy API 还不支持以下功能，不过
你可能可以使用操作系统组件（如 SELinux、OpenVSwitch、IPTables 等等）
或者第七层技术（Ingress 控制器、服务网格实现）或准入控制器来实现一些
替代方案。
如果你对 Kubernetes 中的网络安全性还不太了解，了解使用 NetworkPolicy API
还无法实现下面的用户场景是很值得的。</p>
<!--
- Forcing internal cluster traffic to go through a common gateway (this might be best served with a service mesh or other proxy).
- Anything TLS related (use a service mesh or ingress controller for this).
- Node specific policies (you can use CIDR notation for these, but you cannot target nodes by their Kubernetes identities specifically).
- Targeting of services by name (you can, however, target pods or namespaces by their <a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='labels'>labels</a>, which is often a viable workaround).
- Creation or management of "Policy requests" that are fulfilled by a third party.
-->
<ul>
<li>强制集群内部流量经过某公用网关（这种场景最好通过服务网格或其他代理来实现）；</li>
<li>与 TLS 相关的场景（考虑使用服务网格或者 Ingress 控制器）；</li>
<li>特定于节点的策略（你可以使用 CIDR 来表达这一需求不过你无法使用节点在
Kubernetes 中的其他标识信息来辩识目标节点）；</li>
<li>基于名字来选择服务（不过，你可以使用 <a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>
来选择目标 Pod 或名字空间，这也通常是一种可靠的替代方案）；</li>
<li>创建或管理由第三方来实际完成的“策略请求”；</li>
</ul>
<!--
- Default policies which are applied to all namespaces or pods (there are some third party Kubernetes distributions and projects which can do this).
- Advanced policy querying and reachability tooling.
- The ability to log network security events (for example connections that are blocked or accepted).
- The ability to explicitly deny policies (currently the model for NetworkPolicies are deny by default, with only the ability to add allow rules).
- The ability to prevent loopback or incoming host traffic (Pods cannot currently block localhost access, nor do they have the ability to block access from their resident node).
-->
<ul>
<li>实现适用于所有名字空间或 Pods 的默认策略（某些第三方 Kubernetes 发行版本
或项目可以做到这点）；</li>
<li>高级的策略查询或者可达性相关工具；</li>
<li>生成网络安全事件日志的能力（例如，被阻塞或接收的连接请求）；</li>
<li>显式地拒绝策略的能力（目前，NetworkPolicy 的模型默认采用拒绝操作，
其唯一的能力是添加允许策略）；</li>
<li>禁止本地回路或指向宿主的网络流量（Pod 目前无法阻塞 localhost 访问，
它们也无法禁止来自所在节点的访问请求）。</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
- See the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/)
  walkthrough for further examples.
- See more [recipes](https://github.com/ahmetb/kubernetes-network-policy-recipes) for common scenarios enabled by the NetworkPolicy resource.
-->
<ul>
<li>参阅<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
演练了解更多示例；</li>
<li>有关 NetworkPolicy 资源所支持的常见场景的更多信息，请参见
<a href="https://github.com/ahmetb/kubernetes-network-policy-recipes">此指南</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-21f8d19c60c33914baab66224c3d46a7">5.11 - IPv4/IPv6 双协议栈</h1>
    
	<!--
reviewers:
- lachie83
- khenidak
- aramase
- bridgetkromhout
title: IPv4/IPv6 dual-stack
feature:
  title: IPv4/IPv6 dual-stack
  description: >
    Allocation of IPv4 and IPv6 addresses to Pods and Services

content_type: concept
weight: 70
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code>
</div>


<!--
 IPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> and <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a>.
-->
<p>IPv4/IPv6 双协议栈网络能够将 IPv4 和 IPv6 地址分配给
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 和
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>。</p>
<!--
IPv4/IPv6 dual-stack networking is enabled by default for your Kubernetes cluster starting in 1.21, allowing the simultaneous assignment of both IPv4 and IPv6 addresses.
-->
<p>从 1.21 版本开始，Kubernetes 集群默认启用 IPv4/IPv6 双协议栈网络，
以支持同时分配 IPv4 和 IPv6 地址。</p>
<!-- body -->
<!--
## Supported Features
-->
<h2 id="supported-features">支持的功能 </h2>
<!--
IPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:
-->
<p>Kubernetes 集群的 IPv4/IPv6 双协议栈可提供下面的功能：</p>
<!--
   * Dual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)
   * IPv4 and IPv6 enabled Services
   * Pod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces
-->
<ul>
<li>双协议栈 pod 网络 (每个 pod 分配一个 IPv4 和 IPv6 地址)</li>
<li>IPv4 和 IPv6 启用的服务</li>
<li>Pod 的集群外出口通过 IPv4 和 IPv6 路由</li>
</ul>
<!--
## Prerequisites
-->
<h2 id="prerequisites">先决条件 </h2>
<!--
The following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes clusters:
-->
<p>为了使用 IPv4/IPv6 双栈的 Kubernetes 集群，需要满足以下先决条件：</p>
<!--
   * Kubernetes 1.20 or later  
     For information about using dual-stack services with earlier
     Kubernetes versions, refer to the documentation for that version
     of Kubernetes.
   * Provider support for dual-stack networking (Cloud provider or otherwise must be able to provide Kubernetes nodes with routable IPv4/IPv6 network interfaces)
   * A network plugin that supports dual-stack (such as Kubenet or Calico)
-->
<ul>
<li>Kubernetes 1.20 版本或更高版本，有关更早 Kubernetes 版本的使用双栈服务的信息，
请参考对应版本的 Kubernetes 文档。</li>
<li>提供商支持双协议栈网络（云提供商或其他提供商必须能够为 Kubernetes
节点提供可路由的 IPv4/IPv6 网络接口）</li>
<li>支持双协议栈的网络插件（如 Kubenet 或 Calico）</li>
</ul>
<!--
## Configure IPv4/IPv6 dual-stack
-->
<h2 id="配置-ipv4-ipv6-双协议栈">配置 IPv4/IPv6 双协议栈</h2>
<!--
To use IPv4/IPv6 dual-stack, ensure the `IPv6DualStack` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled for the relevant components of your cluster. (Starting in 1.21, IPv4/IPv6 dual-stack defaults to enabled.)
-->
<p>要使用 IPv4/IPv6 双协议栈，确保为集群的相关组件启用 <code>IPv6DualStack</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>，
（从 1.21 版本开始，IPv4/IPv6 双协议栈默认是被启用的）。</p>
<!--
   * kube-apiserver:
      * `--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>`
   * kube-controller-manager:
      * `--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>`
      * `--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>`
      * `--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6` defaults to /24 for IPv4 and /64 for IPv6
   * kube-proxy:
      * `--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>`
-->
<ul>
<li>kube-apiserver:
<ul>
<li><code>--service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li>
</ul>
</li>
<li>kube-controller-manager:
<ul>
<li><code>--cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li>
<li><code>--service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li>
<li><code>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code> 对于 IPv4 默认为 /24，对于 IPv6 默认为 /64</li>
</ul>
</li>
<li>kube-proxy:
<ul>
<li><code>--cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></li>
</ul>
</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
An example of an IPv4 CIDR: `10.244.0.0/16` (though you would supply your own address range)

An example of an IPv6 CIDR: `fdXY:IJKL:MNOP:15::/64` (this shows the format but is not a valid address - see [RFC 4193](https://tools.ietf.org/html/rfc4193))
-->
<p>IPv4 CIDR 的一个例子：<code>10.244.0.0/16</code>（尽管你会提供你自己的地址范围）。</p>
<p>IPv6 CIDR 的一个例子：<code>fdXY:IJKL:MNOP:15::/64</code>
（这里演示的是格式而非有效地址 - 请看 <a href="https://tools.ietf.org/html/rfc4193">RFC 4193</a>）。</p>
<!--
Starting in 1.21, IPv4/IPv6 dual-stack defaults to enabled.
You can disable it when necessary by specifying `--feature-gates="IPv6DualStack=false"`
on the kube-apiserver, kube-controller-manager, kubelet, and kube-proxy command line.
-->
<p>从 1.21 开始 IPv4/IPv6 双协议栈默认为启用状态。
你可以在必要的时候通过为 kube-apiserver、kube-controller-manager、kubelet
和 kube-proxy 命令行设置 <code>--feature-gates=&quot;IPv6DualStack=false&quot;</code> 来禁用
此特性。</p>

</div>
<!--
## Services
-->
<h2 id="服务">服务</h2>
<!--
You can create <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a> which can use IPv4, IPv6, or both. 

The address family of a Service defaults to the address family of the first service cluster IP range (configured via the `--service-cluster-ip-range` flag to the kube-apiserver).

When you define a Service you can optionally configure it as dual stack. To specify the behavior you want, you
set the `.spec.ipFamilyPolicy` field to one of the following values:
-->
<p>你可以使用 IPv4 或 IPv6 地址来创建
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>。
服务的地址族默认为第一个服务集群 IP 范围的地址族（通过 kube-apiserver 的
<code>--service-cluster-ip-range</code> 参数配置）。
当你定义服务时，可以选择将其配置为双栈。若要指定所需的行为，你可以设置
<code>.spec.ipFamilyPolicy</code> 字段为以下值之一：</p>
<!--
* `SingleStack`: Single-stack service. The control plane allocates a cluster IP for the Service, using the first configured service cluster IP range.
* `PreferDualStack`:
  * Allocates IPv4 and IPv6 cluster IPs for the Service. (If the cluster has `--feature-gates="IPv6DualStack=false"`, this setting follows the same behavior as `SingleStack`.)
* `RequireDualStack`: Allocates Service `.spec.ClusterIPs` from both IPv4 and IPv6 address ranges.
  * Selects the `.spec.ClusterIP` from the list of `.spec.ClusterIPs` based on the address family of the first element in the `.spec.ipFamilies` array.
-->
<ul>
<li><code>SingleStack</code>：单栈服务。控制面使用第一个配置的服务集群 IP 范围为服务分配集群 IP。</li>
<li><code>PreferDualStack</code>：
<ul>
<li>为服务分配 IPv4 和 IPv6 集群 IP 地址。
（如果集群设置了 <code>--feature-gates=&quot;IPv6DualStack=false&quot;</code>，则此设置的行为与
<code>SingleStack</code> 设置相同。）</li>
</ul>
</li>
<li><code>RequireDualStack</code>：从 IPv4 和 IPv6 的地址范围分配服务的 <code>.spec.ClusterIPs</code>
<ul>
<li>从基于在 <code>.spec.ipFamilies</code> 数组中第一个元素的地址族的 <code>.spec.ClusterIPs</code>
列表中选择 <code>.spec.ClusterIP</code></li>
</ul>
</li>
</ul>
<!--
If you would like to define which IP family to use for single stack or define the order of IP families for dual-stack, you can choose the address families by setting an optional field, `.spec.ipFamilies`, on the Service. 
-->
<p>如果你想要定义哪个 IP 族用于单栈或定义双栈 IP 族的顺序，可以通过设置
服务上的可选字段 <code>.spec.ipFamilies</code> 来选择地址族。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The `.spec.ipFamilies` field is immutable because the `.spec.ClusterIP` cannot be reallocated on a Service that already exists. If you want to change `.spec.ipFamilies`, delete and recreate the Service.
-->
<p><code>.spec.ipFamilies</code> 字段是不可变的，因为系统无法为已经存在的服务重新分配
<code>.spec.ClusterIP</code>。如果你想改变 <code>.spec.ipFamilies</code>，则需要删除并重新创建服务。
</div>
<!--
You can set `.spec.ipFamilies` to any of the following array values:
-->
<p>你可以设置 <code>.spec.ipFamily</code> 为以下任何数组值：</p>
<!--
- `["IPv4"]`
- `["IPv6"]`
- `["IPv4","IPv6"]` (dual stack)
- `["IPv6","IPv4"]` (dual stack)

-->
<ul>
<li><code>[&quot;IPv4&quot;]</code></li>
<li><code>[&quot;IPv6&quot;]</code></li>
<li><code>[&quot;IPv4&quot;,&quot;IPv6&quot;]</code> （双栈）</li>
<li><code>[&quot;IPv6&quot;,&quot;IPv4&quot;]</code> （双栈）</li>
</ul>
<!--
The first family you list is used for the legacy `.spec.ClusterIP` field.
-->
<p>你所列出的第一个地址族用于原来的 <code>.spec.ClusterIP</code> 字段。</p>
<!--
### Dual-stack Service configuration scenarios

These examples demonstrate the behavior of various dual-stack Service configuration scenarios.
-->
<h3 id="双栈服务配置场景">双栈服务配置场景</h3>
<p>以下示例演示多种双栈服务配置场景下的行为。</p>
<!--
#### Dual-stack options on new Services
-->
<h4 id="新服务的双栈选项">新服务的双栈选项</h4>
<!--
1. This Service specification does not explicitly define `.spec.ipFamilyPolicy`. When you create this Service, Kubernetes assigns a cluster IP for the Service from the first configured `service-cluster-ip-range` and sets the `.spec.ipFamilyPolicy` to `SingleStack`. ([Services without selectors](/docs/concepts/services-networking/service/#services-without-selectors) and [headless Services](/docs/concepts/services-networking/service/#headless-services) with selectors will behave in this same way.)
-->
<ol>
<li>
<p>此服务规约中没有显式设定 <code>.spec.ipFamilyPolicy</code>。当你创建此服务时，Kubernetes
从所配置的第一个 <code>service-cluster-ip-range</code> 种为服务分配一个集群IP，并设置
<code>.spec.ipFamilyPolicy</code> 为 <code>SingleStack</code>。
（<a href="/zh/docs/concepts/services-networking/service/#services-without-selectors">无选择算符的服务</a>
和<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>的行为方式
与此相同。）</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/dual-stack-default-svc.yaml" download="service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-dual-stack-default-svc-yaml')" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-dual-stack-default-svc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


</li>
</ol>
<!--
1. This Service specification explicitly defines `PreferDualStack` in `.spec.ipFamilyPolicy`. When you create this Service on a dual-stack cluster, Kubernetes assigns both IPv4 and IPv6 addresses for the service. The control plane updates the `.spec` for the Service to record the IP address assignments. The field `.spec.ClusterIPs` is the primary field, and contains both assigned IP addresses; `.spec.ClusterIP` is a secondary field with its value calculated from `.spec.ClusterIPs`.
   
      * For the `.spec.ClusterIP` field, the control plane records the IP address that is from the same address family as the first service cluster IP range. 
      * On a single-stack cluster, the `.spec.ClusterIPs` and `.spec.ClusterIP` fields both only list one address. 
      * On a cluster with dual-stack enabled, specifying `RequireDualStack` in `.spec.ipFamilyPolicy` behaves the same as `PreferDualStack`.

-->
<ol start="2">
<li>
<p>此服务规约显式地将 <code>.spec.ipFamilyPolicy</code> 设置为 <code>PreferDualStack</code>。
当你在双栈集群上创建此服务时，Kubernetes 会为该服务分配 IPv4 和 IPv6 地址。
控制平面更新服务的 <code>.spec</code> 以记录 IP 地址分配。
字段 <code>.spec.ClusterIPs</code> 是主要字段，包含两个分配的 IP 地址；<code>.spec.ClusterIP</code> 是次要字段，
其取值从 <code>.spec.ClusterIPs</code> 计算而来。</p>
<ul>
<li>对于 <code>.spec.ClusterIP</code> 字段，控制面记录来自第一个服务集群 IP 范围
对应的地址族的 IP 地址。</li>
<li>对于单协议栈的集群，<code>.spec.ClusterIPs</code> 和 <code>.spec.ClusterIP</code> 字段都
仅仅列出一个地址。</li>
<li>对于启用了双协议栈的集群，将 <code>.spec.ipFamilyPolicy</code> 设置为
<code>RequireDualStack</code> 时，其行为与 <code>PreferDualStack</code> 相同。</li>
</ul>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/dual-stack-preferred-svc.yaml" download="service/networking/dual-stack-preferred-svc.yaml"><code>service/networking/dual-stack-preferred-svc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-dual-stack-preferred-svc-yaml')" title="Copy service/networking/dual-stack-preferred-svc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-dual-stack-preferred-svc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>PreferDualStack<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


</li>
</ol>
<!--
1. This Service specification explicitly defines `IPv6` and `IPv4` in `.spec.ipFamilies` as well as defining `PreferDualStack` in `.spec.ipFamilyPolicy`. When Kubernetes assigns an IPv6 and IPv4 address in `.spec.ClusterIPs`, `.spec.ClusterIP` is set to the IPv6 address because that is the first element in the `.spec.ClusterIPs` array, overriding the default.
-->
<ol start="3">
<li>
<p>下面的服务规约显式地在 <code>.spec.ipFamilies</code> 中指定 <code>IPv6</code> 和 <code>IPv4</code>，并
将 <code>.spec.ipFamilyPolicy</code> 设定为 <code>PreferDualStack</code>。
当 Kubernetes 为 <code>.spec.ClusterIPs</code> 分配一个 IPv6 和一个 IPv4 地址时，
<code>.spec.ClusterIP</code> 被设置成 IPv6 地址，因为它是 <code>.spec.ClusterIPs</code> 数组中的第一个元素，
覆盖其默认值。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/dual-stack-preferred-ipfamilies-svc.yaml" download="service/networking/dual-stack-preferred-ipfamilies-svc.yaml"><code>service/networking/dual-stack-preferred-ipfamilies-svc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-dual-stack-preferred-ipfamilies-svc-yaml')" title="Copy service/networking/dual-stack-preferred-ipfamilies-svc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-dual-stack-preferred-ipfamilies-svc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>PreferDualStack<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- IPv6<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- IPv4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


</li>
</ol>
<!--
#### Dual-stack defaults on existing Services
-->
<h4 id="现有服务的双栈默认值">现有服务的双栈默认值</h4>
<!--
These examples demonstrate the default behavior when dual-stack is newly enabled on a cluster where Services already exist.  (Upgrading an existing cluster to 1.21 will enable dual-stack unless `--feature-gates="IPv6DualStack=false"` is set.)
-->
<p>下面示例演示了在服务已经存在的集群上新启用双栈时的默认行为。
（将现有集群升级到 1.21 会启用双协议栈支持，除非设置了
<code>--feature-gates=&quot;IPv6DualStack=false&quot;</code>）</p>
<!--
1. When dual-stack is enabled on a cluster, existing Services (whether `IPv4` or `IPv6`) are configured by the control plane to set `.spec.ipFamilyPolicy` to `SingleStack` and set `.spec.ipFamilies` to the address family of the existing Service. The existing Service cluster IP will be stored in `.spec.ClusterIPs`.
-->
<ol>
<li>
<p>在集群上启用双栈时，控制面会将现有服务（无论是 <code>IPv4</code> 还是 <code>IPv6</code>）配置
<code>.spec.ipFamilyPolicy</code> 为 <code>SingleStack</code> 并设置 <code>.spec.ipFamilies</code>
为服务的当前地址族。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/dual-stack-default-svc.yaml" download="service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-dual-stack-default-svc-yaml')" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-dual-stack-default-svc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
You can validate this behavior by using kubectl to inspect an existing service.
-->
<p>你可以通过使用 kubectl 检查现有服务来验证此行为。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc my-service -o yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span><span style="color:#666">10.0.197.123</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIPs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#666">10.0.197.123</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- IPv4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>SingleStack<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>ClusterIP<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">loadBalancer</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
1. When dual-stack is enabled on a cluster, existing [headless Services](/docs/concepts/services-networking/service/#headless-services) with selectors are configured by the control plane to set `.spec.ipFamilyPolicy` to `SingleStack` and set `.spec.ipFamilies` to the address family of the first service cluster IP range (configured via the `--service-cluster-ip-range` flag to the kube-apiserver) even though `.spec.ClusterIP` is set to `None`.
-->
<ol start="2">
<li>
<p>在集群上启用双栈时，带有选择算符的现有
<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a>
由控制面设置 <code>.spec.ipFamilyPolicy</code> 为 <code>SingleStack</code>
并设置 <code>.spec.ipFamilies</code> 为第一个服务集群 IP 范围的地址族（通过配置 kube-apiserver 的
<code>--service-cluster-ip-range</code> 参数），即使 <code>.spec.ClusterIP</code> 的设置值为 <code>None</code> 也如此。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/dual-stack-default-svc.yaml" download="service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-dual-stack-default-svc-yaml')" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-dual-stack-default-svc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
You can validate this behavior by using kubectl to inspect an existing headless service with selectors.
-->
<p>你可以通过使用 kubectl 检查带有选择算符的现有无头服务来验证此行为。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc my-service -o yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIPs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- IPv4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>SingleStack<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp  <span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
#### Switching Services between single-stack and dual-stack
-->
<h4 id="在单栈和双栈之间切换服务">在单栈和双栈之间切换服务</h4>
<!--
Services can be changed from single-stack to dual-stack and from dual-stack to single-stack.
-->
<p>服务可以从单栈更改为双栈，也可以从双栈更改为单栈。</p>
<!--
1. To change a Service from single-stack to dual-stack, change `.spec.ipFamilyPolicy` from `SingleStack` to `PreferDualStack` or `RequireDualStack` as desired. When you change this Service from single-stack to dual-stack, Kubernetes assigns the missing address family so that the Service now has IPv4 and IPv6 addresses.

   Edit the Service specification updating the `.spec.ipFamilyPolicy` from `SingleStack` to `PreferDualStack`.
-->
<ol>
<li>
<p>要将服务从单栈更改为双栈，根据需要将 <code>.spec.ipFamilyPolicy</code> 从 <code>SingleStack</code> 改为
<code>PreferDualStack</code> 或 <code>RequireDualStack</code>。
当你将此服务从单栈更改为双栈时，Kubernetes 将分配缺失的地址族，以便现在
该服务具有 IPv4 和 IPv6 地址。
编辑服务规约将 <code>.spec.ipFamilyPolicy</code> 从 <code>SingleStack</code> 改为 <code>PreferDualStack</code>。</p>
<!--
Before:
-->
<p>之前：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>SingleStack<span style="color:#bbb">
</span></code></pre></div><!--
After:
-->
<p>之后：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>PreferDualStack<span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
1. To change a Service from dual-stack to single-stack, change `.spec.ipFamilyPolicy` from `PreferDualStack` or `RequireDualStack` to `SingleStack`. When you change this Service from dual-stack to single-stack, Kubernetes retains only the first element in the `.spec.ClusterIPs` array, and sets `.spec.ClusterIP` to that IP address and sets `.spec.ipFamilies` to the address family of `.spec.ClusterIPs`.
-->
<ol start="2">
<li>要将服务从双栈更改为单栈，请将 <code>.spec.ipFamilyPolicy</code> 从 <code>PreferDualStack</code> 或
<code>RequireDualStack</code> 改为 <code>SingleStack</code>。
当你将此服务从双栈更改为单栈时，Kubernetes 只保留 <code>.spec.ClusterIPs</code>
数组中的第一个元素，并设置 <code>.spec.ClusterIP</code> 为那个 IP 地址，
并设置 <code>.spec.ipFamilies</code> 为 <code>.spec.ClusterIPs</code> 地址族。</li>
</ol>
<!--
### Headless Services without selector
-->
<h3 id="无选择算符的无头服务">无选择算符的无头服务</h3>
<!--
For [Headless Services without selectors](/docs/concepts/services-networking/service/#without-selectors) and without `.spec.ipFamilyPolicy` explicitly set, the `.spec.ipFamilyPolicy` field defaults to `RequireDualStack`.
-->
<p>对于<a href="/zh/docs/concepts/services-networking/service/#without-selectors">不带选择算符的无头服务</a>，
若没有显式设置 <code>.spec.ipFamilyPolicy</code>，则 <code>.spec.ipFamilyPolicy</code>
字段默认设置为 <code>RequireDualStack</code>。</p>
<!--
### Service type LoadBalancer
-->
<h3 id="loadbalancer-类型服务">LoadBalancer 类型服务</h3>
<!--
To provision a dual-stack load balancer for your Service:
   * Set the `.spec.type` field to `LoadBalancer`
   * Set `.spec.ipFamilyPolicy` field to `PreferDualStack` or `RequireDualStack`
-->
<p>要为你的服务提供双栈负载均衡器：</p>
<ul>
<li>将 <code>.spec.type</code> 字段设置为 <code>LoadBalancer</code></li>
<li>将 <code>.spec.ipFamilyPolicy</code> 字段设置为 <code>PreferDualStack</code> 或者 <code>RequireDualStack</code></li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
To use a dual-stack `LoadBalancer` type Service, your cloud provider must support IPv4 and IPv6 load balancers.
-->
<p>为了使用双栈的负载均衡器类型服务，你的云驱动必须支持 IPv4 和 IPv6 的负载均衡器。
</div>
<!--
## Egress traffic
-->
<h2 id="出站流量">出站流量</h2>
<!--
If you want to enable egress traffic in order to reach off-cluster destinations (eg. the public Internet) from a Pod that uses non-publicly routable IPv6 addresses, you need to enable the Pod to use a publicly routed IPv6 address via a mechanism such as transparent proxying or IP masquerading. The [ip-masq-agent](https://github.com/kubernetes-sigs/ip-masq-agent) project supports IP masquerading on dual-stack clusters.
-->
<p>如果你要启用出站流量，以便使用非公开路由 IPv6 地址的 Pod 到达集群外地址
（例如公网），则需要通过透明代理或 IP 伪装等机制使 Pod 使用公共路由的
IPv6 地址。
<a href="https://github.com/kubernetes-sigs/ip-masq-agent">ip-masq-agent</a>项目
支持在双栈集群上进行 IP 伪装。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Ensure your <a class='glossary-tooltip' title='容器网络接口 (CNI) 插件是遵循 appc/CNI 协议的一类网络插件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni' target='_blank' aria-label='CNI'>CNI</a> provider supports IPv6.
-->
<p>确认你的 <a class='glossary-tooltip' title='容器网络接口 (CNI) 插件是遵循 appc/CNI 协议的一类网络插件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni' target='_blank' aria-label='CNI'>CNI</a> 驱动支持 IPv6。
</div>
<h2 id="what-s-next">What's next</h2>
<!--
* [Validate IPv4/IPv6 dual-stack](/docs/tasks/network/validate-dual-stack) networking
* [Enable dual-stack networking using kubeadm ](/docs/setup/production-environment/tools/kubeadm/dual-stack-support/)
-->
<ul>
<li><a href="/zh/docs/tasks/network/validate-dual-stack">验证 IPv4/IPv6 双协议栈</a>网络</li>
<li><a href="/zh/docs/setup/production-environment/tools/kubeadm/dual-stack-support/">使用 kubeadm 启用双协议栈网络</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f018f568c6723865753f150c3c59bdda">6 - 存储</h1>
    <div class="lead">为集群中的 Pods 提供长期和临时存储的方法。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-27795584640a03bd2024f1fe3b3ab754">6.1 - 卷</h1>
    
	<!--
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: Volumes
content_type: concept
weight: 10
-->
<!-- overview -->
<!--
On-disk files in a Container are ephemeral, which presents some problems for
non-trivial applications when running in containers. One problem
is the loss of files when a container crashes. The kubelet restarts the container
but with a clean state. A second problem occurs when sharing files
between containers running together in a `Pod`.
The Kubernetes <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='volume'>volume</a> abstraction
solves both of these problems.
-->
<p>Container 中的文件在磁盘上是临时存放的，这给 Container 中运行的较重要的应用程序带来一些问题。
问题之一是当容器崩溃时文件丢失。
kubelet 会重新启动容器，但容器会以干净的状态重启。
第二个问题会在同一 <code>Pod</code> 中运行多个容器并共享文件时出现。
Kubernetes <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷（Volume）'>卷（Volume）</a>
这一抽象概念能够解决这两个问题。</p>
<!--
Familiarity with [Pods](/docs/concepts/workloads/pods/) is suggested.
-->
<p>阅读本文前建议你熟悉一下 <a href="/zh/docs/concepts/workloads/pods">Pods</a>。</p>
<!-- body -->
<!--
## Background

Docker has a concept of
[volumes](https://docs.docker.com/storage/), though it is
somewhat looser and less managed. A Docker volume is a directory on
disk or in another container. Docker provides volume
drivers, but the functionality is somewhat limited.
-->
<h2 id="background">背景 </h2>
<p>Docker 也有 <a href="https://docs.docker.com/storage/">卷（Volume）</a> 的概念，但对它只有少量且松散的管理。
Docker 卷是磁盘上或者另外一个容器内的一个目录。
Docker 提供卷驱动程序，但是其功能非常有限。</p>
<!--
Kubernetes supports many types of volumes. A <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
can use any number of volume types simultaneously.
Ephemeral volume types have a lifetime of a pod, but persistent volumes exist beyond
the lifetime of a pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes;
however, Kubernetes does not destroy persistent volumes.
For any kind of volume in a given pod, data is preserved across container restarts.
-->
<p>Kubernetes 支持很多类型的卷。
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 可以同时使用任意数目的卷类型。
临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。
当 Pod 不再存在时，Kubernetes 也会销毁临时卷；不过 Kubernetes 不会销毁持久卷。
对于给定 Pod 中任何类型的卷，在容器重启期间数据都不会丢失。</p>
<!--
At its core, a volume is just a directory, possibly with some data in it, which
is accessible to the containers in a pod. How that directory comes to be, the
medium that backs it, and the contents of it are determined by the particular
volume type used.
-->
<p>卷的核心是一个目录，其中可能存有数据，Pod 中的容器可以访问该目录中的数据。
所采用的特定的卷类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放的内容。</p>
<!--
To use a volume, specify the volumes to provide for the Pod in `.spec.volumes`
and declare where to mount those volumes into containers in `.spec.containers[*].volumeMounts`.
A process in a container sees a filesystem view composed from the initial contents of
the <a class='glossary-tooltip' title='镜像是保存的容器实例，它打包了应用运行所需的一组软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-image' target='_blank' aria-label='container image'>container image</a>, plus volumes
(if defined) mounted inside the container.
The process sees a root filesystem that initially matches the contents of the container
image.
Any writes to within that filesystem hierarchy, if allowed, affect what that process views
when it performs a subsequent filesystem access.
-->
<p>使用卷时, 在 <code>.spec.volumes</code> 字段中设置为 Pod 提供的卷，并在
<code>.spec.containers[*].volumeMounts</code> 字段中声明卷在容器中的挂载位置。
容器中的进程看到的文件系统视图是由它们的 <a class='glossary-tooltip' title='镜像是保存的容器实例，它打包了应用运行所需的一组软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-image' target='_blank' aria-label='容器镜像'>容器镜像</a>
的初始内容以及挂载在容器中的卷（如果定义了的话）所组成的。
其中根文件系统同容器镜像的内容相吻合。
任何在该文件系统下的写入操作，如果被允许的话，都会影响接下来容器中进程访问文件系统时所看到的内容。</p>
<!--
Volumes mount at the [specified paths](#using-subpath) within
the image.
For each container defined within a Pod, you must independently specify where
to mount each volume that the container uses.

Volumes cannot mount within other volumes (but see [Using subPath](#using-subpath)
for a related mechanism). Also, a volume cannot contain a hard link to anything in
a different volume.
-->
<p>卷挂载在镜像中的<a href="#using-subpath">指定路径</a>下。
Pod 配置中的每个容器必须独立指定各个卷的挂载位置。</p>
<p>卷不能挂载到其他卷之上（不过存在一种<a href="#using-subpath">使用 subPath</a> 的相关机制），也不能与其他卷有硬链接。</p>
<!--
## Types of Volumes

Kubernetes supports several types of Volumes:
-->
<h2 id="volume-types">卷类型 </h2>
<p>Kubernetes 支持下列类型的卷：</p>
<h3 id="awselasticblockstore">awsElasticBlockStore</h3>
<!--
An `awsElasticBlockStore` volume mounts an Amazon Web Services (AWS)
[EBS Volume](http://aws.amazon.com/ebs/) into your Pod.  Unlike
`emptyDir`, which is erased when a Pod is removed, the contents of an EBS
volume are persisted and the volume is unmounted. This means that an
EBS volume can be pre-populated with data, and that data can be shared between pods.
-->
<p><code>awsElasticBlockStore</code> 卷将 Amazon Web服务（AWS）<a href="https://aws.amazon.com/ebs/">EBS 卷</a>
挂载到你的 Pod 中。与 <code>emptyDir</code> 在 Pod 被删除时也被删除不同，EBS 卷的内容在删除 Pod
时会被保留，卷只是被卸载掉了。
这意味着 EBS 卷可以预先填充数据，并且该数据可以在 Pod 之间共享。</p>
<!--
You must create an EBS volume using `aws ec2 create-volume` or the AWS API before you can use it.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你在使用 EBS 卷之前必须使用 <code>aws ec2 create-volume</code> 命令或者 AWS API 创建该卷。
</div>
<!--
There are some restrictions when using an `awsElasticBlockStore` volume:

* the nodes on which Pods are running must be AWS EC2 instances
* those instances need to be in the same region and availability-zone as the EBS volume
* EBS only supports a single EC2 instance mounting a volume
-->
<p>使用 <code>awsElasticBlockStore</code> 卷时有一些限制：</p>
<ul>
<li>Pod 运行所在的节点必须是 AWS EC2 实例。</li>
<li>这些实例需要与 EBS 卷在相同的地域（Region）和可用区（Availability-Zone）。</li>
<li>EBS 卷只支持被挂载到单个 EC2 实例上。</li>
</ul>
<!--
#### Creating an EBS volume

Before you can use an EBS volume with a Pod, you need to create it.
-->
<h4 id="创建-ebs-卷">创建 EBS 卷</h4>
<p>在将 EBS 卷用到 Pod 上之前，你首先要创建它。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">aws ec2 create-volume --availability-zone<span style="color:#666">=</span>eu-west-1a --size<span style="color:#666">=</span><span style="color:#666">10</span> --volume-type<span style="color:#666">=</span>gp2
</code></pre></div><!--
Make sure the zone matches the zone you brought up your cluster in. Check that the size and
EBS volume type are suitable for your use.
-->
<p>确保该区域与你的群集所在的区域相匹配。还要检查卷的大小和 EBS 卷类型都适合你的用途。</p>
<!--
#### AWS EBS Example configuration
-->
<h4 id="aws-ebs-配置示例">AWS EBS 配置示例</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-ebs<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/test-webserver<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/test-ebs<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此 AWS EBS 卷必须已经存在</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">awsElasticBlockStore</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeID</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&lt;volume-id&gt;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span></code></pre></div><!--
If the EBS volume is partitioned, you can supply the optional field `partition: "<partition number>"` to specify which parition to mount on.
-->
<p>如果 EBS 卷是分区的，你可以提供可选的字段 <code>partition: &quot;&lt;partition number&gt;&quot;</code> 来指定要挂载到哪个分区上。</p>
<!--
#### AWS EBS CSI migration
-->
<h4 id="aws-ebs-csi-卷迁移">AWS EBS CSI 卷迁移</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [beta]</code>
</div>


<!--
The `CSIMigration` feature for `awsElasticBlockStore`, when enabled, redirects
all plugin operations from the existing in-tree plugin to the `ebs.csi.aws.com` Container
Storage Interface (CSI) driver. In order to use this feature, the [AWS EBS CSI
driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationAWS`
beta features must be enabled.
-->
<p>如果启用了对 <code>awsElasticBlockStore</code> 的 <code>CSIMigration</code>
特性支持，所有插件操作都不再指向树内插件（In-Tree Plugin），转而指向
<code>ebs.csi.aws.com</code> 容器存储接口（Container Storage Interface，CSI）驱动。
为了使用此特性，必须在集群中安装
<a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">AWS EBS CSI 驱动</a>，
并确保 <code>CSIMigration</code> 和 <code>CSIMigrationAWS</code> Beta 功能特性被启用。</p>
<!--
#### AWS EBS CSI migration complete
-->
<h4 id="aws-ebs-csi-迁移结束">AWS EBS CSI 迁移结束</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [alpha]</code>
</div>


<!--
To disable the `awsElasticBlockStore` storage plugin from being loaded by the controller manager
and the kubelet, set the `InTreePluginAWSUnregister` flag to `true`.
-->
<p>要禁止控制器管理器和 kubelet 加载 <code>awsElasticBlockStore</code> 存储插件，
请将 <code>InTreePluginAWSUnregister</code> 标志设置为 <code>true</code>。</p>
<h3 id="azuredisk">azureDisk</h3>
<!--
The `azureDisk` volume type mounts a Microsoft Azure [Data Disk](https://docs.microsoft.com/en-us/azure/aks/csi-storage-drivers) into a pod.

For more details, see the [`azureDisk` volume plugin](https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk/README.md).
-->
<p><code>azureDisk</code> 卷类型用来在 Pod 上挂载 Microsoft Azure
<a href="https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-about-disks-vhds/">数据盘（Data Disk）</a> 。
若需了解更多详情，请参考 <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk/README.md"><code>azureDisk</code> 卷插件</a>。</p>
<!--
#### azureDisk CSI Migration
-->
<h4 id="azuredisk-csi-migration">azureDisk 的 CSI 迁移 </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code>
</div>


<!--
The `CSIMigration` feature for `azureDisk`, when enabled, redirects all plugin operations
from the existing in-tree plugin to the `disk.csi.azure.com` Container
Storage Interface (CSI) Driver. In order to use this feature, the [Azure Disk CSI
Driver](https://github.com/kubernetes-sigs/azuredisk-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationAzureDisk`
features must be enabled.
-->
<p>启用 <code>azureDisk</code> 的 <code>CSIMigration</code> 功能后，所有插件操作从现有的树内插件重定向到
<code>disk.csi.azure.com</code> 容器存储接口（CSI）驱动程序。
为了使用此功能，必须在集群中安装
<a href="https://github.com/kubernetes-sigs/azuredisk-csi-driver">Azure 磁盘 CSI 驱动程序</a>，
并且 <code>CSIMigration</code> 和 <code>CSIMigrationAzureDisk</code> 功能必须被启用。</p>
<!--
#### azureDisk CSI migration complete






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>



To disable the `azureDisk` storage plugin from being loaded by the controller manager
and the kubelet, set the `InTreePluginAzureDiskUnregister` flag to `true`.
-->
<h4 id="azuredisk-csi-迁移完成">azureDisk CSI 迁移完成</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>


<p>要禁止控制器管理器和 kubelet 加载 <code>azureDisk</code> 存储插件，
请将 <code>InTreePluginAzureDiskUnregister</code> 标志设置为 <code>true</code>。</p>
<h3 id="azurefile">azureFile</h3>
<!--
The `azureFile` volume type mounts a Microsoft Azure File volume (SMB 2.1 and 3.0)
into a Pod.

For more details, see the [`azureFile` volume plugin](https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md).
-->
<p><code>azureFile</code> 卷类型用来在 Pod 上挂载 Microsoft Azure 文件卷（File Volume）（SMB 2.1 和 3.0）。
更多详情请参考 <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md"><code>azureFile</code> 卷插件</a>。</p>
<!--
#### azureFile CSI migration
-->
<h4 id="azurefile-csi-migration">azureFile CSI 迁移 </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code>
</div>


<!--
The CSI Migration feature for azureFile, when enabled, redirects all plugin operations
from the existing in-tree plugin to the `file.csi.azure.com` Container
Storage Interface (CSI) Driver. In order to use this feature, the [Azure File CSI
Driver](https://github.com/kubernetes-sigs/azurefile-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationAzureFile`
[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) must be enabled.
-->
<p>启用 <code>azureFile</code> 的 <code>CSIMigration</code> 功能后，所有插件操作将从现有的树内插件重定向到
<code>file.csi.azure.com</code> 容器存储接口（CSI）驱动程序。要使用此功能，必须在集群中安装
<a href="https://github.com/kubernetes-sigs/azurefile-csi-driver">Azure 文件 CSI 驱动程序</a>，
并且 <code>CSIMigration</code> 和 <code>CSIMigrationAzureFile</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
必须被启用。</p>
<!--
Azure File CSI driver does not support using same volume with different fsgroups, if Azurefile CSI migration is enabled, using same volume with different fsgroups won't be supported at all.
-->
<p>Azure 文件 CSI 驱动尚不支持为同一卷设置不同的 fsgroup。
如果 AzureFile CSI 迁移被启用，用不同的 fsgroup 来使用同一卷也是不被支持的。</p>
<!--
#### azureDisk CSI migration complete






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>



To disable the `azureDisk` storage plugin from being loaded by the controller manager
and the kubelet, set the `InTreePluginAzureDiskUnregister` flag to `true`.
-->
<h4 id="azuredisk-csi-迁移完成-1">azureDisk CSI 迁移完成</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>


<p>要禁止控制器管理器和 kubelet 加载 <code>azureDisk</code> 存储插件，
请将 <code>InTreePluginAzureDiskUnregister</code> 标志设置为 <code>true</code>。</p>
<h3 id="cephfs">cephfs</h3>
<!--
A `cephfs` volume allows an existing CephFS volume to be
mounted into your Pod. Unlike `emptyDir`, which is erased when a Pod is
removed, the contents of a `cephfs` volume are preserved and the volume is merely
unmounted. This means that a `cephfs` volume can be pre-populated with data, and
that data can be shared between Pods.  The `cephfs` can be mounted by multiple
writers simultaneously.
-->
<p><code>cephfs</code> 卷允许你将现存的 CephFS 卷挂载到 Pod 中。
不像 <code>emptyDir</code> 那样会在 Pod 被删除的同时也会被删除，<code>cephfs</code>
卷的内容在 Pod 被删除时会被保留，只是卷被卸载了。
这意味着 <code>cephfs</code> 卷可以被预先填充数据，且这些数据可以在
Pod 之间共享。同一 <code>cephfs</code> 卷可同时被多个写者挂载。</p>
<!--
You must have your own Ceph server running with the share exported before you can use it.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在使用 Ceph 卷之前，你的 Ceph 服务器必须已经运行并将要使用的 share 导出（exported）。
</div>
<!--
See the [CephFS example](https://github.com/kubernetes/examples/tree/master/volumes/cephfs/) for more details.
-->
<p>更多信息请参考 <a href="https://github.com/kubernetes/examples/tree/master/volumes/cephfs/">CephFS 示例</a>。</p>
<h3 id="cinder">cinder</h3>
<!--
Kubernetes must be configured with the OpenStack cloud provider.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Kubernetes 必须配置了 OpenStack Cloud Provider。
</div>
<!--
The `cinder` volume type is used to mount the OpenStack Cinder volume into your pod.

#### Cinder Volume Example configuration
-->
<p><code>cinder</code> 卷类型用于将 OpenStack Cinder 卷挂载到 Pod 中。</p>
<h4 id="cinder-卷示例配置">Cinder 卷示例配置</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-cinder<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/test-webserver<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-cinder-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/test-cinder<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此 OpenStack 卷必须已经存在</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cinder</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeID</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&lt;volume-id&gt;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span></code></pre></div><!--
#### OpenStack CSI Migration
-->
<h4 id="openstack-csi-迁移">OpenStack CSI 迁移</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code>
</div>


<!--
The `CSIMigration` feature for Cinder is enabled by default in Kubernetes 1.21.
It redirects all plugin operations from the existing in-tree plugin to the
`cinder.csi.openstack.org` Container Storage Interface (CSI) Driver.
[OpenStack Cinder CSI Driver](https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md)
must be installed on the cluster.
You can disable Cinder CSI migration for your cluster by setting the `CSIMigrationOpenStack`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to `false`.
If you disable the `CSIMigrationOpenStack` feature, the in-tree Cinder volume plugin takes responsibility
for all aspects of Cinder volume storage management.
-->
<p>Cinder 的 <code>CSIMigration</code> 功能在 Kubernetes 1.21 版本中是默认被启用的。
此特性会将插件的所有操作从现有的树内插件重定向到
<code>cinder.csi.openstack.org</code> 容器存储接口（CSI）驱动程序。
为了使用此功能，必须在集群中安装
<a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md">OpenStack Cinder CSI 驱动程序</a>，
你可以通过设置 <code>CSIMigrationOpenStack</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
为 <code>false</code> 来禁止 Cinder CSI 迁移。
如果你禁用了 <code>CSIMigrationOpenStack</code> 功能特性，则树内的 Cinder 卷插件
会负责 Cinder 卷存储管理的方方面面。</p>
<h3 id="configmap">configMap</h3>
<!--
A [ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/)
provides a way to inject configuration data into Pods.
The data stored in a ConfigMap object can be referenced in a volume of type
`configMap` and then consumed by containerized applications running in a Pod.
-->
<p><a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/"><code>configMap</code></a>
卷提供了向 Pod 注入配置数据的方法。
ConfigMap 对象中存储的数据可以被 <code>configMap</code> 类型的卷引用，然后被 Pod 中运行的容器化应用使用。</p>
<!--
When referencing a ConfigMap, you provide the name of the ConfigMap in the
volume. You can customize the path to use for a specific
entry in the ConfigMap. The following configuration shows how to mount
the `log-config` ConfigMap onto a Pod called `configmap-pod`:
-->
<p>引用 configMap 对象时，你可以在 volume 中通过它的名称来引用。
你可以自定义 ConfigMap 中特定条目所要使用的路径。
下面的配置显示了如何将名为 <code>log-config</code> 的 ConfigMap 挂载到名为 <code>configmap-pod</code>
的 Pod 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>configmap-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-vol<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-vol<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log-config<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>log_level<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>log_level<span style="color:#bbb">
</span></code></pre></div><!--
The `log-config` ConfigMap is mounted as a volume, and all contents stored in
its `log_level` entry are mounted into the Pod at path "`/etc/config/log_level`".
Note that this path is derived from the volume's `mountPath` and the `path`
keyed with `log_level`.
-->
<p><code>log-config</code> ConfigMap 以卷的形式挂载，并且存储在 <code>log_level</code>
条目中的所有内容都被挂载到 Pod 的 <code>/etc/config/log_level</code> 路径下。
请注意，这个路径来源于卷的 <code>mountPath</code> 和 <code>log_level</code> 键对应的 <code>path</code>。</p>
<!--
* You must create a [ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/)
  before you can use it.

* A container using a ConfigMap as a [`subPath`](#using-subpath) volume mount will not
  receive ConfigMap updates.

* Text data is exposed as files using the UTF-8 character encoding. For other character encodings, use `binaryData`.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <ul>
<li>在使用 <a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a> 之前你首先要创建它。</li>
<li>容器以 <a href="#using-subpath">subPath</a> 卷挂载方式使用 ConfigMap 时，将无法接收 ConfigMap 的更新。</li>
<li>文本数据挂载成文件时采用 UTF-8 字符编码。如果使用其他字符编码形式，可使用
<code>binaryData</code> 字段。</li>
</ul>

</div>
<h3 id="downwardapi">downwardAPI</h3>
<!--
A `downwardAPI` volume is used to make downward API data available to applications.
It mounts a directory and writes the requested data in plain text files.
-->
<p><code>downwardAPI</code> 卷用于使 downward API 数据对应用程序可用。
这种卷类型挂载一个目录并在纯文本文件中写入所请求的数据。</p>
<!--
A Container using Downward API as a [subPath](#using-subpath) volume mount will not
receive Downward API updates.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 容器以 <a href="#using-subpath">subPath</a> 卷挂载方式使用 downwardAPI 时，将不能接收到它的更新。
</div>
<!--
See the [`downwardAPI` volume example](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)  for more details.
-->
<p>更多详细信息请参考 <a href="/zh/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/"><code>downwardAPI</code> 卷示例</a>。</p>
<h3 id="emptydir">emptyDir</h3>
<!--
An `emptyDir` volume is first created when a Pod is assigned to a Node, and
exists as long as that Pod is running on that node.  As the name says, it is
initially empty.  Containers in the Pod can all read and write the same
files in the `emptyDir` volume, though that volume can be mounted at the same
or different paths in each Container.  When a Pod is removed from a node for
any reason, the data in the `emptyDir` is deleted forever.
-->
<p>当 Pod 分派到某个 Node 上时，<code>emptyDir</code> 卷会被创建，并且在 Pod 在该节点上运行期间，卷一直存在。
就像其名称表示的那样，卷最初是空的。
尽管 Pod 中的容器挂载 <code>emptyDir</code> 卷的路径可能相同也可能不同，这些容器都可以读写
<code>emptyDir</code> 卷中相同的文件。
当 Pod 因为某些原因被从节点上删除时，<code>emptyDir</code> 卷中的数据也会被永久删除。</p>
<!--
A Container crashing does *NOT* remove a Pod from a node, so the data in an `emptyDir` volume is safe across Container crashes.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 容器崩溃并<strong>不</strong>会导致 Pod 被从节点上移除，因此容器崩溃期间 <code>emptyDir</code> 卷中的数据是安全的。
</div>
<!--
Some uses for an `emptyDir` are:

* scratch space, such as for a disk-based merge sort
* checkpointing a long computation for recovery from crashes
* holding files that a content-manager Container fetches while a webserver
  Container serves the data
-->
<p><code>emptyDir</code> 的一些用途：</p>
<ul>
<li>缓存空间，例如基于磁盘的归并排序。</li>
<li>为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。</li>
<li>在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。</li>
</ul>
<!--
Depending on your environment, `emptyDir` volumes are stored on whatever medium that backs the
node such as disk or SSD, or network storage. However, if you set the `emptyDir.medium` field
to `"Memory"`, Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead.
While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on
node reboot and any files you write will count against your Container's
memory limit.
-->
<p>取决于你的环境，<code>emptyDir</code> 卷存储在该节点所使用的介质上；这里的介质可以是磁盘或 SSD
或网络存储。但是，你可以将 <code>emptyDir.medium</code> 字段设置为 <code>&quot;Memory&quot;</code>，以告诉 Kubernetes
为你挂载 tmpfs（基于 RAM 的文件系统）。
虽然 tmpfs 速度非常快，但是要注意它与磁盘不同。
tmpfs 在节点重启时会被清除，并且你所写入的所有文件都会计入容器的内存消耗，受容器内存限制约束。</p>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> If the <code>SizeMemoryBackedVolumes</code> <a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> is enabled,
you can specify a size for memory backed volumes.  If no size is specified, memory
backed volumes are sized to 50% of the memory on a Linux host.
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 当启用 <code>SizeMemoryBackedVolumes</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
时，你可以为基于内存提供的卷指定大小。
如果未指定大小，则基于内存的卷的大小为 Linux 主机上内存的 50％。
</div>
<!--
#### emptyDir configuration example
-->
<h4 id="emptydir-配置示例">emptyDir 配置示例</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-pd<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/test-webserver<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/cache<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cache-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cache-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div><!--
### fc (fibre channel) {#fc}

An `fc` volume type allows an existing fibre channel block storage volume
to mount in a Pod. You can specify single or multiple target world wide names (WWNs)
using the parameter `targetWWNs` in your Volume configuration. If multiple WWNs are specified,
targetWWNs expect that those WWNs are from multi-path connections.
-->
<h3 id="fc">fc (光纤通道)</h3>
<p><code>fc</code> 卷类型允许将现有的光纤通道块存储卷挂载到 Pod 中。
可以使用卷配置中的参数 <code>targetWWNs</code> 来指定单个或多个目标 WWN（World Wide Names）。
如果指定了多个 WWN，targetWWNs 期望这些 WWN 来自多路径连接。</p>
<!--
You must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the target WWNs beforehand so that Kubernetes hosts can access them.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你必须配置 FC SAN Zoning，以便预先向目标 WWN 分配和屏蔽这些 LUN（卷），这样
Kubernetes 主机才可以访问它们。
</div>
<!--
See the [fibre channel example](https://github.com/kubernetes/examples/tree/master/staging/volumes/fibre_channel) for more details.
-->
<p>更多详情请参考 <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/fibre_channel">FC 示例</a>。</p>
<!--
### flocker (deprecated) {#flocker}

[Flocker](https://github.com/ClusterHQ/flocker) is an open-source, clustered
Container data volume manager. Flocker provides management
and orchestration of data volumes backed by a variety of storage backends.
-->
<h3 id="flocker">flocker （已弃用）</h3>
<p><a href="https://github.com/ClusterHQ/flocker">Flocker</a> 是一个开源的、集群化的容器数据卷管理器。
Flocker 提供了由各种存储后端所支持的数据卷的管理和编排。</p>
<!--
A `flocker` volume allows a Flocker dataset to be mounted into a Pod. If the
dataset does not already exist in Flocker, it needs to be first created with the Flocker
CLI or by using the Flocker API. If the dataset already exists it will be
reattached by Flocker to the node that the Pod is scheduled. This means data
can be shared between Pods as required.
-->
<p>使用 <code>flocker</code> 卷可以将一个 Flocker 数据集挂载到 Pod 中。
如果数据集在 Flocker 中不存在，则需要首先使用 Flocker CLI 或 Flocker API 创建数据集。
如果数据集已经存在，那么 Flocker 将把它重新附加到 Pod 被调度的节点。
这意味着数据可以根据需要在 Pod 之间共享。</p>
<!--
You must have your own Flocker installation running before you can use it.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在使用 Flocker 之前你必须先安装运行自己的 Flocker。
</div>
<!--
See the [Flocker example](https://github.com/kubernetes/examples/tree/master/staging/volumes/flocker) for more details.
-->
<p>更多详情请参考 <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/flocker">Flocker 示例</a>。</p>
<!--
### gcePersistentDisk

A `gcePersistentDisk` volume mounts a Google Compute Engine (GCE)
[Persistent Disk](http://cloud.google.com/compute/docs/disks) into your Pod.
Unlike `emptyDir`, which is erased when a Pod is removed, the contents of a PD are
preserved and the volume is merely unmounted.  This means that a PD can be
pre-populated with data, and that data can be shared between pods.
-->
<h3 id="gcepersistentdisk">gcePersistentDisk</h3>
<p><code>gcePersistentDisk</code> 卷能将谷歌计算引擎 (GCE) <a href="http://cloud.google.com/compute/docs/disks">持久盘（PD）</a>
挂载到你的 Pod 中。
不像 <code>emptyDir</code> 那样会在 Pod 被删除的同时也会被删除，持久盘卷的内容在删除 Pod
时会被保留，卷只是被卸载了。
这意味着持久盘卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。</p>
<!--
You must create a PD using `gcloud` or the GCE API or UI before you can use it.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 在使用 PD 前，你必须使用 <code>gcloud</code> 或者 GCE API 或 UI 创建它。
</div>

<!--
There are some restrictions when using a `gcePersistentDisk`:

* the nodes on which Pods are running must be GCE VMs
* those VMs need to be in the same GCE project and zone as the PD
-->
<p>使用 <code>gcePersistentDisk</code> 时有一些限制：</p>
<ul>
<li>运行 Pod 的节点必须是 GCE VM</li>
<li>这些 VM 必须和持久盘位于相同的 GCE 项目和区域（zone）</li>
</ul>
<!--
One feature of GCE persistent disk is concurrent read-only access to a persistent disk.
A `gcePersistentDisk` volume permits multiple consumers to simultaneously
mount a persistent disk as read-only. This means that you can pre-populate a PD with your dataset
and then serve it in parallel from as many Pods as you need. Unfortunately,
PDs can only be mounted by a single consumer in read-write mode. Simultaneous
writers are not allowed.
-->
<p>GCE PD 的一个特点是它们可以同时被多个消费者以只读方式挂载。
这意味着你可以用数据集预先填充 PD，然后根据需要并行地在尽可能多的 Pod 中提供该数据集。
不幸的是，PD 只能由单个使用者以读写模式挂载 —— 即不允许同时写入。</p>
<!--
Using a GCE persistent disk with a Pod controlled by a ReplicaSet will fail unless
the PD is read-only or the replica count is 0 or 1.
-->
<p>在由 ReplicationController 所管理的 Pod 上使用 GCE PD 将会失败，除非 PD
是只读模式或者副本的数量是 0 或 1。</p>
<!--
#### Creating a GCE persistent disk {#gce-create-persistent-disk}

Before you can use a GCE PD with a Pod, you need to create it.
-->
<h4 id="gce-create-persistent-disk">创建 GCE 持久盘（PD）  </h4>
<p>在 Pod 中使用 GCE 持久盘之前，你首先要创建它。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">gcloud compute disks create --size<span style="color:#666">=</span>500GB --zone<span style="color:#666">=</span>us-central1-a my-data-disk
</code></pre></div><!--
#### Example Pod
-->
<h4 id="gce-pd-configuration-example">GCE 持久盘配置示例</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-pd<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/test-webserver<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/test-pd<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此 GCE PD 必须已经存在</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">gcePersistentDisk</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">pdName</span>:<span style="color:#bbb"> </span>my-data-disk<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span></code></pre></div><!--
#### Regional Persistent Disks
-->
<h4 id="regional-persistent-disks">区域持久盘  </h4>
<!--
The [Regional Persistent Disks](https://cloud.google.com/compute/docs/disks/#repds)
feature allows the creation of Persistent Disks that are available in two zones
within the same region. In order to use this feature, the volume must be provisioned
as a PersistentVolume; referencing the volume directly from a Pod is not supported.
-->
<p><a href="https://cloud.google.com/compute/docs/disks/#repds">区域持久盘</a>
功能允许你创建能在同一区域的两个可用区中使用的持久盘。
要使用这个功能，必须以持久卷（PersistentVolume）的方式提供卷；直接从
Pod 引用这种卷是不可以的。</p>
<!--
#### Manually provisioning a Regional PD PersistentVolume

Dynamic provisioning is possible using a [StorageClass for GCE PD](/docs/concepts/storage/storage-classes/#gce).
Before creating a PersistentVolume, you must create the PD:
-->
<h4 id="manually-provisioning-regional-pd-pv">手动供应基于区域 PD 的 PersistentVolume</h4>
<p>使用<a href="/zh/docs/concepts/storage/storage-classes/#gce">为 GCE PD 定义的存储类</a>
可以实现动态供应。在创建 PersistentVolume 之前，你首先要创建 PD。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">gcloud beta compute disks create --size<span style="color:#666">=</span>500GB my-data-disk
    --region us-central1
    --replica-zones us-central1-a,us-central1-b
</code></pre></div><!--
Example PersistentVolume spec:
-->
<p>PersistentVolume 示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">failure-domain.beta.kubernetes.io/zone</span>:<span style="color:#bbb"> </span>us-central1-a__us-central1-b<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>400Gi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">gcePersistentDisk</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pdName</span>:<span style="color:#bbb"> </span>my-data-disk<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">required</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># failure-domain.beta.kubernetes.io/zone 应在 1.21 之前使用</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- us-central1-a<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- us-central1-b<span style="color:#bbb">
</span></code></pre></div><!--
#### GCE CSI Migration
-->
<h4 id="gce-csi-migration">GCE CSI 迁移 </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [beta]</code>
</div>


<!--
The CSI Migration feature for GCE PD, when enabled, shims all plugin operations
from the existing in-tree plugin to the `pd.csi.storage.gke.io` Container
Storage Interface (CSI) Driver. In order to use this feature, the [GCE PD CSI
Driver](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationGCE`
beta features must be enabled.
-->
<p>启用 GCE PD 的 <code>CSIMigration</code> 功能后，所有插件操作将从现有的树内插件重定向到
<code>pd.csi.storage.gke.io</code> 容器存储接口（ CSI ）驱动程序。
为了使用此功能，必须在集群中上安装
<a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE PD CSI驱动程序</a>，
并且 <code>CSIMigration</code> 和 <code>CSIMigrationGCE</code> Beta 功能必须被启用。</p>
<!-- 
#### GCE CSI migration complete






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>



To disable the `gcePersistentDisk` storage plugin from being loaded by the controller manager
and the kubelet, set the `InTreePluginGCEUnregister` flag to `true`.
-->
<h4 id="gce-csi-迁移完成">GCE CSI 迁移完成</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>


<p>要禁止控制器管理器和 kubelet 加载 <code>gcePersistentDisk</code> 存储插件，请将
<code>InTreePluginGCEUnregister</code> 标志设置为 <code>true</code>。</p>
<!--
### gitRepo (deprecated) {#gitrepo}
-->
<h3 id="gitrepo">gitRepo (已弃用)   </h3>
<!--
The gitRepo volume type is deprecated. To provision a container with a git repo, mount an [EmptyDir](#emptydir) into an InitContainer that clones the repo using git, then mount the [EmptyDir](#emptydir) into the Pod's container.
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <code>gitRepo</code> 卷类型已经被废弃。如果需要在容器中提供 git 仓库，请将一个
<a href="#emptydir">EmptyDir</a> 卷挂载到 InitContainer 中，使用 git
命令完成仓库的克隆操作，然后将 <a href="#emptydir">EmptyDir</a> 卷挂载到 Pod 的容器中。
</div>


<!--
A `gitRepo` volume is an example of a volume plugin. This plugin
mounts an empty directory and clones a git repository into this directory
for your Pod to use.

Here is an example of a `gitRepo` volume:
-->
<p><code>gitRepo</code> 卷是一个卷插件的例子。
该查卷挂载一个空目录，并将一个 Git 代码仓库克隆到这个目录中供 Pod 使用。</p>
<p>下面给出一个 <code>gitRepo</code> 卷的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>server<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/mypath<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>git-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>git-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">gitRepo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">repository</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;git@somewhere:me/my-git-repository.git&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">revision</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;22f1d8406d464b0c0874075539c1f2e96c253775&#34;</span><span style="color:#bbb">
</span></code></pre></div><h3 id="glusterfs">glusterfs</h3>
<!--
A `glusterfs` volume allows a [Glusterfs](http://www.gluster.org) (an open
source networked filesystem) volume to be mounted into your Pod.  Unlike
`emptyDir`, which is erased when a Pod is removed, the contents of a
`glusterfs` volume are preserved and the volume is merely unmounted.  This
means that a glusterfs volume can be pre-populated with data, and that data can
be shared between pods. GlusterFS can be mounted by multiple writers
simultaneously.
-->
<p><code>glusterfs</code> 卷能将 <a href="https://www.gluster.org">Glusterfs</a> (一个开源的网络文件系统)
挂载到你的 Pod 中。不像 <code>emptyDir</code> 那样会在删除 Pod 的同时也会被删除，<code>glusterfs</code>
卷的内容在删除 Pod 时会被保存，卷只是被卸载。
这意味着 <code>glusterfs</code> 卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。
GlusterFS 可以被多个写者同时挂载。</p>
<!--
You must have your own GlusterFS installation running before you can use it.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在使用前你必须先安装运行自己的 GlusterFS。
</div>
<!--
See the [GlusterFS example](https://github.com/kubernetes/examples/tree/master/volumes/glusterfs) for more details.
-->
<p>更多详情请参考 <a href="https://github.com/kubernetes/examples/tree/master/volumes/glusterfs">GlusterFS 示例</a>。</p>
<h3 id="hostpath">hostPath</h3>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!-- 
HostPath volumes present many security risks, and it is a best practice to avoid the use of
HostPaths when possible. When a HostPath volume must be used, it should be scoped to only the
required file or directory, and mounted as ReadOnly.

If restricting HostPath access to specific directories through AdmissionPolicy, `volumeMounts` MUST
be required to use `readOnly` mounts for the policy to be effective.
-->
<p>HostPath 卷存在许多安全风险，最佳做法是尽可能避免使用 HostPath。
当必须使用 HostPath 卷时，它的范围应仅限于所需的文件或目录，并以只读方式挂载。</p>
<p>如果通过 AdmissionPolicy 限制 HostPath 对特定目录的访问，则必须要求
<code>volumeMounts</code> 使用 <code>readOnly</code> 挂载以使策略生效。</p>

</div>


<!--
A `hostPath` volume mounts a file or directory from the host node's filesystem
into your Pod. This is not something that most Pods will need, but it offers a
powerful escape hatch for some applications.
-->
<p><code>hostPath</code> 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中。
虽然这不是大多数 Pod 需要的，但是它为一些应用程序提供了强大的逃生舱。</p>
<!--
For example, some uses for a `hostPath` are:

* running a Container that needs access to Docker internals; use a `hostPath`
  of `/var/lib/docker`
* running cAdvisor in a Container; use a `hostPath` of `/sys`
* allowing a Pod to specify whether a given `hostPath` should exist prior to the
  Pod running, whether it should be created, and what it should exist as
-->
<p>例如，<code>hostPath</code> 的一些用法有：</p>
<ul>
<li>运行一个需要访问 Docker 内部机制的容器；可使用 <code>hostPath</code> 挂载 <code>/var/lib/docker</code> 路径。</li>
<li>在容器中运行 cAdvisor 时，以 <code>hostPath</code> 方式挂载 <code>/sys</code>。</li>
<li>允许 Pod 指定给定的 <code>hostPath</code> 在运行 Pod 之前是否应该存在，是否应该创建以及应该以什么方式存在。</li>
</ul>
<!--
In addition to the required `path` property, user can optionally specify a `type` for a `hostPath` volume.

The supported values for field `type` are:
-->
<p>除了必需的 <code>path</code> 属性之外，用户可以选择性地为 <code>hostPath</code> 卷指定 <code>type</code>。</p>
<p>支持的 <code>type</code> 值如下：</p>
<!--
| Value   | Behavior |
|:--------|:---------|
| | Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the hostPath volume. |
| `DirectoryOrCreate` | If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet. |
| `Directory` | A directory must exist at the given path |
| `FileOrCreate` | If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet. |
| `File` | A file must exist at the given path |
| `Socket` | A UNIX socket must exist at the given path |
| `CharDevice` | A character device must exist at the given path |
| `BlockDevice` | A block device must exist at the given path |
-->
<table>
<thead>
<tr>
<th style="text-align:left">取值</th>
<th style="text-align:left">行为</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。</td>
</tr>
<tr>
<td style="text-align:left"><code>DirectoryOrCreate</code></td>
<td style="text-align:left">如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。</td>
</tr>
<tr>
<td style="text-align:left"><code>Directory</code></td>
<td style="text-align:left">在给定路径上必须存在的目录。</td>
</tr>
<tr>
<td style="text-align:left"><code>FileOrCreate</code></td>
<td style="text-align:left">如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。</td>
</tr>
<tr>
<td style="text-align:left"><code>File</code></td>
<td style="text-align:left">在给定路径上必须存在的文件。</td>
</tr>
<tr>
<td style="text-align:left"><code>Socket</code></td>
<td style="text-align:left">在给定路径上必须存在的 UNIX 套接字。</td>
</tr>
<tr>
<td style="text-align:left"><code>CharDevice</code></td>
<td style="text-align:left">在给定路径上必须存在的字符设备。</td>
</tr>
<tr>
<td style="text-align:left"><code>BlockDevice</code></td>
<td style="text-align:left">在给定路径上必须存在的块设备。</td>
</tr>
</tbody>
</table>
<!--
Watch out when using this type of volume, because:

* HostPaths can expose privileged system credentials (such as for the Kubelet) or privileged APIs
  (such as container runtime socket), which can be used for container escape or to attack other
  parts of the cluster.
* Pods with identical configuration (such as created from a PodTemplate) may
  behave differently on different nodes due to different files on the nodes
* The files or directories created on the underlying hosts are only writable by root. You
  either need to run your process as root in a
  [privileged Container](/docs/tasks/configure-pod-container/security-context/) or modify the file
  permissions on the host to be able to write to a `hostPath` volume
-->
<p>当使用这种类型的卷时要小心，因为：</p>
<ul>
<li>HostPath 卷可能会暴露特权系统凭据（例如 Kubelet）或特权
API（例如容器运行时套接字），可用于容器逃逸或攻击集群的其他部分。</li>
<li>具有相同配置（例如基于同一 PodTemplate 创建）的多个 Pod
会由于节点上文件的不同而在不同节点上有不同的行为。</li>
<li>下层主机上创建的文件或目录只能由 root 用户写入。你需要在
<a href="/zh/docs/tasks/configure-pod-container/security-context/">特权容器</a>
中以 root 身份运行进程，或者修改主机上的文件权限以便容器能够写入 <code>hostPath</code> 卷。</li>
</ul>
<!--
#### hostPath configuration example
-->
<h4 id="hostpath-配置示例">hostPath 配置示例：</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-pd<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/test-webserver<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/test-pd<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># 宿主上目录位置</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/data<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># 此字段为可选</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Directory<span style="color:#bbb">
</span></code></pre></div><!--
The `FileOrCreate` mode does not create the parent directory of the file. If the parent directory
of the mounted file does not exist, the pod fails to start. To ensure that this mode works,
you can try to mount directories and files separately, as shown in the
[`FileOrCreate`configuration](#hostpath-fileorcreate-example).
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <code>FileOrCreate</code> 模式不会负责创建文件的父目录。
如果欲挂载的文件的父目录不存在，Pod 启动会失败。
为了确保这种模式能够工作，可以尝试把文件和它对应的目录分开挂载，如
<a href="#hostpath-fileorcreate-example"><code>FileOrCreate</code> 配置</a> 所示。
</div>

<!--
#### hostPath FileOrCreate configuration example {#hostpath-fileorcreate-example}
-->
<h4 id="hostpath-fileorcreate-example">hostPath FileOrCreate 配置示例 </h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-webserver<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-webserver<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/test-webserver:latest<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/local/aaa<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mydir<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/local/aaa/1.txt<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myfile<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mydir<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># 确保文件所在目录成功创建。</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/local/aaa<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>DirectoryOrCreate<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myfile<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/local/aaa/1.txt<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>FileOrCreate<span style="color:#bbb">
</span></code></pre></div><h3 id="iscsi">iscsi</h3>
<!--
An `iscsi` volume allows an existing iSCSI (SCSI over IP) volume to be mounted
into your Pod.  Unlike `emptyDir`, which is erased when a Pod is removed, the
contents of an `iscsi` volume are preserved and the volume is merely
unmounted.  This means that an iscsi volume can be pre-populated with data, and
that data can be shared between pods.
-->
<p><code>iscsi</code> 卷能将 iSCSI (基于 IP 的 SCSI) 卷挂载到你的 Pod 中。
不像 <code>emptyDir</code> 那样会在删除 Pod 的同时也会被删除，<code>iscsi</code>
卷的内容在删除 Pod 时会被保留，卷只是被卸载。
这意味着 <code>iscsi</code> 卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。</p>
<!--
You must have your own iSCSI server running with the volume created before you can use it.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 在使用 iSCSI 卷之前，你必须拥有自己的 iSCSI 服务器，并在上面创建卷。
</div>

<!--
A feature of iSCSI is that it can be mounted as read-only by multiple consumers
simultaneously.  This means that you can pre-populate a volume with your dataset
and then serve it in parallel from as many Pods as you need.  Unfortunately,
iSCSI volumes can only be mounted by a single consumer in read-write mode.
Simultaneous writers are not allowed.
-->
<p>iSCSI 的一个特点是它可以同时被多个用户以只读方式挂载。
这意味着你可以用数据集预先填充卷，然后根据需要在尽可能多的 Pod 上使用它。
不幸的是，iSCSI 卷只能由单个使用者以读写模式挂载。不允许同时写入。</p>
<!--
See the [iSCSI example](https://github.com/kubernetes/examples/tree/master/volumes/iscsi) for more details.
-->
<p>更多详情请参考 <a href="https://github.com/kubernetes/examples/tree/master/volumes/iscsi">iSCSI 示例</a>。</p>
<!--
### local

A `local` volume represents a mounted local storage device such as a disk,
partition or directory.

Local volumes can only be used as a statically created PersistentVolume. Dynamic
provisioning is not supported yet.
-->
<h3 id="local">local</h3>
<p><code>local</code> 卷所代表的是某个被挂载的本地存储设备，例如磁盘、分区或者目录。</p>
<p><code>local</code> 卷只能用作静态创建的持久卷。尚不支持动态配置。</p>
<!--
Compared to `hostPath` volumes, `local` volumes are used in a durable and
portable manner without manually scheduling Pods to nodes. The system is aware
of the volume's node constraints by looking at the node affinity on the PersistentVolume.
-->
<p>与 <code>hostPath</code> 卷相比，<code>local</code> 卷能够以持久和可移植的方式使用，而无需手动将 Pod
调度到节点。系统通过查看 PersistentVolume 的节点亲和性配置，就能了解卷的节点约束。</p>
<!--
However, local volumes are still subject to the availability of the underlying
node and are not suitable for all applications. If a node becomes unhealthy,
then the `local` volume becomes inaccessible by the pod. The Pod using this volume
is unable to run. Applications using local volumes must be able to tolerate this
reduced availability, as well as potential data loss, depending on the
durability characteristics of the underlying disk.

The following is an example of PersistentVolume spec using a `local` volume and
`nodeAffinity`:
-->
<p>然而，<code>local</code> 卷仍然取决于底层节点的可用性，并不适合所有应用程序。
如果节点变得不健康，那么 <code>local</code> 卷也将变得不可被 Pod 访问。使用它的 Pod 将不能运行。
使用 <code>local</code> 卷的应用程序必须能够容忍这种可用性的降低，以及因底层磁盘的耐用性特征而带来的潜在的数据丢失风险。</p>
<p>下面是一个使用 <code>local</code> 卷和 <code>nodeAffinity</code> 的持久卷示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-pv<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>100Gi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">persistentVolumeReclaimPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>local-storage<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">local</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/mnt/disks/ssd1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">required</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>kubernetes.io/hostname<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- example-node<span style="color:#bbb">
</span></code></pre></div><!--
You must set a PersistentVolume `nodeAffinity` when using `local` volumes.
The Kubernetes scheduler uses the PersistentVolume `nodeAffinity` to schedule
these Pods to the correct node.
-->
<p>使用 <code>local</code> 卷时，你需要设置 PersistentVolume 对象的 <code>nodeAffinity</code> 字段。
Kubernetes 调度器使用 PersistentVolume 的 <code>nodeAffinity</code> 信息来将使用 <code>local</code>
卷的 Pod 调度到正确的节点。</p>
<!--
PersistentVolume `volumeMode` can be set to "Block" (instead of the default
value "Filesystem") to expose the local volume as a raw block device.
-->
<p>PersistentVolume 对象的 <code>volumeMode</code> 字段可被设置为 &quot;Block&quot;
（而不是默认值 &quot;Filesystem&quot;），以将 <code>local</code> 卷作为原始块设备暴露出来。</p>
<!--
When using local volumes, it is recommended to create a StorageClass with
`volumeBindingMode` set to `WaitForFirstConsumer`. For more details, see the
local [StorageClass](/docs/concepts/storage/storage-classes/#local) example.
Delaying volume binding ensures that the PersistentVolumeClaim binding decision
will also be evaluated with any other node constraints the Pod may have,
such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.
-->
<p>使用 <code>local</code> 卷时，建议创建一个 StorageClass 并将其 <code>volumeBindingMode</code> 设置为
<code>WaitForFirstConsumer</code>。要了解更多详细信息，请参考
<a href="/zh/docs/concepts/storage/storage-classes/#local">local StorageClass 示例</a>。
延迟卷绑定的操作可以确保 Kubernetes 在为 PersistentVolumeClaim 作出绑定决策时，会评估
Pod 可能具有的其他节点约束，例如：如节点资源需求、节点选择器、Pod亲和性和 Pod 反亲和性。</p>
<!--
An external static provisioner can be run separately for improved management of
the local volume lifecycle. Note that this provisioner does not support dynamic
provisioning yet. For an example on how to run an external local provisioner,
see the [local volume provisioner user
guide](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner).
-->
<p>你可以在 Kubernetes 之外单独运行静态驱动以改进对 local 卷的生命周期管理。
请注意，此驱动尚不支持动态配置。
有关如何运行外部 <code>local</code> 卷驱动，请参考
<a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">local 卷驱动用户指南</a>。</p>
<!--
The local PersistentVolume requires manual cleanup and deletion by the
user if the external static provisioner is not used to manage the volume
lifecycle.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果不使用外部静态驱动来管理卷的生命周期，用户需要手动清理和删除 local 类型的持久卷。
</div>
<h3 id="nfs">nfs</h3>
<!--
An `nfs` volume allows an existing NFS (Network File System) share to be
mounted into your Pod. Unlike `emptyDir`, which is erased when a Pod is
removed, the contents of an `nfs` volume are preserved and the volume is merely
unmounted.  This means that an NFS volume can be pre-populated with data, and
that data can be shared between pods. NFS can be mounted by multiple
writers simultaneously.
-->
<p><code>nfs</code> 卷能将 NFS (网络文件系统) 挂载到你的 Pod 中。
不像 <code>emptyDir</code> 那样会在删除 Pod 的同时也会被删除，<code>nfs</code> 卷的内容在删除 Pod
时会被保存，卷只是被卸载。
这意味着 <code>nfs</code> 卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。</p>
<!--
You must have your own NFS server running with the share exported before you can use it.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 在使用 NFS 卷之前，你必须运行自己的 NFS 服务器并将目标 share 导出备用。
</div>

<!--
See the [NFS example](https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs) for more details.
-->
<p>要了解更多详情请参考 <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs">NFS 示例</a>。</p>
<h3 id="persistentvolumeclaim">persistentVolumeClaim</h3>
<!--
A `persistentVolumeClaim` volume is used to mount a
[PersistentVolume](/docs/concepts/storage/persistent-volumes/) into a Pod.  PersistentVolumeClaims
are a way for users to "claim" durable storage (such as a GCE PersistentDisk or an
iSCSI volume) without knowing the details of the particular cloud environment.
-->
<p><code>persistentVolumeClaim</code> 卷用来将<a href="/zh/docs/concepts/storage/persistent-volumes/">持久卷</a>（PersistentVolume）挂载到 Pod 中。
持久卷申领（PersistentVolumeClaim）是用户在不知道特定云环境细节的情况下“申领”持久存储（例如
GCE PersistentDisk 或者 iSCSI 卷）的一种方法。</p>
<!--
See the [PersistentVolumes example](/docs/concepts/storage/persistent-volumes/) for more
details.
-->
<p>更多详情请参考<a href="/zh/docs/concepts/storage/persistent-volumes/">持久卷示例</a>。</p>
<h3 id="portworxvolume">portworxVolume</h3>
<!--
A `portworxVolume` is an elastic block storage layer that runs hyperconverged with
Kubernetes. [Portworx](https://portworx.com/use-case/kubernetes-storage/) fingerprints storage in a server, tiers based on capabilities,
and aggregates capacity across multiple servers. Portworx runs in-guest in virtual machines or on bare metal Linux nodes.
-->
<p><code>portworxVolume</code> 是一个可伸缩的块存储层，能够以超融合（hyperconverged）的方式与 Kubernetes 一起运行。
<a href="https://portworx.com/use-case/kubernetes-storage/">Portworx</a>
支持对服务器上存储的指纹处理、基于存储能力进行分层以及跨多个服务器整合存储容量。
Portworx 可以以 in-guest 方式在虚拟机中运行，也可以在裸金属 Linux 节点上运行。</p>
<!--
A `portworxVolume` can be dynamically created through Kubernetes or it can also
be pre-provisioned and referenced inside a Kubernetes Pod.
Here is an example Pod referencing a pre-provisioned PortworxVolume:
-->
<p><code>portworxVolume</code> 类型的卷可以通过 Kubernetes 动态创建，也可以预先配备并在
Kubernetes Pod 内引用。
下面是一个引用预先配备的 PortworxVolume 的示例 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-portworx-volume-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/test-webserver<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/mnt<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pxvol<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pxvol<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此 Portworx 卷必须已经存在</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">portworxVolume</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeID</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;pxvol&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&lt;fs-type&gt;&#34;</span><span style="color:#bbb">
</span></code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Make sure you have an existing PortworxVolume with name `pxvol`
before using it in the Pod.
-->
<p>在 Pod 中使用 portworxVolume 之前，你要确保有一个名为 <code>pxvol</code> 的 PortworxVolume 存在。
</div>
<!--
For more details, see the [Portworx volume](https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md) examples.
-->
<p>更多详情可以参考 <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md">Portworx 卷</a>。</p>
<h3 id="projected-投射">projected （投射）</h3>
<!--
A projected volume maps several existing volume sources into the same
directory. For more details, see [projected volumes](/docs/concepts/storage/projected-volumes/).
-->
<p>投射卷能将若干现有的卷来源映射到同一目录上。更多详情请参考<a href="/zh/docs/concepts/storage/projected-volumes/">投射卷</a>。</p>
<h3 id="quobyte">quobyte (已弃用)</h3>
<!--
A `quobyte` volume allows an existing [Quobyte](http://www.quobyte.com) volume to
be mounted into your Pod.
-->
<p><code>quobyte</code> 卷允许将现有的 <a href="https://www.quobyte.com">Quobyte</a> 卷挂载到你的 Pod 中。</p>
<!--
You must have your own Quobyte setup running with the volumes
created before you can use it.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在使用 Quobyte 卷之前，你首先要进行安装 Quobyte 并创建好卷。
</div>
<!--
Quobyte supports the <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='Container Storage Interface'>Container Storage Interface</a>.
CSI is the recommended plugin to use Quobyte volumes inside Kubernetes. Quobyte's
GitHub project has [instructions](https://github.com/quobyte/quobyte-csi#quobyte-csi) for deploying Quobyte using CSI, along with examples.
-->
<p>Quobyte 支持<a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='容器存储接口（CSI）'>容器存储接口（CSI）</a>。
推荐使用 CSI 插件以在 Kubernetes 中使用 Quobyte 卷。
Quobyte 的 GitHub 项目包含以 CSI 形式部署 Quobyte 的
<a href="https://github.com/quobyte/quobyte-csi#quobyte-csi">说明</a>
及使用示例。</p>
<h3 id="rbd">rbd</h3>
<!--
An `rbd` volume allows a
[Rados Block Device](https://docs.ceph.com/en/latest/rbd/) (RBD) volume to mount
into your Pod. Unlike `emptyDir`, which is erased when a pod is removed, the
contents of an `rbd` volume are preserved and the volume is unmounted. This
means that a RBD volume can be pre-populated with data, and that data can be
shared between pods.
-->
<p><code>rbd</code> 卷允许将 <a href="https://docs.ceph.com/en/latest/rbd/">Rados 块设备</a>卷挂载到你的 Pod 中。
不像 <code>emptyDir</code> 那样会在删除 Pod 的同时也会被删除，<code>rbd</code> 卷的内容在删除 Pod 时会被保存，卷只是被卸载。
这意味着 <code>rbd</code> 卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。</p>
<!--
You must have a Ceph installation running before you can use RBD.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在使用 RBD 之前，你必须安装运行 Ceph。
</div>
<!--
A feature of RBD is that it can be mounted as read-only by multiple consumers
simultaneously. This means that you can pre-populate a volume with your dataset
and then serve it in parallel from as many pods as you need. Unfortunately,
RBD volumes can only be mounted by a single consumer in read-write mode.
Simultaneous writers are not allowed.

See the [RBD example](https://github.com/kubernetes/examples/tree/master/volumes/rbd)
for more details.
-->
<p>RBD 的一个特性是它可以同时被多个用户以只读方式挂载。
这意味着你可以用数据集预先填充卷，然后根据需要在尽可能多的 Pod 中并行地使用卷。
不幸的是，RBD 卷只能由单个使用者以读写模式安装。不允许同时写入。</p>
<p>更多详情请参考
<a href="https://github.com/kubernetes/examples/tree/master/volumes/rbd">RBD 示例</a>。</p>
<!--
#### RBD CSI migration
-->
<h4 id="rbd-csi-migration">RBD CSI 迁移</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code>
</div>


<!--
The `CSIMigration` feature for `RBD`, when enabled, redirects all plugin
operations from the existing in-tree plugin to the `rbd.csi.ceph.com` <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> driver. In order to use this
feature, the
[Ceph CSI driver](https://github.com/ceph/ceph-csi)
must be installed on the cluster and the `CSIMigration` and `csiMigrationRBD`
[feature gates](/docs/reference/command-line-tools-reference/feature-gates/)
must be enabled.
-->
<p>启用 RBD 的 <code>CSIMigration</code> 功能后，所有插件操作从现有的树内插件重定向到
<code>rbd.csi.ceph.com</code> <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> 驱动程序。
要使用该功能，必须在集群内安装
<a href="https://github.com/ceph/ceph-csi">Ceph CSI 驱动</a>，并启用 <code>CSIMigration</code> 和 <code>csiMigrationRBD</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。</p>
<!--
As a Kubernetes cluster operator that administers storage, here are the
prerequisites that you must complete before you attempt migration to the
RBD CSI driver:

* You must install the Ceph CSI driver (`rbd.csi.ceph.com`), v3.5.0 or above,
  into your Kubernetes cluster.
* considering the `clusterID` field is a required parameter for CSI driver for
  its operations, but in-tree StorageClass has `monitors` field as a required
  parameter, a Kubernetes storage admin has to create a clusterID based on the
  monitors hash ( ex:`#echo -n
  '<monitors_string>' | md5sum`) in the CSI config map and keep the monitors
  under this clusterID configuration.
* Also, if the value of `adminId` in the in-tree Storageclass is different from
 `admin`, the `adminSecretName` mentioned in the in-tree Storageclass has to be
  patched with the base64 value of the `adminId` parameter value, otherwise this
  step can be skipped.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>作为一位管理存储的 Kubernetes 集群操作者，在尝试迁移到 RBD CSI 驱动前，你必须完成下列先决事项：</p>
<ul>
<li>你必须在集群中安装 v3.5.0 或更高版本的 Ceph CSI 驱动（<code>rbd.csi.ceph.com</code>）。</li>
<li>因为 <code>clusterID</code> 是 CSI 驱动程序必需的参数，而树内存储类又将 <code>monitors</code>
作为一个必需的参数，所以 Kubernetes 存储管理者需要根据 <code>monitors</code>
的哈希值（例：<code>#echo -n '&lt;monitors_string&gt;' | md5sum</code>）来创建
<code>clusterID</code>，并保持该 <code>monitors</code> 存在于该 <code>clusterID</code> 的配置中。</li>
<li>同时，如果树内存储类的 <code>adminId</code> 的值不是 <code>admin</code>，那么其 <code>adminSecretName</code>
就需要被修改成 <code>adminId</code> 参数的 base64 编码值。</li>
</ul>

</div>
<h3 id="secret">secret</h3>
<!--
A `secret` volume is used to pass sensitive information, such as passwords, to
Pods.  You can store secrets in the Kubernetes API and mount them as files for
use by Pods without coupling to Kubernetes directly.  `secret` volumes are
backed by tmpfs (a RAM-backed filesystem) so they are never written to
non-volatile storage.
-->
<p><code>secret</code> 卷用来给 Pod 传递敏感信息，例如密码。你可以将 Secret 存储在 Kubernetes
API 服务器上，然后以文件的形式挂在到 Pod 中，无需直接与 Kubernetes 耦合。
<code>secret</code> 卷由 tmpfs（基于 RAM 的文件系统）提供存储，因此它们永远不会被写入非易失性（持久化的）存储器。</p>
<!--
You must create a secret in the Kubernetes API before you can use it.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 使用前你必须在 Kubernetes API 中创建 secret。
</div>
<!--
A Container using a Secret as a [subPath](#using-subpath) volume mount will not
receive Secret updates.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 容器以 <a href="#using-subpath">subPath</a> 卷挂载方式挂载 Secret 时，将感知不到 Secret 的更新。
</div>
<!--
For more details, see [Configuring Secrets](/docs/concepts/configuration/secret/).
-->
<p>更多详情请参考<a href="/zh/docs/concepts/configuration/secret/">配置 Secrets</a>。</p>
<h3 id="storageos">storageOS (已弃用)</h3>
<!--
A `storageos` volume allows an existing [StorageOS](https://www.storageos.com)
volume to be mounted into your Pod.
-->
<p><code>storageos</code> 卷允许将现有的 <a href="https://www.storageos.com">StorageOS</a> 卷挂载到你的 Pod 中。</p>
<!--
StorageOS runs as a Container within your Kubernetes environment, making local
or attached storage accessible from any node within the Kubernetes cluster.
Data can be replicated to protect against node failure. Thin provisioning and
compression can improve utilization and reduce cost.
-->
<p>StorageOS 在 Kubernetes 环境中以容器的形式运行，这使得应用能够从 Kubernetes
集群中的任何节点访问本地的或挂接的存储。为应对节点失效状况，可以复制数据。
若需提高利用率和降低成本，可以考虑瘦配置（Thin Provisioning）和数据压缩。</p>
<!--
At its core, StorageOS provides block storage to Containers, accessible via a file system.

The StorageOS Container requires 64-bit Linux and has no additional dependencies.
A free developer license is available.
-->
<p>作为其核心能力之一，StorageOS 为容器提供了可以通过文件系统访问的块存储。</p>
<p>StorageOS 容器需要 64 位的 Linux，并且没有其他的依赖关系。
StorageOS 提供免费的开发者授权许可。</p>
<!--
You must run the StorageOS Container on each node that wants to
access StorageOS volumes or that will contribute storage capacity to the pool.
For installation instructions, consult the
[StorageOS documentation](https://docs.storageos.com).
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 你必须在每个希望访问 StorageOS 卷的或者将向存储资源池贡献存储容量的节点上运行
StorageOS 容器。有关安装说明，请参阅 <a href="https://docs.storageos.com">StorageOS 文档</a>。
</div>

<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>master<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-storageos-redis<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>master<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>kubernetes/redis:v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MASTER<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">6379</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/redis-master-data<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis-data<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis-data<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storageos</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># `redis-vol01` 卷必须在 StorageOS 中存在，并位于 `default` 名字空间内</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeName</span>:<span style="color:#bbb"> </span>redis-vol01<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span></code></pre></div><!--
For more information about StorageOS, dynamic provisioning, and PersistentVolumeClaims, see the
[StorageOS examples](https://github.com/kubernetes/examples/blob/master/volumes/storageos).
-->
<p>关于 StorageOS 的进一步信息、动态供应和持久卷申领等等，请参考
<a href="https://github.com/kubernetes/examples/blob/master/volumes/storageos">StorageOS 示例</a>。</p>
<h3 id="vspherevolume">vsphereVolume</h3>
<!--
You must configure the Kubernetes vSphere Cloud Provider. For cloudprovider
configuration, refer to the [vSphere Getting Started guide](https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/).
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你必须配置 Kubernetes 的 vSphere 云驱动。云驱动的配置方法请参考
<a href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/">vSphere 使用指南</a>。
</div>
<!--
A `vsphereVolume` is used to mount a vSphere VMDK Volume into your Pod.  The contents
of a volume are preserved when it is unmounted. It supports both VMFS and VSAN datastore.
-->
<p><code>vsphereVolume</code> 用来将 vSphere VMDK 卷挂载到你的 Pod 中。
在卸载卷时，卷的内容会被保留。
vSphereVolume 卷类型支持 VMFS 和 VSAN 数据仓库。</p>
<!--
You must create VMDK using one of the following methods before using with Pod.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 在挂载到 Pod 之前，你必须用下列方式之一创建 VMDK。
</div>

<!--
#### Creating a VMDK volume {#creating-vmdk-volume}

Choose one of the following methods to create a VMDK.
-->
<h4 id="creating-vmdk-volume">创建 VMDK 卷 </h4>
<p>选择下列方式之一创建 VMDK。</p>
<ul class="nav nav-tabs" id="tabs-volumes" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#tabs-volumes-0" role="tab" aria-controls="tabs-volumes-0" aria-selected="true">使用 vmkfstools 创建</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#tabs-volumes-1" role="tab" aria-controls="tabs-volumes-1">使用 vmware-vdiskmanager 创建</a></li></ul>
<div class="tab-content" id="tabs-volumes"><div id="tabs-volumes-0" class="tab-pane show active" role="tabpanel" aria-labelledby="tabs-volumes-0">

<p><p>首先 ssh 到 ESX，然后使用下面的命令来创建 VMDK：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">vmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk
</code></pre></div></div>
  <div id="tabs-volumes-1" class="tab-pane" role="tabpanel" aria-labelledby="tabs-volumes-1">

<p><p>使用下面的命令创建 VMDK：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">vmware-vdiskmanager -c -t <span style="color:#666">0</span> -s 40GB -a lsilogic myDisk.vmdk
</code></pre></div></div></div>

<!--
#### vSphere VMDK configuration example {#vsphere-vmdk-configuration}
-->
<h4 id="vsphere-vmdk-configuration">vSphere VMDK 配置示例   </h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-vmdk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/test-webserver<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/test-vmdk<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此 VMDK 卷必须已经存在</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">vsphereVolume</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumePath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;[DatastoreName] volumes/myDisk&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span></code></pre></div><!--
For more information, see the [vSphere volume](https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere) examples.
-->
<p>进一步信息可参考
<a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere">vSphere 卷</a>。</p>
<!--
#### vSphere CSI migration {#vsphere-csi-migration}
-->
<h4 id="vsphere-csi-migration">vSphere CSI 迁移 </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code>
</div>


<!--
The `CSIMigration` feature for `vsphereVolume`, when enabled, redirects all plugin operations
from the existing in-tree plugin to the `csi.vsphere.vmware.com` <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> driver. In order to use this feature, the
[vSphere CSI driver](https://github.com/kubernetes-sigs/vsphere-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationvSphere`
[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) must be enabled.
-->
<p>当 <code>vsphereVolume</code> 的 <code>CSIMigration</code> 特性被启用时，所有插件操作都被从树内插件重定向到
<code>csi.vsphere.vmware.com</code> <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> 驱动。
为了使用此功能特性，必须在集群中安装
<a href="https://github.com/kubernetes-sigs/vsphere-csi-driver">vSphere CSI 驱动</a>，并启用
<code>CSIMigration</code> 和 <code>CSIMigrationvSphere</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。</p>
<!--
This also requires minimum vSphere vCenter/ESXi Version to be 7.0u1 and minimum HW Version to be VM version 15.
-->
<p>此特性还要求 vSphere vCenter/ESXi 的版本至少为 7.0u1，且 HW 版本至少为
VM version 15。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The following StorageClass parameters from the built-in `vsphereVolume` plugin are not supported by the vSphere CSI driver:
-->
<p>vSphere CSI 驱动不支持内置 <code>vsphereVolume</code> 的以下 StorageClass 参数：</p>
<ul>
<li><code>diskformat</code></li>
<li><code>hostfailurestotolerate</code></li>
<li><code>forceprovisioning</code></li>
<li><code>cachereservation</code></li>
<li><code>diskstripes</code></li>
<li><code>objectspacereservation</code></li>
<li><code>iopslimit</code></li>
</ul>
<!--
Existing volumes created using these parameters will be migrated to the vSphere CSI driver,
but new volumes created by the vSphere CSI driver will not be honoring these parameters.
-->
<p>使用这些参数创建的现有卷将被迁移到 vSphere CSI 驱动，不过使用 vSphere
CSI 驱动所创建的新卷都不会理会这些参数。</p>

</div>
<!--
#### vSphere CSI migration complete {#vsphere-csi-migration-complete}
-->
<h4 id="vsphere-csi-migration-complete">vSphere CSI 迁移完成  </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code>
</div>


<!--
To turn off the `vsphereVolume` plugin from being loaded by the controller manager and the kubelet, you need to set `InTreePluginvSphereUnregister` feature flag to `true`. You must install a `csi.vsphere.vmware.com` <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> driver on all worker nodes.
-->
<p>为了避免控制器管理器和 kubelet 加载 <code>vsphereVolume</code> 插件，你需要将
<code>InTreePluginvSphereUnregister</code> 特性设置为 <code>true</code>。你还必须在所有工作节点上安装
<code>csi.vsphere.vmware.com</code> <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> 驱动。</p>
<!--
#### Portworx CSI migration
-->
<h4 id="portworx-csi-迁移">Portworx CSI 迁移</h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code>
</div>


<!--
The `CSIMigration` feature for Portworx has been added but disabled by default in Kubernetes 1.23 since it's in alpha state.
It redirects all plugin operations from the existing in-tree plugin to the
`pxd.portworx.com` Container Storage Interface (CSI) Driver.
[Portworx CSI Driver](https://docs.portworx.com/portworx-install-with-kubernetes/storage-operations/csi/)
must be installed on the cluster.
To enable the feature, set `CSIMigrationPortworx=true` in kube-controller-manager and kubelet.
-->
<p>Kubernetes 1.23 中加入了 Portworx 的 <code>CSIMigration</code> 功能，但默认不会启用，因为该功能仍处于 alpha 阶段。
该功能会将所有的插件操作从现有的树内插件重定向到
<code>pxd.portworx.com</code> 容器存储接口（Container Storage Interface, CSI）驱动程序。
集群中必须安装
<a href="https://docs.portworx.com/portworx-install-with-kubernetes/storage-operations/csi/">Portworx CSI 驱动</a>。
要启用此功能，请在 kube-controller-manager 和 kubelet 中设置 <code>CSIMigrationPortworx=true</code>。</p>
<!--
## Using subPath {#using-subpath}

Sometimes, it is useful to share one volume for multiple uses in a single Pod.
The `volumeMounts.subPath` property specifies a sub-path inside the referenced volume
instead of its root.
-->
<h2 id="using-subpath">使用 subPath </h2>
<p>有时，在单个 Pod 中共享卷以供多方使用是很有用的。
<code>volumeMounts.subPath</code> 属性可用于指定所引用的卷内的子路径，而不是其根路径。</p>
<!--
The following example shows how to configure a Pod with a LAMP stack (Linux Apache MySQL PHP)
using a single, shared volume. This sample `subPath` configuration is not recommended
for production use.

The PHP application's code and assets map to the volume's `html` folder and
the MySQL database is stored in the volume's `mysql` folder. For example:
-->
<p>下面例子展示了如何配置某包含 LAMP 堆栈（Linux Apache MySQL PHP）的 Pod 使用同一共享卷。
此示例中的 <code>subPath</code> 配置不建议在生产环境中使用。
PHP 应用的代码和相关数据映射到卷的 <code>html</code> 文件夹，MySQL 数据库存储在卷的 <code>mysql</code> 文件夹中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-lamp-site<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MYSQL_ROOT_PASSWORD<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;rootpasswd&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>site-data<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">subPath</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>php:7.0-apache<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/www/html<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>site-data<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">subPath</span>:<span style="color:#bbb"> </span>html<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>site-data<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>my-lamp-site-data<span style="color:#bbb">
</span></code></pre></div><!--
### Using subPath with expanded environment variables {#using-subpath-expanded-environment}
-->
<h3 id="using-subpath-expanded-environment">使用带有扩展环境变量的 subPath </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code>
</div>


<!--
Use the `subPathExpr` field to construct `subPath` directory names from
Downward API environment variables.
The `subPath` and `subPathExpr` properties are mutually exclusive.
-->
<p>使用 <code>subPathExpr</code> 字段可以基于 Downward API 环境变量来构造 <code>subPath</code> 目录名。
<code>subPath</code> 和 <code>subPathExpr</code> 属性是互斥的。</p>
<!--
In this example, a Pod uses `subPathExpr` to create a directory `pod1` within
the hostPath volume `/var/log/pods`.
The `hostPath` volume takes the `Pod` name from the `downwardAPI`.
The host directory `/var/log/pods/pod1` is mounted at `/logs` in the container.
-->
<p>在这个示例中，Pod 使用 <code>subPathExpr</code> 来 hostPath 卷 <code>/var/log/pods</code> 中创建目录 <code>pod1</code>。
<code>hostPath</code> 卷采用来自 <code>downwardAPI</code> 的 Pod 名称生成目录名。
宿主目录 <code>/var/log/pods/pod1</code> 被挂载到容器的 <code>/logs</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>container1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>POD_NAME<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>metadata.name<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;while [ true ]; do echo &#39;Hello&#39;; sleep 10; done | tee -a /logs/hello.txt&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>workdir1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/logs<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># 包裹变量名的是小括号，而不是大括号</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">subPathExpr</span>:<span style="color:#bbb"> </span>$(POD_NAME)<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>workdir1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/log/pods<span style="color:#bbb">
</span></code></pre></div><!--
## Resources

The storage media (Disk, SSD, etc.) of an `emptyDir` volume is determined by the
medium of the filesystem holding the kubelet root dir (typically
`/var/lib/kubelet`).  There is no limit on how much space an `emptyDir` or
`hostPath` volume can consume, and no isolation between Containers or between
Pods.
-->
<h2 id="resources">资源  </h2>
<p><code>emptyDir</code> 卷的存储介质（磁盘、SSD 等）是由保存 kubelet
数据的根目录（通常是 <code>/var/lib/kubelet</code>）的文件系统的介质确定。
Kubernetes 对 <code>emptyDir</code> 卷或者 <code>hostPath</code> 卷可以消耗的空间没有限制，容器之间或 Pod 之间也没有隔离。</p>
<!--
To learn about requesting space using a resource specification, see
[how to manage resources](/docs/concepts/configuration/manage-resources-containers/).
-->
<p>要了解如何使用资源规约来请求空间，可参考
<a href="/zh/docs/concepts/configuration/manage-resources-containers/">如何管理资源</a>。</p>
<!--
## Out-of-Tree Volume Plugins

The out-of-tree volume plugins include
<a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='Container Storage Interface'>Container Storage Interface</a> (CSI), and also FlexVolume (which is deprecated). These plugins enable storage vendors to create custom storage plugins
without adding their plugin source code to the Kubernetes repository.
-->
<h2 id="out-of-tree-volume-plugins">树外（Out-of-Tree）卷插件   </h2>
<p>Out-of-Tree 卷插件包括
<a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='容器存储接口（CSI）'>容器存储接口（CSI）</a>
和 FlexVolume（已弃用）。
它们使存储供应商能够创建自定义存储插件，而无需将插件源码添加到 Kubernetes 代码仓库。</p>
<!--
Previously, all volume plugins were "in-tree". The "in-tree" plugins were built, linked, compiled,
and shipped with the core Kubernetes binaries. This meant that adding a new storage system to
Kubernetes (a volume plugin) required checking code into the core Kubernetes code repository.
-->
<p>以前，所有卷插件（如上面列出的卷类型）都是“树内（In-Tree）”的。
“树内”插件是与 Kubernetes 的核心组件一同构建、链接、编译和交付的。
这意味着向 Kubernetes 添加新的存储系统（卷插件）需要将代码合并到 Kubernetes 核心代码库中。</p>
<!--
Both CSI and FlexVolume allow volume plugins to be developed independent of
the Kubernetes code base, and deployed (installed) on Kubernetes clusters as
extensions.

For storage vendors looking to create an out-of-tree volume plugin, please refer
to [this FAQ](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md).
-->
<p>CSI 和 FlexVolume 都允许独立于 Kubernetes 代码库开发卷插件，并作为扩展部署（安装）在 Kubernetes 集群上。</p>
<p>对于希望创建树外（Out-Of-Tree）卷插件的存储供应商，请参考
<a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md">卷插件常见问题</a>。</p>
<h3 id="csi">CSI</h3>
<!--
[Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md)
(CSI) defines a standard interface for container orchestration systems (like
Kubernetes) to expose arbitrary storage systems to their container workloads.
-->
<p><a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">容器存储接口</a> (CSI)
为容器编排系统（如 Kubernetes）定义标准接口，以将任意存储系统暴露给它们的容器工作负载。</p>
<!--
Please read the [CSI design proposal](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md) for more information.

CSI support was introduced as alpha in Kubernetes v1.9, moved to beta in
Kubernetes v1.10, and is GA in Kubernetes v1.13.
-->
<p>更多详情请阅读 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md">CSI 设计方案</a>。</p>
<!--
Support for CSI spec versions 0.2 and 0.3 are deprecated in Kubernetes
v1.13 and will be removed in a future release.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Kubernetes v1.13 废弃了对 CSI 规范版本 0.2 和 0.3 的支持，并将在以后的版本中删除。
</div>
<!--
CSI drivers may not be compatible across all Kubernetes releases.
Please check the specific CSI driver's documentation for supported
deployments steps for each Kubernetes release and a compatibility matrix.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> CSI 驱动可能并非兼容所有的 Kubernetes 版本。
请查看特定 CSI 驱动的文档，以了解各个 Kubernetes 版本所支持的部署步骤以及兼容性列表。
</div>
<!--
Once a CSI compatible volume driver is deployed on a Kubernetes cluster, users
may use the `csi` volume type to attach, mount, etc. the volumes exposed by the
CSI driver.

A `csi` volume can be used in a Pod in three different ways:

* through a reference to a [PersistentVolumeClaim](#persistentvolumeclaim)
* with a [generic ephemeral volume](/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volume)
(alpha feature)
* with a [CSI ephemeral volume](/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume)
if the driver supports that (beta feature)
-->
<p>一旦在 Kubernetes 集群上部署了 CSI 兼容卷驱动程序，用户就可以使用
<code>csi</code> 卷类型来挂接、挂载 CSI 驱动所提供的卷。</p>
<p><code>csi</code> 卷可以在 Pod 中以三种方式使用：</p>
<ul>
<li>通过 PersistentVolumeClaim(#persistentvolumeclaim) 对象引用</li>
<li>使用<a href="/zh/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volume">一般性的临时卷</a>
（Alpha 特性）</li>
<li>使用 <a href="/zh/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume">CSI 临时卷</a>，
前提是驱动支持这种用法（Beta 特性）</li>
</ul>
<!--
The following fields are available to storage administrators to configure a CSI
persistent volume:
-->
<p>存储管理员可以使用以下字段来配置 CSI 持久卷：</p>
<!--
- `driver`: A string value that specifies the name of the volume driver to use.
  This value must correspond to the value returned in the `GetPluginInfoResponse`
  by the CSI driver as defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo).
  It is used by Kubernetes to identify which CSI driver to call out to, and by
  CSI driver components to identify which PV objects belong to the CSI driver.
-->
<ul>
<li><code>driver</code>：指定要使用的卷驱动名称的字符串值。
这个值必须与 CSI 驱动程序在 <code>GetPluginInfoResponse</code> 中返回的值相对应；该接口定义在
<a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo">CSI 规范</a>中。
Kubernetes 使用所给的值来标识要调用的 CSI 驱动程序；CSI
驱动程序也使用该值来辨识哪些 PV 对象属于该 CSI 驱动程序。</li>
</ul>
<!--
- `volumeHandle`: A string value that uniquely identifies the volume. This value
  must correspond to the value returned in the `volume.id` field of the
  `CreateVolumeResponse` by the CSI driver as defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume).
  The value is passed as `volume_id` on all calls to the CSI volume driver when
  referencing the volume.
-->
<ul>
<li><code>volumeHandle</code>：唯一标识卷的字符串值。
该值必须与 CSI 驱动在 <code>CreateVolumeResponse</code> 的 <code>volume_id</code> 字段中返回的值相对应；接口定义在
<a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume">CSI 规范</a> 中。
在所有对 CSI 卷驱动程序的调用中，引用该 CSI 卷时都使用此值作为 <code>volume_id</code> 参数。</li>
</ul>
<!--
- `readOnly`: An optional boolean value indicating whether the volume is to be
  "ControllerPublished" (attached) as read only. Default is false. This value is
  passed to the CSI driver via the `readonly` field in the
  `ControllerPublishVolumeRequest`.
-->
<ul>
<li><code>readOnly</code>：一个可选的布尔值，指示通过 <code>ControllerPublished</code> 关联该卷时是否设置该卷为只读。默认值是 false。
该值通过 <code>ControllerPublishVolumeRequest</code> 中的 <code>readonly</code> 字段传递给 CSI 驱动。</li>
</ul>
<!--
- `fsType`: If the PV's `VolumeMode` is `Filesystem` then this field may be used
  to specify the filesystem that should be used to mount the volume. If the
  volume has not been formatted and formatting is supported, this value will be
  used to format the volume.
  This value is passed to the CSI driver via the `VolumeCapability` field of
  `ControllerPublishVolumeRequest`, `NodeStageVolumeRequest`, and
  `NodePublishVolumeRequest`.
-->
<ul>
<li><code>fsType</code>：如果 PV 的 <code>VolumeMode</code> 为 <code>Filesystem</code>，那么此字段指定挂载卷时应该使用的文件系统。
如果卷尚未格式化，并且支持格式化，此值将用于格式化卷。
此值可以通过 <code>ControllerPublishVolumeRequest</code>、<code>NodeStageVolumeRequest</code> 和
<code>NodePublishVolumeRequest</code> 的 <code>VolumeCapability</code> 字段传递给 CSI 驱动。</li>
</ul>
<!--
- `volumeAttributes`: A map of string to string that specifies static properties
  of a volume. This map must correspond to the map returned in the
  `volume.attributes` field of the `CreateVolumeResponse` by the CSI driver as
  defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume).
  The map is passed to the CSI driver via the `volume_attributes` field in the
  `ControllerPublishVolumeRequest`, `NodeStageVolumeRequest`, and
  `NodePublishVolumeRequest`.
-->
<ul>
<li><code>volumeAttributes</code>：一个字符串到字符串的映射表，用来设置卷的静态属性。
该映射必须与 CSI 驱动程序返回的 <code>CreateVolumeResponse</code> 中的 <code>volume.attributes</code>
字段的映射相对应；
<a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume">CSI 规范</a>
中有相应的定义。
该映射通过<code>ControllerPublishVolumeRequest</code>、<code>NodeStageVolumeRequest</code>、和
<code>NodePublishVolumeRequest</code> 中的 <code>volume_attributes</code> 字段传递给 CSI 驱动。</li>
</ul>
<!--
- `controllerPublishSecretRef`: A reference to the secret object containing
  sensitive information to pass to the CSI driver to complete the CSI
  `ControllerPublishVolume` and `ControllerUnpublishVolume` calls. This field is
  optional, and may be empty if no secret is required. If the secret object
  contains more than one secret, all secrets are passed.
-->
<ul>
<li><code>controllerPublishSecretRef</code>：对包含敏感信息的 Secret 对象的引用；
该敏感信息会被传递给 CSI 驱动来完成 CSI <code>ControllerPublishVolume</code> 和
<code>ControllerUnpublishVolume</code> 调用。
此字段是可选的；在不需要 Secret 时可以是空的。
如果 Secret 对象包含多个 Secret 条目，则所有的 Secret 条目都会被传递。</li>
</ul>
<!--
- `nodeStageSecretRef`: A reference to the secret object containing
  sensitive information to pass to the CSI driver to complete the CSI
  `NodeStageVolume` call. This field is optional, and may be empty if no secret
  is required. If the secret object contains more than one secret, all secrets
  are passed.
-->
<ul>
<li><code>nodeStageSecretRef</code>：对包含敏感信息的 Secret 对象的引用。
该信息会传递给 CSI 驱动来完成 CSI <code>NodeStageVolume</code> 调用。
此字段是可选的，如果不需要 Secret，则可能是空的。
如果 Secret 对象包含多个 Secret 条目，则传递所有 Secret 条目。</li>
</ul>
<!--
- `nodePublishSecretRef`: A reference to the secret object containing
  sensitive information to pass to the CSI driver to complete the CSI
  `NodePublishVolume` call. This field is optional, and may be empty if no
  secret is required. If the secret object contains more than one secret, all
  secrets are passed.
-->
<ul>
<li><code>nodePublishSecretRef</code>：对包含敏感信息的 Secret 对象的引用。
该信息传递给 CSI 驱动来完成 CSI <code>NodePublishVolume</code> 调用。
此字段是可选的，如果不需要 Secret，则可能是空的。
如果 Secret 对象包含多个 Secret 条目，则传递所有 Secret 条目。</li>
</ul>
<!--
#### CSI raw block volume support
-->
<h4 id="csi-raw-block-volume-support">CSI 原始块卷支持   </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>


<!--
Vendors with external CSI drivers to implement raw block volumes support
in Kubernetes workloads.
-->
<p>具有外部 CSI 驱动程序的供应商能够在 Kubernetes 工作负载中实现原始块卷支持。</p>
<!--
You can set up your
[PersistentVolume/PersistentVolumeClaim with raw block volume support](/docs/concepts/storage/persistent-volumes/#raw-block-volume-support) as usual, without any CSI specific changes.
-->
<p>你可以和以前一样，安装自己的
<a href="/zh/docs/concepts/storage/persistent-volumes/#raw-block-volume-support">带有原始块卷支持的 PV/PVC</a>，
采用 CSI 对此过程没有影响。</p>
<!--
#### CSI ephemeral volumes
-->
<h4 id="csi-ephemeral-volumes">CSI 临时卷  </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.16 [beta]</code>
</div>


<!--
You can directly configure CSI volumes within the Pod
specification. Volumes specified in this way are ephemeral and do not
persist across pod restarts. See [Ephemeral
Volumes](/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume)
for more information.
-->
<p>你可以直接在 Pod 规约中配置 CSI 卷。采用这种方式配置的卷都是临时卷，
无法在 Pod 重新启动后继续存在。
进一步的信息可参阅<a href="/zh/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume">临时卷</a>。</p>
<!--
For more information on how to develop a CSI driver, refer to the [kubernetes-csi
documentation](https://kubernetes-csi.github.io/docs/)

-->
<p>有关如何开发 CSI 驱动的更多信息，请参考 <a href="https://kubernetes-csi.github.io/docs/">kubernetes-csi 文档</a>。</p>
<!--
#### Migrating to CSI drivers from in-tree plugins
-->
<h4 id="migrating-to-csi-drivers-from-in-tree-plugins">从树内插件迁移到 CSI 驱动程序 </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [beta]</code>
</div>


<!--
The CSI Migration feature, when enabled, directs operations against existing in-tree
plugins to corresponding CSI plugins (which are expected to be installed and configured).
As a result, operators do not have to make any
configuration changes to existing Storage Classes, PersistentVolumes or PersistentVolumeClaims
(referring to in-tree plugins) when transitioning to a CSI driver that supersedes an in-tree plugin.

The operations and features that are supported include:
provisioning/delete, attach/detach, mount/unmount and resizing of volumes.

In-tree plugins that support `CSIMigration` and have a corresponding CSI driver implemented
are listed in [Types of Volumes](#volume-types).
-->
<p>启用 <code>CSIMigration</code> 功能后，针对现有树内插件的操作会被重定向到相应的 CSI 插件（应已安装和配置）。
因此，操作员在过渡到取代树内插件的 CSI 驱动时，无需对现有存储类、PV 或 PVC（指树内插件）进行任何配置更改。</p>
<p>所支持的操作和功能包括：配备（Provisioning）/删除、挂接（Attach）/解挂（Detach）、
挂载（Mount）/卸载（Unmount）和调整卷大小。</p>
<p>上面的<a href="#volume-types">卷类型</a>节列出了支持 <code>CSIMigration</code> 并已实现相应 CSI
驱动程序的树内插件。</p>
<h3 id="flexvolume">flexVolume</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [deprecated]</code>
</div>


<!--
FlexVolume is an out-of-tree plugin interface that uses an exec-based model to interface
with storage drivers. The FlexVolume driver binaries must be installed in a pre-defined
volume plugin path on each node and in some cases the control plane nodes as well.

Pods interact with FlexVolume drivers through the `flexVolume` in-tree volume plugin.
For more details, see the FlexVolume [README](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md#readme) document.
-->
<p>FlexVolume 是一个使用基于 exec 的模型来与驱动程序对接的树外插件接口。
用户必须在每个节点上的预定义卷插件路径中安装 FlexVolume
驱动程序可执行文件，在某些情况下，控制平面节点中也要安装。</p>
<p>Pod 通过 <code>flexvolume</code> 树内插件与 FlexVolume 驱动程序交互。
更多详情请参考 FlexVolume <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md#readme">README</a> 文档。</p>
<!--
FlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate external storage with Kubernetes.

Maintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of FlexVolume drivers to CSI.
Users of FlexVolume should move their workloads to use the equivalent CSI Driver.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>FlexVolume 已弃用。推荐使用树外 CSI 驱动来将外部存储整合进 Kubernetes。</p>
<p>FlexVolume 驱动的维护者应开发一个 CSI 驱动并帮助用户从 FlexVolume 驱动迁移到 CSI。
FlexVolume 用户应迁移工作负载以使用对等的 CSI 驱动。</p>

</div>
<!--
## Mount propagation

Mount propagation allows for sharing volumes mounted by a Container to
other Containers in the same Pod, or even to other Pods on the same node.

Mount propagation of a volume is controlled by `mountPropagation` field in Container.volumeMounts.
Its values are:
-->
<h2 id="mount-propagation">挂载卷的传播  </h2>
<p>挂载卷的传播能力允许将容器安装的卷共享到同一 Pod 中的其他容器，甚至共享到同一节点上的其他 Pod。</p>
<p>卷的挂载传播特性由 <code>Container.volumeMounts</code> 中的 <code>mountPropagation</code> 字段控制。
它的值包括：</p>
<!--
 * `None` - This volume mount will not receive any subsequent mounts
   that are mounted to this volume or any of its subdirectories by the host.
   In similar fashion, no mounts created by the Container will be visible on
   the host. This is the default mode.

   This mode is equal to `private` mount propagation as described in the
   [Linux kernel documentation](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)
-->
<ul>
<li>
<p><code>None</code> - 此卷挂载将不会感知到主机后续在此卷或其任何子目录上执行的挂载变化。
类似的，容器所创建的卷挂载在主机上是不可见的。这是默认模式。</p>
<p>该模式等同于 <a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">Linux 内核文档</a>
中描述的 <code>private</code> 挂载传播选项。</p>
</li>
</ul>
<!--
 * `HostToContainer` - This volume mount will receive all subsequent mounts
   that are mounted to this volume or any of its subdirectories.

   In other words, if the host mounts anything inside the volume mount, the
   Container will see it mounted there.

   Similarly, if any Pod with `Bidirectional` mount propagation to the same
   volume mounts anything there, the Container with `HostToContainer` mount
   propagation will see it.

   This mode is equal to `rslave` mount propagation as described in the
   [Linux kernel documentation](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)
-->
<ul>
<li>
<p><code>HostToContainer</code> - 此卷挂载将会感知到主机后续针对此卷或其任何子目录的挂载操作。</p>
<p>换句话说，如果主机在此挂载卷中挂载任何内容，容器将能看到它被挂载在那里。</p>
<p>类似的，配置了 <code>Bidirectional</code> 挂载传播选项的 Pod 如果在同一卷上挂载了内容，挂载传播设置为
<code>HostToContainer</code> 的容器都将能看到这一变化。</p>
<p>该模式等同于 <a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">Linux 内核文档</a>
中描述的 <code>rslave</code> 挂载传播选项。</p>
</li>
</ul>
<!--
* `Bidirectional` - This volume mount behaves the same the `HostToContainer` mount.
   In addition, all volume mounts created by the Container will be propagated
   back to the host and to all Containers of all Pods that use the same volume.

   A typical use case for this mode is a Pod with a FlexVolume or CSI driver or
   a Pod that needs to mount something on the host using a `hostPath` volume.

   This mode is equal to `rshared` mount propagation as described in the
   [Linux kernel documentation](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)
-->
<ul>
<li>
<p><code>Bidirectional</code> - 这种卷挂载和 <code>HostToContainer</code> 挂载表现相同。
另外，容器创建的卷挂载将被传播回至主机和使用同一卷的所有 Pod 的所有容器。</p>
<p>该模式等同于 <a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">Linux 内核文档</a>
中描述的 <code>rshared</code> 挂载传播选项。</p>
<!--
`Bidirectional` mount propagation can be dangerous. It can damage
the host operating system and therefore it is allowed only in privileged
Containers. Familiarity with Linux kernel behavior is strongly recommended.
In addition, any volume mounts created by Containers in Pods must be destroyed
(unmounted) by the Containers on termination.
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <code>Bidirectional</code> 形式的挂载传播可能比较危险。
它可以破坏主机操作系统，因此它只被允许在特权容器中使用。
强烈建议你熟悉 Linux 内核行为。
此外，由 Pod 中的容器创建的任何卷挂载必须在终止时由容器销毁（卸载）。
</div>


</li>
</ul>
<!--
### Configuration

Before mount propagation can work properly on some deployments (CoreOS,
RedHat/Centos, Ubuntu) mount share must be configured correctly in
Docker as shown below.
-->
<h3 id="configuration">配置 </h3>
<p>在某些部署环境中，挂载传播正常工作前，必须在 Docker 中正确配置挂载共享（mount share），如下所示。</p>
<!--
Edit your Docker's `systemd` service file.  Set `MountFlags` as follows:
-->
<p>编辑你的 Docker <code>systemd</code> 服务文件，按下面的方法设置 <code>MountFlags</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">MountFlags</span><span style="color:#666">=</span>shared
</code></pre></div><!--
Or, remove `MountFlags=slave` if present. Then restart the Docker daemon:
-->
<p>或者，如果存在 <code>MountFlags=slave</code> 就删除掉。然后重启 Docker 守护进程：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
Follow an example of [deploying WordPress and MySQL with Persistent Volumes](/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/).
-->
<p>参考<a href="/zh/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/">使用持久卷部署 WordPress 和 MySQL</a> 示例。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ffd12528a12882b282e1bd19e29f9e75">6.2 - 持久卷</h1>
    
	<!--
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
- xing-yang
title: Persistent Volumes
feature:
  title: Storage orchestration
  description: >
    Automatically mount the storage system of your choice, whether from local storage, a public cloud provider such as <a href="https://cloud.google.com/storage/">GCP</a> or <a href="https://aws.amazon.com/products/storage/">AWS</a>, or a network storage system such as NFS, iSCSI, Gluster, Ceph, Cinder, or Flocker.
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
This document describes _persistent volumes_ in Kubernetes. Familiarity with [volumes](/docs/concepts/storage/volumes/) is suggested.
-->
<p>本文描述 Kubernetes 中的 <em>持久卷（Persistent Volume）</em> 。
建议先熟悉<a href="/zh/docs/concepts/storage/volumes/">卷（Volume）</a>的概念。</p>
<!-- body -->
<!--
## Introduction

Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources:  PersistentVolume and PersistentVolumeClaim.
-->
<h2 id="introduction">介绍 </h2>
<p>存储的管理是一个与计算实例的管理完全不同的问题。PersistentVolume 子系统为用户
和管理员提供了一组 API，将存储如何供应的细节从其如何被使用中抽象出来。
为了实现这点，我们引入了两个新的 API 资源：PersistentVolume 和
PersistentVolumeClaim。</p>
<!--
A _PersistentVolume_ (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using [Storage Classes](/docs/concepts/storage/storage-classes/). It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.
-->
<p>持久卷（PersistentVolume，PV）是集群中的一块存储，可以由管理员事先供应，或者
使用<a href="/zh/docs/concepts/storage/storage-classes/">存储类（Storage Class）</a>来动态供应。
持久卷是集群资源，就像节点也是集群资源一样。PV 持久卷和普通的 Volume 一样，也是使用
卷插件来实现的，只是它们拥有独立于任何使用 PV 的 Pod 的生命周期。
此 API 对象中记述了存储的实现细节，无论其背后是 NFS、iSCSI 还是特定于云平台的存储系统。</p>
<!--
A _PersistentVolumeClaim_ (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory).  Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see [AccessModes](#access-modes)).
-->
<p>持久卷申领（PersistentVolumeClaim，PVC）表达的是用户对存储的请求。概念上与 Pod 类似。
Pod 会耗用节点资源，而 PVC 申领会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU
和内存）；同样 PVC 申领也可以请求特定的大小和访问模式
（例如，可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany
模式之一来挂载，参见<a href="#access-modes">访问模式</a>）。</p>
<!--
While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the _StorageClass_ resource.

See the [detailed walkthrough with working examples](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).
-->
<p>尽管 PersistentVolumeClaim 允许用户消耗抽象的存储资源，常见的情况是针对不同的
问题用户需要的是具有不同属性（如，性能）的 PersistentVolume 卷。
集群管理员需要能够提供不同性质的 PersistentVolume，并且这些 PV 卷之间的差别不
仅限于卷大小和访问模式，同时又不能将卷是如何实现的这些细节暴露给用户。
为了满足这类需求，就有了 <em>存储类（StorageClass）</em> 资源。</p>
<p>参见<a href="/zh/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">基于运行示例的详细演练</a>。</p>
<!--
## Lifecycle of a volume and claim

PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:

### Provisioning

There are two ways PVs may be provisioned: statically or dynamically.
-->
<h2 id="lifecycle-of-a-volume-and-claim">卷和申领的生命周期  </h2>
<p>PV 卷是集群中的资源。PVC 申领是对这些资源的请求，也被用来执行对资源的申领检查。
PV 卷和 PVC 申领之间的互动遵循如下生命周期：</p>
<h3 id="provisioning">供应  </h3>
<p>PV 卷的供应有两种方式：静态供应或动态供应。</p>
<!--
#### Static

A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.
-->
<h4 id="static">静态供应 </h4>
<p>集群管理员创建若干 PV 卷。这些卷对象带有真实存储的细节信息，并且对集群
用户可用（可见）。PV 卷对象存在于 Kubernetes  API 中，可供用户消费（使用）。</p>
<!--
#### Dynamic

When none of the static PVs the administrator created match a user's PersistentVolumeClaim,
the cluster may try to dynamically provision a volume specially for the PVC.
This provisioning is based on StorageClasses: the PVC must request a
[storage class](/docs/concepts/storage/storage-classes/) and
the administrator must have created and configured that class for dynamic
provisioning to occur. Claims that request the class `""` effectively disable
dynamic provisioning for themselves.
-->
<h4 id="dynamic">动态供应    </h4>
<p>如果管理员所创建的所有静态 PV 卷都无法与用户的 PersistentVolumeClaim 匹配，
集群可以尝试为该 PVC 申领动态供应一个存储卷。
这一供应操作是基于 StorageClass 来实现的：PVC 申领必须请求某个
<a href="/zh/docs/concepts/storage/storage-classes/">存储类</a>，同时集群管理员必须
已经创建并配置了该类，这样动态供应卷的动作才会发生。
如果 PVC 申领指定存储类为 <code>&quot;&quot;</code>，则相当于为自身禁止使用动态供应的卷。</p>
<!--
To enable dynamic storage provisioning based on storage class, the cluster administrator
needs to enable the `DefaultStorageClass` [admission controller](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
on the API server. This can be done, for example, by ensuring that `DefaultStorageClass` is
among the comma-delimited, ordered list of values for the `-enable-admission-plugins` flag of
the API server component. For more information on API server command-line flags,
check [kube-apiserver](/docs/admin/kube-apiserver/) documentation.
-->
<p>为了基于存储类完成动态的存储供应，集群管理员需要在 API 服务器上启用
<code>DefaultStorageClass</code> <a href="/zh/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass">准入控制器</a>。
举例而言，可以通过保证 <code>DefaultStorageClass</code> 出现在 API 服务器组件的
<code>--enable-admission-plugins</code> 标志值中实现这点；该标志的值可以是逗号
分隔的有序列表。关于 API 服务器标志的更多信息，可以参考
<a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a>
文档。</p>
<!--
### Binding

A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.

Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.
-->
<h3 id="binding">绑定    </h3>
<p>用户创建一个带有特定存储容量和特定访问模式需求的 PersistentVolumeClaim 对象；
在动态供应场景下，这个 PVC 对象可能已经创建完毕。
主控节点中的控制回路监测新的 PVC 对象，寻找与之匹配的 PV 卷（如果可能的话），
并将二者绑定到一起。
如果为了新的 PVC 申领动态供应了 PV 卷，则控制回路总是将该 PV 卷绑定到这一 PVC 申领。
否则，用户总是能够获得他们所请求的资源，只是所获得的 PV 卷可能会超出所请求的配置。
一旦绑定关系建立，则 PersistentVolumeClaim 绑定就是排他性的，无论该 PVC 申领是
如何与 PV 卷建立的绑定关系。
PVC 申领与 PV 卷之间的绑定是一种一对一的映射，实现上使用 ClaimRef 来记述 PV 卷
与 PVC 申领间的双向绑定关系。</p>
<!--
Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.
-->
<p>如果找不到匹配的 PV 卷，PVC 申领会无限期地处于未绑定状态。
当与之匹配的 PV 卷可用时，PVC 申领会被绑定。
例如，即使某集群上供应了很多 50 Gi 大小的 PV 卷，也无法与请求
100 Gi 大小的存储的 PVC 匹配。当新的 100 Gi PV 卷被加入到集群时，该
PVC 才有可能被绑定。</p>
<!--
### Using

Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.
-->
<h3 id="using">使用   </h3>
<p>Pod 将 PVC 申领当做存储卷来使用。集群会检视 PVC 申领，找到所绑定的卷，并
为 Pod 挂载该卷。对于支持多种访问模式的卷，用户要在 Pod 中以卷的形式使用申领
时指定期望的访问模式。</p>
<!--
Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a `persistentVolumeClaim` section in a Pod's `volumes` block. See [Claims As Volumes](#claims-as-volumes) for more details on this.
-->
<p>一旦用户有了申领对象并且该申领已经被绑定，则所绑定的 PV 卷在用户仍然需要它期间
一直属于该用户。用户通过在 Pod 的 <code>volumes</code> 块中包含 <code>persistentVolumeClaim</code>
节区来调度 Pod，访问所申领的 PV 卷。
相关细节可参阅<a href="#claims-as-volumes">使用申领作为卷</a>。</p>
<!--
### Storage Object in Use Protection

The purpose of the Storage Object in Use Protection feature is to ensure that PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are bound to PVCs are not removed from the system, as this may result in data loss.
-->
<h3 id="storage-object-in-use-protection">保护使用中的存储对象  </h3>
<p>保护使用中的存储对象（Storage Object in Use Protection）这一功能特性的目的
是确保仍被 Pod 使用的 PersistentVolumeClaim（PVC）对象及其所绑定的
PersistentVolume（PV）对象在系统中不会被删除，因为这样做可能会引起数据丢失。</p>
<!--
PVC is in active use by a Pod when a Pod object exists that is using the PVC.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 当使用某 PVC 的 Pod 对象仍然存在时，认为该 PVC 仍被此 Pod 使用。
</div>
<!--
If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no longer actively used by any Pods. Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is no longer bound to a PVC.

You can see that a PVC is protected when the PVC's status is `Terminating` and the `Finalizers` list includes `kubernetes.io/pvc-protection`:
-->
<p>如果用户删除被某 Pod 使用的 PVC 对象，该 PVC 申领不会被立即移除。
PVC 对象的移除会被推迟，直至其不再被任何 Pod 使用。
此外，如果管理员删除已绑定到某 PVC 申领的 PV 卷，该 PV 卷也不会被立即移除。
PV 对象的移除也要推迟到该 PV 不再绑定到 PVC。</p>
<p>你可以看到当 PVC 的状态为 <code>Terminating</code> 且其 <code>Finalizers</code> 列表中包含
<code>kubernetes.io/pvc-protection</code> 时，PVC 对象是处于被保护状态的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pvc hostpath
</code></pre></div><pre><code>Name:          hostpath
Namespace:     default
StorageClass:  example-hostpath
Status:        Terminating
Volume:
Labels:        &lt;none&gt;
Annotations:   volume.beta.kubernetes.io/storage-class=example-hostpath
               volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath
Finalizers:    [kubernetes.io/pvc-protection]
...
</code></pre><!--
You can see that a PV is protected when the PV's status is `Terminating` and the `Finalizers` list includes `kubernetes.io/pv-protection` too:
-->
<p>你也可以看到当 PV 对象的状态为 <code>Terminating</code> 且其 <code>Finalizers</code> 列表中包含
<code>kubernetes.io/pv-protection</code> 时，PV 对象是处于被保护状态的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pv task-pv-volume
</code></pre></div><pre><code>Name:            task-pv-volume
Labels:          type=local
Annotations:     &lt;none&gt;
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Terminating
Claim:
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/data
    HostPathType:
Events:            &lt;none&gt;
</code></pre><!--
### Reclaiming

When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted.
-->
<h3 id="reclaiming">回收  </h3>
<p>当用户不再使用其存储卷时，他们可以从 API 中将 PVC 对象删除，从而允许
该资源被回收再利用。PersistentVolume 对象的回收策略告诉集群，当其被
从申领中释放时如何处理该数据卷。
目前，数据卷可以被 Retained（保留）、Recycled（回收）或 Deleted（删除）。</p>
<!--
#### Retain

The `Retain` reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered "released". But it is not yet available for another claim because the previous claimant's data remains on the volume. An administrator can manually reclaim the volume with the following steps.
-->
<h4 id="retain">保留（Retain）   </h4>
<p>回收策略 <code>Retain</code> 使得用户可以手动回收资源。当 PersistentVolumeClaim 对象
被删除时，PersistentVolume 卷仍然存在，对应的数据卷被视为&quot;已释放（released）&quot;。
由于卷上仍然存在这前一申领人的数据，该卷还不能用于其他申领。
管理员可以通过下面的步骤来手动回收该卷：</p>
<!--
1. Delete the PersistentVolume. The associated storage asset in external infrastructure (such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume) still exists after the PV is deleted.
1. Manually clean up the data on the associated storage asset accordingly.
1. Manually delete the associated storage asset.

If you want to reuse the same storage asset, create a new PersistentVolume with the same storage asset definition.
-->
<ol>
<li>删除 PersistentVolume 对象。与之相关的、位于外部基础设施中的存储资产
（例如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）在 PV 删除之后仍然存在。</li>
<li>根据情况，手动清除所关联的存储资产上的数据。</li>
<li>手动删除所关联的存储资产。</li>
</ol>
<p>如果你希望重用该存储资产，可以基于存储资产的定义创建新的 PersistentVolume 卷对象。</p>
<!--
#### Delete

For volume plugins that support the `Delete` reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume. Volumes that were dynamically provisioned inherit the [reclaim policy of their StorageClass](#reclaim-policy), which defaults to `Delete`. The administrator should configure the StorageClass according to users' expectations; otherwise, the PV must be edited or patched after it is created. See [Change the Reclaim Policy of a PersistentVolume](/docs/tasks/administer-cluster/change-pv-reclaim-policy/).
-->
<h4 id="delete">删除（Delete）   </h4>
<p>对于支持 <code>Delete</code> 回收策略的卷插件，删除动作会将 PersistentVolume 对象从
Kubernetes 中移除，同时也会从外部基础设施（如 AWS EBS、GCE PD、Azure Disk 或
Cinder 卷）中移除所关联的存储资产。
动态供应的卷会继承<a href="#reclaim-policy">其 StorageClass 中设置的回收策略</a>，该策略默认
为 <code>Delete</code>。
管理员需要根据用户的期望来配置 StorageClass；否则 PV 卷被创建之后必须要被
编辑或者修补。参阅<a href="/zh/docs/tasks/administer-cluster/change-pv-reclaim-policy/">更改 PV 卷的回收策略</a>.</p>
<!--
#### Recycle

The `Recycle` reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.

If supported by the underlying volume plugin, the `Recycle` reclaim policy performs a basic scrub (`rm -rf /thevolume/*`) on the volume and makes it available again for a new claim.
-->
<h4 id="recycle">回收（Recycle）    </h4>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 回收策略 <code>Recycle</code> 已被废弃。取而代之的建议方案是使用动态供应。
</div>


<p>如果下层的卷插件支持，回收策略 <code>Recycle</code> 会在卷上执行一些基本的
擦除（<code>rm -rf /thevolume/*</code>）操作，之后允许该卷用于新的 PVC 申领。</p>
<!--
However, an administrator can configure a custom recycler Pod template using
the Kubernetes controller manager command line arguments as described in the
[reference](/docs/reference/command-line-tools-reference/kube-controller-manager/).
The custom recycler Pod template must contain a `volumes` specification, as
shown in the example below:
-->
<p>不过，管理员可以按
<a href="/zh/docs/reference/command-line-tools-reference/kube-controller-manager/">参考资料</a>
中所述，使用 Kubernetes 控制器管理器命令行参数来配置一个定制的回收器（Recycler）
Pod 模板。此定制的回收器 Pod 模板必须包含一个 <code>volumes</code> 规约，如下例所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pv-recycler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>vol<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/any/path/it/will/be/replaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pv-recycler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;k8s.gcr.io/busybox&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \&#34;$(ls -A /scrub)\&#34; || exit 1&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>vol<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/scrub<span style="color:#bbb">
</span></code></pre></div><!--
However, the particular path specified in the custom recycler Pod template in the `volumes` part is replaced with the particular path of the volume that is being recycled.
-->
<p>定制回收器 Pod 模板中在 <code>volumes</code> 部分所指定的特定路径要替换为
正被回收的卷的路径。</p>
<!--
### Reserving a PersistentVolume

The control plane can [bind PersistentVolumeClaims to matching PersistentVolumes](#binding) in the
cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.
-->
<h3 id="reserving-a-persistentvolume">预留 PersistentVolume </h3>
<p>通过在 PersistentVolumeClaim 中指定 PersistentVolume，你可以声明该特定
PV 与 PVC 之间的绑定关系。如果该 PersistentVolume 存在且未被通过其
<code>claimRef</code> 字段预留给 PersistentVolumeClaim，则该 PersistentVolume
会和该 PersistentVolumeClaim 绑定到一起。</p>
<!--
The binding happens regardless of some volume matching criteria, including node affinity.
The control plane still checks that [storage class](/docs/concepts/storage/storage-classes/), access modes, and requested storage size are valid.
-->
<p>绑定操作不会考虑某些卷匹配条件是否满足，包括节点亲和性等等。
控制面仍然会检查
<a href="/zh/docs/concepts/storage/storage-classes/">存储类</a>、访问模式和所请求的
存储尺寸都是合法的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo-pvc<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 此处须显式设置空字符串，否则会被设置为默认的 StorageClass</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeName</span>:<span style="color:#bbb"> </span>foo-pv<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!--
This method does not guarantee any binding privileges to the PersistentVolume. If other PersistentVolumeClaims could use the PV that you specify, you first need to reserve that storage volume. Specify the relevant PersistentVolumeClaim in the `claimRef` field of the PV so that other PVCs can not bind to it.
-->
<p>此方法无法对 PersistentVolume 的绑定特权做出任何形式的保证。
如果有其他 PersistentVolumeClaim 可以使用你所指定的 PV，则你应该首先预留
该存储卷。你可以将 PV 的 <code>claimRef</code> 字段设置为相关的 PersistentVolumeClaim
以确保其他 PVC 不会绑定到该 PV 卷。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo-pv<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">claimRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo-pvc<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!--
This is useful if you want to consume PersistentVolumes that have their `claimPolicy` set
to `Retain`, including cases where you are reusing an existing PV.
-->
<p>如果你想要使用 <code>claimPolicy</code> 属性设置为 <code>Retain</code> 的 PersistentVolume 卷
时，包括你希望复用现有的 PV 卷时，这点是很有用的</p>
<!--
### Expanding Persistent Volumes Claims
-->
<h3 id="expanding-persistent-volumes-claims">扩充 PVC 申领  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code>
</div>


<!--
Support for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand
the following types of volumes:
-->
<p>现在，对扩充 PVC 申领的支持默认处于被启用状态。你可以扩充以下类型的卷：</p>
<ul>
<li>azureDisk</li>
<li>azureFile</li>
<li>awsElasticBlockStore</li>
<li>cinder (deprecated)</li>
<li><a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='csi'>csi</a></li>
<li>flexVolume (deprecated)</li>
<li>gcePersistentDisk</li>
<li>glusterfs</li>
<li>rbd</li>
<li>portworxVolume</li>
</ul>
<!--
You can only expand a PVC if its storage class's `allowVolumeExpansion` field is set to true.
-->
<p>只有当 PVC 的存储类中将 <code>allowVolumeExpansion</code> 设置为 true 时，你才可以扩充该 PVC 申领。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>gluster-vol-default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/glusterfs<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resturl</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;http://192.168.10.100:8080&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restuser</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">secretNamespace</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">allowVolumeExpansion</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div><!--
To request a larger volume for a PVC, edit the PVC object and specify a larger
size. This triggers expansion of the volume that backs the underlying PersistentVolume. A
new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.
-->
<p>如果要为某 PVC 请求较大的存储卷，可以编辑 PVC 对象，设置一个更大的尺寸值。
这一编辑操作会触发为下层 PersistentVolume 提供存储的卷的扩充。
Kubernetes 不会创建新的 PV 卷来满足此申领的请求。
与之相反，现有的卷会被调整大小。</p>
<!--
Directly editing the size of a PersistentVolume can prevent an automatic resize of that volume.
If you edit the capacity of a PersistentVolume, and then edit the `.spec` of a matching
PersistentVolumeClaim to make the size of the PersistentVolumeClaim match the PersistentVolume,
then no storage resize happens.
The Kubernetes control plane will see that the desired state of both resources matches,
conclude that the backing volume size has been manually
increased and that no resize is necessary.
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 直接编辑 PersistentVolume 的大小可以阻止该卷自动调整大小。
如果对 PersistentVolume 的容量进行编辑，然后又将其所对应的
PersistentVolumeClaim 的 <code>.spec</code> 进行编辑，使该 PersistentVolumeClaim
的大小匹配 PersistentVolume 的话，则不会发生存储大小的调整。
Kubernetes 控制平面将看到两个资源的所需状态匹配，并认为其后备卷的大小
已被手动增加，无需调整。
</div>


<!--
#### CSI Volume expansion
-->
<h4 id="csi-volume-expansion">CSI 卷的扩充    </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.16 [beta]</code>
</div>


<!--
Support for expanding CSI volumes is enabled by default but it also requires a specific CSI driver to support volume expansion. Refer to documentation of the specific CSI driver for more information.
-->
<p>对 CSI 卷的扩充能力默认是被启用的，不过扩充 CSI 卷要求 CSI 驱动支持
卷扩充操作。可参阅特定 CSI 驱动的文档了解更多信息。</p>
<!--
#### Resizing a volume containing a file system

You can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.
-->
<h4 id="resizing-a-volume-containing-a-file-system">重设包含文件系统的卷的大小</h4>
<p>只有卷中包含的文件系统是 XFS、Ext3 或者 Ext4 时，你才可以重设卷的大小。</p>
<!--
When a volume contains a file system, the file system is only resized when a new Pod is using
the PersistentVolumeClaim in `ReadWrite` mode. File system expansion is either done when a Pod is starting up
or when a Pod is running and the underlying file system supports online expansion.

FlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with the
`RequiresFSResize` capability to `true`. The FlexVolume can be resized on Pod restart.
-->
<p>当卷中包含文件系统时，只有在 Pod 使用 <code>ReadWrite</code> 模式来使用 PVC 申领的
情况下才能重设其文件系统的大小。
文件系统扩充的操作或者是在 Pod 启动期间完成，或者在下层文件系统支持在线
扩充的前提下在 Pod 运行期间完成。</p>
<p>如果 FlexVolumes 的驱动将 <code>RequiresFSResize</code> 能力设置为 <code>true</code>，则该
FlexVolume 卷（于 Kubernetes v1.23 弃用）可以在 Pod 重启期间调整大小。</p>
<!--
#### Resizing an in-use PersistentVolumeClaim
-->
<h4 id="resizing-an-in-use-persistentvolumevlaim">重设使用中 PVC 申领的大小   </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.15 [beta]</code>
</div>


<!--
Expanding in-use PVCs is available as beta since Kubernetes 1.15, and as alpha since 1.11. The `ExpandInUsePersistentVolumes` feature must be enabled, which is the case automatically for many clusters for beta features. Refer to the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) documentation for more information.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Kubernetes 从 1.15 版本开始将调整使用中 PVC 申领大小这一能力作为 Beta
特性支持；该特性在 1.11 版本以来处于 Alpha 阶段。
<code>ExpandInUsePersistentVolumes</code> 特性必须被启用；在很多集群上，与此类似的
Beta 阶段的特性是自动启用的。
可参考<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
文档了解更多信息。
</div>
<!--
In this case, you don't need to delete and recreate a Pod or deployment that is using an existing PVC.
Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.
This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that
uses the PVC before the expansion can complete.
-->
<p>在这种情况下，你不需要删除和重建正在使用某现有 PVC 的 Pod 或 Deployment。
所有使用中的 PVC 在其文件系统被扩充之后，立即可供其 Pod 使用。
此功能特性对于没有被 Pod 或 Deployment 使用的 PVC 而言没有效果。
你必须在执行扩展操作之前创建一个使用该 PVC 的 Pod。</p>
<!--
Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.
-->
<p>与其他卷类型类似，FlexVolume 卷也可以在被 Pod 使用期间执行扩充操作。</p>
<!--
FlexVolume resize is possible only when the underlying driver supports resize.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> FlexVolume 卷的重设大小只能在下层驱动支持重设大小的时候才可进行。
</div>
<!--
Expanding EBS volumes is a time-consuming operation. Also, there is a per-volume quota of one modification every 6 hours.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 扩充 EBS 卷的操作非常耗时。同时还存在另一个配额限制：
每 6 小时只能执行一次（尺寸）修改操作。
</div>
<!--
#### Recovering from Failure when Expanding Volumes

If a user specifies a new size that is too big to be satisfied by underlying storage system, expansion of PVC will be continuously retried until user or cluster administrator takes some action. This can be undesirable and hence Kubernetes provides following methods of recovering from such failures.
-->
<h4 id="recovering-from-failure-when-expanding-volumes">处理扩充卷过程中的失败     </h4>
<p>如果用户指定的新大小过大，底层存储系统无法满足，PVC 的扩展将不断重试，
直到用户或集群管理员采取一些措施。这种情况是不希望发生的，因此 Kubernetes
提供了以下从此类故障中恢复的方法。</p>
<ul class="nav nav-tabs" id="recovery-methods" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#recovery-methods-0" role="tab" aria-controls="recovery-methods-0" aria-selected="true">集群管理员手动处理</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#recovery-methods-1" role="tab" aria-controls="recovery-methods-1">通过请求扩展为更小尺寸</a></li></ul>
<div class="tab-content" id="recovery-methods"><div id="recovery-methods-0" class="tab-pane show active" role="tabpanel" aria-labelledby="recovery-methods-0">

<p><!--
If expanding underlying storage fails, the cluster administrator can manually recover the Persistent Volume Claim (PVC) state and cancel the resize requests. Otherwise, the resize requests are continuously retried by the controller without administrator intervention.
-->
<p>如果扩充下层存储的操作失败，集群管理员可以手动地恢复 PVC 申领的状态并
取消重设大小的请求。否则，在没有管理员干预的情况下，控制器会反复重试
重设大小的操作。</p>
<!--
1. Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC) with `Retain` reclaim policy.
2. Delete the PVC. Since PV has `Retain` reclaim policy - we will not lose any data when we recreate the PVC.
3. Delete the `claimRef` entry from PV specs, so as new PVC can bind to it. This should make the PV `Available`.
4. Re-create the PVC with smaller size than PV and set `volumeName` field of the PVC to the name of the PV. This should bind new PVC to existing PV.
5. Don't forget to restore the reclaim policy of the PV.
-->
<ol>
<li>将绑定到 PVC 申领的 PV 卷标记为 <code>Retain</code> 回收策略；</li>
<li>删除 PVC 对象。由于 PV 的回收策略为 <code>Retain</code>，我们不会在重建 PVC 时丢失数据。</li>
<li>删除 PV 规约中的 <code>claimRef</code> 项，这样新的 PVC 可以绑定到该卷。
这一操作会使得 PV 卷变为 &quot;可用（Available）&quot;。</li>
<li>使用小于 PV 卷大小的尺寸重建 PVC，设置 PVC 的 <code>volumeName</code> 字段为 PV 卷的名称。
这一操作将把新的 PVC 对象绑定到现有的 PV 卷。</li>
<li>不要忘记恢复 PV 卷上设置的回收策略。</li>
</ol>
</div>
  <div id="recovery-methods-1" class="tab-pane" role="tabpanel" aria-labelledby="recovery-methods-1">

<p><div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code>
</div>
<!--
Recovery from failing PVC expansion by users is available as an alpha feature since Kubernetes 1.23. The `RecoverVolumeExpansionFailure` feature must be enabled for this feature to work. Refer to the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) documentation for more information.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Kubernetes 从 1.23 版本开始将允许用户恢复失败的 PVC 扩展这一能力作为
alpha 特性支持。 <code>RecoverVolumeExpansionFailure</code> 必须被启用以允许使用此功能。
可参考<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
文档了解更多信息。
</div>
<!--
If the feature gates `ExpandPersistentVolumes` and `RecoverVolumeExpansionFailure` are both
enabled in your cluster, and expansion has failed for a PVC, you can retry expansion with a
smaller size than the previously requested value. To request a new expansion attempt with a
smaller proposed size, edit `.spec.resources` for that PVC and choose a value that is less than the
value you previously tried.
This is useful if expansion to a higher value did not succeed because of capacity constraint.
If that has happened, or you suspect that it might have, you can retry expansion by specifying a
size that is within the capacity limits of underlying storage provider. You can monitor status of resize operation by watching `.status.resizeStatus` and events on the PVC.
-->
<p>如果集群中的特性门控 <code>ExpandPersistentVolumes</code> 和 <code>RecoverVolumeExpansionFailure</code>
都已启用，在 PVC 的扩展发生失败时，你可以使用比先前请求的值更小的尺寸来重试扩展。
要使用一个更小的尺寸尝试请求新的扩展，请编辑该 PVC 的 <code>.spec.resources</code> 并选择
一个比你之前所尝试的值更小的值。
如果由于容量限制而无法成功扩展至更高的值，这将很有用。
如果发生了这种情况，或者你怀疑可能发生了这种情况，你可以通过指定一个在底层存储供应容量
限制内的尺寸来重试扩展。你可以通过查看 <code>.status.resizeStatus</code> 以及 PVC 上的事件
来监控调整大小操作的状态。</p>
<!--
Note that,
although you can specify a lower amount of storage than what was requested previously,
the new value must still be higher than `.status.capacity`.
Kubernetes does not support shrinking a PVC to less than its current size.
-->
<p>请注意，
尽管你可以指定比之前的请求更低的存储量，新值必须仍然高于 <code>.status.capacity</code>。
Kubernetes 不支持将 PVC 缩小到小于其当前的尺寸。</p>
</div></div>

<!--
## Types of Persistent Volumes

PersistentVolume types are implemented as plugins.  Kubernetes currently supports the following plugins:
-->
<h2 id="types-of-persistent-volumes">持久卷的类型    </h2>
<p>PV 持久卷是用插件的形式来实现的。Kubernetes 目前支持以下插件：</p>
<!--
* [`awsElasticBlockStore`](/docs/concepts/storage/volumes/#awselasticblockstore) - AWS Elastic Block Store (EBS)
* [`azureDisk`](/docs/concepts/storage/volumes/#azuredisk) - Azure Disk
* [`azureFile`](/docs/concepts/storage/volumes/#azurefile) - Azure File
* [`cephfs`](/docs/concepts/storage/volumes/#cephfs) - CephFS volume
* [`csi`](/docs/concepts/storage/volumes/#csi) - Container Storage Interface (CSI)
* [`fc`](/docs/concepts/storage/volumes/#fc) - Fibre Channel (FC) storage
* [`gcePersistentDisk`](/docs/concepts/storage/volumes/#gcepersistentdisk) - GCE Persistent Disk
* [`glusterfs`](/docs/concepts/storage/volumes/#glusterfs) - Glusterfs volume
* [`hostPath`](/docs/concepts/storage/volumes/#hostpath) - HostPath volume
  (for single node testing only; WILL NOT WORK in a multi-node cluster;
  consider using `local` volume instead)
* [`iscsi`](/docs/concepts/storage/volumes/#iscsi) - iSCSI (SCSI over IP) storage
* [`local`](/docs/concepts/storage/volumes/#local) - local storage devices
  mounted on nodes.
* [`nfs`](/docs/concepts/storage/volumes/#nfs) - Network File System (NFS) storage
* [`portworxVolume`](/docs/concepts/storage/volumes/#portworxvolume) - Portworx volume
* [`rbd`](/docs/concepts/storage/volumes/#rbd) - Rados Block Device (RBD) volume
* [`vsphereVolume`](/docs/concepts/storage/volumes/#vspherevolume) - vSphere VMDK volume
-->
<ul>
<li><a href="/zh/docs/concepts/storage/volumes/#awselasticblockstore"><code>awsElasticBlockStore</code></a> - AWS 弹性块存储（EBS）</li>
<li><a href="/zh/docs/concepts/storage/volumes/#azuredisk"><code>azureDisk</code></a> - Azure Disk</li>
<li><a href="/zh/docs/concepts/storage/volumes/#azurefile"><code>azureFile</code></a> - Azure File</li>
<li><a href="/zh/docs/concepts/storage/volumes/#cephfs"><code>cephfs</code></a> - CephFS volume</li>
<li><a href="/zh/docs/concepts/storage/volumes/#csi"><code>csi</code></a> - 容器存储接口 (CSI)</li>
<li><a href="/zh/docs/concepts/storage/volumes/#fc"><code>fc</code></a> - Fibre Channel (FC) 存储</li>
<li><a href="/zh/docs/concepts/storage/volumes/#gcepersistentdisk"><code>gcePersistentDisk</code></a> - GCE 持久化盘</li>
<li><a href="/zh/docs/concepts/storage/volumes/#glusterfs"><code>glusterfs</code></a> - Glusterfs 卷</li>
<li><a href="/zh/docs/concepts/storage/volumes/#hostpath"><code>hostPath</code></a> - HostPath 卷
（仅供单节点测试使用；不适用于多节点集群；
请尝试使用 <code>local</code> 卷作为替代）</li>
<li><a href="/zh/docs/concepts/storage/volumes/#iscsi"><code>iscsi</code></a> - iSCSI (SCSI over IP) 存储</li>
<li><a href="/zh/docs/concepts/storage/volumes/#local"><code>local</code></a> - 节点上挂载的本地存储设备</li>
<li><a href="/zh/docs/concepts/storage/volumes/#nfs"><code>nfs</code></a> - 网络文件系统 (NFS) 存储</li>
<li><a href="/zh/docs/concepts/storage/volumes/#portworxvolume"><code>portworxVolume</code></a> - Portworx 卷</li>
<li><a href="/zh/docs/concepts/storage/volumes/#rbd"><code>rbd</code></a> - Rados 块设备 (RBD) 卷</li>
<li><a href="/zh/docs/concepts/storage/volumes/#vspherevolume"><code>vsphereVolume</code></a> - vSphere VMDK 卷</li>
</ul>
<!-- 
The following types of PersistentVolume are deprecated. This means that support is still available but will be removed in a future Kubernetes release.

* [`cinder`](/docs/concepts/storage/volumes/#cinder) - Cinder (OpenStack block storage)
  (**deprecated** in v1.18)
* [`flexVolume`](/docs/concepts/storage/volumes/#flexvolume) - FlexVolume
  (**deprecated** in v1.23)
* [`flocker`](/docs/concepts/storage/volumes/#flocker) - Flocker storage
  (**deprecated** in v1.22)
* [`quobyte`](/docs/concepts/storage/volumes/#quobyte) - Quobyte volume
  (**deprecated** in v1.22)
* [`storageos`](/docs/concepts/storage/volumes/#storageos) - StorageOS volume
  (**deprecated** in v1.22)
-->
<p>以下的持久卷已被弃用。这意味着当前仍是支持的，但是 Kubernetes 将来的发行版会将其移除。</p>
<ul>
<li><a href="/docs/concepts/storage/volumes/#cinder"><code>cinder</code></a> - Cinder（OpenStack 块存储）（于 v1.18 <strong>弃用</strong>）</li>
<li><a href="/zh/docs/concepts/storage/volumes/#flexVolume"><code>flexVolume</code></a> - FlexVolume （于 v1.23 <strong>弃用</strong>）</li>
<li><a href="/docs/concepts/storage/volumes/#flocker"><code>flocker</code></a> - Flocker 存储（于 v1.22 <strong>弃用</strong>）</li>
<li><a href="/docs/concepts/storage/volumes/#quobyte"><code>quobyte</code></a> - Quobyte 卷
（于 v1.22 <strong>弃用</strong>）</li>
<li><a href="/docs/concepts/storage/volumes/#storageos"><code>storageos</code></a> - StorageOS 卷（于 v1.22 <strong>弃用</strong>）</li>
</ul>
<!-- 
Older versions of Kubernetes also supported the following in-tree PersistentVolume types:

* `photonPersistentDisk` - Photon controller persistent disk.
  (**not available** after v1.15)
* [`scaleIO`](/docs/concepts/storage/volumes/#scaleio) - ScaleIO volume
  (**not available** after v1.21)
-->
<p>旧版本的 Kubernetes 仍支持这些“树内（In-Tree）”持久卷类型：</p>
<ul>
<li><code>photonPersistentDisk</code> - Photon 控制器持久化盘。（v1.15 之后 <strong>不可用</strong>）</li>
<li><a href="/docs/concepts/storage/volumes/#scaleio"><code>scaleIO</code></a> - ScaleIO 卷（v1.21 之后 <strong>不可用</strong>）</li>
</ul>
<!--
## Persistent Volumes

Each PV contains a spec and status, which is the specification and status of the volume.
The name of a PersistentVolume object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<h2 id="persistent-volumes">持久卷   </h2>
<p>每个 PV 对象都包含 <code>spec</code> 部分和 <code>status</code> 部分，分别对应卷的规约和状态。
PersistentVolume 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pv0003<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>5Gi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">persistentVolumeReclaimPolicy</span>:<span style="color:#bbb"> </span>Recycle<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">mountOptions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- hard<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- nfsvers=4.1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nfs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/tmp<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">server</span>:<span style="color:#bbb"> </span><span style="color:#666">172.17.0.2</span><span style="color:#bbb">
</span></code></pre></div><!--
Helper programs relating to the volume type may be required for consumption of a PersistentVolume within a cluster.  In this example, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在集群中使用持久卷存储通常需要一些特定于具体卷类型的辅助程序。
在这个例子中，PersistentVolume 是 NFS 类型的，因此需要辅助程序 <code>/sbin/mount.nfs</code>
来支持挂载 NFS 文件系统。
</div>
<!--
### Capacity

Generally, a PV will have a specific storage capacity. This is set using the PV's `capacity` attribute. Read the glossary term [Quantity](/docs/reference/glossary/?all=true#term-quantity) to understand the units expected by `capacity`.

Currently, storage size is the only resource that can be set or requested.  Future attributes may include IOPS, throughput, etc.
-->
<h3 id="capacity">容量   </h3>
<p>一般而言，每个 PV 卷都有确定的存储容量。
容量属性是使用 PV 对象的 <code>capacity</code> 属性来设置的。
参考词汇表中的
<a href="/zh/docs/reference/glossary/?all=true#term-quantity">量纲（Quantity）</a>
词条，了解 <code>capacity</code> 字段可以接受的单位。</p>
<p>目前，存储大小是可以设置和请求的唯一资源。
未来可能会包含 IOPS、吞吐量等属性。</p>
<!--
### Volume Mode
-->
<h3 id="volume-mode">卷模式    </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>


<!--
Kubernetes supports two `volumeModes` of PersistentVolumes: `Filesystem` and `Block`.

`volumeMode` is an optional API parameter.
`Filesystem` is the default mode used when `volumeMode` parameter is omitted.

A volume with `volumeMode: Filesystem` is *mounted* into Pods into a directory. If the volume
is backed by a block device and the device is empty, Kubernetes creates a filesystem
on the device before mounting it for the first time.
-->
<p>针对 PV 持久卷，Kubernetes
支持两种卷模式（<code>volumeModes</code>）：<code>Filesystem（文件系统）</code> 和 <code>Block（块）</code>。
<code>volumeMode</code> 是一个可选的 API 参数。
如果该参数被省略，默认的卷模式是 <code>Filesystem</code>。</p>
<p><code>volumeMode</code> 属性设置为 <code>Filesystem</code> 的卷会被 Pod <em>挂载（Mount）</em> 到某个目录。
如果卷的存储来自某块设备而该设备目前为空，Kuberneretes 会在第一次挂载卷之前
在设备上创建文件系统。</p>
<!--
You can set the value of `volumeMode` to `Block` to use a volume as a raw block device.
Such volume is presented into a Pod as a block device, without any filesystem on it.
This mode is useful to provide a Pod the fastest possible way to access a volume, without
any filesystem layer between the Pod and the volume. On the other hand, the application
running in the Pod must know how to handle a raw block device.
See [Raw Block Volume Support](#raw-block-volume-support)
for an example on how to use a volume with `volumeMode: Block` in a Pod.
-->
<p>你可以将 <code>volumeMode</code> 设置为 <code>Block</code>，以便将卷作为原始块设备来使用。
这类卷以块设备的方式交给 Pod 使用，其上没有任何文件系统。
这种模式对于为 Pod 提供一种使用最快可能方式来访问卷而言很有帮助，Pod 和
卷之间不存在文件系统层。另外，Pod 中运行的应用必须知道如何处理原始块设备。
关于如何在 Pod 中使用 <code>volumeMode: Block</code> 的卷，可参阅
<a href="#raw-block-volume-support">原始块卷支持</a>。</p>
<!--
### Access Modes

A PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV's access modes are set to the specific modes supported by that particular volume.  For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV's capabilities.
-->
<h3 id="access-modes">访问模式  </h3>
<p>PersistentVolume 卷可以用资源提供者所支持的任何方式挂载到宿主系统上。
如下表所示，提供者（驱动）的能力不同，每个 PV 卷的访问模式都会设置为
对应卷所支持的模式值。
例如，NFS 可以支持多个读写客户，但是某个特定的 NFS PV 卷可能在服务器
上以只读的方式导出。每个 PV 卷都会获得自身的访问模式集合，描述的是
特定 PV 卷的能力。</p>
<!--
The access modes are:

`ReadWriteOnce` 
: the volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access the volume when the pods are running on the same node.

`ReadOnlyMany`
: the volume can be mounted as read-only by many nodes.

`ReadWriteMany`
: the volume can be mounted as read-write by many nodes.

 `ReadWriteOncePod`
: the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across whole cluster can read that PVC or write to it. This is only supported for CSI volumes and Kubernetes version 1.22+.

The blog article [Introducing Single Pod Access Mode for PersistentVolumes](/blog/2021/09/13/read-write-once-pod-access-mode-alpha/) covers this in more detail.
-->
<p>访问模式有：</p>
<dl>
<dt><code>ReadWriteOnce</code></dt>
<dd>卷可以被一个节点以读写方式挂载。
ReadWriteOnce 访问模式也允许运行在同一节点上的多个 Pod 访问卷。</dd>
<dt><code>ReadOnlyMany</code></dt>
<dd>卷可以被多个节点以只读方式挂载。</dd>
<dt><code>ReadWriteMany</code></dt>
<dd>卷可以被多个节点以读写方式挂载。</dd>
<dt><code>ReadWriteOncePod</code></dt>
<dd>卷可以被单个 Pod 以读写方式挂载。
如果你想确保整个集群中只有一个 Pod 可以读取或写入该 PVC，
请使用ReadWriteOncePod 访问模式。这只支持 CSI 卷以及需要 Kubernetes 1.22 以上版本。</dd>
</dl>
<p>这篇博客文章 <a href="/blog/2021/09/13/read-write-once-pod-access-mode-alpha/">Introducing Single Pod Access Mode for PersistentVolumes</a>
描述了更详细的内容。</p>
<!--
In the CLI, the access modes are abbreviated to:

* RWO - ReadWriteOnce
* ROX - ReadOnlyMany
* RWX - ReadWriteMany
* RWOP - ReadWriteOncePod
-->
<p>在命令行接口（CLI）中，访问模式也使用以下缩写形式：</p>
<ul>
<li>RWO - ReadWriteOnce</li>
<li>ROX - ReadOnlyMany</li>
<li>RWX - ReadWriteMany</li>
<li>RWOP - ReadWriteOncePod</li>
</ul>
<!--
> __Important!__ A volume can only be mounted using one access mode at a time, even if it supports many.  For example, a GCEPersistentDisk can be mounted as ReadWriteOnce by a single node or ReadOnlyMany by many nodes, but not at the same time.
-->
<blockquote>
<p><strong>重要提醒！</strong> 每个卷同一时刻只能以一种访问模式挂载，即使该卷能够支持
多种访问模式。例如，一个 GCEPersistentDisk 卷可以被某节点以 ReadWriteOnce
模式挂载，或者被多个节点以 ReadOnlyMany 模式挂载，但不可以同时以两种模式
挂载。</p>
</blockquote>
<!--
| Volume Plugin        | ReadWriteOnce          | ReadOnlyMany          | ReadWriteMany|
-->
<table>
<thead>
<tr>
<th style="text-align:left">卷插件</th>
<th style="text-align:center">ReadWriteOnce</th>
<th style="text-align:center">ReadOnlyMany</th>
<th style="text-align:center">ReadWriteMany</th>
<th>ReadWriteOncePod</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">AWSElasticBlockStore</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">AzureFile</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">AzureDisk</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">CephFS</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">Cinder</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">CSI</td>
<td style="text-align:center">取决于驱动</td>
<td style="text-align:center">取决于驱动</td>
<td style="text-align:center">取决于驱动</td>
<td>取决于驱动</td>
</tr>
<tr>
<td style="text-align:left">FC</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">FlexVolume</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">取决于驱动</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">Flocker</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">GCEPersistentDisk</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">Glusterfs</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">HostPath</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">iSCSI</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">Quobyte</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">NFS</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">RBD</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">VsphereVolume</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td style="text-align:center">- （Pod 运行于同一节点上时可行）</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">PortworxVolume</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✓</td>
<td>-</td>
</tr>
<tr>
<td style="text-align:left">StorageOS</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td>-</td>
</tr>
</tbody>
</table>
<!--
### Class

A PV can have a class, which is specified by setting the
`storageClassName` attribute to the name of a
[StorageClass](/docs/concepts/storage/storage-classes/).
A PV of a particular class can only be bound to PVCs requesting
that class. A PV with no `storageClassName` has no class and can only be bound
to PVCs that request no particular class.
-->
<h3 id="class">类   </h3>
<p>每个 PV 可以属于某个类（Class），通过将其 <code>storageClassName</code> 属性设置为某个
<a href="/zh/docs/concepts/storage/storage-classes/">StorageClass</a> 的名称来指定。
特定类的 PV 卷只能绑定到请求该类存储卷的 PVC 申领。
未设置 <code>storageClassName</code> 的 PV 卷没有类设定，只能绑定到那些没有指定特定
存储类的 PVC 申领。</p>
<!--
In the past, the annotation `volume.beta.kubernetes.io/storage-class` was used instead
of the `storageClassName` attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.
-->
<p>早前，Kubernetes 使用注解 <code>volume.beta.kubernetes.io/storage-class</code> 而不是
<code>storageClassName</code> 属性。这一注解目前仍然起作用，不过在将来的 Kubernetes
发布版本中该注解会被彻底废弃。</p>
<!--
### Reclaim Policy

Current reclaim policies are:

* Retain -- manual reclamation
* Recycle -- basic scrub (`rm -rf /thevolume/*`)
* Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted

Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk, and Cinder volumes support deletion.
-->
<h3 id="reclaim-policy">回收策略  </h3>
<p>目前的回收策略有：</p>
<ul>
<li>Retain -- 手动回收</li>
<li>Recycle -- 基本擦除   (<code>rm -rf /thevolume/*</code>)</li>
<li>Delete -- 诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除</li>
</ul>
<p>目前，仅 NFS 和 HostPath 支持回收（Recycle）。
AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除（Delete）。</p>
<!--
### Mount Options

A Kubernetes administrator can specify additional mount options for when a Persistent Volume is mounted on a node.
-->
<h3 id="mount-options">挂载选项   </h3>
<p>Kubernetes 管理员可以指定持久卷被挂载到节点上时使用的附加挂载选项。</p>
<!--
Not all Persistent Volume types support mount options.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 并非所有持久卷类型都支持挂载选项。
</div>
<!--
The following volume types support mount options:
-->
<p>以下卷类型支持挂载选项：</p>
<ul>
<li><code>awsElasticBlockStore</code></li>
<li><code>azureDisk</code></li>
<li><code>azureFile</code></li>
<li><code>cephfs</code></li>
<li><code>cinder</code> (<strong>已弃用</strong>于 v1.18)</li>
<li><code>gcePersistentDisk</code></li>
<li><code>glusterfs</code></li>
<li><code>iscsi</code></li>
<li><code>nfs</code></li>
<li><code>quobyte</code> (<strong>已弃用</strong>于 v1.22)</li>
<li><code>rbd</code></li>
<li><code>storageos</code> (<strong>已弃用</strong>于 v1.22)</li>
<li><code>vsphereVolume</code></li>
</ul>
<!--
Mount options are not validated, If a mount option is invalid, the mount fails.
-->
<p>Kubernetes 不对挂载选项执行合法性检查。如果挂载选项是非法的，挂载就会失败。</p>
<!--
In the past, the annotation `volume.beta.kubernetes.io/mount-options` was used instead
of the `mountOptions` attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.
-->
<p>早前，Kubernetes 使用注解 <code>volume.beta.kubernetes.io/mount-options</code> 而不是
<code>mountOptions</code> 属性。这一注解目前仍然起作用，不过在将来的 Kubernetes
发布版本中该注解会被彻底废弃。</p>
<!--
### Node Affinity
-->
<h3 id="node-affinity">节点亲和性  </h3>
<!--
A PV can specify node affinity to define constraints that limit what nodes this volume can be accessed from. Pods that use a PV will only be scheduled to nodes that are selected by the node affinity. To specify node affinity, set `nodeAffinity` in the `.spec` of a PV. The [PersistentVolume](/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/#PersistentVolumeSpec) API reference has more details on this field.
-->
<p>每个 PV 卷可以通过设置节点亲和性来定义一些约束，进而限制从哪些节点上可以访问此卷。
使用这些卷的 Pod 只会被调度到节点亲和性规则所选择的节点上执行。
要设置节点亲和性，配置 PV 卷 <code>.spec</code> 中的 <code>nodeAffinity</code>。
<a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/#PersistentVolumeSpec">持久卷</a>
API 参考关于该字段的更多细节。</p>
<!--
For most volume types, you do not need to set this field. It is automatically populated for [AWS EBS](/docs/concepts/storage/volumes/#awselasticblockstore), [GCE PD](/docs/concepts/storage/volumes/#gcepersistentdisk) and [Azure Disk](/docs/concepts/storage/volumes/#azuredisk) volume block types. You need to explicitly set this for [local](/docs/concepts/storage/volumes/#local) volumes.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 对大多数类型的卷而言，你不需要设置节点亲和性字段。
<a href="/zh/docs/concepts/storage/volumes/#awselasticblockstore">AWS EBS</a>、
<a href="/zh/docs/concepts/storage/volumes/#gcepersistentdisk">GCE PD</a> 和
<a href="/zh/docs/concepts/storage/volumes/#azuredisk">Azure Disk</a> 卷类型都能
自动设置相关字段。
你需要为 <a href="/zh/docs/concepts/storage/volumes/#local">local</a> 卷显式地设置
此属性。
</div>
<!--
### Phase

A volume will be in one of the following phases:

* Available -- a free resource that is not yet bound to a claim
* Bound -- the volume is bound to a claim
* Released -- the claim has been deleted, but the resource is not yet reclaimed by the cluster
* Failed -- the volume has failed its automatic reclamation

The CLI will show the name of the PVC bound to the PV.
-->
<h3 id="phase">阶段  </h3>
<p>每个卷会处于以下阶段（Phase）之一：</p>
<ul>
<li>Available（可用）-- 卷是一个空闲资源，尚未绑定到任何申领；</li>
<li>Bound（已绑定）-- 该卷已经绑定到某申领；</li>
<li>Released（已释放）-- 所绑定的申领已被删除，但是资源尚未被集群回收；</li>
<li>Failed（失败）-- 卷的自动回收操作失败。</li>
</ul>
<p>命令行接口能够显示绑定到某 PV 卷的 PVC 对象。</p>
<h2 id="persistentvolumeclaims">PersistentVolumeClaims</h2>
<!--
Each PVC contains a spec and status, which is the specification and status of the claim.
The name of a PersistentVolumeClaim object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>每个 PVC 对象都有 <code>spec</code> 和 <code>status</code> 部分，分别对应申领的规约和状态。
PersistentVolumeClaim 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myclaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>8Gi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">release</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;stable&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- {<span style="color:#008000;font-weight:bold">key: environment, operator: In, values</span>:<span style="color:#bbb"> </span>[dev]}<span style="color:#bbb">
</span></code></pre></div><!--
### Access Modes

Claims use [the same conventions as volumes](#access-modes) when requesting storage with specific access modes.
-->
<h3 id="access-modes">访问模式</h3>
<p>申领在请求具有特定访问模式的存储时，使用与卷相同的<a href="#access-modes">访问模式约定</a>。</p>
<!--
### Volume Modes

Claims use [the same convention as volumes](#volume-mode) to indicate the consumption of the volume as either a filesystem or block device.
-->
<h3 id="volume-modes">卷模式</h3>
<p>申领使用<a href="#access-modes">与卷相同的约定</a>来表明是将卷作为文件系统还是块设备来使用。</p>
<!--
### Resources

Claims, like Pods, can request specific quantities of a resource. In this case, the request is for storage. The same [resource model](https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md) applies to both volumes and claims.
-->
<h3 id="resources">资源   </h3>
<p>申领和 Pod 一样，也可以请求特定数量的资源。在这个上下文中，请求的资源是存储。
卷和申领都使用相同的
<a href="https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md">资源模型</a>。</p>
<!--
### Selector

Claims can specify a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors) to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim. The selector can consist of two fields:
-->
<h3 id="selector">选择算符   </h3>
<p>申领可以设置<a href="/zh/docs/concepts/overview/working-with-objects/labels/#label-selectors">标签选择算符</a>
来进一步过滤卷集合。只有标签与选择算符相匹配的卷能够绑定到申领上。
选择算符包含两个字段：</p>
<!--
* `matchLabels` - the volume must have a label with this value
* `matchExpressions` - a list of requirements made by specifying key, list of values, and operator that relates the key and values. Valid operators include In, NotIn, Exists, and DoesNotExist.

All of the requirements, from both `matchLabels` and `matchExpressions`, are ANDed together – they must all be satisfied in order to match.
-->
<ul>
<li><code>matchLabels</code> - 卷必须包含带有此值的标签</li>
<li><code>matchExpressions</code> - 通过设定键（key）、值列表和操作符（operator）
来构造的需求。合法的操作符有 In、NotIn、Exists 和 DoesNotExist。</li>
</ul>
<p>来自 <code>matchLabels</code> 和 <code>matchExpressions</code> 的所有需求都按逻辑与的方式组合在一起。
这些需求都必须被满足才被视为匹配。</p>
<!--
### Class

A claim can request a particular class by specifying the name of a
[StorageClass](/docs/concepts/storage/storage-classes/)
using the attribute `storageClassName`.
Only PVs of the requested class, ones with the same `storageClassName` as the PVC, can
be bound to the PVC.
-->
<h3 id="class">类     </h3>
<p>申领可以通过为 <code>storageClassName</code> 属性设置
<a href="/zh/docs/concepts/storage/storage-classes/">StorageClass</a> 的名称来请求特定的存储类。
只有所请求的类的 PV 卷，即 <code>storageClassName</code> 值与 PVC 设置相同的 PV 卷，
才能绑定到 PVC 申领。</p>
<!--
PVCs don't necessarily have to request a class. A PVC with its `storageClassName` set
equal to `""` is always interpreted to be requesting a PV with no class, so it
can only be bound to PVs with no class (no annotation or one set equal to
`""`). A PVC with no `storageClassName` is not quite the same and is treated differently
by the cluster, depending on whether the
[`DefaultStorageClass` admission plugin](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
is turned on.
-->
<p>PVC 申领不必一定要请求某个类。如果 PVC 的 <code>storageClassName</code> 属性值设置为 <code>&quot;&quot;</code>，
则被视为要请求的是没有设置存储类的 PV 卷，因此这一 PVC 申领只能绑定到未设置
存储类的 PV 卷（未设置注解或者注解值为 <code>&quot;&quot;</code> 的 PersistentVolume（PV）对象在系统中不会被删除，因为这样做可能会引起数据丢失。
未设置 <code>storageClassName</code> 的 PVC 与此大不相同，也会被集群作不同处理。
具体筛查方式取决于
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass"><code>DefaultStorageClass</code> 准入控制器插件</a>
是否被启用。</p>
<!--
* If the admission plugin is turned on, the administrator may specify a
  default StorageClass. All PVCs that have no `storageClassName` can be bound only to
  PVs of that default. Specifying a default StorageClass is done by setting the
  annotation `storageclass.kubernetes.io/is-default-class` equal to `true` in
  a StorageClass object. If the administrator does not specify a default, the
  cluster responds to PVC creation as if the admission plugin were turned off. If
  more than one default is specified, the admission plugin forbids the creation of
  all PVCs.
* If the admission plugin is turned off, there is no notion of a default
  StorageClass. All PVCs that have no `storageClassName` can be bound only to PVs that
  have no class. In this case, the PVCs that have no `storageClassName` are treated the
  same way as PVCs that have their `storageClassName` set to `""`.
-->
<ul>
<li>如果准入控制器插件被启用，则管理员可以设置一个默认的 StorageClass。
所有未设置 <code>storageClassName</code> 的 PVC 都只能绑定到隶属于默认存储类的 PV 卷。
设置默认 StorageClass 的工作是通过将对应 StorageClass 对象的注解
<code>storageclass.kubernetes.io/is-default-class</code> 赋值为 <code>true</code> 来完成的。
如果管理员未设置默认存储类，集群对 PVC 创建的处理方式与未启用准入控制器插件
时相同。如果设定的默认存储类不止一个，准入控制插件会禁止所有创建 PVC 操作。</li>
<li>如果准入控制器插件被关闭，则不存在默认 StorageClass 的说法。
所有未设置 <code>storageClassName</code> 的 PVC 都只能绑定到未设置存储类的 PV 卷。
在这种情况下，未设置 <code>storageClassName</code> 的 PVC 与 <code>storageClassName</code> 设置未
<code>&quot;&quot;</code> 的 PVC 的处理方式相同。</li>
</ul>
<!--
Depending on installation method, a default StorageClass may be deployed
to a Kubernetes cluster by addon manager during installation.

When a PVC specifies a `selector` in addition to requesting a StorageClass,
the requirements are ANDed together: only a PV of the requested class and with
the requested labels may be bound to the PVC.
-->
<p>取决于安装方法，默认的 StorageClass 可能在集群安装期间由插件管理器（Addon
Manager）部署到集群中。</p>
<p>当某 PVC 除了请求 StorageClass 之外还设置了 <code>selector</code>，则这两种需求会按
逻辑与关系处理：只有隶属于所请求类且带有所请求标签的 PV 才能绑定到 PVC。</p>
<!--
Currently, a PVC with a non-empty `selector` can't have a PV dynamically provisioned for it.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 目前，设置了非空 <code>selector</code> 的 PVC 对象无法让集群为其动态供应 PV 卷。
</div>
<!--
In the past, the annotation `volume.beta.kubernetes.io/storage-class` was used instead
of `storageClassName` attribute. This annotation is still working; however,
it won't be supported in a future Kubernetes release.
-->
<p>早前，Kubernetes 使用注解 <code>volume.beta.kubernetes.io/storage-class</code> 而不是
<code>storageClassName</code> 属性。这一注解目前仍然起作用，不过在将来的 Kubernetes
发布版本中该注解会被彻底废弃。</p>
<!--
## Claims As Volumes

Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod's namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.
-->
<h2 id="claims-as-volumes">使用申领作为卷    </h2>
<p>Pod 将申领作为卷来使用，并藉此访问存储资源。
申领必须位于使用它的 Pod 所在的同一名字空间内。
集群在 Pod 的名字空间中查找申领，并使用它来获得申领所使用的 PV 卷。
之后，卷会被挂载到宿主上并挂载到 Pod 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myfrontend<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/var/www/html&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypd<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypd<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>myclaim<span style="color:#bbb">
</span></code></pre></div><!--
### A Note on Namespaces

PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with "Many" modes (`ROX`, `RWX`) is only possible within one namespace.
-->
<h3 id="a-note-on-namespaces">关于名字空间的说明   </h3>
<p>PersistentVolume 卷的绑定是排他性的。
由于 PersistentVolumeClaim 是名字空间作用域的对象，使用
&quot;Many&quot; 模式（<code>ROX</code>、<code>RWX</code>）来挂载申领的操作只能在同一名字空间内进行。</p>
<!--
### PersistentVolumes typed `hostPath`

A `hostPath` PersistentVolume uses a file or directory on the Node to emulate network-attached storage.
See [an example of `hostPath` typed volume](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).
-->
<h3 id="persistentvolumes-typed-hostpath">类型为 <code>hostpath</code> 的 PersistentVolume </h3>
<p><code>hostPath</code> PersistentVolume 使用节点上的文件或目录来模拟网络附加（network-attached）存储。
相关细节可参阅<a href="/zh/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume"><code>hostPath</code> 卷示例</a>。</p>
<!--
## Raw Block Volume Support
-->
<h2 id="raw-block-volume-support">原始块卷支持  </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>


<!--
The following volume plugins support raw block volumes, including dynamic provisioning where
applicable:
-->
<p>以下卷插件支持原始块卷，包括其动态供应（如果支持的话）的卷：</p>
<ul>
<li>AWSElasticBlockStore</li>
<li>AzureDisk</li>
<li>CSI</li>
<li>FC （光纤通道）</li>
<li>GCEPersistentDisk</li>
<li>iSCSI</li>
<li>Local 卷</li>
<li>OpenStack Cinder</li>
<li>RBD （Ceph 块设备）</li>
<li>VsphereVolume</li>
</ul>
<!--
### PersistentVolume using a Raw Block Volume {#persistent-volume-using-a-raw-block-volume}
-->
<h3 id="persistent-volume-using-a-raw-block-volume">使用原始块卷的持久卷     </h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>block-pv<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeMode</span>:<span style="color:#bbb"> </span>Block<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">persistentVolumeReclaimPolicy</span>:<span style="color:#bbb"> </span>Retain<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">fc</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetWWNs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;50060e801049cfd1&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">lun</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span></code></pre></div><!--
### PersistentVolumeClaim requesting a Raw Block Volume {#persistent-volume-claim-requesting-a-raw-block-volume}
-->
<h3 id="persistent-volume-claim-requesting-a-raw-block-volume">申请原始块卷的 PVC 申领     </h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>block-pvc<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeMode</span>:<span style="color:#bbb"> </span>Block<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></code></pre></div><!--

## Volume populators and data sources

Kubernetes supports custom volume populators; this alpha feature was introduced
in Kubernetes 1.18. Kubernetes 1.22 reimplemented the mechanism with a redesigned API.
Check that you are reading the version of the Kubernetes documentation that matches your
cluster. 
 To check the version, enter <code>kubectl version</code>.

To use custom volume populators, you must enable the `AnyVolumeDataSource`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for
the kube-apiserver and kube-controller-manager.

Volume populators take advantage of a PVC spec field called `dataSourceRef`. Unlike the
`dataSource` field, which can only contain either a reference to another PersistentVolumeClaim
or to a VolumeSnapshot, the `dataSourceRef` field can contain a reference to any object in the
same namespace, except for core objects other than PVCs. For clusters that have the feature
gate enabled, use of the `dataSourceRef` is preferred over `dataSource`.
-->
<h2 id="volume-populators-and-data-sources">卷填充器（Populator）与数据源     </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>Kubernetes 支持自定义的卷填充器；Kubernetes 1.18 版本引入了这个 alpha 特性。
Kubernetes 1.22 使用重新设计的 API 重新实现了该机制。
确认你正在阅读与你的集群版本一致的 Kubernetes 文档。
To check the version, enter <code>kubectl version</code>.</p>
<p>要使用自定义的卷填充器，你必须为 kube-apiserver 和 kube-controller-manager 启用 <code>AnyVolumeDataSource</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。</p>

</div>
<p>卷填充器利用了 PVC 规约字段 <code>dataSourceRef</code>。
不像 <code>dataSource</code> 字段只能包含对另一个持久卷申领或卷快照的引用，
<code>dataSourceRef</code> 字段可以包含对同一命名空间中任何对象的引用（不包含除 PVC 以外的核心资源）。
对于启用了特性门控的集群，使用 <code>dataSourceRef</code> 比 <code>dataSource</code> 更好。</p>
<!--
## Data source references

The `dataSourceRef` field behaves almost the same as the `dataSource` field. If either one is
specified while the other is not, the API server will give both fields the same value. Neither
field can be changed after creation, and attempting to specify different values for the two
fields will result in a validation error. Therefore the two fields will always have the same
contents.
-->
<h2 id="data-source-references">数据源引用  </h2>
<p><code>dataSourceRef</code> 字段的行为与 <code>dataSource</code> 字段几乎相同。
如果其中一个字段被指定而另一个字段没有被指定，API 服务器将给两个字段相同的值。
这两个字段都不能在创建后改变，如果试图为这两个字段指定不同的值，将导致验证错误。
因此，这两个字段将总是有相同的内容。</p>
<!--
There are two differences between the `dataSourceRef` field and the `dataSource` field that
users should be aware of:
* The `dataSource` field ignores invalid values (as if the field was blank) while the
  `dataSourceRef` field never ignores values and will cause an error if an invalid value is
  used. Invalid values are any core object (objects with no apiGroup) except for PVCs.
* The `dataSourceRef` field may contain different types of objects, while the `dataSource` field
  only allows PVCs and VolumeSnapshots.

Users should always use `dataSourceRef` on clusters that have the feature gate enabled, and
fall back to `dataSource` on clusters that do not. It is not necessary to look at both fields
under any circumstance. The duplicated values with slightly different semantics exist only for
backwards compatibility. In particular, a mixture of older and newer controllers are able to
interoperate because the fields are the same.
-->
<p>在 <code>dataSourceRef</code> 字段和 <code>dataSource</code> 字段之间有两个用户应该注意的区别：</p>
<ul>
<li><code>dataSource</code> 字段会忽略无效的值（如同是空值），
而 <code>dataSourceRef</code> 字段永远不会忽略值，并且若填入一个无效的值，会导致错误。
无效值指的是 PVC 之外的核心对象（没有 apiGroup 的对象）。</li>
<li><code>dataSourceRef</code> 字段可以包含不同类型的对象，而 <code>dataSource</code> 字段只允许 PVC 和卷快照。</li>
</ul>
<p>用户应该始终在启用了特性门控的集群上使用 <code>dataSourceRef</code>，而在没有启用特性门控的集群上使用 <code>dataSource</code>。
在任何情况下都没有必要查看这两个字段。
这两个字段的值看似相同但是语义稍微不一样，是为了向后兼容。
特别是混用旧版本和新版本的控制器时，它们能够互通。</p>
<!--
### Using volume populators

Volume populators are <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controllers'>controllers</a> that can
create non-empty volumes, where the contents of the volume are determined by a Custom Resource.
Users create a populated volume by referring to a Custom Resource using the `dataSourceRef` field:
-->
<h2 id="using-volume-populators">使用卷填充器  </h2>
<p>卷填充器是能创建非空卷的<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>，
其卷的内容通过一个自定义资源决定。
用户通过使用 <code>dataSourceRef</code> 字段引用自定义资源来创建一个被填充的卷：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>populated-pvc<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">dataSourceRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-name<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ExampleDataSource<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>example.storage.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></code></pre></div><!--
Because volume populators are external components, attempts to create a PVC that uses one
can fail if not all the correct components are installed. External controllers should generate
events on the PVC to provide feedback on the status of the creation, including warnings if
the PVC cannot be created due to some missing component.

You can install the alpha [volume data source validator](https://github.com/kubernetes-csi/volume-data-source-validator)
controller into your cluster. That controller generates warning Events on a PVC in the case that no populator
is registered to handle that kind of data source. When a suitable populator is installed for a PVC, it's the
responsibility of that populator controller to report Events that relate to volume creation and issues during
the process.
-->
<p>因为卷填充器是外部组件，如果没有安装所有正确的组件，试图创建一个使用卷填充器的 PVC 就会失败。
外部控制器应该在 PVC 上产生事件，以提供创建状态的反馈，包括在由于缺少某些组件而无法创建 PVC 的情况下发出警告。</p>
<p>你可以把 alpha 版本的<a href="https://github.com/kubernetes-csi/volume-data-source-validator">卷数据源验证器</a>
控制器安装到你的集群中。
如果没有填充器处理该数据源的情况下，该控制器会在 PVC 上产生警告事件。
当一个合适的填充器被安装到 PVC 上时，该控制器的职责是上报与卷创建有关的事件，以及在该过程中发生的问题。</p>
<!--
### Pod specification adding Raw Block Device path in container
-->
<h3 id="在容器中添加原始块设备路径的-pod-规约">在容器中添加原始块设备路径的 Pod 规约</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod-with-block-volume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fc-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>fedora:26<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;tail -f /dev/null&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeDevices</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">devicePath</span>:<span style="color:#bbb"> </span>/dev/xvda<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>block-pvc<span style="color:#bbb">
</span></code></pre></div><!--
When adding a raw block device for a Pod, you specify the device path in the container instead of a mount path.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 向 Pod 中添加原始块设备时，你要在容器内设置设备路径而不是挂载路径。
</div>
<!--
### Binding Block Volumes

If a user requests a raw block volume by indicating this using the `volumeMode` field in the PersistentVolumeClaim spec, the binding rules differ slightly from previous releases that didn't consider this mode as part of the spec.
Listed is a table of possible combinations the user and admin might specify for requesting a raw block device. The table indicates if the volume will be bound or not given the combinations:
Volume binding matrix for statically provisioned volumes:
-->
<h3 id="binding-block-volumes">绑定块卷    </h3>
<p>如果用户通过 PersistentVolumeClaim 规约的 <code>volumeMode</code> 字段来表明对原始
块设备的请求，绑定规则与之前版本中未在规约中考虑此模式的实现略有不同。
下面列举的表格是用户和管理员可以为请求原始块设备所作设置的组合。
此表格表明在不同的组合下卷是否会被绑定。</p>
<p>静态供应卷的卷绑定矩阵：</p>
<!--
| PV volumeMode | PVC volumeMode  | Result           |
| --------------|:---------------:| ----------------:|
|   unspecified | unspecified     | BIND             |
|   unspecified | Block           | NO BIND          |
|   unspecified | Filesystem      | BIND             |
|   Block       | unspecified     | NO BIND          |
|   Block       | Block           | BIND             |
|   Block       | Filesystem      | NO BIND          |
|   Filesystem  | Filesystem      | BIND             |
|   Filesystem  | Block           | NO BIND          |
|   Filesystem  | unspecified     | BIND             |
-->
<table>
<thead>
<tr>
<th>PV volumeMode</th>
<th style="text-align:center">PVC volumeMode</th>
<th style="text-align:right">Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>未指定</td>
<td style="text-align:center">未指定</td>
<td style="text-align:right">绑定</td>
</tr>
<tr>
<td>未指定</td>
<td style="text-align:center">Block</td>
<td style="text-align:right">不绑定</td>
</tr>
<tr>
<td>未指定</td>
<td style="text-align:center">Filesystem</td>
<td style="text-align:right">绑定</td>
</tr>
<tr>
<td>Block</td>
<td style="text-align:center">未指定</td>
<td style="text-align:right">不绑定</td>
</tr>
<tr>
<td>Block</td>
<td style="text-align:center">Block</td>
<td style="text-align:right">绑定</td>
</tr>
<tr>
<td>Block</td>
<td style="text-align:center">Filesystem</td>
<td style="text-align:right">不绑定</td>
</tr>
<tr>
<td>Filesystem</td>
<td style="text-align:center">Filesystem</td>
<td style="text-align:right">绑定</td>
</tr>
<tr>
<td>Filesystem</td>
<td style="text-align:center">Block</td>
<td style="text-align:right">不绑定</td>
</tr>
<tr>
<td>Filesystem</td>
<td style="text-align:center">未指定</td>
<td style="text-align:right">绑定</td>
</tr>
</tbody>
</table>
<!--
Only statically provisioned volumes are supported for alpha release. Administrators should take care to consider these values when working with raw block devices.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Alpha 发行版本中仅支持静态供应的卷。
管理员需要在处理原始块设备时小心处理这些值。
</div>
<!--
## Volume Snapshot and Restore Volume from Snapshot Support
-->
<h2 id="对卷快照及从卷快照中恢复卷的支持">对卷快照及从卷快照中恢复卷的支持</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [beta]</code>
</div>


<!--
Volume snapshot feature was added to support CSI Volume Plugins only. For details, see [volume snapshots](/docs/concepts/storage/volume-snapshots/).

To enable support for restoring a volume from a volume snapshot data source, enable the
`VolumeSnapshotDataSource` feature gate on the apiserver and controller-manager.
-->
<p>卷快照（Volume Snapshot）功能的添加仅是为了支持 CSI 卷插件。
有关细节可参阅<a href="/zh/docs/concepts/storage/volume-snapshots/">卷快照</a>文档。</p>
<p>要启用从卷快照数据源恢复数据卷的支持，可在 API 服务器和控制器管理器上启用
<code>VolumeSnapshotDataSource</code> 特性门控。</p>
<!--
### Create a PersistentVolumeClaim from a Volume Snapshot {#create-persistent-volume-claim-from-volume-snapshot}
-->
<h3 id="create-persistent-volume-claim-from-volume-snapshot">基于卷快照创建 PVC 申领    </h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>restore-pvc<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>csi-hostpath-sc<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">dataSource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>new-snapshot-test<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshot<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></code></pre></div><!--
## Volume Cloning

[Volume Cloning](/docs/concepts/storage/volume-pvc-datasource/) only available for CSI volume plugins.
-->
<h2 id="volume-cloning">卷克隆    </h2>
<p><a href="/zh/docs/concepts/storage/volume-pvc-datasource/">卷克隆</a>功能特性仅适用于
CSI 卷插件。</p>
<!--
### Create PersistentVolumeClaim from an existing PVC {#create-persistent-volume-claim-from-an-existing-pvc}
-->
<h3 id="create-persistent-volume-claim-from-an-existing-pvc">基于现有 PVC 创建新的 PVC 申领   </h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloned-pvc<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>my-csi-plugin<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">dataSource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>existing-src-pvc-name<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></code></pre></div><!--
## Writing Portable Configuration

If you're writing configuration templates or examples that run on a wide range of clusters
and need persistent storage, it is recommended that you use the following pattern:
-->
<h2 id="writing-portable-configuration">编写可移植的配置  </h2>
<p>如果你要编写配置模板和示例用来在很多集群上运行并且需要持久性存储，建议你使用以下模式：</p>
<!--
- Include PersistentVolumeClaim objects in your bundle of config (alongside
  Deployments, ConfigMaps, etc).
- Do not include PersistentVolume objects in the config, since the user instantiating
  the config may not have permission to create PersistentVolumes.
-->
<ul>
<li>将 PersistentVolumeClaim 对象包含到你的配置包（Bundle）中，和 Deployment
以及 ConfigMap 等放在一起。</li>
<li>不要在配置中包含 PersistentVolume 对象，因为对配置进行实例化的用户很可能
没有创建 PersistentVolume 的权限。</li>
</ul>
<!--
- Give the user the option of providing a storage class name when instantiating
  the template.
  - If the user provides a storage class name, put that value into the
    `persistentVolumeClaim.storageClassName` field.
    This will cause the PVC to match the right storage
    class if the cluster has StorageClasses enabled by the admin.
  - If the user does not provide a storage class name, leave the
    `persistentVolumeClaim.storageClassName` field as nil. This will cause a
    PV to be automatically provisioned for the user with the default StorageClass
    in the cluster. Many cluster environments have a default StorageClass installed,
    or administrators can create their own default StorageClass.
-->
<ul>
<li>为用户提供在实例化模板时指定存储类名称的能力。
<ul>
<li>仍按用户提供存储类名称，将该名称放到 <code>persistentVolumeClaim.storageClassName</code> 字段中。
这样会使得 PVC 在集群被管理员启用了存储类支持时能够匹配到正确的存储类，</li>
<li>如果用户未指定存储类名称，将 <code>persistentVolumeClaim.storageClassName</code> 留空（nil）。
这样，集群会使用默认 <code>StorageClass</code> 为用户自动供应一个存储卷。
很多集群环境都配置了默认的 <code>StorageClass</code>，或者管理员也可以自行创建默认的
<code>StorageClass</code>。</li>
</ul>
</li>
</ul>
<!--
- In your tooling, watch for PVCs that are not getting bound after some time
  and surface this to the user, as this may indicate that the cluster has no
  dynamic storage support (in which case the user should create a matching PV)
  or the cluster has no storage system (in which case the user cannot deploy
  config requiring PVCs).
-->
<ul>
<li>在你的工具链中，监测经过一段时间后仍未被绑定的 PVC 对象，要让用户知道这些对象，
因为这可能意味着集群不支持动态存储（因而用户必须先创建一个匹配的 PV），或者
集群没有配置存储系统（因而用户无法配置需要 PVC 的工作负载配置）。</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [Creating a PersistentVolume](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).
* Learn more about [Creating a PersistentVolumeClaim](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim).
* Read the [Persistent Storage design document](https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md).
-->
<ul>
<li>进一步了解<a href="/zh/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume">创建持久卷</a>.</li>
<li>进一步学习<a href="/zh/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim">创建 PVC 申领</a>.</li>
<li>阅读<a href="https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md">持久存储的设计文档</a>.</li>
</ul>
<!--
### API references {#reference}

Read about the APIs described in this page:

* [`PersistentVolume`](/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/)
* [`PersistentVolumeClaim`](/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/)
-->
<h3 id="reference">API 参考   </h3>
<p>阅读以下页面中描述的 API：</p>
<ul>
<li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/"><code>PersistentVolume</code></a></li>
<li><a href="/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/"><code>PersistentVolumeClaim</code></a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2db414b26d4daec3ebed19dd837830c3">6.3 - 投射卷</h1>
    
	<!--
reviewers:
- marosset
- jsturtevant
- zshihang
title: Projected Volumes
content_type: concept
weight: 21 # just after persistent volumes
-->
<!-- overview -->
<!--
This document describes _projected volumes_ in Kubernetes. Familiarity with [volumes](/docs/concepts/storage/volumes/) is suggested.
-->
<p>本文档描述 Kubernet 中的<em>投射卷（Projected Volumes）</em>。
建议先熟悉<a href="/zh/docs/concepts/storage/volumes/">卷</a>概念。</p>
<!-- body -->
<!--
## Introduction

A `projected` volume maps several existing volume sources into the same directory.

Currently, the following types of volume sources can be projected:

* [`secret`](/docs/concepts/storage/volumes/#secret)
* [`downwardAPI`](/docs/concepts/storage/volumes/#downwardapi)
* [`configMap`](/docs/concepts/storage/volumes/#configmap)
* [`serviceAccountToken`](#serviceaccounttoken)
-->
<h2 id="introduction">介绍   </h2>
<p>一个 <code>projected</code> 卷可以将若干现有的卷源映射到同一个目录之上。</p>
<p>目前，以下类型的卷源可以被投射：</p>
<ul>
<li><a href="/zh/docs/concepts/storage/volumes/#secret"><code>secret</code></a></li>
<li><a href="/zh/docs/concepts/storage/volumes/#downwardapi"><code>downwardAPI</code></a></li>
<li><a href="/zh/docs/concepts/storage/volumes/#configmap"><code>configMap</code></a></li>
<li><a href="#serviceaccounttoken"><code>serviceAccountToken</code></a></li>
</ul>
<!--
All sources are required to be in the same namespace as the Pod. For more details,
see the [all-in-one volume design document](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/all-in-one-volume.md).
-->
<p>所有的卷源都要求处于 Pod 所在的同一个名字空间内。进一步的详细信息，可参考
<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/all-in-one-volume.md">一体化卷设计文档</a>。</p>
<!--
### Example configuration with a secret, a downwardAPI, and a configMap {#example-configuration-secret-downwardapi-configmap}
-->
<h3 id="example-configuration-secret-downwardapi-configmap">带有 Secret、DownwardAPI 和 ConfigMap 的配置示例</h3>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/storage/projected-secret-downwardapi-configmap.yaml" download="pods/storage/projected-secret-downwardapi-configmap.yaml"><code>pods/storage/projected-secret-downwardapi-configmap.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-storage-projected-secret-downwardapi-configmap-yaml')" title="Copy pods/storage/projected-secret-downwardapi-configmap.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-storage-projected-secret-downwardapi-configmap-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>volume-test<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>container-test<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>all-in-one<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/projected-volume&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>all-in-one<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">projected</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">sources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>username<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>my-group/my-username<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">downwardAPI</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;labels&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>metadata.labels<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;cpu_limit&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">resourceFieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">containerName</span>:<span style="color:#bbb"> </span>container-test<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>limits.cpu<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myconfigmap<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>my-group/my-config<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
### Example configuration: secrets with a non-default permission mode set {#example-configuration-secrets-nondefault-permission-mode}
-->
<h3 id="example-configuration-secrets-nondefault-permission-mode">带有非默认权限模式设置的 Secret 的配置示例</h3>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/storage/projected-secrets-nondefault-permission-mode.yaml" download="pods/storage/projected-secrets-nondefault-permission-mode.yaml"><code>pods/storage/projected-secrets-nondefault-permission-mode.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-storage-projected-secrets-nondefault-permission-mode-yaml')" title="Copy pods/storage/projected-secrets-nondefault-permission-mode.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-storage-projected-secrets-nondefault-permission-mode-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>volume-test<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>container-test<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>all-in-one<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/projected-volume&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>all-in-one<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">projected</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">sources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>username<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>my-group/my-username<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret2<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>password<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>my-group/my-password<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">mode</span>:<span style="color:#bbb"> </span><span style="color:#666">511</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Each projected volume source is listed in the spec under `sources`. The
parameters are nearly the same with two exceptions:

* For secrets, the `secretName` field has been changed to `name` to be consistent
  with ConfigMap naming.
* The `defaultMode` can only be specified at the projected level and not for each
  volume source. However, as illustrated above, you can explicitly set the `mode`
  for each individual projection.
-->
<p>每个被投射的卷源都列举在规约中的 <code>sources</code> 下面。参数几乎相同，只有两个例外：</p>
<ul>
<li>对于 Secret，<code>secretName</code> 字段被改为 <code>name</code> 以便于 ConfigMap 的命名一致；</li>
<li><code>defaultMode</code> 只能在投射层级设置，不能在卷源层级设置。不过，正如上面所展示的，
你可以显式地为每个投射单独设置 <code>mode</code> 属性。</li>
</ul>
<!--
## serviceAccountToken projected volumes {#serviceaccounttoken}
When the `TokenRequestProjection` feature is enabled, you can inject the token
for the current [service account](/docs/reference/access-authn-authz/authentication/#service-account-tokens)
into a Pod at a specified path. For example:
-->
<h2 id="serviceaccounttoken">serviceAccountToken 投射卷</h2>
<p>当 <code>TokenRequestProjection</code> 特性被启用时，你可以将当前
<a href="/zh/docs/reference/access-authn-authz/authentication/#service-account-tokens">服务账号</a>
的令牌注入到 Pod 中特定路径下。例如：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/storage/projected-service-account-token.yaml" download="pods/storage/projected-service-account-token.yaml"><code>pods/storage/projected-service-account-token.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-storage-projected-service-account-token-yaml')" title="Copy pods/storage/projected-service-account-token.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-storage-projected-service-account-token-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sa-token-test<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>container-test<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>token-vol<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/service-account&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceAccountName</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>token-vol<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">projected</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">sources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">serviceAccountToken</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">audience</span>:<span style="color:#bbb"> </span>api<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">expirationSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">3600</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>token<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
The example Pod has a projected volume containing the injected service account
token. Containers in this Pod can use that token to access the Kubernetes API
server, authenticating with the identity of [the pod's ServiceAccount](/docs/tasks/configure-pod-container/configure-service-account/).
The `audience` field contains the intended audience of the
token. A recipient of the token must identify itself with an identifier specified
in the audience of the token, and otherwise should reject the token. This field
is optional and it defaults to the identifier of the API server.
-->
<p>示例 Pod 中包含一个投射卷，其中包含注入的服务账号令牌。
此 Pod 中的容器可以使用该令牌访问 Kubernetes API 服务器， 使用
<a href="/zh/docs/tasks/configure-pod-container/configure-service-account/">pod 的 ServiceAccount</a>
进行身份验证。<code>audience</code> 字段包含令牌所针对的受众。
收到令牌的主体必须使用令牌受众中所指定的某个标识符来标识自身，否则应该拒绝该令牌。
此字段是可选的，默认值为 API 服务器的标识。</p>
<!--
The `expirationSeconds` is the expected duration of validity of the service account
token. It defaults to 1 hour and must be at least 10 minutes (600 seconds). An administrator
can also limit its maximum value by specifying the `--service-account-max-token-expiration`
option for the API server. The `path` field specifies a relative path to the mount point
of the projected volume.
-->
<p>字段 <code>expirationSeconds</code> 是服务账号令牌预期的生命期长度。默认值为 1 小时，
必须至少为 10 分钟（600 秒）。管理员也可以通过设置 API 服务器的命令行参数
<code>--service-account-max-token-expiration</code> 来为其设置最大值上限。<code>path</code> 字段给出
与投射卷挂载点之间的相对路径。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
A container using a projected volume source as a [`subPath`](/docs/concepts/storage/volumes/#using-subpath)
volume mount will not receive updates for those volume sources.
-->
<p>以 <a href="/zh/docs/concepts/storage/volumes/#using-subpath"><code>subPath</code></a>
形式使用投射卷源的容器无法收到对应卷源的更新。
</div>
<!--
## SecurityContext interactions
-->
<h2 id="securitycontext-interactions">与 SecurityContext 间的关系   </h2>
<!--
The [proposal for file permission handling in projected service account volume](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2451-service-account-token-volumes#token-volume-projection)
enhancement introduced the projected files having the the correct owner
permissions set.
-->
<p><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2451-service-account-token-volumes#token-volume-projection">关于在投射的服务账号卷中处理文件访问权限的提案</a>
介绍了如何使得所投射的文件具有合适的属主访问权限。</p>
<h3 id="linux">Linux</h3>
<!--
In Linux pods that have a projected volume and `RunAsUser` set in the Pod
[`SecurityContext`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context),
the projected files have the correct ownership set including container user
ownership.
-->
<p>在包含了投射卷并在
<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context"><code>SecurityContext</code></a>
中设置了 <code>RunAsUser</code> 属性的 Linux Pod 中，投射文件具有正确的属主属性设置，
其中包含了容器用户属主。</p>
<h3 id="windows">Windows</h3>
<!--
In Windows pods that have a projected volume and `RunAsUsername` set in the
Pod `SecurityContext`, the ownership is not enforced due to the way user
accounts are managed in Windows. Windows stores and manages local user and group
accounts in a database file called Security Account Manager (SAM). Each
container maintains its own instance of the SAM database, to which the host has
no visibility into while the container is running. Windows containers are
designed to run the user mode portion of the OS in isolation from the host,
hence the maintenance of a virtual SAM database. As a result, the kubelet running
on the host does not have the ability to dynamically configure host file
ownership for virtualized container accounts. It is recommended that if files on
the host machine are to be shared with the container then they should be placed
into their own volume mount outside of `C:\`.
-->
<p>在包含了投射卷并在 <code>SecurityContext</code> 中设置了 <code>RunAsUsername</code> 的 Windows Pod 中,
由于 Windows 中用户账号的管理方式问题，文件的属主无法正确设置。
Windows 在名为安全账号管理器（Security Account Manager，SAM）
的数据库中保存本地用户和组信息。每个容器会维护其自身的 SAM 数据库实例，
宿主系统无法窥视到容器运行期间数据库内容。Windows 容器被设计用来运行操作系统的用户态部分，
与宿主系统之间隔离，因此维护了一个虚拟的 SAM 数据库。
所以，在宿主系统上运行的 kubelet 无法动态为虚拟的容器账号配置宿主文件的属主。
如果需要将宿主机器上的文件与容器共享，建议将它们放到挂载于 <code>C:\</code> 之外
的独立卷中。</p>
<!--
By default, the projected files will have the following ownership as shown for
an example projected volume file:
-->
<p>默认情况下，所投射的文件会具有如下例所示的属主属性设置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">PS </span>C:\&gt; <span style="color:#a2f">Get-Acl</span> C:\var\run\secrets\kubernetes.io\serviceaccount\..2021_08_31_22_22_18.318230061\ca.crt | <span style="color:#a2f">Format-List</span>

Path   <span style="">:</span> Microsoft.PowerShell.Core\FileSystem::C:\var\run\secrets\kubernetes.io\serviceaccount\..2021_08_31_22_22_18.318230061\ca.crt
Owner  <span style="">:</span> BUILTIN\Administrators
<span style="color:#a2f">Group </span> <span style="">:</span> NT AUTHORITY\SYSTEM
Access <span style="">:</span> NT AUTHORITY\SYSTEM Allow  FullControl
         BUILTIN\Administrators Allow  FullControl
         BUILTIN\Users Allow  ReadAndExecute, Synchronize
Audit  <span style="">:</span>
Sddl   <span style="">:</span> O:BAG<span style="">:</span>SYD<span style="">:</span>AI(A;ID;FA;;;SY)(A;ID;FA;;;BA)(A;ID;0x1200a9;;;BU)
</code></pre></div><!--
This implies all administrator users like `ContainerAdministrator` will have
read, write and execute access while, non-administrator users will have read and
execute access.
-->
<p>这意味着，所有类似 <code>ContainerAdministrator</code> 的管理员用户都具有读、写和执行访问权限，
而非管理员用户将具有读和执行访问权限。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In general, granting the container access to the host is discouraged as it can
open the door for potential security exploits.

Creating a Windows Pod with `RunAsUser` in it's `SecurityContext` will result in
the Pod being stuck at `ContainerCreating` forever. So it is advised to not use
the Linux only `RunAsUser` option with Windows Pods.
-->
<p>总体而言，为容器授予访问宿主系统的权限这种做法是不推荐的，因为这样做可能会打开潜在的安全性攻击之门。</p>
<p>在创建 Windows Pod 时，如过在其 <code>SecurityContext</code> 中设置了 <code>RunAsUser</code>，
Pod 会一直阻塞在 <code>ContainerCreating</code> 状态。因此，建议不要在 Windows
节点上使用仅针对 Linux 的 <code>RunAsUser</code> 选项。</p>

</div>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-df33eab51202c17bb0fe551d1d5cc5d2">6.4 - 临时卷</h1>
    
	<!--
reviewers:
- jsafrane
- saad-ali
- msau42
- xing-yang
- pohly
title: Ephemeral Volumes
content_type: concept
weight: 30
-->
<!-- overview -->
<!--
This document describes _ephemeral volumes_ in Kubernetes. Familiarity
with [volumes](/docs/concepts/storage/volumes/) is suggested, in
particular PersistentVolumeClaim and PersistentVolume.
-->
<p>本文档描述 Kubernetes 中的 <em>临时卷（Ephemeral Volume）</em>。
建议先了解<a href="/zh/docs/concepts/storage/volumes/">卷</a>，特别是 PersistentVolumeClaim 和 PersistentVolume。</p>
<!-- body -->
<!--
Some application need additional storage but don't care whether that
data is stored persistently across restarts. For example, caching
services are often limited by memory size and can move infrequently
used data into storage that is slower than memory with little impact
on overall performance.
-->
<p>有些应用程序需要额外的存储，但并不关心数据在重启后仍然可用。
例如，缓存服务经常受限于内存大小，将不常用的数据转移到比内存慢、但对总体性能的影响很小的存储中。</p>
<!--
Other applications expect some read-only input data to be present in
files, like configuration data or secret keys.
-->
<p>另有些应用程序需要以文件形式注入的只读数据，比如配置数据或密钥。</p>
<!--
_Ephemeral volumes_ are designed for these use cases. Because volumes
follow the Pod's lifetime and get created and deleted along with the
Pod, Pods can be stopped and restarted without being limited to where
some persistent volume is available.
-->
<p><em>临时卷</em> 就是为此类用例设计的。因为卷会遵从 Pod 的生命周期，与 Pod 一起创建和删除，
所以停止和重新启动 Pod 时，不会受持久卷在何处可用的限制。</p>
<!--
Ephemeral volumes are specified _inline_ in the Pod spec, which
simplifies application deployment and management.
-->
<p>临时卷在 Pod 规范中以 <em>内联</em> 方式定义，这简化了应用程序的部署和管理。</p>
<!--
### Types of ephemeral volumes
-->
<h3 id="types-of-ephemeral-volumes">临时卷的类型</h3>
<!--
Kubernetes supports several different kinds of ephemeral volumes for
different purposes:
- [emptyDir](/docs/concepts/storage/volumes/#emptydir): empty at Pod startup,
  with storage coming locally from the kubelet base directory (usually
  the root disk) or RAM
- [configMap](/docs/concepts/storage/volumes/#configmap),
  [downwardAPI](/docs/concepts/storage/volumes/#downwardapi),
  [secret](/docs/concepts/storage/volumes/#secret): inject different
  kinds of Kubernetes data into a Pod
- [CSI ephemeral volumes](#csi-ephemeral-volumes):
  similar to the previous volume kinds, but provided by special
  [CSI drivers](https://github.com/container-storage-interface/spec/blob/master/spec.md)
  which specifically [support this feature](https://kubernetes-csi.github.io/docs/drivers.html)
- [generic ephemeral volumes](#generic-ephemeral-volumes), which
  can be provided by all storage drivers that also support persistent volumes
-->
<p>Kubernetes 为了不同的目的，支持几种不同类型的临时卷：</p>
<ul>
<li><a href="/zh/docs/concepts/storage/volumes/#emptydir">emptyDir</a>：
Pod 启动时为空，存储空间来自本地的 kubelet 根目录（通常是根磁盘）或内存</li>
<li><a href="/zh/docs/concepts/storage/volumes/#configmap">configMap</a>、
<a href="/zh/docs/concepts/storage/volumes/#downwardapi">downwardAPI</a>、
<a href="/zh/docs/concepts/storage/volumes/#secret">secret</a>：
将不同类型的 Kubernetes 数据注入到 Pod 中</li>
<li><a href="/zh/docs/concepts/storage/volumes/#csi-ephemeral-volumes">CSI 临时卷</a>：
类似于前面的卷类型，但由专门<a href="https://kubernetes-csi.github.io/docs/drivers.html">支持此特性</a>
的指定
<a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI 驱动程序</a>提供</li>
<li><a href="#generic-ephemeral-volumes">通用临时卷</a>：
它可以由所有支持持久卷的存储驱动程序提供</li>
</ul>
<!--
`emptyDir`, `configMap`, `downwardAPI`, `secret` are provided as
[local ephemeral
storage](/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage).
They are managed by kubelet on each node.

CSI ephemeral volumes *must* be provided by third-party CSI storage
drivers.
-->
<p><code>emptyDir</code>、<code>configMap</code>、<code>downwardAPI</code>、<code>secret</code> 是作为
<a href="/zh/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">本地临时存储</a>
提供的。它们由各个节点上的 kubelet 管理。</p>
<p>CSI 临时卷 <em>必须</em> 由第三方 CSI 存储驱动程序提供。</p>
<!--
Generic ephemeral volumes *can* be provided by third-party CSI storage
drivers, but also by any other storage driver that supports dynamic
provisioning. Some CSI drivers are written specifically for CSI
ephemeral volumes and do not support dynamic provisioning: those then
cannot be used for generic ephemeral volumes.
-->
<p>通用临时卷 <em>可以</em> 由第三方 CSI 存储驱动程序提供，也可以由支持动态配置的任何其他存储驱动程序提供。
一些专门为 CSI 临时卷编写的 CSI 驱动程序，不支持动态供应：因此这些驱动程序不能用于通用临时卷。</p>
<!--
The advantage of using third-party drivers is that they can offer
functionality that Kubernetes itself does not support, for example
storage with different performance characteristics than the disk that
is managed by kubelet, or injecting different data.
-->
<p>使用第三方驱动程序的优势在于，它们可以提供 Kubernetes 本身不支持的功能，
例如，与 kubelet 管理的磁盘具有不同运行特征的存储，或者用来注入不同的数据</p>
<!--
### CSI ephemeral volumes
-->
<h3 id="csi-ephemeral-volumes">CSI 临时卷</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.16 [beta]</code>
</div>


<!--
This feature requires the `CSIInlineVolume` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be enabled. It
is enabled by default starting with Kubernetes 1.16.

CSI ephemeral volumes are only supported by a subset of CSI drivers.
The Kubernetes CSI [Drivers list](https://kubernetes-csi.github.io/docs/drivers.html)
shows which drivers support ephemeral volumes.
-->
<p>该特性需要启用参数 <code>CSIInlineVolume</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控（feature gate）</a>。
该参数从 Kubernetes 1.16 开始默认启用。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 只有一部分 CSI 驱动程序支持 CSI 临时卷。Kubernetes CSI
<a href="https://kubernetes-csi.github.io/docs/drivers.html">驱动程序列表</a>
显示了支持临时卷的驱动程序。
</div>
<!--
Conceptually, CSI ephemeral volumes are similar to `configMap`,
`downwardAPI` and `secret` volume types: the storage is managed locally on each
 node and is created together with other local resources after a Pod has been
scheduled onto a node. Kubernetes has no concept of rescheduling Pods
anymore at this stage. Volume creation has to be unlikely to fail,
otherwise Pod startup gets stuck. In particular, [storage capacity
aware Pod scheduling](/docs/concepts/storage/storage-capacity/) is *not*
supported for these volumes. They are currently also not covered by
the storage resource usage limits of a Pod, because that is something
that kubelet can only enforce for storage that it manages itself.


Here's an example manifest for a Pod that uses CSI ephemeral storage:
-->
<p>从概念上讲，CSI 临时卷类似于 <code>configMap</code>、<code>downwardAPI</code> 和 <code>secret</code> 类型的卷：
其存储在每个节点本地管理，并在将 Pod 调度到节点后与其他本地资源一起创建。
在这个阶段，Kubernetes 没有重新调度 Pods 的概念。卷创建不太可能失败，否则 Pod 启动将会受阻。
特别是，这些卷 <strong>不</strong> 支持<a href="/zh/docs/concepts/storage/storage-capacity/">感知存储容量的 Pod 调度</a>。
它们目前也没包括在 Pod 的存储资源使用限制中，因为 kubelet 只能对它自己管理的存储强制执行。</p>
<p>下面是使用 CSI 临时存储的 Pod 的示例清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-csi-app<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-frontend<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/data&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-csi-inline-vol<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;sleep&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;1000000&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-csi-inline-vol<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">csi</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">driver</span>:<span style="color:#bbb"> </span>inline.storage.kubernetes.io<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeAttributes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>bar<span style="color:#bbb">
</span></code></pre></div><!--
The `volumeAttributes` determine what volume is prepared by the
driver. These attributes are specific to each driver and not
standardized. See the documentation of each CSI driver for further
instructions.

-->
<p><code>volumeAttributes</code> 决定驱动程序准备什么样的卷。这些属性特定于每个驱动程序，且没有实现标准化。
有关进一步的说明，请参阅每个 CSI 驱动程序的文档。</p>
<!--
### CSI driver restrictions

As a cluster administrator, you can use a [PodSecurityPolicy](/docs/concepts/security/pod-security-policy/) to control which CSI drivers can be used in a Pod, specified with the
[`allowedCSIDrivers` field](/docs/reference/generated/kubernetes-api/v1.23/#podsecuritypolicyspec-v1beta1-policy).
-->
<h3 id="csi-driver-restrictions">CSI 驱动程序限制</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code>
</div>


<p>作为一个集群管理员，你可以使用
<a href="/zh/docs/concepts/security/pod-security-policy/">PodSecurityPolicy</a>
来控制在 Pod 中可以使用哪些 CSI 驱动程序，
具体则是通过 <a href="/docs/reference/generated/kubernetes-api/v1.23/#podsecuritypolicyspec-v1beta1-policy"><code>allowedCSIDrivers</code> 字段</a>
指定。</p>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> PodSecurityPolicy is deprecated and will be removed in the Kubernetes v1.25 release.
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> PodSecurityPolicy 已弃用，并将在 Kubernetes v1.25 版本中移除。
</div>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> CSI ephemeral volumes are only supported by a subset of CSI drivers.
The Kubernetes CSI <a href="https://kubernetes-csi.github.io/docs/drivers.html">Drivers list</a>
shows which drivers support ephemeral volumes.
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> CSI 临时卷仅有 CSI 驱动程序的一个子集支持。
Kubernetes CSI <a href="https://kubernetes-csi.github.io/docs/drivers.html">驱动列表</a>显示了哪些驱动程序支持临时卷。
</div>
<!--
### Generic ephemeral volumes
-->
<h3 id="generic-ephemeral-volumes">通用临时卷</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code>
</div>


<!--
Generic ephemeral volumes are similar to `emptyDir` volumes in the
sense that they provide a per-pod directory for scratch data that is
usually empty after provisioning. But they may also have additional
features:

- Storage can be local or network-attached.
- Volumes can have a fixed size that Pods are not able to exceed.
- Volumes may have some initial data, depending on the driver and
  parameters.
- Typical operations on volumes are supported assuming that the driver
  supports them, including
  [snapshotting](/docs/concepts/storage/volume-snapshots/),
  [cloning](/docs/concepts/storage/volume-pvc-datasource/),
  [resizing](/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims),
  and [storage capacity tracking](/docs/concepts/storage/storage-capacity/).

Example:
-->
<p>通用临时卷类似于 <code>emptyDir</code> 卷，因为它为每个 Pod 提供临时数据存放目录，
在最初制备完毕时一般为空。不过通用临时卷也有一些额外的功能特性：</p>
<ul>
<li>存储可以是本地的，也可以是网络连接的。</li>
<li>卷可以有固定的大小，pod不能超量使用。</li>
<li>卷可能有一些初始数据，这取决于驱动程序和参数。</li>
<li>当驱动程序支持，卷上的典型操作将被支持，包括
（<a href="/zh/docs/concepts/storage/volume-snapshots/">快照</a>、
<a href="/zh/docs/concepts/storage/volume-pvc-datasource/">克隆</a>、
<a href="/zh/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims">调整大小</a>和
<a href="/zh/docs/concepts/storage/storage-capacity/">存储容量跟踪</a>）。</li>
</ul>
<p>示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-frontend<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/scratch&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>scratch-volume<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;sleep&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;1000000&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>scratch-volume<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ephemeral</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeClaimTemplate</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>my-frontend-volume<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;ReadWriteOnce&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;scratch-storage-class&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></code></pre></div><!--
### Lifecycle and PersistentVolumeClaim
-->
<h3 id="lifecycle-and-persistentvolumeclaim">生命周期和 PersistentVolumeClaim</h3>
<!--
The key design idea is that the
[parameters for a volume claim](/docs/reference/generated/kubernetes-api/v1.23/#ephemeralvolumesource-v1alpha1-core)
are allowed inside a volume source of the Pod. Labels, annotations and
the whole set of fields for a PersistentVolumeClaim are supported. When such a Pod gets
created, the ephemeral volume controller then creates an actual PersistentVolumeClaim
object in the same namespace as the Pod and ensures that the PersistentVolumeClaim
gets deleted when the Pod gets deleted.
-->
<p>关键的设计思想是在 Pod 的卷来源中允许使用
<a href="/docs/reference/generated/kubernetes-api/v1.23/#ephemeralvolumesource-v1alpha1-core">卷申领的参数</a>。
PersistentVolumeClaim 的标签、注解和整套字段集均被支持。
创建这样一个 Pod 后，
临时卷控制器在 Pod 所属的命名空间中创建一个实际的 PersistentVolumeClaim 对象，
并确保删除 Pod 时，同步删除 PersistentVolumeClaim。</p>
<!--
That triggers volume binding and/or provisioning, either immediately if
the <a class='glossary-tooltip' title='StorageClass 是管理员用来描述可用的不同存储类型的一种方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/storage-classes/' target='_blank' aria-label='StorageClass'>StorageClass</a> uses immediate volume binding or when the Pod is
tentatively scheduled onto a node (`WaitForFirstConsumer` volume
binding mode). The latter is recommended for generic ephemeral volumes
because then the scheduler is free to choose a suitable node for
the Pod. With immediate binding, the scheduler is forced to select a node that has
access to the volume once it is available.
-->
<p>如上设置将触发卷的绑定与/或准备操作，相应动作或者在
<a class='glossary-tooltip' title='StorageClass 是管理员用来描述可用的不同存储类型的一种方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/storage-classes/' target='_blank' aria-label='StorageClass'>StorageClass</a>
使用即时卷绑定时立即执行，
或者当 Pod 被暂时性调度到某节点时执行 (<code>WaitForFirstConsumer</code> 卷绑定模式)。
对于常见的临时卷，建议采用后者，这样调度器就可以自由地为 Pod 选择合适的节点。
对于即时绑定，调度器则必须选出一个节点，使得在卷可用时，能立即访问该卷。</p>
<!--
In terms of [resource ownership](/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents),
a Pod that has generic ephemeral storage is the owner of the PersistentVolumeClaim(s)
that provide that ephemeral storage. When the Pod is deleted,
the Kubernetes garbage collector deletes the PVC, which then usually
triggers deletion of the volume because the default reclaim policy of
storage classes is to delete volumes. You can create quasi-ephemeral local storage
using a StorageClass with a reclaim policy of `retain`: the storage outlives the Pod,
and in this case you need to ensure that volume clean up happens separately.
-->
<p>就<a href="/zh/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents">资源所有权</a>而言，
拥有通用临时存储的 Pod 是提供临时存储 (ephemeral storage) 的 PersistentVolumeClaim 的所有者。
当 Pod 被删除时，Kubernetes 垃圾收集器会删除 PVC，
然后 PVC 通常会触发卷的删除，因为存储类的默认回收策略是删除卷。
你可以使用带有 <code>retain</code> 回收策略的 StorageClass 创建准临时 (quasi-ephemeral) 本地存储：
该存储比 Pod 寿命长，在这种情况下，你需要确保单独进行卷清理。</p>
<!--
While these PVCs exist, they can be used like any other PVC. In
particular, they can be referenced as data source in volume cloning or
snapshotting. The PVC object also holds the current status of the
volume.
-->
<p>当这些 PVC 存在时，它们可以像其他 PVC 一样使用。
特别是，它们可以被引用作为批量克隆或快照的数据源。
PVC对象还保持着卷的当前状态。</p>
<!--
### PersistentVolumeClaim naming
-->
<h3 id="persistentvolumeclaim-naming">PersistentVolumeClaim 的命名</h3>
<!--
Naming of the automatically created PVCs is deterministic: the name is
a combination of Pod name and volume name, with a hyphen (`-`) in the
middle. In the example above, the PVC name will be
`my-app-scratch-volume`.  This deterministic naming makes it easier to
interact with the PVC because one does not have to search for it once
the Pod name and volume name are known.
-->
<p>自动创建的 PVCs 的命名是确定的：此名称是 Pod 名称和卷名称的组合，中间由连字符(<code>-</code>)连接。
在上面的示例中，PVC 将命名为 <code>my-app-scratch-volume</code> 。
这种确定性命名方式使得与 PVC 交互变得更容易，因为一旦知道 Pod 名称和卷名，就不必搜索它。</p>
<!--
The deterministic naming also introduces a potential conflict between different
Pods (a Pod "pod-a" with volume "scratch" and another Pod with name
"pod" and volume "a-scratch" both end up with the same PVC name
"pod-a-scratch") and between Pods and manually created PVCs.
-->
<p>这种确定性命名方式也引入了潜在的冲突，
比如在不同的 Pod 之间（名为 “Pod-a” 的 Pod 挂载名为 &quot;scratch&quot; 的卷，
和名为 &quot;pod&quot; 的 Pod 挂载名为 “a-scratch” 的卷，这两者均会生成名为
&quot;pod-a-scratch&quot; 的PVC），或者在 Pod 和手工创建的 PVC 之间。</p>
<!--
Such conflicts are detected: a PVC is only used for an ephemeral
volume if it was created for the Pod. This check is based on the
ownership relationship. An existing PVC is not overwritten or
modified. But this does not resolve the conflict because without the
right PVC, the Pod cannot start.
-->
<p>以下冲突会被检测到：如果 PVC 是为 Pod 创建的，那么它只用于临时卷。
此检测基于所有权关系。现有的 PVC 不会被覆盖或修改。
但这并不能解决冲突，因为如果没有正确的 PVC，Pod 就无法启动。</p>
<!--
Take care when naming Pods and volumes inside the
same namespace, so that these conflicts can't occur.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 当命名 Pods 和卷出现在同一个命名空间中时，要小心，以防止发生此类冲突。
</div>

<!--
### Security
-->
<h3 id="security">安全</h3>
<!--
Enabling the GenericEphemeralVolume feature allows users to create
PVCs indirectly if they can create Pods, even if they do not have
permission to create PVCs directly. Cluster administrators must be
aware of this. If this does not fit their security model, they have
two choices:
-->
<p>启用 GenericEphemeralVolume 特性会导致那些没有 PVCs 创建权限的用户，
在创建 Pods 时，被允许间接的创建 PVCs。
集群管理员必须意识到这一点。
如果这不符合他们的安全模型，他们有如下选择：</p>
<!--
- Use an [admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/)
  that rejects objects like Pods that have a generic ephemeral
  volume.
- Use a [Pod Security
  Policy](/docs/concepts/policy/pod-security-policy/) where the
  `volumes` list does not contain the `ephemeral` volume type
  (deprecated in Kubernetes 1.21).
-->
<ul>
<li>通过特性门控显式禁用该特性。</li>
<li>使用一个<a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/">准入 Webhook</a>
拒绝包含通用临时卷的 Pods。</li>
<li>当 <code>volumes</code> 列表不包含 <code>ephemeral</code> 卷类型时，使用
<a href="/zh/docs/concepts/policy/pod-security-policy/">Pod 安全策略</a>。
（这一方式在 Kubernetes 1.21 版本已经弃用）</li>
</ul>
<!--
The normal [namespace quota for PVCs](/docs/concepts/policy/resource-quotas/#storage-resource-quota) still applies, so
even if users are allowed to use this new mechanism, they cannot use
it to circumvent other policies.
-->
<p><a href="/zh/docs/concepts/policy/resource-quotas/#storage-resource-quota">为 PVC 卷所设置的逐名字空间的配额</a>
仍然有效，因此即使允许用户使用这种新机制，他们也不能使用它来规避其他策略。</p>
<h2 id="what-s-next">What's next</h2>
<!--
### Ephemeral volumes managed by kubelet

See [local ephemeral storage](/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage).
-->
<h3 id="ephemeral-volumes-managed-by-kubelet">kubelet 管理的临时卷</h3>
<p>参阅<a href="/zh/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">本地临时存储</a>。</p>
<!--
### CSI ephemeral volumes

- For more information on the design, see the [Ephemeral Inline CSI
  volumes KEP](https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md).
- For more information on further development of this feature, see the [enhancement tracking issue #596](https://github.com/kubernetes/enhancements/issues/596).
-->
<h3 id="csi-ephemeral-volumes">CSI 临时卷</h3>
<ul>
<li>有关设计的更多信息，参阅
<a href="https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md">Ephemeral Inline CSI volumes KEP</a>。</li>
<li>本特性下一步开发的更多信息，参阅
<a href="https://github.com/kubernetes/enhancements/issues/596">enhancement tracking issue #596</a>。</li>
</ul>
<!--
### Generic ephemeral volumes

- For more information on the design, see the
[Generic ephemeral inline volumes KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md).
-->
<h3 id="generic-ephemeral-volumes">通用临时卷</h3>
<ul>
<li>有关设计的更多信息，参阅
<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md">Generic ephemeral inline volumes KEP</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f0276d05eef111249272a1c932a91e2c">6.5 - 存储类</h1>
    
	<!--
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: Storage Classes
content_type: concept
weight: 30
-->
<!-- overview -->
<!--
This document describes the concept of a StorageClass in Kubernetes. Familiarity
with [volumes](/docs/concepts/storage/volumes/) and
[persistent volumes](/docs/concepts/storage/persistent-volumes) is suggested.
-->
<p>本文描述了 Kubernetes 中 StorageClass 的概念。建议先熟悉
<a href="/zh/docs/concepts/storage/volumes/">卷</a>和
<a href="/zh/docs/concepts/storage/persistent-volumes">持久卷</a>的概念。</p>
<!-- body -->
<!--
## Introduction

A StorageClass provides a way for administrators to describe the "classes" of
storage they offer. Different classes might map to quality-of-service levels,
or to backup policies, or to arbitrary policies determined by the cluster
administrators. Kubernetes itself is unopinionated about what classes
represent. This concept is sometimes called "profiles" in other storage
systems.
-->
<h2 id="介绍">介绍</h2>
<p>StorageClass 为管理员提供了描述存储 &quot;类&quot; 的方法。
不同的类型可能会映射到不同的服务质量等级或备份策略，或是由集群管理员制定的任意策略。
Kubernetes 本身并不清楚各种类代表的什么。这个类的概念在其他存储系统中有时被称为 &quot;配置文件&quot;。</p>
<!--
## The StorageClass Resource

Each StorageClass contains the fields `provisioner`, `parameters`, and
`reclaimPolicy`, which are used when a PersistentVolume belonging to the
class needs to be dynamically provisioned.

-->
<h2 id="storageclass-资源">StorageClass 资源</h2>
<p>每个 StorageClass 都包含 <code>provisioner</code>、<code>parameters</code> 和 <code>reclaimPolicy</code> 字段，
这些字段会在 StorageClass 需要动态分配 PersistentVolume 时会使用到。</p>
<!--
The name of a StorageClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating StorageClass objects, and the objects cannot
be updated once they are created.
 -->
<p>StorageClass 对象的命名很重要，用户使用这个命名来请求生成一个特定的类。
当创建 StorageClass 对象时，管理员设置 StorageClass 对象的命名和其他参数，一旦创建了对象就不能再对其更新。</p>
<!--
Administrators can specify a default StorageClass only for PVCs that don't
request any particular class to bind to: see the
[PersistentVolumeClaim section](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)
for details.
 -->
<p>管理员可以为没有申请绑定到特定 StorageClass 的 PVC 指定一个默认的存储类：
更多详情请参阅
<a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim 章节</a>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>standard<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/aws-ebs<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>gp2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">reclaimPolicy</span>:<span style="color:#bbb"> </span>Retain<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">allowVolumeExpansion</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">mountOptions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- debug<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">volumeBindingMode</span>:<span style="color:#bbb"> </span>Immediate<span style="color:#bbb">
</span></code></pre></div><!--
### Provisioner

Each StorageClass has a provisioner that determines what volume plugin is used
for provisioning PVs. This field must be specified.
 -->
<h3 id="provisioner">存储制备器 </h3>
<p>每个 StorageClass 都有一个制备器（Provisioner），用来决定使用哪个卷插件制备 PV。
该字段必须指定。</p>
<!--
| Volume Plugin        | Internal Provisioner| Config Example                       |
-->
<table>
<thead>
<tr>
<th style="text-align:left">卷插件</th>
<th style="text-align:center">内置制备器</th>
<th style="text-align:center">配置例子</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">AWSElasticBlockStore</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#aws-ebs">AWS EBS</a></td>
</tr>
<tr>
<td style="text-align:left">AzureFile</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#azure-%E6%96%87%E4%BB%B6">Azure File</a></td>
</tr>
<tr>
<td style="text-align:left">AzureDisk</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#azure-%E7%A3%81%E7%9B%98">Azure Disk</a></td>
</tr>
<tr>
<td style="text-align:left">CephFS</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:left">Cinder</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#openstack-cinder">OpenStack Cinder</a></td>
</tr>
<tr>
<td style="text-align:left">FC</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:left">FlexVolume</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:left">Flocker</td>
<td style="text-align:center">✓</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:left">GCEPersistentDisk</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#gce-pd">GCE PD</a></td>
</tr>
<tr>
<td style="text-align:left">Glusterfs</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#glusterfs">Glusterfs</a></td>
</tr>
<tr>
<td style="text-align:left">iSCSI</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:left">Quobyte</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#quobyte">Quobyte</a></td>
</tr>
<tr>
<td style="text-align:left">NFS</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="#nfs">NFS</a></td>
</tr>
<tr>
<td style="text-align:left">RBD</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#ceph-rbd">Ceph RBD</a></td>
</tr>
<tr>
<td style="text-align:left">VsphereVolume</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#vsphere">vSphere</a></td>
</tr>
<tr>
<td style="text-align:left">PortworxVolume</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#portworx-%E5%8D%B7">Portworx Volume</a></td>
</tr>
<tr>
<td style="text-align:left">ScaleIO</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#scaleio">ScaleIO</a></td>
</tr>
<tr>
<td style="text-align:left">StorageOS</td>
<td style="text-align:center">✓</td>
<td style="text-align:center"><a href="#storageos">StorageOS</a></td>
</tr>
<tr>
<td style="text-align:left">Local</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="#%E6%9C%AC%E5%9C%B0">Local</a></td>
</tr>
</tbody>
</table>
<!--
You are not restricted to specifying the "internal" provisioners
listed here (whose names are prefixed with "kubernetes.io" and shipped
alongside Kubernetes). You can also run and specify external provisioners,
which are independent programs that follow a [specification](https://github.com/kubernetes/design-proposals-archive/blob/main/storage/volume-provisioning.md))
defined by Kubernetes. Authors of external provisioners have full discretion
over where their code lives, how the provisioner is shipped, how it needs to be
run, what volume plugin it uses (including Flex), etc. The repository
[kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner)
houses a library for writing external provisioners that implements the bulk of
the specification. Some external provisioners are listed under the repository
[kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner).
 -->
<p>你不限于指定此处列出的 &quot;内置&quot; 制备器（其名称前缀为 &quot;kubernetes.io&quot; 并打包在 Kubernetes 中）。
你还可以运行和指定外部制备器，这些独立的程序遵循由 Kubernetes 定义的
<a href="https://github.com/kubernetes/design-proposals-archive/blob/main/storage/volume-provisioning.md">规范</a>。
外部供应商的作者完全可以自由决定他们的代码保存于何处、打包方式、运行方式、使用的插件（包括 Flex）等。
代码仓库 <a href="https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner">kubernetes-sigs/sig-storage-lib-external-provisioner</a>
包含一个用于为外部制备器编写功能实现的类库。你可以访问代码仓库
<a href="https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner">kubernetes-sigs/sig-storage-lib-external-provisioner</a>
了解外部驱动列表。</p>
<!--
For example, NFS doesn't provide an internal provisioner, but an external
provisioner can be used. There are also cases when 3rd party storage
vendors provide their own external provisioner.
 -->
<p>例如，NFS 没有内部制备器，但可以使用外部制备器。
也有第三方存储供应商提供自己的外部制备器。</p>
<!--
### Reclaim Policy

PersistentVolumes that are dynamically created by a StorageClass will have the
reclaim policy specified in the `reclaimPolicy` field of the class, which can be
either `Delete` or `Retain`. If no `reclaimPolicy` is specified when a
StorageClass object is created, it will default to `Delete`.

PersistentVolumes that are created manually and managed via a StorageClass will have
whatever reclaim policy they were assigned at creation.
 -->
<h3 id="回收策略">回收策略</h3>
<p>由 StorageClass 动态创建的 PersistentVolume 会在类的 <code>reclaimPolicy</code> 字段中指定回收策略，可以是
<code>Delete</code> 或者 <code>Retain</code>。如果 StorageClass 对象被创建时没有指定 <code>reclaimPolicy</code>，它将默认为 <code>Delete</code>。</p>
<p>通过 StorageClass 手动创建并管理的 PersistentVolume 会使用它们被创建时指定的回收政策。</p>
<!--
### Allow Volume Expansion
-->
<h3 id="允许卷扩展">允许卷扩展</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code>
</div>


<!--
PersistentVolumes can be configured to be expandable. This feature when set to `true`,
allows the users to resize the volume by editing the corresponding PVC object.

The following types of volumes support volume expansion, when the underlying
StorageClass has the field `allowVolumeExpansion` set to true.
-->
<p>PersistentVolume 可以配置为可扩展。将此功能设置为 <code>true</code> 时，允许用户通过编辑相应的 PVC 对象来调整卷大小。</p>
<p>当下层 StorageClass 的 <code>allowVolumeExpansion</code> 字段设置为 true 时，以下类型的卷支持卷扩展。</p>





<!-- 
Volume type | Required Kubernetes version
-->
<table><caption style="display: none;">Table of Volume types and the version of Kubernetes they require</caption>
<thead>
<tr>
<th style="text-align:left">卷类型</th>
<th style="text-align:left">Kubernetes 版本要求</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">gcePersistentDisk</td>
<td style="text-align:left">1.11</td>
</tr>
<tr>
<td style="text-align:left">awsElasticBlockStore</td>
<td style="text-align:left">1.11</td>
</tr>
<tr>
<td style="text-align:left">Cinder</td>
<td style="text-align:left">1.11</td>
</tr>
<tr>
<td style="text-align:left">glusterfs</td>
<td style="text-align:left">1.11</td>
</tr>
<tr>
<td style="text-align:left">rbd</td>
<td style="text-align:left">1.11</td>
</tr>
<tr>
<td style="text-align:left">Azure File</td>
<td style="text-align:left">1.11</td>
</tr>
<tr>
<td style="text-align:left">Azure Disk</td>
<td style="text-align:left">1.11</td>
</tr>
<tr>
<td style="text-align:left">Portworx</td>
<td style="text-align:left">1.11</td>
</tr>
<tr>
<td style="text-align:left">FlexVolume</td>
<td style="text-align:left">1.13</td>
</tr>
<tr>
<td style="text-align:left">CSI</td>
<td style="text-align:left">1.14 (alpha), 1.16 (beta)</td>
</tr>
</tbody>
</table>

<!--
You can only use the volume expansion feature to grow a Volume, not to shrink it.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 此功能仅可用于扩容卷，不能用于缩小卷。
</div>
<!--
### Mount Options

PersistentVolumes that are dynamically created by a StorageClass will have the
mount options specified in the `mountOptions` field of the class.

If the volume plugin does not support mount options but mount options are
specified, provisioning will fail. Mount options are not validated on either
the class or PV, If a mount option is invalid, the PV mount fails.
 -->
<h3 id="挂载选项">挂载选项</h3>
<p>由 StorageClass 动态创建的 PersistentVolume 将使用类中 <code>mountOptions</code> 字段指定的挂载选项。</p>
<p>如果卷插件不支持挂载选项，却指定了挂载选项，则制备操作会失败。
挂载选项在 StorageClass 和 PV 上都不会做验证，如果其中一个挂载选项无效，那么这个 PV 挂载操作就会失败。</p>
<!--
### Volume Binding Mode
 -->
<h3 id="卷绑定模式">卷绑定模式</h3>
<!--
The `volumeBindingMode` field controls when [volume binding and dynamic
provisioning](/docs/concepts/storage/persistent-volumes/#provisioning) should occur.
 -->
<p><code>volumeBindingMode</code> 字段控制了<a href="/zh/docs/concepts/storage/persistent-volumes/#provisioning">卷绑定和动态制备</a>
应该发生在什么时候。</p>
<!--
By default, the `Immediate` mode indicates that volume binding and dynamic
provisioning occurs once the PersistentVolumeClaim is created. For storage
backends that are topology-constrained and not globally accessible from all Nodes
in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling
requirements. This may result in unschedulable Pods.
 -->
<p>默认情况下，<code>Immediate</code> 模式表示一旦创建了 PersistentVolumeClaim 也就完成了卷绑定和动态制备。
对于由于拓扑限制而非集群所有节点可达的存储后端，PersistentVolume
会在不知道 Pod 调度要求的情况下绑定或者制备。</p>
<!--
A cluster administrator can address this issue by specifying the `WaitForFirstConsumer` mode which
will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.
PersistentVolumes will be selected or provisioned conforming to the topology that is
specified by the Pod's scheduling constraints. These include, but are not limited to, [resource
requirements](/docs/concepts/configuration/manage-resources-containers/),
[node selectors](/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector),
[pod affinity and
anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity),
and [taints and tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration).
-->
<p>集群管理员可以通过指定 <code>WaitForFirstConsumer</code> 模式来解决此问题。
该模式将延迟 PersistentVolume 的绑定和制备，直到使用该 PersistentVolumeClaim 的 Pod 被创建。
PersistentVolume 会根据 Pod 调度约束指定的拓扑来选择或制备。这些包括但不限于
<a href="/zh/docs/concepts/configuration/manage-resources-containers/">资源需求</a>、
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector">节点筛选器</a>、
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity/">pod 亲和性和互斥性</a>、
以及<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration">污点和容忍度</a>。</p>
<!--
The following plugins support `WaitForFirstConsumer` with dynamic provisioning:

* [AWSElasticBlockStore](#aws-ebs)
* [GCEPersistentDisk](#gce-pd)
* [AzureDisk](#azure-disk)
-->
<p>以下插件支持动态供应的 <code>WaitForFirstConsumer</code> 模式:</p>
<ul>
<li><a href="#aws-ebs">AWSElasticBlockStore</a></li>
<li><a href="#gce-pd">GCEPersistentDisk</a></li>
<li><a href="#azure-disk">AzureDisk</a></li>
</ul>
<!--
The following plugins support `WaitForFirstConsumer` with pre-created PersistentVolume binding:

* All of the above
* [Local](#local)
-->
<p>以下插件支持预创建绑定 PersistentVolume 的 <code>WaitForFirstConsumer</code> 模式：</p>
<ul>
<li>上述全部</li>
<li><a href="#local">Local</a></li>
</ul>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code>
</div>


<!--
[CSI volumes](/docs/concepts/storage/volumes/#csi) are also supported with dynamic provisioning
and pre-created PVs, but you'll need to look at the documentation for a specific CSI driver
to see its supported topology keys and examples.
-->
<p>动态配置和预先创建的 PV 也支持 <a href="/zh/docs/concepts/storage/volumes/#csi">CSI卷</a>，
但是你需要查看特定 CSI 驱动程序的文档以查看其支持的拓扑键名和例子。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
   If you choose to use `WaitForFirstConsumer`, do not use `nodeName` in the Pod spec
   to specify node affinity. If `nodeName` is used in this case, the scheduler will be bypassed and PVC will remain in `pending` state.

   Instead, you can use node selector for hostname in this case as shown below.
-->
<p>如果你选择使用 <code>WaitForFirstConsumer</code>，请不要在 Pod 规约中使用 <code>nodeName</code> 来指定节点亲和性。
如果在这种情况下使用 <code>nodeName</code>，Pod 将会绕过调度程序，PVC 将停留在 <code>pending</code> 状态。</p>
<p>相反，在这种情况下，你可以使用节点选择器作为主机名，如下所示</p>

</div>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/hostname</span>:<span style="color:#bbb"> </span>kube-01<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-storage<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>task-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;http-server&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/usr/share/nginx/html&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-storage<span style="color:#bbb">
</span></code></pre></div><!--
### Allowed Topologies
-->
<h3 id="allowed-topologies">允许的拓扑结构 </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.12 [beta]</code>
</div>


<!--
When a cluster operator specifies the `WaitForFirstConsumer` volume binding mode, it is no longer necessary
to restrict provisioning to specific topologies in most situations. However,
if still required, `allowedTopologies` can be specified.
-->
<p>当集群操作人员使用了 <code>WaitForFirstConsumer</code> 的卷绑定模式，
在大部分情况下就没有必要将制备限制为特定的拓扑结构。
然而，如果还有需要的话，可以使用 <code>allowedTopologies</code>。</p>
<!--
This example demonstrates how to restrict the topology of provisioned volumes to specific
zones and should be used as a replacement for the `zone` and `zones` parameters for the
supported plugins.
-->
<p>这个例子描述了如何将供应卷的拓扑限制在特定的区域，在使用时应该根据插件
支持情况替换 <code>zone</code> 和 <code>zones</code> 参数。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>standard<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/gce-pd<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>pd-standard<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">volumeBindingMode</span>:<span style="color:#bbb"> </span>WaitForFirstConsumer<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">allowedTopologies</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">matchLabelExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>failure-domain.beta.kubernetes.io/zone<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- us-central-1a<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- us-central-1b<span style="color:#bbb">
</span></code></pre></div><!--
## Parameters

Storage Classes have parameters that describe volumes belonging to the storage
class. Different parameters may be accepted depending on the `provisioner`. For
 example, the value `io1`, for the parameter `type`, and the parameter
`iopsPerGB` are specific to EBS. When a parameter is omitted, some default is
used.

There can be at most 512 parameters defined for a StorageClass.
The total length of the parameters object including its keys and values cannot
exceed 256 KiB.
 -->
<h2 id="参数">参数</h2>
<p>Storage Classes 的参数描述了存储类的卷。取决于制备器，可以接受不同的参数。
例如，参数 type 的值 io1 和参数 iopsPerGB 特定于 EBS PV。
当参数被省略时，会使用默认值。</p>
<p>一个 StorageClass 最多可以定义 512 个参数。这些参数对象的总长度不能
超过 256 KiB, 包括参数的键和值。</p>
<h3 id="aws-ebs">AWS EBS</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/aws-ebs<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>io1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">iopsPerGB</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span></code></pre></div><!--
* `type`: `io1`, `gp2`, `gp2`, `sc1`, `st1`. See
* `type`: `io1`, `gp2`, `sc1`, `st1`. See
  [AWS docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html)
  for details. Default: `gp2`.
* `zone` (Deprecated): AWS zone. If neither `zone` nor `zones` is specified, volumes are
  generally round-robin-ed across all active zones where Kubernetes cluster
  has a node. `zone` and `zones` parameters must not be used at the same time.
* `zones` (Deprecated): A comma separated list of AWS zone(s). If neither `zone` nor `zones`
  is specified, volumes are generally round-robin-ed across all active zones
  where Kubernetes cluster has a node. `zone` and `zones` parameters must not
  be used at the same time.
* `iopsPerGB`: only for `io1` volumes. I/O operations per second per GiB. AWS
  volume plugin multiplies this with size of requested volume to compute IOPS
  of the volume and caps it at 20 000 IOPS (maximum supported by AWS, see
  [AWS docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html).
  A string is expected here, i.e. `"10"`, not `10`.
* `fsType`: fsType that is supported by kubernetes. Default: `"ext4"`.
* `encrypted`: denotes whether the EBS volume should be encrypted or not.
  Valid values are `"true"` or `"false"`. A string is expected here,
  i.e. `"true"`, not `true`.
* `kmsKeyId`: optional. The full Amazon Resource Name of the key to use when
  encrypting the volume. If none is supplied but `encrypted` is true, a key is
  generated by AWS. See AWS docs for valid ARN value.
-->
<ul>
<li><code>type</code>：<code>io1</code>，<code>gp2</code>，<code>sc1</code>，<code>st1</code>。详细信息参见
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html">AWS 文档</a>。默认值：<code>gp2</code>。</li>
<li><code>zone</code>(弃用)：AWS 区域。如果没有指定 <code>zone</code> 和 <code>zones</code>，
通常卷会在 Kubernetes 集群节点所在的活动区域中轮询调度分配。
<code>zone</code> 和 <code>zones</code> 参数不能同时使用。</li>
<li><code>zones</code>(弃用)：以逗号分隔的 AWS 区域列表。
如果没有指定 <code>zone</code> 和 <code>zones</code>，通常卷会在 Kubernetes 集群节点所在的
活动区域中轮询调度分配。<code>zone</code>和<code>zones</code>参数不能同时使用。</li>
<li><code>iopsPerGB</code>：只适用于 <code>io1</code> 卷。每 GiB 每秒 I/O 操作。
AWS 卷插件将其与请求卷的大小相乘以计算 IOPS 的容量，
并将其限制在 20000 IOPS（AWS 支持的最高值，请参阅
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html">AWS 文档</a>。
这里需要输入一个字符串，即 <code>&quot;10&quot;</code>，而不是 <code>10</code>。</li>
<li><code>fsType</code>：受 Kubernetes 支持的文件类型。默认值：<code>&quot;ext4&quot;</code>。</li>
<li><code>encrypted</code>：指定 EBS 卷是否应该被加密。合法值为 <code>&quot;true&quot;</code> 或者 <code>&quot;false&quot;</code>。
这里需要输入字符串，即 <code>&quot;true&quot;</code>, 而非 <code>true</code>。</li>
<li><code>kmsKeyId</code>：可选。加密卷时使用密钥的完整 Amazon 资源名称。
如果没有提供，但 <code>encrypted</code> 值为 true，AWS 生成一个密钥。关于有效的 ARN 值，请参阅 AWS 文档。</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
`zone` and `zones` parameters are deprecated and replaced with
[allowedTopologies](#allowed-topologies)
 -->
<p><code>zone</code> 和 <code>zones</code> 已被弃用并被 <a href="#allowed-topologies">允许的拓扑结构</a> 取代。
</div>
<h3 id="gce-pd">GCE PD</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/gce-pd<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>pd-standard<span style="color:#bbb">
</span><span style="color:#bbb">   </span><span style="color:#008000;font-weight:bold">fstype</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replication-type</span>:<span style="color:#bbb"> </span>none<span style="color:#bbb">
</span></code></pre></div><!--
* `type`: `pd-standard` or `pd-ssd`. Default: `pd-standard`
* `zone` (Deprecated): GCE zone. If neither `zone` nor `zones` is specified, volumes are
  generally round-robin-ed across all active zones where Kubernetes cluster has
  a node. `zone` and `zones` parameters must not be used at the same time.
* `zones` (Deprecated): A comma separated list of GCE zone(s). If neither `zone` nor `zones`
  is specified, volumes are generally round-robin-ed across all active zones
  where Kubernetes cluster has a node. `zone` and `zones` parameters must not
  be used at the same time.
* `fstype`: `ext4` or `xfs`. Default: `ext4`. The defined filesystem type must be supported by the host operating system.
* `replication-type`: `none` or `regional-pd`. Default: `none`.
-->
<ul>
<li><code>type</code>：<code>pd-standard</code> 或者 <code>pd-ssd</code>。默认：<code>pd-standard</code></li>
<li><code>zone</code>(弃用)：GCE 区域。如果没有指定 <code>zone</code> 和 <code>zones</code>，通常
卷会在 Kubernetes 集群节点所在的活动区域中轮询调度分配。
<code>zone</code> 和 <code>zones</code> 参数不能同时使用。</li>
<li><code>zones</code>(弃用)：逗号分隔的 GCE 区域列表。如果没有指定 <code>zone</code> 和 <code>zones</code>，
通常卷会在 Kubernetes 集群节点所在的活动区域中轮询调度（round-robin）分配。
<code>zone</code> 和 <code>zones</code> 参数不能同时使用。</li>
<li><code>fstype</code>: <code>ext4</code> 或 <code>xfs</code>。 默认: <code>ext4</code>。宿主机操作系统必须支持所定义的文件系统类型。</li>
<li><code>replication-type</code>：<code>none</code> 或者 <code>regional-pd</code>。默认值：<code>none</code>。</li>
</ul>
<!--
If `replication-type` is set to `none`, a regular (zonal) PD will be provisioned.
-->
<p>如果 <code>replication-type</code> 设置为 <code>none</code>，会制备一个常规（当前区域内的）持久化磁盘。</p>
<!--
If `replication-type` is set to `regional-pd`, a
[Regional Persistent Disk](https://cloud.google.com/compute/docs/disks/#repds)
will be provisioned. It's highly recommended to have
`volumeBindingMode: WaitForFirstConsumer` set, in which case when you create
a Pod that consumes a PersistentVolumeClaim which uses this StorageClass, a
Regional Persistent Disk is provisioned with two zones. One zone is the same
as the zone that the Pod is scheduled in. The other zone is randomly picked
from the zones available to the cluster. Disk zones can be further constrained
using `allowedTopologies`.
-->
<p>如果 <code>replication-type</code> 设置为 <code>regional-pd</code>，会制备一个
<a href="https://cloud.google.com/compute/docs/disks/#repds">区域性持久化磁盘（Regional Persistent Disk）</a>。</p>
<p>强烈建议设置 <code>volumeBindingMode: WaitForFirstConsumer</code>，这样设置后，
当你创建一个 Pod，它使用的 PersistentVolumeClaim 使用了这个 StorageClass，
区域性持久化磁盘会在两个区域里制备。 其中一个区域是 Pod 所在区域。
另一个区域是会在集群管理的区域中任意选择。磁盘区域可以通过 <code>allowedTopologies</code> 加以限制。</p>
<!--
`zone` and `zones` parameters are deprecated and replaced with
[allowedTopologies](#allowed-topologies)
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <code>zone</code> 和 <code>zones</code> 已被弃用并被 <a href="#allowed-topologies">allowedTopologies</a> 取代。
</div>
<h3 id="glusterfs">Glusterfs</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/glusterfs<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resturl</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;http://127.0.0.1:8081&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;630372ccdc720a92c681fb928f27b53f&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restauthenabled</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restuser</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;admin&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">secretNamespace</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;default&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;heketi-secret&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">gidMin</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;40000&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">gidMax</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;50000&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumetype</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;replicate:3&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
* `resturl`: Gluster REST service/Heketi service url which provision gluster
  volumes on demand. The general format should be `IPaddress:Port` and this is
  a mandatory parameter for GlusterFS dynamic provisioner. If Heketi service is
  exposed as a routable service in openshift/kubernetes setup, this can have a
  format similar to `http://heketi-storage-project.cloudapps.mystorage.com`
  where the fqdn is a resolvable Heketi service url.
* `restauthenabled` : Gluster REST service authentication boolean that enables
  authentication to the REST server. If this value is `"true"`, `restuser` and
  `restuserkey` or `secretNamespace` + `secretName` have to be filled. This
  option is deprecated, authentication is enabled when any of `restuser`,
  `restuserkey`, `secretName` or `secretNamespace` is specified.
* `restuser` : Gluster REST service/Heketi user who has access to create volumes
  in the Gluster Trusted Pool.
* `restuserkey` : Gluster REST service/Heketi user's password which will be used
  for authentication to the REST server. This parameter is deprecated in favor
  of `secretNamespace` + `secretName`.
-->
<ul>
<li><code>resturl</code>：制备 gluster 卷的需求的 Gluster REST 服务/Heketi 服务 url。
通用格式应该是 <code>IPaddress:Port</code>，这是 GlusterFS 动态制备器的必需参数。
如果 Heketi 服务在 OpenShift/kubernetes 中安装并暴露为可路由服务，则可以使用类似于
<code>http://heketi-storage-project.cloudapps.mystorage.com</code> 的格式，其中 fqdn 是可解析的 heketi 服务网址。</li>
<li><code>restauthenabled</code>：Gluster REST 服务身份验证布尔值，用于启用对 REST 服务器的身份验证。
如果此值为 'true'，则必须填写 <code>restuser</code> 和 <code>restuserkey</code> 或 <code>secretNamespace</code> + <code>secretName</code>。
此选项已弃用，当在指定 <code>restuser</code>、<code>restuserkey</code>、<code>secretName</code> 或  <code>secretNamespace</code> 时，身份验证被启用。</li>
<li><code>restuser</code>：在 Gluster 可信池中有权创建卷的 Gluster REST服务/Heketi 用户。</li>
<li><code>restuserkey</code>：Gluster REST 服务/Heketi 用户的密码将被用于对 REST 服务器进行身份验证。
此参数已弃用，取而代之的是 <code>secretNamespace</code> + <code>secretName</code>。</li>
</ul>
<!--
* `secretNamespace`, `secretName` : Identification of Secret instance that
  contains user password to use when talking to Gluster REST service. These
  parameters are optional, empty password will be used when both
  `secretNamespace` and `secretName` are omitted. The provided secret must have
  type `"kubernetes.io/glusterfs"`, for example created in this way:

    ```
    kubectl create secret generic heketi-secret \
      --type="kubernetes.io/glusterfs" --from-literal=key='opensesame' \
      --namespace=default
    ```

    Example of a secret can be found in
    [glusterfs-provisioning-secret.yaml](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/glusterfs/glusterfs-secret.yaml).
-->
<ul>
<li>
<p><code>secretNamespace</code>，<code>secretName</code>：Secret 实例的标识，包含与 Gluster
REST 服务交互时使用的用户密码。
这些参数是可选的，<code>secretNamespace</code> 和 <code>secretName</code> 都省略时使用空密码。
所提供的 Secret 必须将类型设置为 &quot;kubernetes.io/glusterfs&quot;，例如以这种方式创建：</p>
<pre><code>kubectl create secret generic heketi-secret \
  --type=&quot;kubernetes.io/glusterfs&quot; --from-literal=key='opensesame' \
  --namespace=default
</code></pre><p>Secret 的例子可以在 <a href="https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/glusterfs/glusterfs-secret.yaml">glusterfs-provisioning-secret.yaml</a> 中找到。</p>
</li>
</ul>
<!--
* `clusterid`: `630372ccdc720a92c681fb928f27b53f` is the ID of the cluster
  which will be used by Heketi when provisioning the volume. It can also be a
  list of clusterids, for example:
  `"8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397"`. This
  is an optional parameter.
* `gidMin`, `gidMax` : The minimum and maximum value of GID range for the
  StorageClass. A unique value (GID) in this range ( gidMin-gidMax ) will be
  used for dynamically provisioned volumes. These are optional values. If not
  specified, the volume will be provisioned with a value between 2000-2147483647
  which are defaults for gidMin and gidMax respectively.
-->
<ul>
<li><code>clusterid</code>：<code>630372ccdc720a92c681fb928f27b53f</code> 是集群的 ID，当制备卷时，
Heketi 将会使用这个文件。它也可以是一个 clusterid 列表，例如：
<code>&quot;8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397&quot;</code>。这个是可选参数。</li>
<li><code>gidMin</code>，<code>gidMax</code>：StorageClass GID 范围的最小值和最大值。
在此范围（gidMin-gidMax）内的唯一值（GID）将用于动态制备卷。这些是可选的值。
如果不指定，所制备的卷为一个 2000-2147483647 之间的值，这是 gidMin 和
gidMax 的默认值。</li>
</ul>
<!--
* `volumetype` : The volume type and its parameters can be configured with this
  optional value. If the volume type is not mentioned, it's up to the provisioner
  to decide the volume type.

    For example:
    * Replica volume: `volumetype: replicate:3` where '3' is replica count.
    * Disperse/EC volume: `volumetype: disperse:4:2` where '4' is data and '2' is the redundancy count.
    * Distribute volume: `volumetype: none`

    For available volume types and administration options, refer to the
    [Administration Guide](https://access.redhat.com/documentation/en-US/Red_Hat_Storage/3.1/html/Administration_Guide/part-Overview.html).

    For further reference information, see
    [How to configure Heketi](https://github.com/heketi/heketi/wiki/Setting-up-the-topology).

    When persistent volumes are dynamically provisioned, the Gluster plugin
    automatically creates an endpoint and a headless service in the name
    `gluster-dynamic-<claimname>`. The dynamic endpoint and service are automatically
    deleted when the persistent volume claim is deleted.
-->
<ul>
<li>
<p><code>volumetype</code>：卷的类型及其参数可以用这个可选值进行配置。如果未声明卷类型，则
由制备器决定卷的类型。
例如：</p>
<ul>
<li>'Replica volume': <code>volumetype: replicate:3</code> 其中 '3' 是 replica 数量.</li>
<li>'Disperse/EC volume': <code>volumetype: disperse:4:2</code> 其中 '4' 是数据，'2' 是冗余数量.</li>
<li>'Distribute volume': <code>volumetype: none</code></li>
</ul>
<p>有关可用的卷类型和管理选项，请参阅
<a href="https://access.redhat.com/documentation/en-US/Red_Hat_Storage/3.1/html/Administration_Guide/part-Overview.html">管理指南</a>。</p>
<p>更多相关的参考信息，请参阅
<a href="https://github.com/heketi/heketi/wiki/Setting-up-the-topology">如何配置 Heketi</a>。</p>
<p>当动态制备持久卷时，Gluster 插件自动创建名为 <code>gluster-dynamic-&lt;claimname&gt;</code>
的端点和无头服务。在 PVC 被删除时动态端点和无头服务会自动被删除。</p>
</li>
</ul>
<h3 id="nfs">NFS</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-nfs<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>example.com/external-nfs<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">server</span>:<span style="color:#bbb"> </span>nfs-server.example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/share<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;false&#34;</span><span style="color:#bbb">
</span></code></pre></div><!-- 
* `server`: Server is the hostname or IP address of the NFS server.
* `path`: Path that is exported by the NFS server.
* `readOnly`: A flag indicating whether the storage will be mounted as read only (default false).
-->
<ul>
<li><code>server</code>：NFS 服务器的主机名或 IP 地址。</li>
<li><code>path</code>：NFS 服务器导出的路径。</li>
<li><code>readOnly</code>：是否将存储挂载为只读的标志（默认为 false）。</li>
</ul>
<!-- 
Kubernetes doesn't include an internal NFS provisioner. You need to use an external provisioner to create a StorageClass for NFS.
Here are some examples:
* [NFS Ganesha server and external provisioner](https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner)
* [NFS subdir external provisioner](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)
-->
<p>Kubernetes 不包含内部 NFS 驱动。你需要使用外部驱动为 NFS 创建 StorageClass。
这里有些例子：</p>
<ul>
<li><a href="https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner">NFS Ganesha 服务器和外部驱动</a></li>
<li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">NFS subdir 外部驱动</a></li>
</ul>
<h3 id="openstack-cinder">OpenStack Cinder</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>gold<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/cinder<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">availability</span>:<span style="color:#bbb"> </span>nova<span style="color:#bbb">
</span></code></pre></div><!--
* `availability`: Availability Zone. If not specified, volumes are generally
  round-robin-ed across all active zones where Kubernetes cluster has a node.
-->
<ul>
<li><code>availability</code>：可用区域。如果没有指定，通常卷会在 Kubernetes 集群节点
所在的活动区域中轮转调度。</li>
</ul>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.11 [deprecated]</code>
</div>
<p>This internal provisioner of OpenStack is deprecated. Please use <a href="https://github.com/kubernetes/cloud-provider-openstack">the external cloud provider for OpenStack</a>.
</div>
 -->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.11 [deprecated]</code>
</div>
<p>OpenStack 的内部驱动已经被弃用。请使用
<a href="https://github.com/kubernetes/cloud-provider-openstack">OpenStack 的外部云驱动</a>。
</div>
<h3 id="vsphere">vSphere</h3>
<!--
There are two types of provisioners for vSphere storage classes: 

- [CSI provisioner](#vsphere-provisioner-csi): `csi.vsphere.vmware.com`
- [vCP provisioner](#vcp-provisioner): `kubernetes.io/vsphere-volume`

In-tree provisioners are [deprecated](/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi). For more information on the CSI provisioner, see [Kubernetes vSphere CSI Driver](https://vsphere-csi-driver.sigs.k8s.io/) and [vSphereVolume CSI migration](/docs/concepts/storage/volumes/#csi-migration-5).
-->
<p>vSphere 存储类有两种制备器</p>
<ul>
<li><a href="#vsphere-provisioner-csi">CSI 制备器</a>: <code>csi.vsphere.vmware.com</code></li>
<li><a href="#vcp-provisioner">vCP 制备器</a>: <code>kubernetes.io/vsphere-volume</code></li>
</ul>
<p>树内制备器已经被
<a href="/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi">弃用</a>。
更多关于 CSI 制备器的详情，请参阅
<a href="https://vsphere-csi-driver.sigs.k8s.io/">Kubernetes vSphere CSI 驱动</a>
和 <a href="/zh/docs/concepts/storage/volumes/#csi-migration-5">vSphereVolume CSI 迁移</a>。</p>
<!--
#### CSI Provisioner {#vsphere-provisioner-csi}

The vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters. For an example, refer to the [vSphere CSI repository](https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/master/example/vanilla-k8s-RWM-filesystem-volumes/example-sc.yaml).
-->
<h4 id="vsphere-provisioner-csi">CSI 制备器</h4>
<p>vSphere CSI StorageClass 制备器在 Tanzu Kubernetes 集群下运行。示例请参
<a href="https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/master/example/vanilla-k8s-RWM-filesystem-volumes/example-sc.yaml">vSphere CSI 仓库</a>。</p>
<!--
#### vCP Provisioner 

The following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.  
-->
<h4 id="vcp-provisioner">vCP 制备器</h4>
<p>以下示例使用 VMware Cloud Provider (vCP) StorageClass 调度器</p>
<!--
1. Create a StorageClass with a user specified disk format.
 -->
<ol>
<li>
<p>使用用户指定的磁盘格式创建一个 StorageClass。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/vsphere-volume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">diskformat</span>:<span style="color:#bbb"> </span>zeroedthick<span style="color:#bbb">
</span></code></pre></div><!--
`diskformat`: `thin`, `zeroedthick` and `eagerzeroedthick`. Default: `"thin"`.
-->
<p><code>diskformat</code>: <code>thin</code>, <code>zeroedthick</code> 和 <code>eagerzeroedthick</code>。默认值: <code>&quot;thin&quot;</code>。</p>
</li>
</ol>
<!--
2. Create a StorageClass with a disk format on a user specified datastore.
-->
<ol start="2">
<li>
<p>在用户指定的数据存储上创建磁盘格式的 StorageClass。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/vsphere-volume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">diskformat</span>:<span style="color:#bbb"> </span>zeroedthick<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">datastore</span>:<span style="color:#bbb"> </span>VSANDatastore<span style="color:#bbb">
</span></code></pre></div><!--
`datastore`: The user can also specify the datastore in the StorageClass.
The volume will be created on the datastore specified in the storage class,
which in this case is `VSANDatastore`. This field is optional. If the
datastore is not specified, then the volume will be created on the datastore
specified in the vSphere config file used to initialize the vSphere Cloud
Provider.
-->
<p><code>datastore</code>：用户也可以在 StorageClass 中指定数据存储。
卷将在 storage class 中指定的数据存储上创建，在这种情况下是 <code>VSANDatastore</code>。
该字段是可选的。
如果未指定数据存储，则将在用于初始化 vSphere Cloud Provider 的 vSphere
配置文件中指定的数据存储上创建该卷。</p>
</li>
</ol>
<!--
3. Storage Policy Management inside kubernetes
-->
<ol start="3">
<li>
<p>Kubernetes 中的存储策略管理</p>
<!--
* Using existing vCenter SPBM policy

  One of the most important features of vSphere for Storage Management is
  policy based Management. Storage Policy Based Management (SPBM) is a
  storage policy framework that provides a single unified control plane
  across a broad range of data services and storage solutions. SPBM enables
  vSphere administrators to overcome upfront storage provisioning challenges,
  such as capacity planning, differentiated service levels and managing
  capacity headroom.

  The SPBM policies can be specified in the StorageClass using the
  `storagePolicyName` parameter.
 -->
<ul>
<li>
<p>使用现有的 vCenter SPBM 策略</p>
<p>vSphere 用于存储管理的最重要特性之一是基于策略的管理。
基于存储策略的管理（SPBM）是一个存储策略框架，提供单一的统一控制平面的
跨越广泛的数据服务和存储解决方案。
SPBM 使能 vSphere 管理员克服先期的存储配置挑战，如容量规划，差异化服务等级和管理容量空间。</p>
<p>SPBM 策略可以在 StorageClass 中使用 <code>storagePolicyName</code> 参数声明。</p>
</li>
</ul>
 <!--
 * Virtual SAN policy support inside Kubernetes

   Vsphere Infrastructure (VI) Admins will have the ability to specify custom
   Virtual SAN Storage Capabilities during dynamic volume provisioning. You
   can now define storage requirements, such as performance and availability,
   in the form of storage capabilities during dynamic volume provisioning.
   The storage capability requirements are converted into a Virtual SAN
   policy which are then pushed down to the Virtual SAN layer when a
   persistent volume (virtual disk) is being created. The virtual disk is
   distributed across the Virtual SAN datastore to meet the requirements.

   You can see [Storage Policy Based Management for dynamic provisioning of volumes](https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/policy-based-mgmt.html)
   for more details on how to use storage policies for persistent volumes
   management.
 -->
<ul>
<li>
<p>Kubernetes 内的 Virtual SAN 策略支持</p>
<p>Vsphere Infrastructure（VI）管理员将能够在动态卷配置期间指定自定义 Virtual SAN
存储功能。你现在可以在动态制备卷期间以存储能力的形式定义存储需求，例如性能和可用性。
存储能力需求会转换为 Virtual SAN 策略，之后当持久卷（虚拟磁盘）被创建时，
会将其推送到 Virtual SAN 层。虚拟磁盘分布在 Virtual SAN 数据存储中以满足要求。</p>
<p>你可以参考<a href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/policy-based-mgmt.html">基于存储策略的动态制备卷管理</a>，
进一步了解有关持久卷管理的存储策略的详细信息。</p>
</li>
</ul>
</li>
</ol>
<!--
There are few
[vSphere examples](https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere)
which you try out for persistent volume management inside Kubernetes for vSphere.
-->
<p>有几个 <a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere">vSphere 例子</a>
供你在 Kubernetes for vSphere 中尝试进行持久卷管理。</p>
<h3 id="ceph-rbd">Ceph RBD</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/rbd<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">monitors</span>:<span style="color:#bbb"> </span><span style="color:#666">10.16.153.105</span>:<span style="color:#666">6789</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">adminId</span>:<span style="color:#bbb"> </span>kube<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">adminSecretName</span>:<span style="color:#bbb"> </span>ceph-secret<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">adminSecretNamespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">pool</span>:<span style="color:#bbb"> </span>kube<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">userId</span>:<span style="color:#bbb"> </span>kube<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">userSecretName</span>:<span style="color:#bbb"> </span>ceph-secret-user<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">userSecretNamespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">imageFormat</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">imageFeatures</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;layering&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
* `monitors`: Ceph monitors, comma delimited. This parameter is required.
* `adminId`: Ceph client ID that is capable of creating images in the pool.
  Default is "admin".
* `adminSecretName`: Secret Name for `adminId`. This parameter is required.
  The provided secret must have type "kubernetes.io/rbd".
* `adminSecretNamespace`: The namespace for `adminSecretName`. Default is "default".
* `pool`: Ceph RBD pool. Default is "rbd".
* `userId`: Ceph client ID that is used to map the RBD image. Default is the
  same as `adminId`.
-->
<ul>
<li><code>monitors</code>：Ceph monitor，逗号分隔。该参数是必需的。</li>
<li><code>adminId</code>：Ceph 客户端 ID，用于在池 ceph 池中创建映像。默认是 &quot;admin&quot;。</li>
<li><code>adminSecret</code>：<code>adminId</code> 的 Secret 名称。该参数是必需的。
提供的 secret 必须有值为 &quot;kubernetes.io/rbd&quot; 的 type 参数。</li>
<li><code>adminSecretNamespace</code>：<code>adminSecret</code> 的命名空间。默认是 &quot;default&quot;。</li>
<li><code>pool</code>: Ceph RBD 池. 默认是 &quot;rbd&quot;。</li>
<li><code>userId</code>：Ceph 客户端 ID，用于映射 RBD 镜像。默认与 <code>adminId</code> 相同。</li>
</ul>
<!--
* `userSecretName`: The name of Ceph Secret for `userId` to map RBD image. It
  must exist in the same namespace as PVCs. This parameter is required.
  The provided secret must have type "kubernetes.io/rbd", e.g. created in this
  way:

    ```shell
    kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \
      --from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==' \
      --namespace=kube-system
    ```
-->
<ul>
<li>
<p><code>userSecretName</code>：用于映射 RBD 镜像的 <code>userId</code> 的 Ceph Secret 的名字。
它必须与 PVC 存在于相同的 namespace 中。该参数是必需的。
提供的 secret 必须具有值为 &quot;kubernetes.io/rbd&quot; 的 type 参数，例如以这样的方式创建：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic ceph-secret --type<span style="color:#666">=</span><span style="color:#b44">&#34;kubernetes.io/rbd&#34;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --from-literal<span style="color:#666">=</span><span style="color:#b8860b">key</span><span style="color:#666">=</span><span style="color:#b44">&#39;QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --namespace<span style="color:#666">=</span>kube-system
</code></pre></div></li>
</ul>
<!--
* `userSecretNamespace`: The namespace for `userSecretName`.
* `fsType`: fsType that is supported by kubernetes. Default: `"ext4"`.
* `imageFormat`: Ceph RBD image format, "1" or "2". Default is "2".
* `imageFeatures`: This parameter is optional and should only be used if you
  set `imageFormat` to "2". Currently supported features are `layering` only.
  Default is "", and no features are turned on.
-->
<ul>
<li><code>userSecretNamespace</code>：<code>userSecretName</code> 的命名空间。</li>
<li><code>fsType</code>：Kubernetes 支持的 fsType。默认：<code>&quot;ext4&quot;</code>。</li>
<li><code>imageFormat</code>：Ceph RBD 镜像格式，&quot;1&quot; 或者 &quot;2&quot;。默认值是 &quot;1&quot;。</li>
<li><code>imageFeatures</code>：这个参数是可选的，只能在你将 <code>imageFormat</code> 设置为 &quot;2&quot; 才使用。
目前支持的功能只是 <code>layering</code>。默认是 &quot;&quot;，没有功能打开。</li>
</ul>
<h3 id="quobyte">Quobyte</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [deprecated]</code>
</div>


<!-- 
The Quobyte in-tree storage plugin is deprecated, an 
[example](https://github.com/quobyte/quobyte-csi/blob/master/example/StorageClass.yaml)
`StorageClass` for the out-of-tree Quobyte plugin can be found at the Quobyte CSI repository.
-->
<p>Quobyte 树内（in-tree）存储插件已弃用，
你可以在 Quobyte CSI 仓库中找到用于树外（out-of-tree）Quobyte 插件的 <code>StorageClass</code>
<a href="https://github.com/quobyte/quobyte-csi/blob/master/example/StorageClass.yaml">示例</a>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">   </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/quobyte<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">quobyteAPIServer</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;http://138.68.74.142:7860&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">registry</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;138.68.74.142:7861&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">adminSecretName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;quobyte-admin-secret&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">adminSecretNamespace</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kube-system&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;root&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;root&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">quobyteConfig</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;BASE&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">quobyteTenant</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;DEFAULT&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
* `quobyteAPIServer`: API Server of Quobyte in the format
  `"http(s)://api-server:7860"`
* `registry`: Quobyte registry to use to mount the volume. You can specify the
  registry as ``<host>:<port>`` pair or if you want to specify multiple
  registries, put a comma between them.
  ``<host1>:<port>,<host2>:<port>,<host3>:<port>``.
  The host can be an IP address or if you have a working DNS you can also
  provide the DNS names.
* `adminSecretNamespace`: The namespace for `adminSecretName`.
  Default is "default".
-->
<ul>
<li><code>quobyteAPIServer</code>：Quobyte API 服务器的格式是 <code>&quot;http(s)://api-server:7860&quot;</code></li>
<li><code>registry</code>：用于挂载卷的 Quobyte 仓库。你可以指定仓库为 <code>&lt;host&gt;:&lt;port&gt;</code>
或者如果你想指定多个 registry，在它们之间添加逗号，例如
<code>&lt;host1&gt;:&lt;port&gt;,&lt;host2&gt;:&lt;port&gt;,&lt;host3&gt;:&lt;port&gt;</code>。
主机可以是一个 IP 地址，或者如果你有正在运行的 DNS，你也可以提供 DNS 名称。</li>
<li><code>adminSecretNamespace</code>：<code>adminSecretName</code> 的名字空间。
默认值是 &quot;default&quot;。</li>
</ul>
<!--
* `adminSecretName`: secret that holds information about the Quobyte user and
  the password to authenticate against the API server. The provided secret
  must have type "kubernetes.io/quobyte" and the keys `user` and `password`,
  e.g. created in this way:

    ```shell
    kubectl create secret generic quobyte-admin-secret \
      --type="kubernetes.io/quobyte" --from-literal=user='admin' --from-literal=password='opensesame' \
      --namespace=kube-system
    ```
-->
<ul>
<li>
<p><code>adminSecretName</code>：保存关于 Quobyte 用户和密码的 Secret，用于对 API 服务器进行身份验证。
提供的 secret 必须有值为 &quot;kubernetes.io/quobyte&quot; 的 type 参数和 <code>user</code>
与 <code>password</code> 的键值，
例如以这种方式创建：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic quobyte-admin-secret <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --type<span style="color:#666">=</span><span style="color:#b44">&#34;kubernetes.io/quobyte&#34;</span> --from-literal<span style="color:#666">=</span><span style="color:#b8860b">key</span><span style="color:#666">=</span><span style="color:#b44">&#39;opensesame&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --namespace<span style="color:#666">=</span>kube-system
</code></pre></div></li>
</ul>
<!--
* `user`: maps all access to this user. Default is "root".
* `group`: maps all access to this group. Default is "nfsnobody".
* `quobyteConfig`: use the specified configuration to create the volume. You
  can create a new configuration or modify an existing one with the Web
  console or the quobyte CLI. Default is "BASE".
* `quobyteTenant`: use the specified tenant ID to create/delete the volume.
  This Quobyte tenant has to be already present in Quobyte.
  Default is "DEFAULT".
-->
<ul>
<li><code>user</code>：对这个用户映射的所有访问权限。默认是 &quot;root&quot;。</li>
<li><code>group</code>：对这个组映射的所有访问权限。默认是 &quot;nfsnobody&quot;。</li>
<li><code>quobyteConfig</code>：使用指定的配置来创建卷。你可以创建一个新的配置，
或者，可以修改 Web 控制台或 quobyte CLI 中现有的配置。默认是 &quot;BASE&quot;。</li>
<li><code>quobyteTenant</code>：使用指定的租户 ID 创建/删除卷。这个 Quobyte 租户必须
已经于 Quobyte 中存在。默认是 &quot;DEFAULT&quot;。</li>
</ul>
<!--
### Azure Disk
-->
<h3 id="azure-磁盘">Azure 磁盘</h3>
<!--
#### Azure Unmanaged Disk Storage Class {#azure-disk-storage-class}
-->
<h4 id="azure-unmanaged-disk-storage-class">Azure Unmanaged Disk Storage Class（非托管磁盘存储类）</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/azure-disk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">skuName</span>:<span style="color:#bbb"> </span>Standard_LRS<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">location</span>:<span style="color:#bbb"> </span>eastus<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageAccount</span>:<span style="color:#bbb"> </span>azure_storage_account_name<span style="color:#bbb">
</span></code></pre></div><!--
* `skuName`: Azure storage account Sku tier. Default is empty.
* `location`: Azure storage account location. Default is empty.
* `storageAccount`: Azure storage account name. If a storage account is provided,
  it must reside in the same resource group as the cluster, and `location` is
  ignored. If a storage account is not provided, a new storage account will be
  created in the same resource group as the cluster.
-->
<ul>
<li><code>skuName</code>：Azure 存储帐户 Sku 层。默认为空。</li>
<li><code>location</code>：Azure 存储帐户位置。默认为空。</li>
<li><code>storageAccount</code>：Azure 存储帐户名称。
如果提供存储帐户，它必须位于与集群相同的资源组中，并且 <code>location</code>
是被忽略的。如果未提供存储帐户，则会在与群集相同的资源组中创建新的存储帐户。</li>
</ul>
<!--
#### Azure Disk Storage Class (starting from v1.7.2) {#azure-disk-storage-class}
-->
<h4 id="azure-disk-storage-class">Azure 磁盘 Storage Class（从 v1.7.2 开始）</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/azure-disk<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageaccounttype</span>:<span style="color:#bbb"> </span>Standard_LRS<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>managed<span style="color:#bbb">
</span></code></pre></div><!--
* `storageaccounttype`: Azure storage account Sku tier. Default is empty.
* `kind`: Possible values are `shared`, `dedicated`, and `managed` (default).
  When `kind` is `shared`, all unmanaged disks are created in a few shared
  storage accounts in the same resource group as the cluster. When `kind` is
  `dedicated`, a new dedicated storage account will be created for the new
  unmanaged disk in the same resource group as the cluster. When `kind` is
  `managed`, all managed disks are created in the same resource group as
  the cluster.
* `resourceGroup`: Specify the resource group in which the Azure disk will be created. 
   It must be an existing resource group name. If it is unspecified, the disk will be 
   placed in the same resource group as the current Kubernetes cluster.
-->
<ul>
<li><code>storageaccounttype</code>：Azure 存储帐户 Sku 层。默认为空。</li>
<li><code>kind</code>：可能的值是 <code>shared</code>、<code>dedicated</code> 和 <code>managed</code>（默认）。
当 <code>kind</code> 的值是 <code>shared</code> 时，所有非托管磁盘都在集群的同一个资源组中的几个共享存储帐户中创建。
当 <code>kind</code> 的值是 <code>dedicated</code> 时，将为在集群的同一个资源组中新的非托管磁盘创建新的专用存储帐户。</li>
<li><code>resourceGroup</code>: 指定要创建 Azure 磁盘所属的资源组。必须是已存在的资源组名称。
若未指定资源组，磁盘会默认放入与当前 Kubernetes 集群相同的资源组中。</li>
</ul>
<!--
- Premium VM can attach both Standard_LRS and Premium_LRS disks, while Standard
  VM can only attach Standard_LRS disks.
- Managed VM can only attach managed disks and unmanaged VM can only attach
  unmanaged disks.
-->
<ul>
<li>Premium VM 可以同时添加 Standard_LRS 和 Premium_LRS 磁盘，而 Standard
虚拟机只能添加 Standard_LRS 磁盘。</li>
<li>托管虚拟机只能连接托管磁盘，非托管虚拟机只能连接非托管磁盘。</li>
</ul>
<!--
### Azure File
-->
<h3 id="azure-文件">Azure 文件</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>azurefile<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/azure-file<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">skuName</span>:<span style="color:#bbb"> </span>Standard_LRS<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">location</span>:<span style="color:#bbb"> </span>eastus<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageAccount</span>:<span style="color:#bbb"> </span>azure_storage_account_name<span style="color:#bbb">
</span></code></pre></div><!--
* `skuName`: Azure storage account Sku tier. Default is empty.
* `location`: Azure storage account location. Default is empty.
* `storageAccount`: Azure storage account name.  Default is empty. If a storage
  account is not provided, all storage accounts associated with the resource
  group are searched to find one that matches `skuName` and `location`. If a
  storage account is provided, it must reside in the same resource group as the
  cluster, and `skuName` and `location` are ignored.
* `secretNamespace`: the namespace of the secret that contains the Azure Storage
  Account Name and Key. Default is the same as the Pod.
* `secretName`: the name of the secret that contains the Azure Storage Account Name and
  Key. Default is `azure-storage-account-<accountName>-secret`
* `readOnly`: a flag indicating whether the storage will be mounted as read only.
  Defaults to false which means a read/write mount. This setting will impact the
  `ReadOnly` setting in VolumeMounts as well.
-->
<ul>
<li><code>skuName</code>：Azure 存储帐户 Sku 层。默认为空。</li>
<li><code>location</code>：Azure 存储帐户位置。默认为空。</li>
<li><code>storageAccount</code>：Azure 存储帐户名称。默认为空。
如果不提供存储帐户，会搜索所有与资源相关的存储帐户，以找到一个匹配
<code>skuName</code> 和 <code>location</code> 的账号。
如果提供存储帐户，它必须存在于与集群相同的资源组中，<code>skuName</code> 和 <code>location</code> 会被忽略。</li>
<li><code>secretNamespace</code>：包含 Azure 存储帐户名称和密钥的密钥的名称空间。
默认值与 Pod 相同。</li>
<li><code>secretName</code>：包含 Azure 存储帐户名称和密钥的密钥的名称。
默认值为 <code>azure-storage-account-&lt;accountName&gt;-secret</code></li>
<li><code>readOnly</code>：指示是否将存储安装为只读的标志。默认为 false，表示&quot;读/写&quot;挂载。
该设置也会影响VolumeMounts中的 <code>ReadOnly</code> 设置。</li>
</ul>
<!--
During storage provisioning, a secret named by `secretName` is created for the
mounting credentials. If the cluster has enabled both
[RBAC](/docs/reference/access-authn-authz/rbac/) and
[Controller Roles](/docs/reference/access-authn-authz/rbac/#controller-roles),
add the `create` permission of resource `secret` for clusterrole
`system:controller:persistent-volume-binder`.
-->
<p>在存储制备期间，为挂载凭证创建一个名为 <code>secretName</code> 的 Secret。如果集群同时启用了
<a href="/zh/docs/reference/access-authn-authz/rbac/">RBAC</a> 和
<a href="/zh/docs/reference/access-authn-authz/rbac/#controller-roles">控制器角色</a>，
为 <code>system:controller:persistent-volume-binder</code> 的 clusterrole 添加
<code>Secret</code> 资源的 <code>create</code> 权限。</p>
<!--
In a multi-tenancy context, it is strongly recommended to set the value for
`secretNamespace` explicitly, otherwise the storage account credentials may
be read by other users.
-->
<p>在多租户上下文中，强烈建议显式设置 <code>secretNamespace</code> 的值，否则
其他用户可能会读取存储帐户凭据。</p>
<!--
### Portworx Volume
 -->
<h3 id="portworx-卷">Portworx 卷</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>portworx-io-priority-high<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/portworx-volume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">repl</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">snap_interval</span>:<span style="color:#bbb">   </span><span style="color:#b44">&#34;70&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">priority_io</span>:<span style="color:#bbb">  </span><span style="color:#b44">&#34;high&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div><!--
* `fs`: filesystem to be laid out: `none/xfs/ext4` (default: `ext4`).
* `block_size`: block size in Kbytes (default: `32`).
* `repl`: number of synchronous replicas to be provided in the form of
  replication factor `1..3` (default: `1`) A string is expected here i.e.
  `"1"` and not `1`.
* `priority_io`: determines whether the volume will be created from higher
  performance or a lower priority storage `high/medium/low` (default: `low`).
* `snap_interval`: clock/time interval in minutes for when to trigger snapshots.
  Snapshots are incremental based on difference with the prior snapshot, 0
  disables snaps (default: `0`). A string is expected here i.e.
  `"70"` and not `70`.
* `aggregation_level`: specifies the number of chunks the volume would be
  distributed into, 0 indicates a non-aggregated volume (default: `0`). A string
  is expected here i.e. `"0"` and not `0`
* `ephemeral`: specifies whether the volume should be cleaned-up after unmount
  or should be persistent. `emptyDir` use case can set this value to true and
  `persistent volumes` use case such as for databases like Cassandra should set
  to false, `true/false` (default `false`). A string is expected here i.e.
  `"true"` and not `true`.
-->
<ul>
<li><code>fs</code>：选择的文件系统：<code>none/xfs/ext4</code>（默认：<code>ext4</code>）。</li>
<li><code>block_size</code>：以 Kbytes 为单位的块大小（默认值：<code>32</code>）。</li>
<li><code>repl</code>：同步副本数量，以复制因子 <code>1..3</code>（默认值：<code>1</code>）的形式提供。
这里需要填写字符串，即，<code>&quot;1&quot;</code> 而不是 <code>1</code>。</li>
<li><code>io_priority</code>：决定是否从更高性能或者较低优先级存储创建卷
<code>high/medium/low</code>（默认值：<code>low</code>）。</li>
<li><code>snap_interval</code>：触发快照的时钟/时间间隔（分钟）。
快照是基于与先前快照的增量变化，0 是禁用快照（默认：<code>0</code>）。
这里需要填写字符串，即，是 <code>&quot;70&quot;</code> 而不是 <code>70</code>。</li>
<li><code>aggregation_level</code>：指定卷分配到的块数量，0 表示一个非聚合卷（默认：<code>0</code>）。
这里需要填写字符串，即，是 <code>&quot;0&quot;</code> 而不是 <code>0</code>。</li>
<li><code>ephemeral</code>：指定卷在卸载后进行清理还是持久化。
<code>emptyDir</code> 的使用场景可以将这个值设置为 true ，
<code>persistent volumes</code> 的使用场景可以将这个值设置为 false
（例如 Cassandra 这样的数据库）
<code>true/false</code>（默认为 <code>false</code>）。这里需要填写字符串，即，
是 <code>&quot;true&quot;</code> 而不是 <code>true</code>。</li>
</ul>
<h3 id="scaleio">ScaleIO</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/scaleio<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">gateway</span>:<span style="color:#bbb"> </span>https://192.168.99.200:443/api<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">system</span>:<span style="color:#bbb"> </span>scaleio<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">protectionDomain</span>:<span style="color:#bbb"> </span>pd0<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storagePool</span>:<span style="color:#bbb"> </span>sp1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageMode</span>:<span style="color:#bbb"> </span>ThinProvisioned<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">secretRef</span>:<span style="color:#bbb"> </span>sio-secret<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;false&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span>xfs<span style="color:#bbb">
</span></code></pre></div><!--
* `provisioner`: attribute is set to `kubernetes.io/scaleio`
* `gateway`: address to a ScaleIO API gateway (required)
* `system`: the name of the ScaleIO system (required)
* `protectionDomain`: the name of the ScaleIO protection domain (required)
* `storagePool`: the name of the volume storage pool (required)
* `storageMode`: the storage provision mode: `ThinProvisioned` (default) or
  `ThickProvisioned`
* `secretRef`: reference to a configured Secret object (required)
* `readOnly`: specifies the access mode to the mounted volume (default false)
* `fsType`: the file system to use for the volume (default ext4)
-->
<ul>
<li><code>provisioner</code>：属性设置为 <code>kubernetes.io/scaleio</code></li>
<li><code>gateway</code> 到 ScaleIO API 网关的地址（必需）</li>
<li><code>system</code>：ScaleIO 系统的名称（必需）</li>
<li><code>protectionDomain</code>：ScaleIO 保护域的名称（必需）</li>
<li><code>storagePool</code>：卷存储池的名称（必需）</li>
<li><code>storageMode</code>：存储提供模式：<code>ThinProvisioned</code>（默认）或 <code>ThickProvisioned</code></li>
<li><code>secretRef</code>：对已配置的 Secret 对象的引用（必需）</li>
<li><code>readOnly</code>：指定挂载卷的访问模式（默认为 false）</li>
<li><code>fsType</code>：卷的文件系统（默认是 ext4）</li>
</ul>
<!--
The ScaleIO Kubernetes volume plugin requires a configured Secret object.
The secret must be created with type `kubernetes.io/scaleio` and use the same
namespace value as that of the PVC where it is referenced
as shown in the following command:

```shell
kubectl create secret generic sio-secret --type="kubernetes.io/scaleio" \
--from-literal=username=sioadmin --from-literal=password=d2NABDNjMA== \
--namespace=default
```
-->
<p>ScaleIO Kubernetes 卷插件需要配置一个 Secret 对象。
Secret 必须用 <code>kubernetes.io/scaleio</code> 类型创建，并与引用它的
PVC 所属的名称空间使用相同的值。如下面的命令所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic sio-secret --type<span style="color:#666">=</span><span style="color:#b44">&#34;kubernetes.io/scaleio&#34;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --from-literal<span style="color:#666">=</span><span style="color:#b8860b">username</span><span style="color:#666">=</span>sioadmin --from-literal<span style="color:#666">=</span><span style="color:#b8860b">password</span><span style="color:#666">=</span><span style="color:#b8860b">d2NABDNjMA</span><span style="color:#666">==</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --namespace<span style="color:#666">=</span>default
</code></pre></div><h3 id="storageos">StorageOS</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/storageos<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">pool</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">description</span>:<span style="color:#bbb"> </span>Kubernetes volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">fsType</span>:<span style="color:#bbb"> </span>ext4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">adminSecretNamespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">adminSecretName</span>:<span style="color:#bbb"> </span>storageos-secret<span style="color:#bbb">
</span></code></pre></div><!--
* `pool`: The name of the StorageOS distributed capacity pool to provision the
  volume from.  Uses the `default` pool which is normally present if not specified.
* `description`: The description to assign to volumes that were created dynamically.
  All volume descriptions will be the same for the storage class, but different
  storage classes can be used to allow descriptions for different use cases.
  Defaults to `Kubernetes volume`.
* `fsType`: The default filesystem type to request. Note that user-defined rules
  within StorageOS may override this value.  Defaults to `ext4`.
* `adminSecretNamespace`: The namespace where the API configuration secret is
  located. Required if adminSecretName set.
* `adminSecretName`: The name of the secret to use for obtaining the StorageOS
  API credentials. If not specified, default values will be attempted.
-->
<ul>
<li><code>pool</code>：制备卷的 StorageOS 分布式容量池的名称。如果未指定，则使用
通常存在的 <code>default</code> 池。</li>
<li><code>description</code>：指定给动态创建的卷的描述。所有卷描述对于存储类而言都是相同的，
但不同的 storage class 可以使用不同的描述，以区分不同的使用场景。
默认为 <code>Kubernetes volume</code>。</li>
<li><code>fsType</code>：请求的默认文件系统类型。
请注意，在 StorageOS 中用户定义的规则可以覆盖此值。默认为 <code>ext4</code></li>
<li><code>adminSecretNamespace</code>：API 配置 secret 所在的命名空间。
如果设置了 adminSecretName，则是必需的。</li>
<li><code>adminSecretName</code>：用于获取 StorageOS API 凭证的 secret 名称。
如果未指定，则将尝试默认值。</li>
</ul>
<!--
The StorageOS Kubernetes volume plugin can use a Secret object to specify an
endpoint and credentials to access the StorageOS API. This is only required when
the defaults have been changed.
The secret must be created with type `kubernetes.io/storageos` as shown in the
following command:

```shell
kubectl create secret generic storageos-secret \
--type="kubernetes.io/storageos" \
--from-literal=apiAddress=tcp://localhost:5705 \
--from-literal=apiUsername=storageos \
--from-literal=apiPassword=storageos \
--namespace=default
```
-->
<p>StorageOS Kubernetes 卷插件可以使 Secret 对象来指定用于访问 StorageOS API 的端点和凭据。
只有当默认值已被更改时，这才是必须的。
Secret 必须使用 <code>kubernetes.io/storageos</code> 类型创建，如以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic storageos-secret <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--type<span style="color:#666">=</span><span style="color:#b44">&#34;kubernetes.io/storageos&#34;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--from-literal<span style="color:#666">=</span><span style="color:#b8860b">apiAddress</span><span style="color:#666">=</span>tcp://localhost:5705 <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--from-literal<span style="color:#666">=</span><span style="color:#b8860b">apiUsername</span><span style="color:#666">=</span>storageos <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--from-literal<span style="color:#666">=</span><span style="color:#b8860b">apiPassword</span><span style="color:#666">=</span>storageos <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--namespace<span style="color:#666">=</span>default
</code></pre></div><!--
Secrets used for dynamically provisioned volumes may be created in any namespace
and referenced with the `adminSecretNamespace` parameter. Secrets used by
pre-provisioned volumes must be created in the same namespace as the PVC that
references it.
-->
<p>用于动态制备卷的 Secret 可以在任何名称空间中创建，并通过
<code>adminSecretNamespace</code> 参数引用。
预先配置的卷使用的 Secret 必须在与引用它的 PVC 在相同的名称空间中。</p>
<!--
### Local
-->
<h3 id="本地">本地</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code>
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>local-storage<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/no-provisioner<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">volumeBindingMode</span>:<span style="color:#bbb"> </span>WaitForFirstConsumer<span style="color:#bbb">
</span></code></pre></div><!--
Local volumes do not currently support dynamic provisioning, however a StorageClass
should still be created to delay volume binding until pod scheduling. This is
specified by the `WaitForFirstConsumer` volume binding mode.
-->
<p>本地卷还不支持动态制备，然而还是需要创建 StorageClass 以延迟卷绑定，
直到完成 Pod 的调度。这是由 <code>WaitForFirstConsumer</code> 卷绑定模式指定的。</p>
<!--
Delaying volume binding allows the scheduler to consider all of a pod's
scheduling constraints when choosing an appropriate PersistentVolume for a
PersistentVolumeClaim.
-->
<p>延迟卷绑定使得调度器在为 PersistentVolumeClaim 选择一个合适的
PersistentVolume 时能考虑到所有 Pod 的调度限制。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-018f0a7fc6e2f6d16da37702fc39b4f3">6.6 - 动态卷供应</h1>
    
	<!--
title: Dynamic Volume Provisioning
content_type: concept
weight: 40
-->
<!-- overview -->
<!--
Dynamic volume provisioning allows storage volumes to be created on-demand.
Without dynamic provisioning, cluster administrators have to manually make
calls to their cloud or storage provider to create new storage volumes, and
then create [`PersistentVolume` objects](/docs/concepts/storage/persistent-volumes/)
to represent them in Kubernetes. The dynamic provisioning feature eliminates
the need for cluster administrators to pre-provision storage. Instead, it
automatically provisions storage when it is requested by users.
-->
<p>动态卷供应允许按需创建存储卷。
如果没有动态供应，集群管理员必须手动地联系他们的云或存储提供商来创建新的存储卷，
然后在 Kubernetes 集群创建
<a href="/zh/docs/concepts/storage/persistent-volumes/"><code>PersistentVolume</code> 对象</a>来表示这些卷。
动态供应功能消除了集群管理员预先配置存储的需要。 相反，它在用户请求时自动供应存储。</p>
<!-- body -->
<!--
## Background
-->
<h2 id="背景">背景</h2>
<!--
The implementation of dynamic volume provisioning is based on the API object `StorageClass`
from the API group `storage.k8s.io`. A cluster administrator can define as many
`StorageClass` objects as needed, each specifying a *volume plugin* (aka
*provisioner*) that provisions a volume and the set of parameters to pass to
that provisioner when provisioning.
-->
<p>动态卷供应的实现基于 <code>storage.k8s.io</code> API 组中的 <code>StorageClass</code> API 对象。
集群管理员可以根据需要定义多个 <code>StorageClass</code> 对象，每个对象指定一个<em>卷插件</em>（又名 <em>provisioner</em>），
卷插件向卷供应商提供在创建卷时需要的数据卷信息及相关参数。</p>
<!--
A cluster administrator can define and expose multiple flavors of storage (from
the same or different storage systems) within a cluster, each with a custom set
of parameters. This design also ensures that end users don't have to worry
about the complexity and nuances of how storage is provisioned, but still
have the ability to select from multiple storage options.
-->
<p>集群管理员可以在集群中定义和公开多种存储（来自相同或不同的存储系统），每种都具有自定义参数集。
该设计也确保终端用户不必担心存储供应的复杂性和细微差别，但仍然能够从多个存储选项中进行选择。</p>
<!--
More information on storage classes can be found
[here](/docs/concepts/storage/storage-classes/).
-->
<p>点击<a href="/zh/docs/concepts/storage/storage-classes/">这里</a>查阅有关存储类的更多信息。</p>
<!--
## Enabling Dynamic Provisioning
-->
<h2 id="enabling-dynamic-provisioning">启用动态卷供应 </h2>
<!--
To enable dynamic provisioning, a cluster administrator needs to pre-create
one or more StorageClass objects for users.
StorageClass objects define which provisioner should be used and what parameters
should be passed to that provisioner when dynamic provisioning is invoked.
The name of a StorageClass object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

The following manifest creates a storage class "slow" which provisions standard
disk-like persistent disks.
-->
<p>要启用动态供应功能，集群管理员需要为用户预先创建一个或多个 <code>StorageClass</code> 对象。
<code>StorageClass</code> 对象定义当动态供应被调用时，哪一个驱动将被使用和哪些参数将被传递给驱动。
StorageClass 对象的名字必须是一个合法的 <a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。
以下清单创建了一个 <code>StorageClass</code> 存储类 &quot;slow&quot;，它提供类似标准磁盘的永久磁盘。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>slow<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/gce-pd<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>pd-standard<span style="color:#bbb">
</span></code></pre></div><!--
The following manifest creates a storage class "fast" which provisions
SSD-like persistent disks.
-->
<p>以下清单创建了一个 &quot;fast&quot; 存储类，它提供类似 SSD 的永久磁盘。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>kubernetes.io/gce-pd<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>pd-ssd<span style="color:#bbb">
</span></code></pre></div><!--
## Using Dynamic Provisioning
-->
<h2 id="使用动态卷供应">使用动态卷供应</h2>
<!--
Users request dynamically provisioned storage by including a storage class in
their `PersistentVolumeClaim`. Before Kubernetes v1.6, this was done via the
`volume.beta.kubernetes.io/storage-class` annotation. However, this annotation
is deprecated since v1.9. Users now can and should instead use the
`storageClassName` field of the `PersistentVolumeClaim` object. The value of
this field must match the name of a `StorageClass` configured by the
administrator (see [below](#enabling-dynamic-provisioning)).
-->
<p>用户通过在 <code>PersistentVolumeClaim</code> 中包含存储类来请求动态供应的存储。
在 Kubernetes v1.9 之前，这通过 <code>volume.beta.kubernetes.io/storage-class</code> 注解实现。然而，这个注解自 v1.6 起就不被推荐使用了。
用户现在能够而且应该使用 <code>PersistentVolumeClaim</code> 对象的 <code>storageClassName</code> 字段。
这个字段的值必须能够匹配到集群管理员配置的 <code>StorageClass</code> 名称（见<a href="#enabling-dynamic-provisioning">下面</a>）。</p>
<!--
To select the "fast" storage class, for example, a user would create the
following PersistentVolumeClaim:
-->
<p>例如，要选择 “fast” 存储类，用户将创建如下的 PersistentVolumeClaim：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>claim1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>fast<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>30Gi<span style="color:#bbb">
</span></code></pre></div><!--
This claim results in an SSD-like Persistent Disk being automatically
provisioned. When the claim is deleted, the volume is destroyed.
-->
<p>该声明会自动供应一块类似 SSD 的永久磁盘。
在删除该声明后，这个卷也会被销毁。</p>
<!--
## Defaulting Behavior
-->
<h2 id="设置默认值的行为">设置默认值的行为</h2>
<!--
Dynamic provisioning can be enabled on a cluster such that all claims are
dynamically provisioned if no storage class is specified. A cluster administrator
can enable this behavior by:
-->
<p>可以在群集上启用动态卷供应，以便在未指定存储类的情况下动态设置所有声明。
集群管理员可以通过以下方式启用此行为：</p>
<!--
- Marking one `StorageClass` object as *default*;
- Making sure that the [`DefaultStorageClass` admission controller](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
  is enabled on the API server.
-->
<ul>
<li>标记一个 <code>StorageClass</code> 为 <em>默认</em>；</li>
<li>确保 <a href="/zh/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass"><code>DefaultStorageClass</code> 准入控制器</a>在 API 服务端被启用。</li>
</ul>
<!--
An administrator can mark a specific `StorageClass` as default by adding the
`storageclass.kubernetes.io/is-default-class` annotation to it.
When a default `StorageClass` exists in a cluster and a user creates a
`PersistentVolumeClaim` with `storageClassName` unspecified, the
`DefaultStorageClass` admission controller automatically adds the
`storageClassName` field pointing to the default storage class.
-->
<p>管理员可以通过向其添加 <code>storageclass.kubernetes.io/is-default-class</code> 注解来将特定的 <code>StorageClass</code> 标记为默认。
当集群中存在默认的 <code>StorageClass</code> 并且用户创建了一个未指定 <code>storageClassName</code> 的 <code>PersistentVolumeClaim</code> 时，
<code>DefaultStorageClass</code> 准入控制器会自动向其中添加指向默认存储类的 <code>storageClassName</code> 字段。</p>
<!--
Note that there can be at most one *default* storage class on a cluster, or
a `PersistentVolumeClaim` without `storageClassName` explicitly specified cannot
be created.
-->
<p>请注意，群集上最多只能有一个 <em>默认</em> 存储类，否则无法创建没有明确指定
<code>storageClassName</code> 的 <code>PersistentVolumeClaim</code>。</p>
<!--
## Topology Awareness
-->
<h2 id="拓扑感知">拓扑感知</h2>
<!--
In [Multi-Zone](/docs/setup/multiple-zones) clusters, Pods can be spread across
Zones in a Region. Single-Zone storage backends should be provisioned in the Zones where
Pods are scheduled. This can be accomplished by setting the [Volume Binding
Mode](/docs/concepts/storage/storage-classes/#volume-binding-mode).
-->
<p>在<a href="/zh/docs/setup/best-practices/multiple-zones/">多区域</a>集群中，Pod 可以被分散到多个区域。
单区域存储后端应该被供应到 Pod 被调度到的区域。
这可以通过设置<a href="/zh/docs/concepts/storage/storage-classes/#volume-binding-mode">卷绑定模式</a>来实现。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c262af210c6828dec445d2f55a1d877a">6.7 - 卷快照</h1>
    
	<!--
title: Volume Snapshots
content_type: concept
weight: 40
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.17 [beta]</code>
</div>


<!--
In Kubernetes, a _VolumeSnapshot_ represents a snapshot of a volume on a storage system. This document assumes that you are already familiar with Kubernetes [persistent volumes](/docs/concepts/storage/persistent-volumes/).
-->
<p>在 Kubernetes 中，卷快照是一个存储系统上卷的快照，本文假设你已经熟悉了 Kubernetes
的 <a href="/zh/docs/concepts/storage/persistent-volumes/">持久卷</a>。</p>
<!-- body -->
<!--
## Introduction
-->
<h2 id="introduction">介绍</h2>
<!--
Similar to how API resources `PersistentVolume` and `PersistentVolumeClaim` are used to provision volumes for users and administrators, `VolumeSnapshotContent` and `VolumeSnapshot` API resources are provided to create volume snapshots for users and administrators.
-->
<p>与 <code>PersistentVolume</code> 和 <code>PersistentVolumeClaim</code> 两个 API 资源用于给用户和管理员提供卷类似，<code>VolumeSnapshotContent</code> 和 <code>VolumeSnapshot</code> 两个 API 资源用于给用户和管理员创建卷快照。</p>
<!--
A `VolumeSnapshotContent` is a snapshot taken from a volume in the cluster that has been provisioned by an administrator. It is a resource in the cluster just like a PersistentVolume is a cluster resource.
-->
<p><code>VolumeSnapshotContent</code> 是一种快照，从管理员已提供的集群中的卷获取。就像持久卷是集群的资源一样，它也是集群中的资源。</p>
<!--
A `VolumeSnapshot` is a request for snapshot of a volume by a user. It is similar to a PersistentVolumeClaim.
-->
<p><code>VolumeSnapshot</code> 是用户对于卷的快照的请求。它类似于持久卷声明。</p>
<!--
`VolumeSnapshotClass` allows you to specify different attributes belonging to a `VolumeSnapshot`. These attributes may differ among snapshots taken from the same volume on the storage system and therefore cannot be expressed by using the same `StorageClass` of a `PersistentVolumeClaim`.
-->
<p><code>VolumeSnapshotClass</code> 允许指定属于 <code>VolumeSnapshot</code> 的不同属性。在从存储系统的相同卷上获取的快照之间，这些属性可能有所不同，因此不能通过使用与 <code>PersistentVolumeClaim</code> 相同的 <code>StorageClass</code> 来表示。</p>
<!--
Volume snapshots provide Kubernetes users with a standardized way to copy a volume's contents at a particular point in time without creating an entirely new volume. This functionality enables, for example, database administrators to backup databases before performing edit or delete modifications.
-->
<p>卷快照能力为 Kubernetes 用户提供了一种标准的方式来在指定时间点
复制卷的内容，并且不需要创建全新的卷。例如，这一功能使得数据库管理员
能够在执行编辑或删除之类的修改之前对数据库执行备份。</p>
<!--
Users need to be aware of the following when using this feature:
-->
<p>当使用该功能时，用户需要注意以下几点：</p>
<!--
* API Objects `VolumeSnapshot`, `VolumeSnapshotContent`, and `VolumeSnapshotClass` are <a class='glossary-tooltip' title='通过定制化的代码给您的 Kubernetes API 服务器增加资源对象，而无需编译完整的定制 API 服务器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/' target='_blank' aria-label='CRDs'>CRDs</a>, not part of the core API.
* `VolumeSnapshot` support is only available for CSI drivers.
* As part of the deployment process of `VolumeSnapshot`, the Kubernetes team provides a snapshot controller to be deployed into the control plane, and a sidecar helper container called csi-snapshotter to be deployed together with the CSI driver.  The snapshot controller watches `VolumeSnapshot` and `VolumeSnapshotContent` objects and is responsible for the creation and deletion of `VolumeSnapshotContent` object.  The sidecar csi-snapshotter watches `VolumeSnapshotContent` objects and triggers `CreateSnapshot` and `DeleteSnapshot` operations against a CSI endpoint.
* There is also a validating webhook server which provides tightened validation on snapshot objects. This should be installed by the Kubernetes distros along with the snapshot controller and CRDs, not CSI drivers. It should be installed in all Kubernetes clusters that has the snapshot feature enabled.
* CSI drivers may or may not have implemented the volume snapshot functionality. The CSI drivers that have provided support for volume snapshot will likely use the csi-snapshotter. See [CSI Driver documentation](https://kubernetes-csi.github.io/docs/) for details.
* The CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.
-->
<ul>
<li>API 对象 <code>VolumeSnapshot</code>，<code>VolumeSnapshotContent</code> 和 <code>VolumeSnapshotClass</code>
是 <a class='glossary-tooltip' title='通过定制化的代码给您的 Kubernetes API 服务器增加资源对象，而无需编译完整的定制 API 服务器。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/' target='_blank' aria-label='CRDs'>CRDs</a>，
不属于核心 API。</li>
<li><code>VolumeSnapshot</code> 支持仅可用于 CSI 驱动。</li>
<li>作为 <code>VolumeSnapshot</code> 部署过程的一部分，Kubernetes 团队提供了一个部署于控制平面的快照控制器，
并且提供了一个叫做 <code>csi-snapshotter</code> 的边车（Sidecar）辅助容器，和 CSI 驱动程序一起部署。
快照控制器监视 <code>VolumeSnapshot</code> 和 <code>VolumeSnapshotContent</code> 对象，
并且负责创建和删除 <code>VolumeSnapshotContent</code> 对象。
边车 csi-snapshotter 监视 <code>VolumeSnapshotContent</code> 对象，
并且触发针对 CSI 端点的 <code>CreateSnapshot</code> 和 <code>DeleteSnapshot</code> 的操作。</li>
<li>还有一个验证性质的 Webhook 服务器，可以对快照对象进行更严格的验证。
Kubernetes 发行版应将其与快照控制器和 CRD（而非 CSI 驱动程序）一起安装。
此服务器应该安装在所有启用了快照功能的 Kubernetes 集群中。</li>
<li>CSI 驱动可能实现，也可能没有实现卷快照功能。CSI 驱动可能会使用 csi-snapshotter
来提供对卷快照的支持。详见 <a href="https://kubernetes-csi.github.io/docs/">CSI 驱动程序文档</a></li>
<li>Kubernetes 负责 CRDs 和快照控制器的安装。</li>
</ul>
<!--
## Lifecycle of a volume snapshot and volume snapshot content

`VolumeSnapshotContents` are resources in the cluster. `VolumeSnapshots` are requests for those resources. The interaction between `VolumeSnapshotContents` and `VolumeSnapshots` follow this lifecycle:
-->
<h2 id="lifecycle-of-a-volume-snapshot-and-volume-snapshot-content">卷快照和卷快照内容的生命周期</h2>
<p><code>VolumeSnapshotContents</code> 是集群中的资源。<code>VolumeSnapshots</code> 是对于这些资源的请求。<code>VolumeSnapshotContents</code> 和 <code>VolumeSnapshots</code> 之间的交互遵循以下生命周期：</p>
<!--
### Provisioning Volume Snapshot

There are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.
-->
<h3 id="provisioning-volume-snapshot">供应卷快照</h3>
<p>快照可以通过两种方式进行配置：预配置或动态配置。</p>
<!--
#### Pre-provisioned {#static}
A cluster administrator creates a number of `VolumeSnapshotContents`. They carry the details of the real volume snapshot on the storage system which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.
-->
<h4 id="static">预配置</h4>
<p>集群管理员创建多个 <code>VolumeSnapshotContents</code>。它们带有存储系统上实际卷快照的详细信息，可以供集群用户使用。它们存在于 Kubernetes API 中，并且能够被使用。</p>
<!--
#### Dynamic

Instead of using a pre-existing snapshot, you can request that a snapshot to be dynamically taken from a PersistentVolumeClaim. The [VolumeSnapshotClass](/docs/concepts/storage/volume-snapshot-classes/) specifies storage provider-specific parameters to use when taking a snapshot.
-->
<h4 id="dynamic">动态的</h4>
<p>可以从 <code>PersistentVolumeClaim</code> 中动态获取快照，而不用使用已经存在的快照。
在获取快照时，<a href="/zh/docs/concepts/storage/volume-snapshot-classes/">卷快照类</a>
指定要用的特定于存储提供程序的参数。</p>
<!--
### Binding

The snapshot controller handles the binding of a `VolumeSnapshot` object with an appropriate `VolumeSnapshotContent` object, in both pre-provisioned and dynamically provisioned scenarios. The binding is a one-to-one mapping.
-->
<h3 id="binding">绑定</h3>
<p>在预配置和动态配置场景下，快照控制器处理绑定 <code>VolumeSnapshot</code> 对象和其合适的 <code>VolumeSnapshotContent</code> 对象。绑定关系是一对一的。</p>
<!--
In the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the requested VolumeSnapshotContent object is created.
-->
<p>在预配置快照绑定场景下，<code>VolumeSnapshotContent</code> 对象创建之后，才会和 <code>VolumeSnapshot</code> 进行绑定。</p>
<!--
### Persistent Volume Claim as Snapshot Source Protection

The purpose of this protection is to ensure that in-use
<a class='glossary-tooltip' title='声明在持久卷中定义的存储资源，以便可以将其挂载为容器中的卷。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/persistent-volumes/' target='_blank' aria-label='PersistentVolumeClaim'>PersistentVolumeClaim</a>
API objects are not removed from the system while a snapshot is being taken from it (as this may result in data loss).

-->
<h3 id="快照源的持久性卷声明保护">快照源的持久性卷声明保护</h3>
<p>这种保护的目的是确保在从系统中获取快照时，不会将正在使用的
<a class='glossary-tooltip' title='声明在持久卷中定义的存储资源，以便可以将其挂载为容器中的卷。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/persistent-volumes/' target='_blank' aria-label='PersistentVolumeClaim'>PersistentVolumeClaim</a>
API 对象从系统中删除（因为这可能会导致数据丢失）。</p>
<!--

While a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim is in-use. If you delete a PersistentVolumeClaim API object in active use as a snapshot source, the PersistentVolumeClaim object is not removed immediately. Instead, removal of the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.
-->
<p>如果一个 PVC 正在被快照用来作为源进行快照创建，则该 PVC 是使用中的。如果用户删除正作为快照源的 PVC API 对象，则 PVC 对象不会立即被删除掉。相反，PVC 对象的删除将推迟到任何快照不在主动使用它为止。当快照的 <code>Status</code> 中的 <code>ReadyToUse</code>值为 <code>true</code> 时，PVC 将不再用作快照源。</p>
<p>当从 <code>PersistentVolumeClaim</code> 中生成快照时，<code>PersistentVolumeClaim</code> 就在被使用了。如果删除一个作为快照源的 <code>PersistentVolumeClaim</code> 对象，这个 <code>PersistentVolumeClaim</code> 对象不会立即被删除的。相反，删除 <code>PersistentVolumeClaim</code> 对象的动作会被放弃，或者推迟到快照的 Status 为 ReadyToUse时再执行。</p>
<!--
### Delete

Deletion is triggered by deleting the `VolumeSnapshot` object, and the `DeletionPolicy` will be followed. If the `DeletionPolicy` is `Delete`, then the underlying storage snapshot will be deleted along with the `VolumeSnapshotContent` object. If the `DeletionPolicy` is `Retain`, then both the underlying snapshot and `VolumeSnapshotContent` remain.
-->
<h3 id="delete">删除</h3>
<p>删除 <code>VolumeSnapshot</code> 对象触发删除 <code>VolumeSnapshotContent</code> 操作，并且 <code>DeletionPolicy</code> 会紧跟着执行。如果 <code>DeletionPolicy</code> 是 <code>Delete</code>，那么底层存储快照会和 <code>VolumeSnapshotContent</code> 一起被删除。如果 <code>DeletionPolicy</code> 是 <code>Retain</code>，那么底层快照和 <code>VolumeSnapshotContent</code> 都会被保留。</p>
<!--
## VolumeSnapshots

Each VolumeSnapshot contains a spec and a status.
-->
<h2 id="volume-snapshots">卷快照</h2>
<p>每个 <code>VolumeSnapshot</code> 包含一个 spec 和一个状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshot<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>new-snapshot-test<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeSnapshotClassName</span>:<span style="color:#bbb"> </span>csi-hostpath-snapclass<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">source</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">persistentVolumeClaimName</span>:<span style="color:#bbb"> </span>pvc-test<span style="color:#bbb">
</span></code></pre></div><!--
`persistentVolumeClaimName` is the name of the PersistentVolumeClaim data source for the snapshot. This field is required for dynamically provisioning a snapshot.

A volume snapshot can request a particular class by specifying the name of a
[VolumeSnapshotClass](/docs/concepts/storage/volume-snapshot-classes/)
using the attribute `volumeSnapshotClassName`. If nothing is set, then the default class is used if available.
-->
<p><code>persistentVolumeClaimName</code> 是 <code>PersistentVolumeClaim</code> 数据源对快照的名称。
这个字段是动态配置快照中的必填字段。</p>
<p>卷快照可以通过指定 <a href="/zh/docs/concepts/storage/volume-snapshot-classes/">VolumeSnapshotClass</a>
使用 <code>volumeSnapshotClassName</code> 属性来请求特定类。如果没有设置，那么使用默认类（如果有）。</p>
<!--
For pre-provisioned snapshots, you need to specify a `volumeSnapshotContentName` as the source for the snapshot as shown in the following example. The `volumeSnapshotContentName` source field is required for pre-provisioned snapshots.
-->
<p>如下面例子所示，对于预配置的快照，需要给快照指定 <code>volumeSnapshotContentName</code> 来作为源。
对于预配置的快照 <code>source</code> 中的<code>volumeSnapshotContentName</code> 字段是必填的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshot<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-snapshot<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">source</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeSnapshotContentName</span>:<span style="color:#bbb"> </span>test-content<span style="color:#bbb">
</span></code></pre></div><!--
## Volume Snapshot Contents

Each VolumeSnapshot contains a spec and a status, which is the specification and status of the volume snapshot.
Each VolumeSnapshotContent contains a spec and status. In dynamic provisioning, the snapshot common controller creates `VolumeSnapshotContent` objects. Here is an example:
-->
<p>每个 VolumeSnapshotContent 对象包含 spec 和 status。在动态配置时，快照通用控制器创建 <code>VolumeSnapshotContent</code> 对象。下面是例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotContent<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>snapcontent-72d9a349-aacd-42d2-a240-d775650d2455<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">deletionPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">driver</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">source</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeHandle</span>:<span style="color:#bbb"> </span>ee0cfb94-f8d4-11e9-b2d8-0242ac110002<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeSnapshotClassName</span>:<span style="color:#bbb"> </span>csi-hostpath-snapclass<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeSnapshotRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>new-snapshot-test<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>72d9a349-aacd-42d2-a240-d775650d2455<span style="color:#bbb">
</span></code></pre></div><!--
`volumeHandle` is the unique identifier of the volume created on the storage backend and returned by the CSI driver during the volume creation. This field is required for dynamically provisioning a snapshot. It specifies the volume source of the snapshot.

For pre-provisioned snapshots, you (as cluster administrator) are responsible for creating the `VolumeSnapshotContent` object as follows.
-->
<p><code>volumeHandle</code> 是存储后端创建卷的唯一标识符，在卷创建期间由 CSI 驱动程序返回。动态设置快照需要此字段。它指出了快照的卷源。</p>
<p>对于预配置快照，你（作为集群管理员）要按如下命令来创建 <code>VolumeSnapshotContent</code> 对象。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotContent<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>new-snapshot-content-test<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">deletionPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">driver</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">source</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">snapshotHandle</span>:<span style="color:#bbb"> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeSnapshotRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>new-snapshot-test<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span></code></pre></div><!--
`snapshotHandle` is the unique identifier of the volume snapshot created on the storage backend. This field is required for the pre-provisioned snapshots. It specifies the CSI snapshot id on the storage system that this `VolumeSnapshotContent` represents.
-->
<p><code>snapshotHandle</code> 是存储后端创建卷的唯一标识符。对于预设置快照，这个字段是必须的。它指定此 <code>VolumeSnapshotContent</code> 表示的存储系统上的 CSI 快照 id。</p>
<!--
## Provisioning Volumes from Snapshots
-->
<h2 id="从快照供应卷">从快照供应卷</h2>
<!--
You can provision a new volume, pre-populated with data from a snapshot, by using
the *dataSource* field in the `PersistentVolumeClaim` object.
-->
<p>你可以配置一个新卷，该卷预填充了快照中的数据，在 <code>持久卷声明</code> 对象中使用 <em>dataSource</em> 字段。</p>
<!--
For more details, see
[Volume Snapshot and Restore Volume from Snapshot](/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support).
-->
<p>更多详细信息，请参阅
<a href="/zh/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support">卷快照和从快照还原卷</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4d00116c86dade62bdd5be7dc2afa1ca">6.8 - 卷快照类</h1>
    
	<!-- overview -->
<!--
This document describes the concept of VolumeSnapshotClass in Kubernetes. Familiarity
with [volume snapshots](/docs/concepts/storage/volume-snapshots/) and
[storage classes](/docs/concepts/storage/storage-classes) is suggested.
-->
<p>本文档描述了 Kubernetes 中 VolumeSnapshotClass 的概念。建议熟悉
<a href="/zh/docs/concepts/storage/volume-snapshots/">卷快照（Volume Snapshots）</a>和
<a href="/zh/docs/concepts/storage/storage-classes">存储类（Storage Class）</a>。</p>
<!-- body -->
<!--
## Introduction

Just like StorageClass provides a way for administrators to describe the "classes"
of storage they offer when provisioning a volume, VolumeSnapshotClass provides a
way to describe the "classes" of storage when provisioning a volume snapshot.
-->
<h2 id="introduction">介绍</h2>
<p>就像 StorageClass 为管理员提供了一种在配置卷时描述存储“类”的方法，
VolumeSnapshotClass 提供了一种在配置卷快照时描述存储“类”的方法。</p>
<!--
## The VolumeSnapshotClass Resource

Each VolumeSnapshotClass contains the fields `driver`, `deletionPolicy`, and `parameters`,
which are used when a VolumeSnapshot belonging to the class needs to be
dynamically provisioned.

The name of a VolumeSnapshotClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating VolumeSnapshotClass objects, and the objects cannot
be updated once they are created.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Installation of the CRDs is the responsibility of the Kubernetes distribution. Without the required CRDs present, the creation of a VolumeSnapshotClass fails.
</div>

-->
<h2 id="the-volumesnapshortclass-resource">VolumeSnapshotClass 资源 </h2>
<p>每个 VolumeSnapshotClass 都包含 <code>driver</code>、<code>deletionPolicy</code> 和 <code>parameters</code> 字段，
在需要动态配置属于该类的 VolumeSnapshot 时使用。</p>
<p>VolumeSnapshotClass 对象的名称很重要，是用户可以请求特定类的方式。
管理员在首次创建 VolumeSnapshotClass 对象时设置类的名称和其他参数，
对象一旦创建就无法更新。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> CRD 的安装是 Kubernetes 发行版的责任。 如果不存在所需的 CRD，则 VolumeSnapshotClass 的创建将失败。
</div>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>csi-hostpath-snapclass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">driver</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">deletionPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span></code></pre></div><!--
Administrators can specify a default VolumeSnapshotClass for VolumeSnapshots
that don't request any particular class to bind to by adding the
`snapshot.storage.kubernetes.io/is-default-class: "true"` annotation:
-->
<p>管理员可以为未请求任何特定类绑定的 VolumeSnapshots 指定默认的 VolumeSnapshotClass，
方法是设置注解 <code>snapshot.storage.kubernetes.io/is-default-class: &quot;true&quot;</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>csi-hostpath-snapclass<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">snapshot.storage.kubernetes.io/is-default-class</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">driver</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">deletionPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb">
</span></code></pre></div><!--
### Driver

Volume snapshot classes have a driver that determines what CSI volume plugin is
used for provisioning VolumeSnapshots. This field must be specified.
-->
<h3 id="driver">驱动程序</h3>
<p>卷快照类有一个驱动程序，用于确定配置 VolumeSnapshot 的 CSI 卷插件。
此字段必须指定。</p>
<!--
### DeletionPolicy

Volume snapshot classes have a deletionPolicy. It enables you to configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object it is bound to is to be deleted. The deletionPolicy of a volume snapshot class can either be `Retain` or `Delete`. This field must be specified.

If the deletionPolicy is `Delete`, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the deletionPolicy is `Retain`, then both the underlying snapshot and VolumeSnapshotContent remain.
-->
<h3 id="deletion-policy">删除策略</h3>
<p>卷快照类具有 <code>deletionPolicy</code> 属性。用户可以配置当所绑定的 VolumeSnapshot
对象将被删除时，如何处理 VolumeSnapshotContent 对象。
卷快照类的这个策略可以是 <code>Retain</code> 或者 <code>Delete</code>。这个策略字段必须指定。</p>
<p>如果删除策略是 <code>Delete</code>，那么底层的存储快照会和 VolumeSnapshotContent 对象
一起删除。如果删除策略是 <code>Retain</code>，那么底层快照和 VolumeSnapshotContent
对象都会被保留。</p>
<!--
## Parameters

Volume snapshot classes have parameters that describe volume snapshots belonging to
the volume snapshot class. Different parameters may be accepted depending on the
`driver`.
-->
<h2 id="parameters">参数</h2>
<p>卷快照类具有描述属于该卷快照类的卷快照的参数，可根据 <code>driver</code> 接受不同的参数。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-707ca81a34eb1ca202f34692e9917d1e">6.9 - CSI 卷克隆</h1>
    
	<!--
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: CSI Volume Cloning
content_type: concept
weight: 60
-->
<!-- overview -->
<!--
This document describes the concept of cloning existing CSI Volumes in Kubernetes.  Familiarity with [Volumes](/docs/concepts/storage/volumes) is suggested.
-->
<p>本文档介绍 Kubernetes 中克隆现有 CSI 卷的概念。阅读前建议先熟悉
<a href="/zh/docs/concepts/storage/volumes">卷</a>。</p>
<!-- body -->
<!--
## Introduction

The <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> Volume Cloning feature adds support for specifying existing <a class='glossary-tooltip' title='声明在持久卷中定义的存储资源，以便可以将其挂载为容器中的卷。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/persistent-volumes/' target='_blank' aria-label='PVC'>PVC</a>s in the `dataSource` field to indicate a user would like to clone a <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷（Volume）'>卷（Volume）</a>.
-->
<h2 id="介绍">介绍</h2>
<p><a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> 卷克隆功能增加了通过在
<code>dataSource</code> 字段中指定存在的
<a class='glossary-tooltip' title='声明在持久卷中定义的存储资源，以便可以将其挂载为容器中的卷。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/persistent-volumes/' target='_blank' aria-label='PVC'>PVC</a>，
来表示用户想要克隆的 <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷（Volume）'>卷（Volume）</a>。</p>
<!--
A Clone is defined as a duplicate of an existing Kubernetes Volume that can be consumed as any standard Volume would be.  The only difference is that upon provisioning, rather than creating a "new" empty Volume, the back end device creates an exact duplicate of the specified Volume.
-->
<p>克隆（Clone），意思是为已有的 Kubernetes 卷创建副本，它可以像任何其它标准卷一样被使用。
唯一的区别就是配置后，后端设备将创建指定完全相同的副本，而不是创建一个“新的”空卷。</p>
<!--
The implementation of cloning, from the perspective of the Kubernetes API, adds the ability to specify an existing PVC as a dataSource during new PVC creation. The source PVC must be bound and available (not in use).

Users need to be aware of the following when using this feature:
-->
<p>从 Kubernetes API 的角度看，克隆的实现只是在创建新的 PVC 时，
增加了指定一个现有 PVC 作为数据源的能力。源 PVC 必须是 bound
状态且可用的（不在使用中）。</p>
<p>用户在使用该功能时，需要注意以下事项：</p>
<!--
* Cloning support (`VolumePVCDataSource`) is only available for CSI drivers.
* Cloning support is only available for dynamic provisioners.
* CSI drivers may or may not have implemented the volume cloning functionality.
* You can only clone a PVC when it exists in the same namespace as the destination PVC (source and destination must be in the same namespace).
* Cloning is only supported within the same Storage Class.
    - Destination volume must be the same storage class as the source
    - Default storage class can be used and storageClassName omitted in the spec
* Cloning can only be performed between two volumes that use the same VolumeMode setting (if you request a block mode volume, the source MUST also be block mode)
-->
<ul>
<li>克隆支持（<code>VolumePVCDataSource</code>）仅适用于 CSI 驱动。</li>
<li>克隆支持仅适用于 动态供应器。</li>
<li>CSI 驱动可能实现，也可能未实现卷克隆功能。</li>
<li>仅当 PVC 与目标 PVC 存在于同一命名空间（源和目标 PVC 必须在相同的命名空间）时，才可以克隆 PVC。</li>
<li>仅在同一存储类中支持克隆。
<ul>
<li>目标卷必须和源卷具有相同的存储类</li>
<li>可以使用默认的存储类并且 storageClassName 字段在规格中忽略了</li>
</ul>
</li>
<li>克隆只能在两个使用相同 VolumeMode 设置的卷中进行
（如果请求克隆一个块存储模式的卷，源卷必须也是块存储模式）。</li>
</ul>
<!--
## Provisioning

Clones are provisioned like any other PVC with the exception of adding a dataSource that references an existing PVC in the same namespace.
-->
<h2 id="制备">制备</h2>
<p>克隆卷与其他任何 PVC 一样配置，除了需要增加 dataSource 来引用同一命名空间中现有的 PVC。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>clone-of-pvc-1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>myns<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>cloning<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>5Gi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">dataSource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pvc-1<span style="color:#bbb">
</span></code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
You must specify a capacity value for `spec.resources.requests.storage`, 
and the value you specify must be the same or larger than the capacity of the source volume.
-->
<p>你必须为 <code>spec.resources.requests.storage</code> 指定一个值，并且你指定的值必须大于或等于源卷的值。
</div>
<!--
The result is a new PVC with the name `clone-of-pvc-1` that has the exact same content as the specified source `pvc-1`.
-->
<p>结果是一个名称为 <code>clone-of-pvc-1</code> 的新 PVC 与指定的源 <code>pvc-1</code> 拥有相同的内容。</p>
<!--
## Usage

Upon availability of the new PVC, the cloned PVC is consumed the same as other PVC.  It's also expected at this point that the newly created PVC is an independent object.  It can be consumed, cloned, snapshotted, or deleted independently and without consideration for it's original dataSource PVC.  This also implies that the source is not linked in any way to the newly created clone, it may also be modified or deleted without affecting the newly created clone.
-->
<h2 id="使用">使用</h2>
<p>一旦新的 PVC 可用，被克隆的 PVC 像其他 PVC 一样被使用。
可以预期的是，新创建的 PVC 是一个独立的对象。
可以独立使用、克隆、快照或删除它，而不需要考虑它的原始数据源 PVC。
这也意味着，源没有以任何方式链接到新创建的 PVC，它也可以被修改或删除，而不会影响到新创建的克隆。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-00cd24f4570b7acaac75c2551c948bc7">6.10 - 存储容量</h1>
    
	<!-- overview -->
<!--
Storage capacity is limited and may vary depending on the node on
which a pod runs: network-attached storage might not be accessible by
all nodes, or storage is local to a node to begin with.






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code>
</div>



This page describes how Kubernetes keeps track of storage capacity and
how the scheduler uses that information to schedule Pods onto nodes
that have access to enough storage capacity for the remaining missing
volumes. Without storage capacity tracking, the scheduler may choose a
node that doesn't have enough capacity to provision a volume and
multiple scheduling retries will be needed.

Tracking storage capacity is supported for <a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='Container Storage Interface'>Container Storage Interface</a> (CSI) drivers and
[needs to be enabled](#enabling-storage-capacity-tracking) when installing a CSI driver.
-->
<p>存储容量是有限的，并且会因为运行 Pod 的节点不同而变化：
网络存储可能并非所有节点都能够访问，或者对于某个节点存储是本地的。</p>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code>
</div>


<p>本页面描述了 Kubernetes 如何跟踪存储容量以及调度程序如何为了余下的尚未挂载的卷使用该信息将
Pod 调度到能够访问到足够存储容量的节点上。
如果没有跟踪存储容量，调度程序可能会选择一个没有足够容量来提供卷的节点，并且需要多次调度重试。</p>
<p><a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='容器存储接口'>容器存储接口</a>（CSI）驱动程序支持跟踪存储容量，
并且在安装 CSI 驱动程序时<a href="#enabling-storage-capacity-tracking">需要启用</a>该功能。</p>
<!-- body -->
<!--
## API

There are two API extensions for this feature:
- CSIStorageCapacity objects:
  these get produced by a CSI driver in the namespace
  where the driver is installed. Each object contains capacity
  information for one storage class and defines which nodes have
  access to that storage.
- [The `CSIDriverSpec.StorageCapacity` field](/docs/reference/generated/kubernetes-api/v1.23/#csidriverspec-v1-storage-k8s-io):
  when set to `true`, the Kubernetes scheduler will consider storage
  capacity for volumes that use the CSI driver.
-->
<h2 id="api">API</h2>
<p>这个特性有两个 API 扩展接口：</p>
<ul>
<li>CSIStorageCapacity 对象：这些对象由 CSI 驱动程序在安装驱动程序的命名空间中产生。
每个对象都包含一个存储类的容量信息，并定义哪些节点可以访问该存储。</li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.23/#csidriverspec-v1-storage-k8s-io"><code>CSIDriverSpec.StorageCapacity</code> 字段</a>：
设置为 true 时，Kubernetes 调度程序将考虑使用 CSI 驱动程序的卷的存储容量。</li>
</ul>
<!--
## Scheduling

Storage capacity information is used by the Kubernetes scheduler if:
- the `CSIStorageCapacity` feature gate is true,
- a Pod uses a volume that has not been created yet,
- that volume uses a <a class='glossary-tooltip' title='StorageClass 是管理员用来描述可用的不同存储类型的一种方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/storage-classes/' target='_blank' aria-label='StorageClass'>StorageClass</a> which references a CSI driver and
  uses `WaitForFirstConsumer` [volume binding
  mode](/docs/concepts/storage/storage-classes/#volume-binding-mode),
  and
- the `CSIDriver` object for the driver has `StorageCapacity` set to
  true.

In that case, the scheduler only considers nodes for the Pod which
have enough storage available to them. This check is very
simplistic and only compares the size of the volume against the
capacity listed in `CSIStorageCapacity` objects with a topology that
includes the node.

For volumes with `Immediate` volume binding mode, the storage driver
decides where to create the volume, independently of Pods that will
use the volume. The scheduler then schedules Pods onto nodes where the
volume is available after the volume has been created.

For [CSI ephemeral volumes](/docs/concepts/storage/volumes/#csi),
scheduling always happens without considering storage capacity. This
is based on the assumption that this volume type is only used by
special CSI drivers which are local to a node and do not need
significant resources there.
-->
<h2 id="调度">调度</h2>
<p>如果有以下情况，存储容量信息将会被 Kubernetes 调度程序使用：</p>
<ul>
<li><code>CSIStorageCapacity</code> 特性门控被设置为 true，</li>
<li>Pod 使用的卷还没有被创建，</li>
<li>卷使用引用了 CSI 驱动的 <a class='glossary-tooltip' title='StorageClass 是管理员用来描述可用的不同存储类型的一种方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/storage-classes/' target='_blank' aria-label='StorageClass'>StorageClass</a>，
并且使用了 <code>WaitForFirstConsumer</code> <a href="/zh/docs/concepts/storage/storage-classes/#volume-binding-mode">卷绑定模式</a>，</li>
<li>驱动程序的 <code>CSIDriver</code> 对象的 <code>StorageCapacity</code> 被设置为 true。</li>
</ul>
<p>在这种情况下，调度程序仅考虑将 Pod 调度到有足够存储容量的节点上。这个检测非常简单，
仅将卷的大小与 <code>CSIStorageCapacity</code> 对象中列出的容量进行比较，并使用包含该节点的拓扑。</p>
<p>对于具有 <code>Immediate</code> 卷绑定模式的卷，存储驱动程序将决定在何处创建该卷，而不取决于将使用该卷的 Pod。
然后，调度程序将 Pod 调度到创建卷后可使用该卷的节点上。</p>
<p>对于 <a href="/zh/docs/concepts/storage/volumes/#csi">CSI 临时卷</a>，调度总是在不考虑存储容量的情况下进行。
这是基于这样的假设：该卷类型仅由节点本地的特殊 CSI 驱动程序使用，并且不需要大量资源。</p>
<!--
## Rescheduling

When a node has been selected for a Pod with `WaitForFirstConsumer`
volumes, that decision is still tentative. The next step is that the
CSI storage driver gets asked to create the volume with a hint that the
volume is supposed to be available on the selected node.

Because Kubernetes might have chosen a node based on out-dated
capacity information, it is possible that the volume cannot really be
created. The node selection is then reset and the Kubernetes scheduler
tries again to find a node for the Pod.
-->
<h2 id="重新调度">重新调度</h2>
<p>当为带有 <code>WaitForFirstConsumer</code> 的卷的 Pod 来选择节点时，该决定仍然是暂定的。
下一步是要求 CSI 存储驱动程序创建卷，并提示该卷在被选择的节点上可用。</p>
<p>因为 Kubernetes 可能会根据已经过时的存储容量信息来选择一个节点，因此可能无法真正创建卷。
然后就会重置节点选择，Kubernetes 调度器会再次尝试为 Pod 查找节点。</p>
<!--
## Limitations

Storage capacity tracking increases the chance that scheduling works
on the first try, but cannot guarantee this because the scheduler has
to decide based on potentially out-dated information. Usually, the
same retry mechanism as for scheduling without any storage capacity
information handles scheduling failures.

One situation where scheduling can fail permanently is when a Pod uses
multiple volumes: one volume might have been created already in a
topology segment which then does not have enough capacity left for
another volume. Manual intervention is necessary to recover from this,
for example by increasing capacity or deleting the volume that was
already created. [Further
work](https://github.com/kubernetes/enhancements/pull/1703) is needed
to handle this automatically.
-->
<h2 id="限制">限制</h2>
<p>存储容量跟踪增加了调度器第一次尝试即成功的机会，但是并不能保证这一点，因为调度器必须根据可能过期的信息来进行决策。
通常，与没有任何存储容量信息的调度相同的重试机制可以处理调度失败。</p>
<p>当 Pod 使用多个卷时，调度可能会永久失败：一个卷可能已经在拓扑段中创建，而该卷又没有足够的容量来创建另一个卷，
要想从中恢复，必须要进行手动干预，比如通过增加存储容量或者删除已经创建的卷。
需要<a href="https://github.com/kubernetes/enhancements/pull/1703">进一步工作</a>来自动处理此问题。</p>
<!--
## Enabling storage capacity tracking

Storage capacity tracking is a beta feature and enabled by default in 
a Kubernetes cluster since Kubernetes 1.21. In addition to having the
feature enabled in the cluster, a CSI driver also has to support
it. Please refer to the driver's documentation for details.
-->
<h2 id="开启存储容量跟踪">开启存储容量跟踪</h2>
<p>存储容量跟踪是一个 Beta 特性，从 Kubernetes 1.21 版本起在 Kubernetes 集群
中默认被启用。除了在集群中启用此功能特性之外，还要求 CSI 驱动支持此特性。
请参阅驱动的文档了解详细信息。</p>
<h2 id="what-s-next">What's next</h2>
<!--
 - For more information on the design, see the
[Storage Capacity Constraints for Pod Scheduling KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md).
- For more information on further development of this feature, see the [enhancement tracking issue #1472](https://github.com/kubernetes/enhancements/issues/1472).
- Learn about [Kubernetes Scheduler](/docs/concepts/scheduling-eviction/kube-scheduler/)
-->
<ul>
<li>想要获得更多该设计的信息，查看
<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md">Storage Capacity Constraints for Pod Scheduling KEP</a>。</li>
<li>有关此功能的下一步开发信息，查看
<a href="https://github.com/kubernetes/enhancements/issues/1472">enhancement tracking issue #1472</a>。</li>
<li>学习 <a href="/zh/docs/concepts/scheduling-eviction/kube-scheduler/">Kubernetes 调度器</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4f40cb95a671e51b4f0156a409d95c6d">6.11 - 卷健康监测</h1>
    
	<!-- 
reviewers:
- jsafrane
- saad-ali
- msau42
- xing-yang
title: Volume Health Monitoring
content_type: concept
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>


<!-- 
<a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> volume health monitoring allows CSI Drivers to detect abnormal volume conditions from the underlying storage systems and report them as events on <a class='glossary-tooltip' title='声明在持久卷中定义的存储资源，以便可以将其挂载为容器中的卷。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/persistent-volumes/' target='_blank' aria-label='PVCs'>PVCs</a> or <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>.
-->
<p><a class='glossary-tooltip' title='容器存储接口 （CSI）定义了存储系统暴露给容器的标准接口。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/#csi' target='_blank' aria-label='CSI'>CSI</a> 卷健康监测支持 CSI 驱动从底层的存储系统着手，
探测异常的卷状态，并以事件的形式上报到 <a class='glossary-tooltip' title='声明在持久卷中定义的存储资源，以便可以将其挂载为容器中的卷。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/persistent-volumes/' target='_blank' aria-label='PVCs'>PVCs</a>
或 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>.</p>
<!-- body -->
<!-- 
## Volume health monitoring
-->
<h2 id="volume-health-monitoring">卷健康监测</h2>
<!-- 
Kubernetes _volume health monitoring_ is part of how Kubernetes implements the Container Storage Interface (CSI). Volume health monitoring feature is implemented in two components: an External Health Monitor controller, and the <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>.

If a CSI Driver supports Volume Health Monitoring feature from the controller side, an event will be reported on the related <a class='glossary-tooltip' title='声明在持久卷中定义的存储资源，以便可以将其挂载为容器中的卷。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/persistent-volumes/' target='_blank' aria-label='PersistentVolumeClaim'>PersistentVolumeClaim</a> (PVC) when an abnormal volume condition is detected on a CSI volume.
-->
<p>Kubernetes <em>卷健康监测</em> 是 Kubernetes 容器存储接口（CSI）实现的一部分。
卷健康监测特性由两个组件实现：外部健康监测控制器和 <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>。</p>
<p>如果 CSI 驱动器通过控制器的方式支持卷健康监测特性，那么只要在 CSI 卷上监测到异常卷状态，就会在
<a class='glossary-tooltip' title='声明在持久卷中定义的存储资源，以便可以将其挂载为容器中的卷。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/persistent-volumes/' target='_blank' aria-label='PersistentVolumeClaim'>PersistentVolumeClaim</a> (PVC)
中上报一个事件。</p>
<!--
The External Health Monitor <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> also watches for node failure events. You can enable node failure monitoring by setting the `enable-node-watcher` flag to true. When the external health monitor detects a node failure event, the controller reports an Event will be reported on the PVC to indicate that pods using this PVC are on a failed node.

If a CSI Driver supports Volume Health Monitoring feature from the node side, an Event will be reported on every Pod using the PVC when an abnormal volume condition is detected on a CSI volume.
-->
<p>外部健康监测<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>也会监测节点失效事件。
如果要启动节点失效监测功能，你可以设置标志 <code>enable-node-watcher</code> 为 <code>true</code>。
当外部健康监测器检测到节点失效事件，控制器会报送一个事件，该事件会在 PVC 上继续上报，
以表明使用此 PVC 的 Pod 正位于一个失效的节点上。</p>
<p>如果 CSI 驱动程序支持节点测的卷健康检测，那当在 CSI 卷上检测到异常卷时，
会在使用该 PVC 的每个Pod 上触发一个事件。</p>
<!-- 
You need to enable the `CSIVolumeHealth` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to use this feature from the node side.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 你需要启用 <code>CSIVolumeHealth</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>，
才能在节点上使用此特性。
</div>
<h2 id="what-s-next">What's next</h2>
<!-- 
See the [CSI driver documentation](https://kubernetes-csi.github.io/docs/drivers.html) to find out which CSI drivers have implemented this feature.
-->
<p>参阅 <a href="https://kubernetes-csi.github.io/docs/drivers.html">CSI 驱动程序文档</a>，
可以找出有哪些 CSI 驱动程序实现了此特性。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b2e4b16ac37988c678a3312a4a6639f8">6.12 - 特定于节点的卷数限制</h1>
    
	<!-- ---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: Node-specific Volume Limits
content_type: concept
---
 -->
<!-- overview -->
<!-- This page describes the maximum number of volumes that can be attached
to a Node for various cloud providers. -->
<p>此页面描述了各个云供应商可关联至一个节点的最大卷数。</p>
<!-- Cloud providers like Google, Amazon, and Microsoft typically have a limit on
how many volumes can be attached to a Node. It is important for Kubernetes to
respect those limits. Otherwise, Pods scheduled on a Node could get stuck
waiting for volumes to attach. -->
<p>谷歌、亚马逊和微软等云供应商通常对可以关联到节点的卷数量进行限制。
Kubernetes 需要尊重这些限制。 否则，在节点上调度的 Pod 可能会卡住去等待卷的关联。</p>
<!-- body -->
<!--
## Kubernetes default limits

The Kubernetes scheduler has default limits on the number of volumes
that can be attached to a Node:
-->
<h2 id="kubernetes-的默认限制">Kubernetes 的默认限制</h2>
<p>The Kubernetes 调度器对关联于一个节点的卷数有默认限制：</p>
<!--
<table>
  <tr><th>Cloud service</th><th>Maximum volumes per Node</th></tr>
  <tr><td><a href="https://aws.amazon.com/ebs/">Amazon Elastic Block Store (EBS)</a></td><td>39</td></tr>
  <tr><td><a href="https://cloud.google.com/persistent-disk/">Google Persistent Disk</a></td><td>16</td></tr>
  <tr><td><a href="https://azure.microsoft.com/en-us/services/storage/main-disks/">Microsoft Azure Disk Storage</a></td><td>16</td></tr>
</table>
 -->
<table>
  <tr><th>云服务</th><th>每节点最大卷数</th></tr>
  <tr><td><a href="https://aws.amazon.com/ebs/">Amazon Elastic Block Store (EBS)</a></td><td>39</td></tr>
  <tr><td><a href="https://cloud.google.com/persistent-disk/">Google Persistent Disk</a></td><td>16</td></tr>
  <tr><td><a href="https://azure.microsoft.com/en-us/services/storage/main-disks/">Microsoft Azure Disk Storage</a></td><td>16</td></tr>
</table>
<!--
## Custom limits

You can change these limits by setting the value of the
`KUBE_MAX_PD_VOLS` environment variable, and then starting the scheduler.
CSI drivers might have a different procedure, see their documentation
on how to customize their limits.

Use caution if you set a limit that is higher than the default limit. Consult
the cloud provider's documentation to make sure that Nodes can actually support
the limit you set.

The limit applies to the entire cluster, so it affects all Nodes.
-->
<h2 id="自定义限制">自定义限制</h2>
<p>您可以通过设置 <code>KUBE_MAX_PD_VOLS</code> 环境变量的值来设置这些限制，然后再启动调度器。
CSI 驱动程序可能具有不同的过程，关于如何自定义其限制请参阅相关文档。</p>
<p>如果设置的限制高于默认限制，请谨慎使用。请参阅云提供商的文档以确保节点可支持您设置的限制。</p>
<p>此限制应用于整个集群，所以它会影响所有节点。</p>
<!--
## Dynamic volume limits
-->
<h2 id="动态卷限制">动态卷限制</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code>
</div>


<!--
Dynamic volume limits are supported for following volume types.

- Amazon EBS
- Google Persistent Disk
- Azure Disk
- CSI
-->
<p>以下卷类型支持动态卷限制。</p>
<ul>
<li>Amazon EBS</li>
<li>Google Persistent Disk</li>
<li>Azure Disk</li>
<li>CSI</li>
</ul>
<!--
For volumes managed by in-tree volume plugins, Kubernetes automatically determines the Node
type and enforces the appropriate maximum number of volumes for the node. For example:
-->
<p>对于由内建插件管理的卷，Kubernetes 会自动确定节点类型并确保节点上可关联的卷数目合规。 例如：</p>
<!--
* On
<a href="https://cloud.google.com/compute/">Google Compute Engine</a>,
up to 127 volumes can be attached to a node, [depending on the node
type](https://cloud.google.com/compute/docs/disks/#pdnumberlimits).

* For Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only 25
volumes to be attached to a Node. For other instance types on
<a href="https://aws.amazon.com/ec2/">Amazon Elastic Compute Cloud (EC2)</a>,
Kubernetes allows 39 volumes to be attached to a Node.

* On Azure, up to 64 disks can be attached to a node, depending on the node type. For more details, refer to [Sizes for virtual machines in Azure](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes).

* If a CSI storage driver advertises a maximum number of volumes for a Node (using `NodeGetInfo`), the <a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='kube-scheduler'>kube-scheduler</a> honors that limit.
Refer to the [CSI specifications](https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo) for details.

* For volumes managed by in-tree plugins that have been migrated to a CSI driver, the maximum number of volumes will be the one reported by the CSI driver.
-->
<ul>
<li>
<p>在
<a href="https://cloud.google.com/compute/">Google Compute Engine</a>环境中,
<a href="https://cloud.google.com/compute/docs/disks/#pdnumberlimits">根据节点类型</a>最多可以将127个卷关联到节点。</p>
</li>
<li>
<p>对于 M5、C5、R5、T3 和 Z1D 类型实例的 Amazon EBS 磁盘，Kubernetes 仅允许 25 个卷关联到节点。
对于 ec2 上的其他实例类型
<a href="https://aws.amazon.com/ec2/">Amazon Elastic Compute Cloud (EC2)</a>,
Kubernetes 允许 39 个卷关联至节点。</p>
</li>
<li>
<p>在 Azure 环境中, 根据节点类型，最多 64 个磁盘可以关联至一个节点。
更多详细信息，请参阅<a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes">Azure 虚拟机的数量大小</a>。</p>
</li>
<li>
<p>如果 CSI 存储驱动程序（使用 <code>NodeGetInfo</code> ）为节点通告卷数上限，则 <a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='kube-scheduler'>kube-scheduler</a> 将遵守该限制值。
参考 <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo">CSI 规范</a> 获取更多详细信息。</p>
</li>
<li>
<p>对于由已迁移到 CSI 驱动程序的树内插件管理的卷，最大卷数将是 CSI 驱动程序报告的卷数。</p>
</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-275bea454e1cf4c5adeca4058b5af988">7 - 配置</h1>
    <div class="lead">Kubernetes 为配置 Pods 提供的资源。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-ddef6fd0e47bb51c6f05e8e7fb11d2dd">7.1 - 配置最佳实践</h1>
    
	<!--
title: Configuration Best Practices
content_type: concept
weight: 10
-->
<!-- overview -->
<!--
This document highlights and consolidates configuration best practices that are introduced throughout the user guide, Getting Started documentation, and examples.
-->
<p>本文档重点介绍并整合了整个用户指南、入门文档和示例中介绍的配置最佳实践。</p>
<!--
This is a living document. If you think of something that is not on this list but might be useful to others, please don't hesitate to file an issue or submit a PR.
-->
<p>这是一份不断改进的文件。
如果你认为某些内容缺失但可能对其他人有用，请不要犹豫，提交 Issue 或提交 PR。</p>
<!-- body -->
<!--
## General Configuration Tips
-->
<h2 id="general-configuration-tips">一般配置提示 </h2>
<!--
- When defining configurations, specify the latest stable API version.
-->
<ul>
<li>定义配置时，请指定最新的稳定 API 版本。</li>
</ul>
<!--
- Configuration files should be stored in version control before being pushed to the cluster. This allows you to quickly roll back a configuration change if necessary. It also aids cluster re-creation and restoration.
-->
<ul>
<li>在推送到集群之前，配置文件应存储在版本控制中。
这允许你在必要时快速回滚配置更改。
它还有助于集群重新创建和恢复。</li>
</ul>
<!--
- Write your configuration files using YAML rather than JSON. Though these formats can be used interchangeably in almost all scenarios, YAML tends to be more user-friendly.
-->
<ul>
<li>使用 YAML 而不是 JSON 编写配置文件。虽然这些格式几乎可以在所有场景中互换使用，但 YAML 往往更加用户友好。</li>
</ul>
<!--
- Group related objects into a single file whenever it makes sense. One file is often easier to manage than several. See the [guestbook-all-in-one.yaml](https://github.com/kubernetes/examples/tree/master/guestbook/all-in-one/guestbook-all-in-one.yaml) file as an example of this syntax.
-->
<ul>
<li>只要有意义，就将相关对象分组到一个文件中。
一个文件通常比几个文件更容易管理。
请参阅 <a href="https://github.com/kubernetes/examples/tree/master/guestbook/all-in-one/guestbook-all-in-one.yaml">guestbook-all-in-one.yaml</a> 文件作为此语法的示例。</li>
</ul>
<!--
- Note also that many `kubectl` commands can be called on a directory. For example, you can call `kubectl apply` on a directory of config files.
-->
<ul>
<li>另请注意，可以在目录上调用许多<code>kubectl</code>命令。
例如，你可以在配置文件的目录中调用<code>kubectl apply</code>。</li>
</ul>
<!--
- Don't specify default values unnecessarily: simple, minimal configuration will make errors less likely.
-->
<ul>
<li>除非必要，否则不指定默认值：简单的最小配置会降低错误的可能性。</li>
</ul>
<!--
- Put object descriptions in annotations, to allow better introspection.
-->
<ul>
<li>将对象描述放在注释中，以便更好地进行内省。</li>
</ul>
<!--
## "Naked" Pods vs ReplicaSets, Deployments, and Jobs
-->
<h2 id="naked-pods-与-replicaset-deployment-和-jobs">“Naked” Pods 与 ReplicaSet，Deployment 和 Jobs</h2>
<!--
- Don't use naked Pods (that is, Pods not bound to a [ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) or [Deployment](/docs/concepts/workloads/controllers/deployment/)) if you can avoid it. Naked Pods will not be rescheduled in the event of a node failure.
-->
<ul>
<li>如果可能，不要使用独立的 Pods（即，未绑定到
<a href="/zh/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> 或
<a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a> 的 Pod）。
如果节点发生故障，将不会重新调度独立的 Pods。</li>
</ul>
<!--
  A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is always available, and specifies a strategy to replace Pods (such as [RollingUpdate](/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment)), is almost always preferable to creating Pods directly, except for some explicit [`restartPolicy: Never`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) scenarios. A [Job](/docs/concepts/workloads/controllers/jobs-run-to-completion/) may also be appropriate.
-->
<p>Deployment 既可以创建一个 ReplicaSet 来确保预期个数的 Pod 始终可用，也可以指定替换 Pod 的策略（例如
<a href="/zh/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment">RollingUpdate</a>）。
除了一些显式的 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy"><code>restartPolicy: Never</code></a>
场景外，Deployment 通常比直接创建 Pod 要好得多。<a href="/zh/docs/concepts/workloads/controllers/job/">Job</a> 也可能是合适的选择。</p>
<!--
## Services
-->
<h2 id="services">服务  </h2>
<!--
- Create a [Service](/docs/concepts/services-networking/service/) before its corresponding backend workloads (Deployments or ReplicaSets), and before any workloads that need to access it. When Kubernetes starts a container, it provides environment variables pointing to all the Services which were running when the container was started. For example, if a Service named `foo` exists, all containers will get the following variables in their initial environment:
-->
<ul>
<li>
<p>在创建相应的后端工作负载（Deployment 或 ReplicaSet），以及在需要访问它的任何工作负载之前创建
<a href="/zh/docs/concepts/services-networking/service/">服务</a>。
当 Kubernetes 启动容器时，它提供指向启动容器时正在运行的所有服务的环境变量。
例如，如果存在名为 <code>foo</code> 的服务，则所有容器将在其初始环境中获得以下变量。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">FOO_SERVICE_HOST</span><span style="color:#666">=</span>&lt;the host the Service is running on&gt;
<span style="color:#b8860b">FOO_SERVICE_PORT</span><span style="color:#666">=</span>&lt;the port the Service is running on&gt;
</code></pre></div></li>
</ul>
<!--
  *This does imply an ordering requirement* - any `Service` that a `Pod` wants to access must be created before the `Pod` itself, or else the environment variables will not be populated.  DNS does not have this restriction.
-->
<p><em>这确实意味着在顺序上的要求</em> - 必须在 <code>Pod</code> 本身被创建之前创建 <code>Pod</code> 想要访问的任何 <code>Service</code>，
否则将环境变量不会生效。DNS 没有此限制。</p>
<!--
- An optional (though strongly recommended) [cluster add-on](/docs/concepts/cluster-administration/addons/) is a DNS server.  The
DNS server watches the Kubernetes API for new `Services` and creates a set of DNS records for each.  If DNS has been enabled throughout the cluster then all `Pods` should be able to do name resolution of `Services` automatically.
-->
<ul>
<li>一个可选（尽管强烈推荐）的<a href="/zh/docs/concepts/cluster-administration/addons/">集群插件</a>
是 DNS 服务器。DNS 服务器为新的 <code>Services</code> 监视 Kubernetes API，并为每个创建一组 DNS 记录。
如果在整个集群中启用了 DNS，则所有 <code>Pods</code> 应该能够自动对 <code>Services</code> 进行名称解析。</li>
</ul>
<!--
- Don't specify a `hostPort` for a Pod unless it is absolutely necessary. When you bind a Pod to a `hostPort`, it limits the number of places the Pod can be scheduled, because each <`hostIP`, `hostPort`, `protocol`> combination must be unique. If you don't specify the `hostIP` and `protocol` explicitly, Kubernetes will use `0.0.0.0` as the default `hostIP` and `TCP` as the default `protocol`.
-->
<ul>
<li>除非绝对必要，否则不要为 Pod 指定 <code>hostPort</code>。
将 Pod 绑定到<code>hostPort</code>时，它会限制 Pod 可以调度的位置数，因为每个
<code>&lt;hostIP, hostPort, protocol&gt;</code>组合必须是唯一的。
如果你没有明确指定 <code>hostIP</code> 和 <code>protocol</code>，Kubernetes 将使用 <code>0.0.0.0</code> 作为默认
<code>hostIP</code> 和 <code>TCP</code> 作为默认 <code>protocol</code>。</li>
</ul>
<!--
  If you only need access to the port for debugging purposes, you can use the [apiserver proxy](/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls) or [`kubectl port-forward`](/docs/tasks/access-application-cluster/port-forward-access-application-cluster/).
-->
<p>如果你只需要访问端口以进行调试，则可以使用
<a href="/zh/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls">apiserver proxy</a>或
<a href="/zh/docs/tasks/access-application-cluster/port-forward-access-application-cluster/"><code>kubectl port-forward</code></a>。</p>
<!--
  If you explicitly need to expose a Pod's port on the node, consider using a [NodePort](/docs/concepts/services-networking/service/#type-nodeport) Service before resorting to `hostPort`.
-->
<p>如果你明确需要在节点上公开 Pod 的端口，请在使用 <code>hostPort</code> 之前考虑使用
<a href="/zh/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> 服务。</p>
<!--
- Avoid using `hostNetwork`, for the same reasons as `hostPort`.
-->
<ul>
<li>避免使用 <code>hostNetwork</code>，原因与 <code>hostPort</code> 相同。</li>
</ul>
<!--
- Use [headless Services](/docs/concepts/services-networking/service/#headless-
services) (which have a `ClusterIP` of `None`) for service discovery when you don't need `kube-proxy` load balancing.
-->
<ul>
<li>当你不需要 <code>kube-proxy</code> 负载均衡时，使用
<a href="/zh/docs/concepts/services-networking/service/#headless-services">无头服务</a><br>
(<code>ClusterIP</code> 被设置为 <code>None</code>)以便于服务发现。</li>
</ul>
<!--
## Using Labels
-->
<h2 id="using-labels">使用标签  </h2>
<!--
- Define and use [labels](/docs/concepts/overview/working-with-objects/labels/) that identify __semantic attributes__ of your application or Deployment, such as `{ app: myapp, tier: frontend, phase: test, deployment: v3 }`. You can use these labels to select the appropriate Pods for other resources; for example, a Service that selects all `tier: frontend` Pods, or all `phase: test` components of `app: myapp`. See the [guestbook](https://github.com/kubernetes/examples/tree/master/guestbook/) app for examples of this approach.
-->
<ul>
<li>定义并使用<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签</a>来识别应用程序
或 Deployment 的 <strong>语义属性</strong>，例如<code>{ app: myapp, tier: frontend, phase: test, deployment: v3 }</code>。
你可以使用这些标签为其他资源选择合适的 Pod；
例如，一个选择所有 <code>tier: frontend</code> Pod 的服务，或者 <code>app: myapp</code> 的所有 <code>phase: test</code> 组件。
有关此方法的示例，请参阅 <a href="https://github.com/kubernetes/examples/tree/master/guestbook/">guestbook</a> 。</li>
</ul>
<!--
A Service can be made to span multiple Deployments by omitting release-specific labels from its selector. [Deployments](/docs/concepts/workloads/controllers/deployment/) make it easy to update a running service without downtime.
-->
<p>通过从选择器中省略特定发行版的标签，可以使服务跨越多个 Deployment。
当你需要不停机的情况下更新正在运行的服务，可以使用<a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a>。</p>
<!--
A desired state of an object is described by a Deployment, and if changes to that spec are _applied_, the deployment controller changes the actual state to the desired state at a controlled rate.
-->
<p>Deployment 描述了对象的期望状态，并且如果对该规范的更改被成功应用，
则 Deployment 控制器以受控速率将实际状态改变为期望状态。</p>
<!--
- Use the [Kubernetes common labels](/docs/concepts/overview/working-with-objects/common-labels/) for common use cases. These standardized labels enrich the metadata in a way that allows tools, including `kubectl` and [dashboard](/docs/tasks/access-application-cluster/web-ui-dashboard), to work in an interoperable way.
-->
<ul>
<li>对于常见场景，应使用 <a href="/zh/docs/concepts/overview/working-with-objects/common-labels/">Kubernetes 通用标签</a>。
这些标准化的标签丰富了对象的元数据，使得包括 <code>kubectl</code> 和
<a href="/zh/docs/tasks/access-application-cluster/web-ui-dashboard">仪表板（Dashboard）</a>
这些工具能够以可互操作的方式工作。</li>
</ul>
<!--
- You can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) and Services match to Pods using selector labels, removing the relevant labels from a Pod will stop it from being considered by a controller or from being served traffic by a Service. If you remove the labels of an existing Pod, its controller will create a new Pod to take its place. This is a useful way to debug a previously "live" Pod in a "quarantine" environment. To interactively remove or add labels, use [`kubectl label`](/docs/reference/generated/kubectl/kubectl-commands#label).
-->
<ul>
<li>你可以操纵标签进行调试。
由于 Kubernetes 控制器（例如 ReplicaSet）和服务使用选择器标签来匹配 Pod，
从 Pod 中删除相关标签将阻止其被控制器考虑或由服务提供服务流量。
如果删除现有 Pod 的标签，其控制器将创建一个新的 Pod 来取代它。
这是在&quot;隔离&quot;环境中调试先前&quot;活跃&quot;的 Pod 的有用方法。
要以交互方式删除或添加标签，请使用 <a href="/docs/reference/generated/kubectl/kubectl-commands#label"><code>kubectl label</code></a>。</li>
</ul>
<!--
## Using kubectl
-->
<h2 id="using-kubectl">使用 kubectl  </h2>
<!--
- Use `kubectl apply -f <directory>`. This looks for Kubernetes configuration in all `.yaml`, `.yml`, and `.json` files in `<directory>` and passes it to `apply`.
-->
<ul>
<li>使用 <code>kubectl apply -f &lt;directory&gt;</code>。
它在 <code>&lt;directory&gt;</code> 中的所有<code> .yaml</code>、<code>.yml</code> 和 <code>.json</code> 文件中查找 Kubernetes 配置，并将其传递给 <code>apply</code>。</li>
</ul>
<!--
- Use label selectors for `get` and `delete` operations instead of specific object names. See the sections on [label selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors) and [using labels effectively](/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively).
-->
<ul>
<li>使用标签选择器进行 <code>get</code> 和 <code>delete</code> 操作，而不是特定的对象名称。</li>
<li>请参阅<a href="/zh/docs/concepts/overview/working-with-objects/labels/#label-selectors">标签选择器</a>和
<a href="/zh/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively">有效使用标签</a>部分。</li>
</ul>
<!--
- Use `kubectl run` and `kubectl expose` to quickly create single-container Deployments and Services. See [Use a Service to Access an Application in a Cluster](/docs/tasks/access-application-cluster/service-access-application-cluster/) for an example.
-->
<ul>
<li>使用<code>kubectl run</code>和<code>kubectl expose</code>来快速创建单容器部署和服务。
有关示例，请参阅<a href="/zh/docs/tasks/access-application-cluster/service-access-application-cluster/">使用服务访问集群中的应用程序</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-6b5ccadd699df0904e8e9917c5450c4b">7.2 - ConfigMap</h1>
    
	<!-- overview -->
<!--
---
title: ConfigMap
id: configmap
date: 2018-04-12
full_link: /docs/concepts/configuration/configmap/
short_description: >
  An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume.

aka: 
tags:
- core-object
---
-->
<!--
 An API object used to store non-confidential data in key-value pairs.
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> can consume ConfigMaps as
environment variables, command-line arguments, or as configuration files in a
<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='volume'>volume</a>.
-->
<p>ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。</p>
<!--
A ConfigMap allows you to decouple environment-specific configuration from your <a class='glossary-tooltip' title='镜像是保存的容器实例，它打包了应用运行所需的一组软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-image' target='_blank' aria-label='container images'>container images</a>, so that your applications are easily portable.
-->
<p>ConfigMap 将您的环境配置信息和 <a class='glossary-tooltip' title='镜像是保存的容器实例，它打包了应用运行所需的一组软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-image' target='_blank' aria-label='容器镜像'>容器镜像</a> 解耦，便于应用配置的修改。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
ConfigMap does not provide secrecy or encryption.
If the data you want to store are confidential, use a
<a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a> rather than a ConfigMap,
or use additional (third party) tools to keep your data private.
-->
<p>ConfigMap 并不提供保密或者加密功能。
如果你想存储的数据是机密的，请使用 <a class='glossary-tooltip' title='Secret 用于存储敏感信息，如密码、 OAuth 令牌和 SSH 密钥。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/configuration/secret/' target='_blank' aria-label='Secret'>Secret</a>，
或者使用其他第三方工具来保证你的数据的私密性，而不是用 ConfigMap。
</div>

<!-- body -->
<!--
## Motivation

Use a ConfigMap for setting configuration data separately from application code.

For example, imagine that you are developing an application that you can run on your
own computer (for development) and in the cloud (to handle real traffic).
You write the code to look in an environment variable named `DATABASE_HOST`.
Locally, you set that variable to `localhost`. In the cloud, you set it to
refer to a Kubernetes <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Service'>Service</a>
that exposes the database component to your cluster.
This lets you fetch a container image running in the cloud and
debug the exact same code locally if needed.
-->
<h2 id="motivation">动机  </h2>
<p>使用 ConfigMap 来将你的配置数据和应用程序代码分开。</p>
<p>比如，假设你正在开发一个应用，它可以在你自己的电脑上（用于开发）和在云上
（用于实际流量）运行。
你的代码里有一段是用于查看环境变量 <code>DATABASE_HOST</code>，在本地运行时，
你将这个变量设置为 <code>localhost</code>，在云上，你将其设置为引用 Kubernetes 集群中的
公开数据库组件的 <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务'>服务</a>。</p>
<p>这让你可以获取在云中运行的容器镜像，并且如果有需要的话，在本地调试完全相同的代码。</p>
<!--
A ConfigMap is not designed to hold large chunks of data. The data stored in a
ConfigMap cannot exceed 1 MiB. If you need to store settings that are
larger than this limit, you may want to consider mounting a volume or use a
separate database or file service.
-->
<p>ConfigMap 在设计上不是用来保存大量数据的。在 ConfigMap 中保存的数据不可超过
1 MiB。如果你需要保存超出此尺寸限制的数据，你可能希望考虑挂载存储卷
或者使用独立的数据库或者文件服务。</p>
<!--
## ConfigMap object

A ConfigMap is an API [object](/docs/concepts/overview/working-with-objects/kubernetes-objects/)
that lets you store configuration for other objects to use. Unlike most
Kubernetes objects that have a `spec`, a ConfigMap has `data` and `binaryData`
fields. These fields accept key-value pairs as their values.  Both the `data`
field and the `binaryData` are optional. The `data` field is designed to
contain UTF-8 strings while the `binaryData` field is designed to
contain binary data as base64-encoded strings.

The name of a ConfigMap must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<h2 id="configmap-对象">ConfigMap 对象</h2>
<p>ConfigMap 是一个 API <a href="/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/">对象</a>，
让你可以存储其他对象所需要使用的配置。
和其他 Kubernetes 对象都有一个 <code>spec</code> 不同的是，ConfigMap 使用 <code>data</code> 和
<code>binaryData</code> 字段。这些字段能够接收键-值对作为其取值。<code>data</code> 和 <code>binaryData</code>
字段都是可选的。<code>data</code> 字段设计用来保存 UTF-8 字符串，而 <code>binaryData</code>
则被设计用来保存二进制数据作为 base64 编码的字串。</p>
<p>ConfigMap 的名字必须是一个合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
Each key under the `data` or the `binaryData` field must consist of
alphanumeric characters, `-`, `_` or `.`. The keys stored in `data` must not
overlap with the keys in the `binaryData` field.

Starting from v1.19, you can add an `immutable` field to a ConfigMap
definition to create an [immutable ConfigMap](#configmap-immutable).
-->
<p><code>data</code> 或 <code>binaryData</code> 字段下面的每个键的名称都必须由字母数字字符或者
<code>-</code>、<code>_</code> 或 <code>.</code> 组成。在 <code>data</code> 下保存的键名不可以与在 <code>binaryData</code>
下出现的键名有重叠。</p>
<p>从 v1.19 开始，你可以添加一个 <code>immutable</code> 字段到 ConfigMap 定义中，
创建<a href="#configmap-immutable">不可变更的 ConfigMap</a>。</p>
<!--
## ConfigMaps and Pods

You can write a Pod `spec` that refers to a ConfigMap and configures the container(s)
in that Pod based on the data in the ConfigMap. The Pod and the ConfigMap must be in
the same <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespace'>namespace</a>.
-->
<h2 id="configmaps-和-pods">ConfigMaps 和 Pods</h2>
<p>你可以写一个引用 ConfigMap 的 Pod 的 <code>spec</code>，并根据 ConfigMap 中的数据在该
Pod 中配置容器。这个 Pod 和 ConfigMap 必须要在同一个
<a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a> 中。</p>
<!--
The `spec` of a <a class='glossary-tooltip' title='静态Pod（Static Pod）是指由特定节点上的 kubelet 守护进程直接管理的 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/static-pod/' target='_blank' aria-label='static Pod'>static Pod</a> cannot refer to a ConfigMap
or any other API objects.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <a class='glossary-tooltip' title='静态Pod（Static Pod）是指由特定节点上的 kubelet 守护进程直接管理的 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/static-pod/' target='_blank' aria-label='静态 Pod'>静态 Pod</a> 中的 <code>spec</code>
字段不能引用 ConfigMap 或任何其他 API 对象。
</div>
<!--
Here's an example ConfigMap that has some keys with single values,
and other keys where the value looks like a fragment of a configuration
format.
-->
<p>这是一个 ConfigMap 的示例，它的一些键只有一个值，其他键的值看起来像是
配置的片段格式。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>game-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 类属性键；每一个键都映射到一个简单的值</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">player_initial_lives</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;3&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ui_properties_file_name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;user-interface.properties&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 类文件键</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">game.properties</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    enemy.types=aliens,monsters
</span><span style="color:#b44;font-style:italic">    player.maximum-lives=5</span><span style="color:#bbb">    
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user-interface.properties</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    color.good=purple
</span><span style="color:#b44;font-style:italic">    color.bad=yellow
</span><span style="color:#b44;font-style:italic">    allow.textmode=true</span><span style="color:#bbb">    
</span></code></pre></div><!--
There are four different ways that you can use a ConfigMap to configure
a container inside a Pod:

1. Inside a container command and args
1. Environment variables for a container
1. Add a file in read-only volume, for the application to read
1. Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap

These different methods lend themselves to different ways of modeling
the data being consumed.
For the first three methods, the
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> uses the data from
the ConfigMap when it launches container(s) for a Pod.
-->
<p>你可以使用四种方式来使用 ConfigMap 配置 Pod 中的容器：</p>
<ol>
<li>在容器命令和参数内</li>
<li>容器的环境变量</li>
<li>在只读卷里面添加一个文件，让应用来读取</li>
<li>编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap</li>
</ol>
<p>这些不同的方法适用于不同的数据使用方式。
对前三个方法，<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>
使用 ConfigMap 中的数据在 Pod 中启动容器。</p>
<!--
The fourth method means you have to write code to read the ConfigMap and its data.
However, because you're using the Kubernetes API directly, your application can
subscribe to get updates whenever the ConfigMap changes, and react
when that happens. By accessing the Kubernetes API directly, this
technique also lets you access a ConfigMap in a different namespace.

Here's an example Pod that uses values from `game-demo` to configure a Pod:
-->
<p>第四种方法意味着你必须编写代码才能读取 ConfigMap 和它的数据。然而，
由于你是直接使用 Kubernetes API，因此只要 ConfigMap 发生更改，
你的应用就能够通过订阅来获取更新，并且在这样的情况发生的时候做出反应。
通过直接进入 Kubernetes API，这个技术也可以让你能够获取到不同的名字空间里的 ConfigMap。</p>
<p>下面是一个 Pod 的示例，它通过使用 <code>game-demo</code> 中的值来配置一个 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>configmap-demo-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>demo<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>alpine<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;sleep&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;3600&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 定义环境变量</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>PLAYER_INITIAL_LIVES<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 请注意这里和 ConfigMap 中的键名是不一样的</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">configMapKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>game-demo          <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 这个值来自 ConfigMap</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>player_initial_lives<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 需要取值的键</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>UI_PROPERTIES_FILE_NAME<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">configMapKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>game-demo<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>ui_properties_file_name<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/config&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 你可以在 Pod 级别设置卷，然后将其挂载到 Pod 内的容器中</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 提供你想要挂载的 ConfigMap 的名字</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>game-demo<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 来自 ConfigMap 的一组键，将被创建为文件</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;game.properties&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;game.properties&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;user-interface.properties&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;user-interface.properties&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
A ConfigMap doesn't differentiate between single line property values and
multi-line file-like values.
What matters how Pods and other objects consume those values.

For this example, defining a volume and mounting it inside the `demo`
container as `/config` creates two files,
`/config/game.properties` and `/config/user-interface.properties`,
even though there are four keys in the ConfigMap. This is because the Pod
definition specifies an `items` array in the `volumes` section.
If you omit the `items` array entirely, every key  in the ConfigMap becomes
a file with the same name as the key, and you get 4 files.
-->
<p>ConfigMap 不会区分单行属性值和多行类似文件的值，重要的是 Pods
和其他对象如何使用这些值。</p>
<p>上面的例子定义了一个卷并将它作为 <code>/config</code> 文件夹挂载到 <code>demo</code> 容器内，
创建两个文件，<code>/config/game.properties</code> 和
<code>/config/user-interface.properties</code>，
尽管 ConfigMap 中包含了四个键。
这是因为 Pod 定义中在 <code>volumes</code> 节指定了一个 <code>items</code> 数组。
如果你完全忽略 <code>items</code> 数组，则 ConfigMap 中的每个键都会变成一个与该键同名的文件，
因此你会得到四个文件。</p>
<!--
## Using ConfigMaps

ConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other
parts of the system, without being directly exposed to the Pod. For example,
ConfigMaps can hold data that other parts of the system should use for configuration.
-->
<h2 id="using-configmaps">使用 ConfigMap  </h2>
<p>ConfigMap 可以作为数据卷挂载。ConfigMap 也可被系统的其他组件使用，
而不一定直接暴露给 Pod。例如，ConfigMap 可以保存系统中其他组件要使用的配置数据。</p>
<!--
The most common way to use ConfigMaps is to configure settings for
containers running in a Pod in the same namespace. You can also use a
ConfigMap separately.

For example, you
might encounter <a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='addons'>addons</a>
or <a class='glossary-tooltip' title='一种用于管理自定义资源的专用控制器' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/operator/' target='_blank' aria-label='operators'>operators</a> that
adjust their behavior based on a ConfigMap.
-->
<p>ConfigMap 最常见的用法是为同一命名空间里某 Pod 中运行的容器执行配置。
你也可以单独使用 ConfigMap。</p>
<p>比如，你可能会遇到基于 ConfigMap 来调整其行为的
<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='插件'>插件</a> 或者
<a class='glossary-tooltip' title='一种用于管理自定义资源的专用控制器' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/operator/' target='_blank' aria-label='operator'>operator</a>。</p>
<!--
### Using ConfigMaps as files from a Pod

To consume a ConfigMap in a volume in a Pod:
-->
<h3 id="在-pod-中将-configmap-当做文件使用">在 Pod 中将 ConfigMap 当做文件使用</h3>
<p>要在一个 Pod 的存储卷中使用 ConfigMap:</p>
<!--
1. Create a ConfigMap or use an existing one. Multiple Pods can reference the
   same ConfigMap.
1. Modify your Pod definition to add a volume under `.spec.volumes[]`. Name
   the volume anything, and have a `.spec.volumes[].configMap.name` field set
   to reference your ConfigMap object.
1. Add a `.spec.containers[].volumeMounts[]` to each container that needs the
   ConfigMap. Specify `.spec.containers[].volumeMounts[].readOnly = true` and
   `.spec.containers[].volumeMounts[].mountPath` to an unused directory name
   where you would like the ConfigMap to appear.
1. Modify your image or command line so that the program looks for files in
   that directory. Each key in the ConfigMap `data` map becomes the filename
   under `mountPath`.
-->
<ol>
<li>创建一个 ConfigMap 对象或者使用现有的 ConfigMap 对象。多个 Pod 可以引用同一个
ConfigMap。</li>
<li>修改 Pod 定义，在 <code>spec.volumes[]</code> 下添加一个卷。
为该卷设置任意名称，之后将 <code>spec.volumes[].configMap.name</code> 字段设置为对你的
ConfigMap 对象的引用。</li>
<li>为每个需要该 ConfigMap 的容器添加一个 <code>.spec.containers[].volumeMounts[]</code>。
设置 <code>.spec.containers[].volumeMounts[].readOnly=true</code> 并将
<code>.spec.containers[].volumeMounts[].mountPath</code> 设置为一个未使用的目录名，
ConfigMap 的内容将出现在该目录中。</li>
<li>更改你的镜像或者命令行，以便程序能够从该目录中查找文件。ConfigMap 中的每个
<code>data</code> 键会变成 <code>mountPath</code> 下面的一个文件名。</li>
</ol>
<!--
This is an example of a Pod that mounts a ConfigMap in a volume:
-->
<p>下面是一个将 ConfigMap 以卷的形式进行挂载的 Pod 示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/etc/foo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myconfigmap<span style="color:#bbb">
</span></code></pre></div><!--
Each ConfigMap you want to use needs to be referred to in `.spec.volumes`.

If there are multiple containers in the Pod, then each container needs its
own `volumeMounts` block, but only one `.spec.volumes` is needed per ConfigMap.
-->
<p>你希望使用的每个 ConfigMap 都需要在 <code>spec.volumes</code> 中被引用到。</p>
<p>如果 Pod 中有多个容器，则每个容器都需要自己的 <code>volumeMounts</code> 块，但针对每个
ConfigMap，你只需要设置一个 <code>spec.volumes</code> 块。</p>
<!--
#### Mounted ConfigMaps are updated automatically

When a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well.
The kubelet checks whether the mounted ConfigMap is fresh on every periodic sync.
However, the kubelet uses its local cache for getting the current value of the ConfigMap.
The type of the cache is configurable using the `ConfigMapAndSecretChangeDetectionStrategy` field in
the [KubeletConfiguration struct](/docs/reference/config-api/kubelet-config.v1beta1/).
-->
<h4 id="被挂载的-configmap-内容会被自动更新">被挂载的 ConfigMap 内容会被自动更新</h4>
<p>当卷中使用的 ConfigMap 被更新时，所投射的键最终也会被更新。
kubelet 组件会在每次周期性同步时检查所挂载的 ConfigMap 是否为最新。
不过，kubelet 使用的是其本地的高速缓存来获得 ConfigMap 的当前值。
高速缓存的类型可以通过
<a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/">KubeletConfiguration 结构</a>.
的 <code>ConfigMapAndSecretChangeDetectionStrategy</code> 字段来配置。</p>
<!--
A ConfigMap can be either propagated by watch (default), ttl-based, or by redirecting
all requests directly to the API server.
As a result, the total delay from the moment when the ConfigMap is updated to the moment
when new keys are projected to the Pod can be as long as the kubelet sync period + cache
propagation delay, where the cache propagation delay depends on the chosen cache type
(it equals to watch propagation delay, ttl of cache, or zero correspondingly).
-->
<p>ConfigMap 既可以通过 watch 操作实现内容传播（默认形式），也可实现基于 TTL
的缓存，还可以直接经过所有请求重定向到 API 服务器。
因此，从 ConfigMap 被更新的那一刻算起，到新的主键被投射到 Pod 中去，
这一时间跨度可能与 kubelet 的同步周期加上高速缓存的传播延迟相等。
这里的传播延迟取决于所选的高速缓存类型
（分别对应 watch 操作的传播延迟、高速缓存的 TTL 时长或者 0）。</p>
<!--
ConfigMaps consumed as environment variables are not updated automatically and require a pod restart.
-->
<p>以环境变量方式使用的 ConfigMap 数据不会被自动更新。
更新这些数据需要重新启动 Pod。</p>
<!--
A container using a ConfigMap as a [subPath](/docs/concepts/storage/volumes#using-subpath) volume mount will not receive ConfigMap updates.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 使用 ConfigMap 作为 <a href="/zh/docs/concepts/storage/volumes#using-subpath">subPath</a> 卷挂载的容器将不会收到 ConfigMap 的更新。
</div>
<!--
## Immutable ConfigMaps {#configmap-immutable}
-->
<h2 id="configmap-immutable">不可变更的 ConfigMap    </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>


<!--
The Kubernetes feature _Immutable Secrets and ConfigMaps_ provides an option to set
individual Secrets and ConfigMaps as immutable. For clusters that extensively use ConfigMaps
(at least tens of thousands of unique ConfigMap to Pod mounts), preventing changes to their
data has the following advantages:
-->
<p>Kubernetes 特性 <em>Immutable Secret 和 ConfigMaps</em> 提供了一种将各个
Secret 和 ConfigMap 设置为不可变更的选项。对于大量使用 ConfigMap 的集群
（至少有数万个各不相同的 ConfigMap 给 Pod 挂载）而言，禁止更改
ConfigMap 的数据有以下好处：</p>
<!--
- protects you from accidental (or unwanted) updates that could cause applications outages
- improves performance of your cluster by significantly reducing load on kube-apiserver, by
  closing watches for ConfigMaps marked as immutable.
-->
<ul>
<li>保护应用，使之免受意外（不想要的）更新所带来的负面影响。</li>
<li>通过大幅降低对 kube-apiserver 的压力提升集群性能，
这是因为系统会关闭对已标记为不可变更的 ConfigMap 的监视操作。</li>
</ul>
<!--
This feature is controlled by the `ImmutableEphemeralVolumes`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/).
You can create an immutable ConfigMap by setting the `immutable` field to `true`.
For example:
-->
<p>此功能特性由 <code>ImmutableEphemeralVolumes</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>来控制。
你可以通过将 <code>immutable</code> 字段设置为 <code>true</code> 创建不可变更的 ConfigMap。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">immutable</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div><!--
Once a ConfigMap is marked as immutable, it is _not_ possible to revert this change
nor to mutate the contents of the `data` or the `binaryData` field. You can
only delete and recreate the ConfigMap. Because existing Pods maintain a mount point
to the deleted ConfigMap, it is recommended to recreate these pods.
-->
<p>一旦某 ConfigMap 被标记为不可变更，则 <em>无法</em> 逆转这一变化，，也无法更改
<code>data</code> 或 <code>binaryData</code> 字段的内容。你只能删除并重建 ConfigMap。
因为现有的 Pod 会维护一个已被删除的 ConfigMap 的挂载点，建议重新创建这些 Pods。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about [Secrets](/docs/concepts/configuration/secret/).
* Read [Configure a Pod to Use a ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/).
* Read [The Twelve-Factor App](https://12factor.net/) to understand the motivation for
  separating code from configuration.
-->
<ul>
<li>阅读 <a href="/zh/docs/concepts/configuration/secret/">Secret</a>。</li>
<li>阅读<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">配置 Pod 使用 ConfigMap</a>。</li>
<li>阅读 <a href="https://12factor.net/zh_cn/">Twelve-Factor 应用</a>来了解将代码和配置分开的动机。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e511ed821ada65d0053341dbd8ad2bb5">7.3 - Secret</h1>
    
	<!--
reviewers:
- mikedanese
title: Secrets
content_type: concept
feature:
  title: Secret and configuration management
  description: >
    Deploy and update secrets and application configuration without rebuilding your image
    and without exposing secrets in your stack configuration.
weight: 30
-->
<!-- overview -->
<!--
A Secret is an object that contains a small amount of sensitive data such as
a password, a token, or a key. Such information might otherwise be put in a
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> specification or in a
<a class='glossary-tooltip' title='镜像是保存的容器实例，它打包了应用运行所需的一组软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-image' target='_blank' aria-label='container image'>container image</a>. Using a
Secret means that you don't need to include confidential data in your
application code.
-->
<p>Secret 是一种包含少量敏感信息例如密码、令牌或密钥的对象。
这样的信息可能会被放在 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 规约中或者镜像中。
使用 Secret 意味着你不需要在应用程序代码中包含机密数据。</p>
<!-- 
Because Secrets can be created independently of the Pods that use them, there
is less risk of the Secret (and its data) being exposed during the workflow of
creating, viewing, and editing Pods. Kubernetes, and applications that run in
your cluster, can also take additional precautions with Secrets, such as avoiding
writing confidential data to nonvolatile storage.

Secrets are similar to <a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMaps'>ConfigMaps</a>
but are specifically intended to hold confidential data.
-->
<p>由于创建 Secret 可以独立于使用它们的 Pod，
因此在创建、查看和编辑 Pod 的工作流程中暴露 Secret（及其数据）的风险较小。
Kubernetes 和在集群中运行的应用程序也可以对 Secret 采取额外的预防措施，
例如避免将机密数据写入非易失性存储。</p>
<p>Secret 类似于 <a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a>
但专门用于保存机密数据。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
Kubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store (etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd.
Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to create a Deployment.

In order to safely use Secrets, take at least the following steps:

1. [Enable Encryption at Rest](/docs/tasks/administer-cluster/encrypt-data/) for Secrets.
1. Enable or configure [RBAC rules](/docs/reference/access-authn-authz/authorization/) that
   restrict reading and writing data in Secrets. Be aware that secrets can be obtained
   implicitly by anyone with the permission to create a Pod.
1. Where appropriate, also use mechanisms such as RBAC to limit which principals are allowed
   to create new Secrets or replace existing ones.
-->
<p>默认情况下，Kubernetes Secret 未加密地存储在 API 服务器的底层数据存储（etcd）中。
任何拥有 API 访问权限的人都可以检索或修改 Secret，任何有权访问 etcd 的人也可以。
此外，任何有权限在命名空间中创建 Pod 的人都可以使用该访问权限读取该命名空间中的任何 Secret；
这包括间接访问，例如创建 Deployment 的能力。</p>
<p>为了安全地使用 Secret，请至少执行以下步骤：</p>
<ol>
<li>为 Secret <a href="/zh/docs/tasks/administer-cluster/encrypt-data/">启用静态加密</a>；</li>
<li>启用或配置 <a href="/zh/docs/reference/access-authn-authz/authorization/">RBAC 规则</a>来限制读取和写入
Secret 的数据（包括通过间接方式）。需要注意的是，被准许创建 Pod 的人也隐式地被授权获取
Secret 内容。</li>
<li>在适当的情况下，还可以使用 RBAC 等机制来限制允许哪些主体创建新 Secret 或替换现有 Secret。</li>
</ol>

</div>

<!--
See [Information security for Secrets](#information-security-for-secrets) for more details.
-->
<p>参见 <a href="#information-security-for-secrets">Secret 的信息安全</a>了解详情。</p>
<!-- body -->
<!--
## Uses for Secrets

There are three main ways for a Pod to use a Secret:
- As [files](#using-secrets-as-files-from-a-pod) in a
  <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='volume'>volume</a> mounted on one or more of
  its containers.
- As [container environment variable](#using-secrets-as-environment-variables).
- By the [kubelet when pulling images](#using-imagepullsecrets) for the Pod.

The Kubernetes control plane also uses Secrets; for example,
[bootstrap token Secrets](#bootstrap-token-secrets) are a mechanism to
help automate node registration.
-->
<h2 id="uses-for-secrets">Secret 的使用</h2>
<p>Pod 可以用三种方式之一来使用 Secret：</p>
<ul>
<li>作为挂载到一个或多个容器上的<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>
中的<a href="#using-secrets-as-files-from-a-pod">文件</a>。</li>
<li>作为<a href="#using-secrets-as-environment-variables">容器的环境变量</a>。</li>
<li>由 <a href="#using-imagepullsecrets">kubelet 在为 Pod 拉取镜像时使用</a>。</li>
</ul>
<p>Kubernetes 控制面也使用 Secret；
例如，<a href="#bootstrap-token-secrets">引导令牌 Secret</a>
是一种帮助自动化节点注册的机制。</p>
<!--
### Alternatives to Secrets

Rather than using a Secret to protect confidential data, you can pick from alternatives.

Here are some of your options:
-->
<h3 id="alternatives-to-secrets">Secret 的替代方案 </h3>
<p>除了使用 Secret 来保护机密数据，你也可以选择一些替代方案。</p>
<p>下面是一些选项：</p>
<!--
- if your cloud-native component needs to authenticate to another application that you
  know is running within the same Kubernetes cluster, you can use a
  [ServiceAccount](/docs/reference/access-authn-authz/authentication/#service-account-tokens)
  and its tokens to identify your client.
- there are third-party tools that you can run, either within or outside your cluster,
  that provide secrets management. For example, a service that Pods access over HTTPS,
  that reveals a secret if the client correctly authenticates (for example, with a ServiceAccount
  token).
-->
<ul>
<li>如果你的云原生组件需要执行身份认证来访问你所知道的、在同一 Kubernetes 集群中运行的另一个应用，
你可以使用 <a href="/zh/docs/reference/access-authn-authz/authentication/#service-account-tokens">ServiceAccount</a>
及其令牌来标识你的客户端身份。</li>
<li>你可以运行的第三方工具也有很多，这些工具可以运行在集群内或集群外，提供机密数据管理。
例如，这一工具可能是 Pod 通过 HTTPS 访问的一个服务，该服务在客户端能够正确地通过身份认证
（例如，通过 ServiceAccount 令牌）时，提供机密数据内容。</li>
</ul>
<!--
- for authentication, you can implement a custom signer for X.509 certificates, and use
  [CertificateSigningRequests](/docs/reference/access-authn-authz/certificate-signing-requests/)
  to let that custom signer issue certificates to Pods that need them.
- you can use a [device plugin](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
  to expose node-local encryption hardware to a specific Pod. For example, you can schedule
  trusted Pods onto nodes that provide a Trusted Platform Module, configured out-of-band.
-->
<ul>
<li>就身份认证而言，你可以为 X.509 证书实现一个定制的签名者，并使用
<a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/">CertificateSigningRequest</a>
来让该签名者为需要证书的 Pod 发放证书。</li>
<li>你可以使用一个<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">设备插件</a>
来将节点本地的加密硬件暴露给特定的 Pod。例如，你可以将可信任的 Pod
调度到提供可信平台模块（Trusted Platform Module，TPM）的节点上。
这类节点是另行配置的。</li>
</ul>
<!--
You can also combine two or more of those options, including the option to use Secret objects themselves.

For example: implement (or deploy) an <a class='glossary-tooltip' title='一种用于管理自定义资源的专用控制器' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/operator/' target='_blank' aria-label='operator'>operator</a>
that fetches short-lived session tokens from an external service, and then creates Secrets based
on those short-lived session tokens. Pods running in your cluster can make use of the session tokens,
and operator ensures they are valid. This separation means that you can run Pods that are unaware of
the exact mechanisms for issuing and refreshing those session tokens.
-->
<p>你还可以将如上选项的两种或多种进行组合，包括直接使用 Secret 对象本身也是一种选项。</p>
<p>例如：实现（或部署）一个 <a class='glossary-tooltip' title='一种用于管理自定义资源的专用控制器' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/operator/' target='_blank' aria-label='operator'>operator</a>，
从外部服务取回生命期很短的会话令牌，之后基于这些生命期很短的会话令牌来创建 Secret。
运行在集群中的 Pod 可以使用这些会话令牌，而 Operator 则确保这些令牌是合法的。
这种责权分离意味着你可以运行那些不了解会话令牌如何发放与刷新的确切机制的 Pod。</p>
<!--
## Working with Secrets

### Creating a Secret

There are several options to create a Secret:

- [create Secret using `kubectl` command](/docs/tasks/configmap-secret/managing-secret-using-kubectl/)
- [create Secret from config file](/docs/tasks/configmap-secret/managing-secret-using-config-file/)
- [create Secret using kustomize](/docs/tasks/configmap-secret/managing-secret-using-kustomize/)
-->
<h2 id="working-with-secrets">使用 Secret </h2>
<h3 id="creating-a-secret">创建 Secret </h3>
<ul>
<li><a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kubectl/">使用 <code>kubectl</code> 命令来创建 Secret</a></li>
<li><a href="/zh/docs/tasks/configmap-secret/managing-secret-using-config-file/">基于配置文件来创建 Secret</a></li>
<li><a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kustomize/">使用 kustomize 来创建 Secret</a></li>
</ul>
<!--
#### Constraints on Secret names and data {#restriction-names-data}

The name of a Secret object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<h4 id="restriction-names-data">对 Secret 名称与数据的约束</h4>
<p>Secret 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
You can specify the `data` and/or the `stringData` field when creating a
configuration file for a Secret.  The `data` and the `stringData` fields are optional.
The values for all keys in the `data` field have to be base64-encoded strings.
If the conversion to base64 string is not desirable, you can choose to specify
the `stringData` field instead, which accepts arbitrary strings as values.

The keys of `data` and `stringData` must consist of alphanumeric characters,
`-`, `_` or `.`. All key-value pairs in the `stringData` field are internally
merged into the `data` field. If a key appears in both the `data` and the
`stringData` field, the value specified in the `stringData` field takes
precedence.
-->
<p>在为创建 Secret 编写配置文件时，你可以设置 <code>data</code> 与/或 <code>stringData</code> 字段。
<code>data</code> 和 <code>stringData</code> 字段都是可选的。<code>data</code> 字段中所有键值都必须是 base64
编码的字符串。如果不希望执行这种 base64 字符串的转换操作，你可以选择设置
<code>stringData</code> 字段，其中可以使用任何字符串作为其取值。</p>
<p><code>data</code> 和 <code>stringData</code> 中的键名只能包含字母、数字、<code>-</code>、<code>_</code> 或 <code>.</code> 字符。
<code>stringData</code> 字段中的所有键值对都会在内部被合并到 <code>data</code> 字段中。
如果某个主键同时出现在 <code>data</code> 和 <code>stringData</code> 字段中，<code>stringData</code>
所指定的键值具有高优先级。</p>
<!--
#### Size limit {#restriction-data-size}

Individual secrets are limited to 1MiB in size. This is to discourage creation
of very large secrets that could exhaust the API server and kubelet memory.
However, creation of many smaller secrets could also exhaust memory. You can
use a [resource quota](/docs/concepts/policy/resource-quotas/) to limit the
number of Secrets (or other resources) in a namespace.
-->
<h4 id="restriction-data-size">尺寸限制  </h4>
<p>每个 Secret 的尺寸最多为 1MiB。施加这一限制是为了避免用户创建非常大的 Secret，
进而导致 API 服务器和 kubelet 内存耗尽。不过创建很多小的 Secret 也可能耗尽内存。
你可以使用<a href="/zh/docs/concepts/policy/resource-quotas/">资源配额</a>来约束每个名字空间中
Secret（或其他资源）的个数。</p>
<!--
### Editing a Secret

You can edit an existing Secret using kubectl:
-->
<h3 id="editing-a-secret">编辑 Secret   </h3>
<p>你可以使用 kubectl 来编辑一个已有的 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit secrets mysecret
</code></pre></div><p>这一命令会启动你的默认编辑器，允许你更新 <code>data</code> 字段中存放的 base64 编码的 Secret 值；
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># and an empty file will abort the edit. If an error occurs while saving this file, it will be</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># reopened with the relevant failures.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>YWRtaW4=<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>MWYyZDFlMmU2N2Rm<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubectl.kubernetes.io/last-applied-configuration</span>:<span style="color:#bbb"> </span>{<span style="color:#bbb"> </span>... }<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2020-01-22T18:41:56Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;164619&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>cfee02d6-c137-11e5-8d73-42010af00002<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Opaque<span style="color:#bbb">
</span></code></pre></div><!--
That example manifest defines a Secret with two keys in the `data` field: `username` and `password`.
The values are Base64 strings in the manifest; however, when you use the Secret with a Pod
then the kubelet provides the _decoded_ data to the Pod and its containers.

You can package many keys and values into one Secret, or use many Secrets, whichever is convenient.
-->
<p>这一示例清单定义了一个 Secret，其 <code>data</code> 字段中包含两个主键：<code>username</code> 和 <code>password</code>。
清单中的字段值是 Base64 字符串，不过，当你在 Pod 中使用 Secret 时，kubelet 为 Pod
及其中的容器提供的是解码后的数据。</p>
<p>你可以在一个 Secret 中打包多个主键和数值，也可以选择使用多个 Secret，
完全取决于哪种方式最方便。</p>
<!--
### Using a Secret

Secrets can be mounted as data volumes or exposed as
<a class='glossary-tooltip' title='容器环境变量提供了 name=value 形式的、运行容器化应用所必须的一些重要信息。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/containers/container-environment/' target='_blank' aria-label='environment variables'>environment variables</a>
to be used by a container in a Pod. Secrets can also be used by other parts of the
system, without being directly exposed to the Pod. For example, Secrets can hold
credentials that other parts of the system should use to interact with external
systems on your behalf.
-->
<h3 id="using-a-secret">使用 Secret   </h3>
<p>Secret 可以以数据卷的形式挂载，也可以作为<a class='glossary-tooltip' title='容器环境变量提供了 name=value 形式的、运行容器化应用所必须的一些重要信息。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/containers/container-environment/' target='_blank' aria-label='环境变量'>环境变量</a>
暴露给 Pod 中的容器使用。Secret 也可用于系统中的其他部分，而不是一定要直接暴露给 Pod。
例如，Secret 也可以包含系统中其他部分在替你与外部系统交互时要使用的凭证数据。</p>
<!--
Secret volume sources are validated to ensure that the specified object
reference actually points to an object of type Secret. Therefore, a Secret
needs to be created before any Pods that depend on it.  

If the Secret cannot be fetched (perhaps because it does not exist, or
due to a temporary lack of connection to the API server) the kubelet
periodically retries running that Pod. The kubelet also reports an Event
for that Pod, including details of the problem fetching the Secret.
-->
<p>Kubernetes 会检查 Secret 的卷数据源，确保所指定的对象引用确实指向类型为 Secret
的对象。因此，如果 Pod 依赖于某 Secret，该 Secret 必须先于 Pod 被创建。</p>
<p>如果 Secret 内容无法取回（可能因为 Secret 尚不存在或者临时性地出现 API
服务器网络连接问题），kubelet 会周期性地重试 Pod 运行操作。kubelet 也会为该 Pod
报告 Event 事件，给出读取 Secret 时遇到的问题细节。</p>
<!--
#### Optional Secrets {#restriction-secret-must-exist}

When you define a container environment variable based on a Secret,
you can mark it as _optional_. The default is for the Secret to be
required.

None of a Pod's containers will start until all non-optional Secrets are
available.

If a Pod references a specific key in a Secret and that Secret does exist, but
is missing the named key, the Pod fails during startup.
-->
<h4 id="restriction-secret-must-exist">可选的 Secret  </h4>
<p>当你定义一个基于 Secret 的环境变量时，你可以将其标记为可选。
默认情况下，所引用的 Secret 都是必需的。</p>
<p>只有所有非可选的 Secret 都可用时，Pod 中的容器才能启动运行。</p>
<p>如果 Pod 引用了 Secret 中的特定主键，而虽然 Secret 本身存在，对应的主键不存在，
Pod 启动也会失败。</p>
<!--
### Using Secrets as files from a Pod {#using-secrets-as-files-from-a-pod}

If you want to access data from a Secret in a Pod, one way to do that is to
have Kubernetes make the value of that Secret be available as a file inside
the filesystem of one or more of the Pod's containers.

To configure that, you:
-->
<h3 id="using-secrets-as-files-from-a-pod">在 Pod 中以文件形式使用 Secret </h3>
<p>如果你希望在 Pod 中访问 Secret 内的数据，一种方式是让 Kubernetes 将 Secret
以 Pod 中一个或多个容器的文件系统中的文件的形式呈现出来。</p>
<p>要配置这种行为，你需要：</p>
<!--
1. Create a secret or use an existing one. Multiple Pods can reference the same secret.
1. Modify your Pod definition to add a volume under `.spec.volumes[]`. Name the volume anything, and have a `.spec.volumes[].secret.secretName` field equal to the name of the Secret object.
1. Add a `.spec.containers[].volumeMounts[]` to each container that needs the secret. Specify `.spec.containers[].volumeMounts[].readOnly = true` and `.spec.containers[].volumeMounts[].mountPath` to an unused directory name where you would like the secrets to appear.
1. Modify your image or command line so that the program looks for files in that directory. Each key in the secret `data` map becomes the filename under `mountPath`.

This is an example of a Pod that mounts a Secret named `mysecret` in a volume:
-->
<ol>
<li>创建一个 Secret 或者使用已有的 Secret。多个 Pod 可以引用同一个 Secret。</li>
<li>更改 Pod 定义，在 <code>.spec.volumes[]</code> 下添加一个卷。根据需要为卷设置其名称，
并将 <code>.spec.volumes[].secret.secretName</code> 字段设置为 Secret 对象的名称。</li>
<li>为每个需要该 Secret 的容器添加 <code>.spec.containers[].volumeMounts[]</code>。
并将 <code>.spec.containers[].volumeMounts[].readyOnly</code> 设置为 <code>true</code>，
将 <code>.spec.containers[].volumeMounts[].mountPath</code> 设置为希望 Secret
被放置的、目前尚未被使用的路径名。</li>
<li>更改你的镜像或命令行，以便程序读取所设置的目录下的文件。Secret 的 <code>data</code>
映射中的每个主键都成为 <code>mountPath</code> 下面的文件名。</li>
</ol>
<p>下面是一个通过卷来挂载名为 <code>mysecret</code> 的 Secret 的 Pod 示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/etc/foo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">optional</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 默认设置，意味着 &#34;mysecret&#34; 必须已经存在</span><span style="color:#bbb">
</span></code></pre></div><!--
Each Secret you want to use needs to be referred to in `.spec.volumes`.

If there are multiple containers in the Pod, then each container needs its
own `volumeMounts` block, but only one `.spec.volumes` is needed per Secret.
-->
<p>你要访问的每个 Secret 都需要通过 <code>.spec.volumes</code> 来引用。</p>
<p>如果 Pod 中包含多个容器，则每个容器需要自己的 <code>volumeMounts</code> 块，
不过针对每个 Secret 而言，只需要一份 <code>.spec.volumes</code> 设置。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Versions of Kubernetes before v1.22 automatically created credentials for accessing
the Kubernetes API. This older mechanism was based on creating token Secrets that
could then be mounted into running Pods.
In more recent versions, including Kubernetes v1.23, API credentials
are obtained directly by using the [TokenRequest](/docs/reference/kubernetes-api/authentication-resources/token-request-v1/) API,
and are mounted into Pods using a [projected volume](/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume).
The tokens obtained using this method have bounded lifetimes, and are automatically
invalidated when the Pod they are mounted into is deleted.
-->
<p>Kubernetes v1.22 版本之前都会自动创建用来访问 Kubernetes API 的凭证。
这一老的机制是基于创建可被挂载到 Pod 中的令牌 Secret 来实现的。
在最近的版本中，包括 Kubernetes v1.23 中，API 凭据是直接通过
<a href="/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">TokenRequest</a>
API 来获得的，这一凭据会使用<a href="/zh/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume">投射卷</a>
挂载到 Pod 中。使用这种方式获得的令牌有确定的生命期，并且在挂载它们的 Pod
被删除时自动作废。</p>
<!--
You can still [manually create](/docs/tasks/configure-pod-container/configure-service-account/#manually-create-a-service-account-api-token)
a service account token Secret; for example, if you need a token that never expires.
However, using the [TokenRequest](/docs/reference/kubernetes-api/authentication-resources/token-request-v1/)
subresource to obtain a token to access the API is recommended instead.
-->
<p>你仍然可以<a href="/zh/docs/tasks/configure-pod-container/configure-service-account/#manually-create-a-service-account-api-token">手动创建</a>
服务账号令牌。例如，当你需要一个永远都不过期的令牌时。
不过，仍然建议使用 <a href="/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">TokenRequest</a>
子资源来获得访问 API 服务器的令牌。</p>

</div>
<!--
#### Projection of Secret keys to specific paths

You can also control the paths within the volume where Secret keys are projected.
You can use the `.spec.volumes[].secret.items` field to change the target path of each key:
-->
<h4 id="将-secret-键投射到特定目录">将 Secret 键投射到特定目录</h4>
<p>你也可以控制 Secret 键所投射到的卷中的路径。
你可以使用 <code>.spec.volumes[].secret.items</code> 字段来更改每个主键的目标路径：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/etc/foo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>username<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>my-group/my-username<span style="color:#bbb">
</span></code></pre></div><!--
What will happen:

* the `username` key from `mysecret` is available to the container at the path
  `/etc/foo/my-group/my-username` instead of at `/etc/foo/username`.
* the `password` key from that Secret object is not projected.

If `.spec.volumes[].secret.items` is used, only keys specified in `items` are projected.
To consume all keys from the Secret, all of them must be listed in the `items` field.

If you list keys explicitly, then all listed keys must exist in the corresponding Secret.
Otherwise, the volume is not created.
-->
<p>将发生的事情如下：</p>
<ul>
<li><code>mysecret</code> 中的键 <code>username</code> 会出现在容器中的路径为 <code>/etc/foo/my-group/my-username</code>，
而不是 <code>/etc/foo/username</code>。</li>
<li>Secret 对象的 <code>password</code> 键不会被投射。</li>
</ul>
<p>如果使用了 <code>.spec.volumes[].secret.items</code>，则只有 <code>items</code> 中指定了的主键会被投射。
如果要使用 Secret 中的所有主键，则需要将它们全部枚举到 <code>items</code> 字段中。</p>
<p>如果你显式地列举了主键，则所列举的主键都必须在对应的 Secret 中存在。
否则所在的卷不会被创建。</p>
<!--
#### Secret files permissions

You can set the POSIX file access permission bits for a single Secret key.
If you don't specify any permissions, `0644` is used by default.
You can also set a default mode for the entire Secret volume and override per key if needed.

For example, you can specify a default mode like this:
-->
<h4 id="secret-文件的访问权限">Secret 文件的访问权限</h4>
<p>你可以为某个 Secret 主键设置 POSIX 文件访问权限位。
如果你不指定访问权限，默认会使用 <code>0644</code>。
你也可以为整个 Secret 卷设置默认的访问模式，然后再根据需要在主键层面重载。</p>
<p>例如，你可以像下面这样设置默认的模式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/etc/foo&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>foo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">defaultMode</span>:<span style="color:#bbb"> </span><span style="color:#666">0400</span><span style="color:#bbb">
</span></code></pre></div><!--
The secret is mounted on `/etc/foo`; all the files created by the
secret volume mount have permission `0400`.
-->
<p>该 Secret 被挂载在 <code>/etc/foo</code> 下，Secret 卷挂载所创建的所有文件的访问模式都是 <code>0400</code>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you're defining a Pod or a Pod template using JSON, beware that the JSON
specification doesn't support octal notation. You can use the decimal value
for the `defaultMode` (for example, 0400 in octal is 256 in decimal) instead.  
If you're writing YAML, you can write the `defaultMode` in octal.
-->
<p>如果你是使用 JSON 来定义 Pod 或 Pod 模板，需要注意 JSON 规范不支持八进制的记数方式。
你可以在 <code>defaultMode</code> 中设置十进制的值（例如，八进制中的 0400 在十进制中为 256）。
如果你使用 YAML 来编写定义，你可以用八进制值来设置 <code>defaultMode</code>。
</div>
<!--
#### Consuming Secret values from volumes

Inside the container that mounts a secret volume, the secret keys appear as
files. The secret values are base64 decoded and stored inside these files.

This is the result of commands executed inside the container from the example above:
-->
<h4 id="consuming-secret-values-from-volumes">使用来自卷中的 Secret 值 </h4>
<p>在挂载了 Secret 卷的容器内，Secret 的主键都呈现为文件。
Secret 的取值都是 Base64 编码的，保存在这些文件中。</p>
<p>下面是在上例中的容器内执行命令的结果：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ls /etc/foo/
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>username
password
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat /etc/foo/username
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>admin
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat /etc/foo/password
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>1f2d1e2e67df
</code></pre><!--
The program in a container is responsible for reading the secret data from these
files, as needed.
-->
<p>容器中的程序要负责根据需要读取 Secret 数据。</p>
<!--
#### Mounted Secrets are updated automatically

When a volume contains data from a Secret, and that Secret is updated, Kubernetes tracks
this and updates the data in the volume, using an eventually-consistent approach.
-->
<h4 id="mounted-secrets-are-updated-automatically">挂载的 Secret 是被自动更新的 </h4>
<p>当卷中包含来自 Secret 的数据，而对应的 Secret 被更新，Kubernetes
会跟踪到这一操作并更新卷中的数据。更新的方式是保证最终一致性。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
A container using a Secret as a
[subPath](/docs/concepts/storage/volumes#using-subpath) volume mount does not receive
automated Secret updates.
-->
<p>对于以 <a href="/zh/docs/concepts/storage/volumes#using-subpath">subPath</a> 形式挂载 Secret 卷的容器而言，
它们无法收到自动的 Secret 更新。
</div>
<!--
The kubelet keeps a cache of the current keys and values for the Secrets that are used in
volumes for pods on that node.
You can configure the way that the kubelet detects changes from the cached values. The `configMapAndSecretChangeDetectionStrategy` field in
the [kubelet configuration](/docs/reference/config-api/kubelet-config.v1beta1/) controls which strategy the kubelet uses. The default strategy is `Watch`.
-->
<p>Kubelet 组件会维护一个缓存，在其中保存节点上 Pod 卷中使用的 Secret 的当前主键和取值。
你可以配置 kubelet 如何检测所缓存数值的变化。
<a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/">kubelet 配置</a>中的
<code>configMapAndSecretChangeDetectionStrategy</code> 字段控制 kubelet 所采用的策略。
默认的策略是 <code>Watch</code>。</p>
<!--
Updates to Secrets can be either propagated by an API watch mechanism (the default), based on
a cache with a defined time-to-live, or polled from the cluster API server on each kubelet
synchronisation loop.
-->
<p>对 Secret 的更新操作既可以通过 API 的 watch 机制（默认）来传播，
基于设置了生命期的缓存获取，也可以通过 kubelet 的同步回路来从集群的 API
服务器上轮询获取。</p>
<!--
As a result, the total delay from the moment when the Secret is updated to the moment
when new keys are projected to the Pod can be as long as the kubelet sync period + cache
propagation delay, where the cache propagation delay depends on the chosen cache type
(following the same order listed in the previous paragraph, these are:
watch propagation delay, the configured cache TTL, or zero for direct polling).
-->
<p>因此，从 Secret 被更新到新的主键被投射到 Pod 中，中间存在一个延迟。
这一延迟的上限是 kubelet 的同步周期加上缓存的传播延迟，
其中缓存的传播延迟取决于所选择的缓存类型。
对应上一段中提到的几种传播机制，延迟时长为 watch 的传播延迟、所配置的缓存 TTL
或者对于直接轮询而言是零。</p>
<!--
### Using Secrets as environment variables

To use a Secret in an <a class='glossary-tooltip' title='容器环境变量提供了 name=value 形式的、运行容器化应用所必须的一些重要信息。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/containers/container-environment/' target='_blank' aria-label='environment variable'>environment variable</a>
in a Pod:
-->
<h3 id="using-secrets-as-environment-variables">以环境变量的方式使用 Secret  </h3>
<p>如果需要在 Pod 中以<a class='glossary-tooltip' title='容器环境变量提供了 name=value 形式的、运行容器化应用所必须的一些重要信息。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/containers/container-environment/' target='_blank' aria-label='环境变量'>环境变量</a>
的形式使用 Secret：</p>
<!--
1. Create a Secret (or use an existing one).  Multiple Pods can reference the same Secret.
1. Modify your Pod definition in each container that you wish to consume the value of a secret
   key to add an environment variable for each secret key you wish to consume. The environment
   variable that consumes the secret key should populate the secret's name and key in `env[].valueFrom.secretKeyRef`.
1. Modify your image and/or command line so that the program looks for values in the specified
   environment variables.
-->
<ol>
<li>创建 Secret（或者使用现有 Secret）。多个 Pod 可以引用同一个 Secret。</li>
<li>更改 Pod 定义，在要使用 Secret 键值的每个容器中添加与所使用的主键对应的环境变量。
读取 Secret 主键的环境变量应该在 <code>env[].valueFrom.secretKeyRef</code> 中填写 Secret
的名称和主键名称。</li>
<li>更改你的镜像或命令行，以便程序读取环境变量中保存的值。</li>
</ol>
<!--
This is an example of a Pod that uses a Secret via environment variables:
-->
<p>下面是一个通过环境变量来使用 Secret 的示例 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-env-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mycontainer<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>SECRET_USERNAME<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secretKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>username<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">optional</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 此值为默认值；意味着 &#34;mysecret&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">                            </span><span style="color:#080;font-style:italic"># 必须存在且包含名为 &#34;username&#34; 的主键</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>SECRET_PASSWORD<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secretKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>password<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">optional</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 此值为默认值；意味着 &#34;mysecret&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">                            </span><span style="color:#080;font-style:italic"># 必须存在且包含名为 &#34;password&#34; 的主键</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div><!--
#### Invalid environment variables {#restriction-env-from-invalid}

Secrets used to populate environment variables by the `envFrom` field that have keys
that are considered invalid environment variable names will have those keys
skipped. The Pod is allowed to start.
-->
<h4 id="restriction-env-from-invalid">非法环境变量   </h4>
<p>对于通过 <code>envFrom</code> 字段来填充环境变量的 Secret 而言，
如果其中包含的主键不能被当做合法的环境变量名，这些主键会被忽略掉。
Pod 仍然可以启动。</p>
<!--
If you define a Pod with an invalid variable name, the failed Pod startup includes
an event with the reason set to `InvalidVariableNames` and a message that lists the
skipped invalid keys. The following example shows a Pod that refers to a Secret
named `mysecret`, where `mysecret` contains 2 invalid keys: `1badkey` and `2alsobad`.
-->
<p>如果你定义的 Pod 中包含非法的变量名称，则 Pod 可能启动失败，
会形成 reason 为 <code>InvalidVariableNames</code> 的事件，以及列举被略过的非法主键的消息。
下面的例子中展示了一个 Pod，引用的是名为 <code>mysecret</code> 的 Secret，
其中包含两个非法的主键：<code>1badkey</code> 和 <code>2alsobad</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get events
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>LASTSEEN   FIRSTSEEN   COUNT     NAME            KIND      SUBOBJECT                         TYPE      REASON
0s         0s          1         dapi-test-pod   Pod                                         Warning   InvalidEnvironmentVariableNames   kubelet, 127.0.0.1      Keys [1badkey, 2alsobad] from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names.
</code></pre><!--
#### Consuming Secret values from environment variables

Inside a container that consumes a Secret using environment variables, the secret keys appear
as normal environment variables. The values of those variables are the base64 decoded values
of the secret data.

This is the result of commands executed inside the container from the example above:
-->
<h4 id="通过环境变量使用-secret-值">通过环境变量使用 Secret 值</h4>
<p>在通过环境变量来使用 Secret 的容器中，Secret 主键展现为普通的环境变量。
这些变量的取值是 Secret 数据的 Base64 解码值。</p>
<p>下面是在前文示例中的容器内执行命令的结果：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">echo</span> <span style="color:#b44">&#34;</span><span style="color:#b8860b">$SECRET_USERNAME</span><span style="color:#b44">&#34;</span>
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>admin
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">echo</span> <span style="color:#b44">&#34;</span><span style="color:#b8860b">$SECRET_PASSWORD</span><span style="color:#b44">&#34;</span>
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>1f2d1e2e67df
</code></pre><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If a container already consumes a Secret in an environment variable,
a Secret update will not be seen by the container unless it is
restarted. There are third party solutions for triggering restarts when
secrets change.
-->
<p>如果容器已经在通过环境变量来使用 Secret，Secret 更新在容器内是看不到的，
除非容器被重启。有一些第三方的解决方案，能够在 Secret 发生变化时触发容器重启。
</div>
<!--
### Container image pull secrets {#using-imagepullsecrets}

If you want to fetch container images from a private repository, you need a way for
the kubelet on each node to authenticate to that repository. You can configure
_image pull secrets_ to make this possible. These secrets are configured at the Pod
level.
-->
<h3 id="using-imagepullsecrets">容器镜像拉取 Secret </h3>
<p>如果你尝试从私有仓库拉取容器镜像，你需要一种方式让每个节点上的 kubelet
能够完成与镜像库的身份认证。你可以配置 <em>镜像拉取 Secret</em> 来实现这点。
Secret 是在 Pod 层面来配置的。</p>
<!--
The `imagePullSecrets` field for a Pod is a list of references to Secrets in the same namespace
as the Pod.
You can use an `imagePullSecrets` to pass image registry access credentials to
the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.
See `PodSpec` in the [Pod API reference](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)
for more information about the `imagePullSecrets` field.
-->
<p>Pod 的 <code>imagePullSecrets</code> 字段是一个对 Pod 所在的名字空间中的 Secret
的引用列表。你可以使用 <code>imagePullSecrets</code> 来将镜像仓库访问凭据传递给 kubelet。
kubelet 使用这个信息来替你的 Pod 拉取私有镜像。
参阅 <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec">Pod API 参考</a>
中的 <code>PodSpec</code> 进一步了解 <code>imagePullSecrets</code> 字段。</p>
<!--
#### Using imagePullSecrets

The `imagePullSecrets` field is a list of references to secrets in the same namespace.
You can use an `imagePullSecrets` to pass a secret that contains a Docker (or other) image registry
password to the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.
See the [PodSpec API](/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core) for more information about the `imagePullSecrets` field.
-->
<h4 id="使用-imagepullsecrets">使用 imagePullSecrets</h4>
<p><code>imagePullSecrets</code> 字段是一个列表，包含对同一名字空间中 Secret 的引用。
你可以使用 <code>imagePullSecrets</code> 将包含 Docker（或其他）镜像仓库密码的 Secret
传递给 kubelet。kubelet 使用此信息来替 Pod 拉取私有镜像。
参阅 <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec">PodSpec API </a>
进一步了解 <code>imagePullSecrets</code> 字段。</p>
<!--
##### Manually specifying an imagePullSecret

You can learn how to specify `imagePullSecrets` from the [container images](/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod)
documentation.
-->
<h5 id="手动设定-imagepullsecret">手动设定 imagePullSecret</h5>
<p>你可以通过阅读<a href="/zh/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">容器镜像</a>
文档了解如何设置 <code>imagePullSecrets</code>。</p>
<!--
##### Arranging for imagePullSecrets to be automatically attached

You can manually create `imagePullSecrets`, and reference these from
a ServiceAccount. Any Pods created with that ServiceAccount
or created with that ServiceAccount by default, will get their `imagePullSecrets`
field set to that of the service account.
See [Add ImagePullSecrets to a service account](/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account)
 for a detailed explanation of that process.
-->
<h5 id="设置-imagepullsecrets-为自动挂载">设置 imagePullSecrets 为自动挂载</h5>
<p>你可以手动创建 <code>imagePullSecret</code>，并在一个 ServiceAccount 中引用它。
对使用该 ServiceAccount 创建的所有 Pod，或者默认使用该 ServiceAccount 创建的 Pod
而言，其 <code>imagePullSecrets</code> 字段都会设置为该服务账号。
请阅读<a href="/zh/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account">向服务账号添加 ImagePullSecrets</a>
来详细了解这一过程。</p>
<!--
### Using Secrets with static Pods {#restriction-static-pod}

You cannot use ConfigMaps or Secrets with
<a class='glossary-tooltip' title='静态Pod（Static Pod）是指由特定节点上的 kubelet 守护进程直接管理的 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/static-pod/' target='_blank' aria-label='static Pods'>static Pods</a>.
-->
<h3 id="restriction-static-pod">在静态 Pod 中使用 Secret   </h3>
<p>你不可以在<a class='glossary-tooltip' title='静态Pod（Static Pod）是指由特定节点上的 kubelet 守护进程直接管理的 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/static-pod/' target='_blank' aria-label='静态 Pod'>静态 Pod</a>.
中使用 ConfigMap 或 Secret。</p>
<!--
## Use cases

### Use case: As container environment variables

Create a secret
-->
<h2 id="use-case">使用场景 </h2>
<h3 id="使用场景-作为容器环境变量">使用场景：作为容器环境变量</h3>
<p>创建 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Opaque<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">USER_NAME</span>:<span style="color:#bbb"> </span>YWRtaW4=<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">PASSWORD</span>:<span style="color:#bbb"> </span>MWYyZDFlMmU2N2Rm<span style="color:#bbb">
</span></code></pre></div><!--
Create the Secret:
-->
<p>创建 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f mysecret.yaml
</code></pre></div><!--
Use `envFrom` to define all of the Secret's data as container environment variables. The key from the Secret becomes the environment variable name in the Pod.
-->
<p>使用 <code>envFrom</code> 来将 Secret 的所有数据定义为容器的环境变量。
来自 Secret 的主键成为 Pod 中的环境变量名称：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-test-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;env&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">envFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">secretRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div><!--
### Use case: Pod with SSH keys

Create a Secret containing some SSH keys:
-->
<h3 id="使用场景-带-ssh-密钥的-pod">使用场景：带 SSH 密钥的 Pod</h3>
<p>创建包含一些 SSH 密钥的 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic ssh-key-secret --from-file<span style="color:#666">=</span>ssh-privatekey<span style="color:#666">=</span>/path/to/.ssh/id_rsa --from-file<span style="color:#666">=</span>ssh-publickey<span style="color:#666">=</span>/path/to/.ssh/id_rsa.pub
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>secret &quot;ssh-key-secret&quot; created
</code></pre><!--
You can also create a `kustomization.yaml` with a `secretGenerator` field containing ssh keys.
-->
<p>你也可以创建一个 <code>kustomization.yaml</code> 文件，在其 <code>secretGenerator</code>
字段中包含 SSH 密钥。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
Think carefully before sending your own SSH keys: other users of the cluster may have access
to the Secret.
-->
<p>在提供你自己的 SSH 密钥之前要仔细思考：集群的其他用户可能有权访问该 Secret。</p>
<!--
You could instead create an SSH private key representing a service identity that you want to be
accessible to all the users with whom you share the Kubernetes cluster, and that you can revoke
if the credentials are compromised.
-->
<p>你也可以创建一个 SSH 私钥，代表一个你希望与你共享 Kubernetes 集群的其他用户分享的服务标识。
当凭据信息被泄露时，你可以收回该访问权限。</p>

</div>

<!--
Now you can create a Pod which references the secret with the SSH key and
consumes it in a volume:
-->
<p>现在你可以创建一个 Pod，在其中访问包含 SSH 密钥的 Secret，并通过卷的方式来使用它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-test-pod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-test<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>ssh-key-secret<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ssh-test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mySshImage<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/etc/secret-volume&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
When the container's command runs, the pieces of the key will be available in:
-->
<p>容器命令执行时，秘钥的数据可以在下面的位置访问到：</p>
<pre><code>/etc/secret-volume/ssh-publickey
/etc/secret-volume/ssh-privatekey
</code></pre><!--
The container is then free to use the secret data to establish an SSH connection.
-->
<p>容器就可以随便使用 Secret 数据来建立 SSH 连接。</p>
<!--
### Use case: Pods with prod / test credentials

This example illustrates a Pod which consumes a secret containing production
credentials and another Pod which consumes a secret with test environment
credentials.

You can create a `kustomization.yaml` with a `secretGenerator` field or run
`kubectl create secret`.
-->
<h3 id="使用场景-带有生产-测试环境凭据的-pod">使用场景：带有生产、测试环境凭据的 Pod</h3>
<p>这一示例所展示的一个 Pod 会使用包含生产环境凭据的 Secret，另一个 Pod
使用包含测试环境凭据的 Secret。</p>
<p>你可以创建一个带有 <code>secretGenerator</code> 字段的 <code>kustomization.yaml</code> 文件或者运行
<code>kubectl create secret</code> 来创建 Secret。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic prod-db-secret --from-literal<span style="color:#666">=</span><span style="color:#b8860b">username</span><span style="color:#666">=</span>produser --from-literal<span style="color:#666">=</span><span style="color:#b8860b">password</span><span style="color:#666">=</span>Y4nys7f11
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>secret &quot;prod-db-secret&quot; created
</code></pre><!--
You can also create a secret for test environment credentials.
-->
<p>你也可以创建一个包含测试环境凭据的 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic test-db-secret --from-literal<span style="color:#666">=</span><span style="color:#b8860b">username</span><span style="color:#666">=</span>testuser --from-literal<span style="color:#666">=</span><span style="color:#b8860b">password</span><span style="color:#666">=</span>iluvtests
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>secret &quot;test-db-secret&quot; created
</code></pre><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Special characters such as `$`, `\`, `*`, `=`, and `!` will be interpreted by your [shell](https://en.wikipedia.org/wiki/Shell_(computing)) and require escaping.
-->
<p>特殊字符（例如 <code>$</code>、<code>\</code>、<code>*</code>、<code>=</code> 和 <code>!</code>）会被你的
<a href="https://en.wikipedia.org/wiki/Shell_(computing)">Shell</a>解释，因此需要转义。</p>
<!--
In most shells, the easiest way to escape the password is to surround it with single quotes (`'`).
For example, if your actual password is `S!B\*d$zDsb=`, you should execute the command this way:
-->
<p>在大多数 Shell 中，对密码进行转义的最简单方式是用单引号（<code>'</code>）将其括起来。
例如，如果你的实际密码是 <code>S!B\*d$zDsb</code>，则应通过以下方式执行命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic dev-db-secret --from-literal<span style="color:#666">=</span><span style="color:#b8860b">username</span><span style="color:#666">=</span>devuser --from-literal<span style="color:#666">=</span><span style="color:#b8860b">password</span><span style="color:#666">=</span><span style="color:#b44">&#39;S!B\*d$zDsb=&#39;</span>
</code></pre></div><!--
You do not need to escape special characters in passwords from files (`--from-file`).
-->
<p>你无需对文件中的密码（<code>--from-file</code>）中的特殊字符进行转义。</p>

</div>
<!--
Now make the Pods:
-->
<p>现在生成 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt; pod.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: List
</span><span style="color:#b44">items:
</span><span style="color:#b44">- kind: Pod
</span><span style="color:#b44">  apiVersion: v1
</span><span style="color:#b44">  metadata:
</span><span style="color:#b44">    name: prod-db-client-pod
</span><span style="color:#b44">    labels:
</span><span style="color:#b44">      name: prod-db-client
</span><span style="color:#b44">  spec:
</span><span style="color:#b44">    volumes:
</span><span style="color:#b44">    - name: secret-volume
</span><span style="color:#b44">      secret:
</span><span style="color:#b44">        secretName: prod-db-secret
</span><span style="color:#b44">    containers:
</span><span style="color:#b44">    - name: db-client-container
</span><span style="color:#b44">      image: myClientImage
</span><span style="color:#b44">      volumeMounts:
</span><span style="color:#b44">      - name: secret-volume
</span><span style="color:#b44">        readOnly: true
</span><span style="color:#b44">        mountPath: &#34;/etc/secret-volume&#34;
</span><span style="color:#b44">- kind: Pod
</span><span style="color:#b44">  apiVersion: v1
</span><span style="color:#b44">  metadata:
</span><span style="color:#b44">    name: test-db-client-pod
</span><span style="color:#b44">    labels:
</span><span style="color:#b44">      name: test-db-client
</span><span style="color:#b44">  spec:
</span><span style="color:#b44">    volumes:
</span><span style="color:#b44">    - name: secret-volume
</span><span style="color:#b44">      secret:
</span><span style="color:#b44">        secretName: test-db-secret
</span><span style="color:#b44">    containers:
</span><span style="color:#b44">    - name: db-client-container
</span><span style="color:#b44">      image: myClientImage
</span><span style="color:#b44">      volumeMounts:
</span><span style="color:#b44">      - name: secret-volume
</span><span style="color:#b44">        readOnly: true
</span><span style="color:#b44">        mountPath: &#34;/etc/secret-volume&#34;
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Add the pods to the same `kustomization.yaml`:
-->
<p>将 Pod 添加到同一 <code>kustomization.yaml</code> 文件中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt;&gt; kustomization.yaml
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- pod.yaml
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Apply all those objects on the API server by running:
-->
<p>通过下面的命令在 API 服务器上应用所有这些对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -k .
</code></pre></div><!--
Both containers will have the following files present on their filesystems with the values
for each container's environment:
-->
<p>两个文件都会在其文件系统中出现下面面的文件，文件中内容是各个容器的环境值：</p>
<pre><code>/etc/secret-volume/username
/etc/secret-volume/password
</code></pre><!--
Note how the specs for the two Pods differ only in one field; this facilitates
creating Pods with different capabilities from a common Pod template.
-->
<p>注意这两个 Pod 的规约中只有一个字段不同。
这便于基于相同的 Pod 模板生成具有不同能力的 Pod。</p>
<!--
You could further simplify the base Pod specification by using two service accounts:

1. `prod-user` with the `prod-db-secret`
1. `test-user` with the `test-db-secret`

The Pod specification is shortened to:
-->
<p>你可以通过使用两个服务账号来进一步简化这一基本的 Pod 规约：</p>
<ol>
<li><code>prod-user</code> 服务账号使用 <code>prod-db-secret</code></li>
<li><code>test-user</code> 服务账号使用 <code>test-db-secret</code></li>
</ol>
<p>Pod 规约简化为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>prod-db-client-pod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>prod-db-client<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceAccount</span>:<span style="color:#bbb"> </span>prod-db-client<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>db-client-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>myClientImage<span style="color:#bbb">
</span></code></pre></div><!--
### Use case: dotfiles in a secret volume

You can make your data "hidden" by defining a key that begins with a dot.
This key represents a dotfile or "hidden" file. For example, when the following secret
is mounted into a volume, `secret-volume`:
-->
<h3 id="使用场景-在-secret-卷中带句点的文件">使用场景：在 Secret 卷中带句点的文件</h3>
<p>通过定义以句点（<code>.</code>）开头的主键，你可以“隐藏”你的数据。
这些主键代表的是以句点开头的文件或“隐藏”文件。
例如，当下面的 Secret 被挂载到 <code>secret-volume</code> 卷中时：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dotfile-secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">.secret-file</span>:<span style="color:#bbb"> </span>dmFsdWUtMg0KDQo=<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-dotfiles-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>dotfile-secret<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dotfile-test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ls<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;-l&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;/etc/secret-volume&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/etc/secret-volume&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
The volume will contain a single file, called `.secret-file`, and
the `dotfile-test-container` will have this file present at the path
`/etc/secret-volume/.secret-file`.
-->
<p>卷中会包含一个名为 <code>.secret-file</code> 的文件，并且容器 <code>dotfile-test-container</code>
中此文件位于路径 <code>/etc/secret-volume/.secret-file</code> 处。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Files beginning with dot characters are hidden from the output of  `ls -l`;
you must use `ls -la` to see them when listing directory contents.
-->
<p>以句点开头的文件会在 <code>ls -l</code> 的输出中被隐藏起来；
列举目录内容时你必须使用 <code>ls -la</code> 才能看到它们。
</div>
<!--
### Use case: Secret visible to one container in a Pod

Consider a program that needs to handle HTTP requests, do some complex business
logic, and then sign some messages with an HMAC. Because it has complex
application logic, there might be an unnoticed remote file reading exploit in
the server, which could expose the private key to an attacker.
-->
<h3 id="使用场景-仅对-pod-中一个容器可见的-secret">使用场景：仅对 Pod 中一个容器可见的 Secret</h3>
<p>考虑一个需要处理 HTTP 请求，执行某些复杂的业务逻辑，之后使用 HMAC
来对某些消息进行签名的程序。因为这一程序的应用逻辑很复杂，
其中可能包含未被注意到的远程服务器文件读取漏洞，
这种漏洞可能会把私钥暴露给攻击者。</p>
<!--
This could be divided into two processes in two containers: a frontend container
which handles user interaction and business logic, but which cannot see the
private key; and a signer container that can see the private key, and responds
to simple signing requests from the frontend (for example, over localhost networking).
-->
<p>这一程序可以分隔成两个容器中的两个进程：前端容器要处理用户交互和业务逻辑，
但无法看到私钥；签名容器可以看到私钥，并对来自前端的简单签名请求作出响应
（例如，通过本地主机网络）。</p>
<!--
With this partitioned approach, an attacker now has to trick the application
server into doing something rather arbitrary, which may be harder than getting
it to read a file.
-->
<p>采用这种划分的方法，攻击者现在必须欺骗应用服务器来做一些其他操作，
而这些操作可能要比读取一个文件要复杂很多。</p>
<!--
## Types of Secret {#secret-types}

When creating a Secret, you can specify its type using the `type` field of
the [Secret](/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/)
resource, or certain equivalent `kubectl` command line flags (if available).
The Secret type is used to facilitate programmatic handling of the Secret data.

Kubernetes provides several builtin types for some common usage scenarios.
These types vary in terms of the validations performed and the constraints
Kubernetes imposes on them.
-->
<h2 id="secret-types">Secret 的类型 </h2>
<p>创建 Secret 时，你可以使用 <a href="/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/">Secret</a>
资源的 <code>type</code> 字段，或者与其等价的 <code>kubectl</code> 命令行参数（如果有的话）为其设置类型。
Secret 类型有助于对 Secret 数据进行编程处理。</p>
<p>Kubernetes 提供若干种内置的类型，用于一些常见的使用场景。
针对这些类型，Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同。</p>
<!--
| Builtin Type | Usage |
|--------------|-------|
| `Opaque`     |  arbitrary user-defined data |
| `kubernetes.io/service-account-token` | service account token |
| `kubernetes.io/dockercfg` | serialized `~/.dockercfg` file |
| `kubernetes.io/dockerconfigjson` | serialized `~/.docker/config.json` file |
| `kubernetes.io/basic-auth` | credentials for basic authentication |
| `kubernetes.io/ssh-auth` | credentials for SSH authentication |
| `kubernetes.io/tls` | data for a TLS client or server |
| `bootstrap.kubernetes.io/token` | bootstrap token data |
-->
<table>
<thead>
<tr>
<th>内置类型</th>
<th>用法</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Opaque</code></td>
<td>用户定义的任意数据</td>
</tr>
<tr>
<td><code>kubernetes.io/service-account-token</code></td>
<td>服务账号令牌</td>
</tr>
<tr>
<td><code>kubernetes.io/dockercfg</code></td>
<td><code>~/.dockercfg</code> 文件的序列化形式</td>
</tr>
<tr>
<td><code>kubernetes.io/dockerconfigjson</code></td>
<td><code>~/.docker/config.json</code> 文件的序列化形式</td>
</tr>
<tr>
<td><code>kubernetes.io/basic-auth</code></td>
<td>用于基本身份认证的凭据</td>
</tr>
<tr>
<td><code>kubernetes.io/ssh-auth</code></td>
<td>用于 SSH 身份认证的凭据</td>
</tr>
<tr>
<td><code>kubernetes.io/tls</code></td>
<td>用于 TLS 客户端或者服务器端的数据</td>
</tr>
<tr>
<td><code>bootstrap.kubernetes.io/token</code></td>
<td>启动引导令牌数据</td>
</tr>
</tbody>
</table>
<!--
You can define and use your own Secret type by assigning a non-empty string as the
`type` value for a Secret object. An empty string is treated as an `Opaque` type.
-->
<p>通过为 Secret 对象的 <code>type</code> 字段设置一个非空的字符串值，你也可以定义并使用自己
Secret 类型。如果 <code>type</code> 值为空字符串，则被视为 <code>Opaque</code> 类型。</p>
<!--
Kubernetes doesn't impose any constraints on the type name. However, if you
are using one of the builtin types, you must meet all the requirements defined
for that type.
-->
<p>Kubernetes 并不对类型的名称作任何限制。不过，如果你要使用内置类型之一，
则你必须满足为该类型所定义的所有要求。</p>
<!--
If you are defining a type of secret that's for public use, follow the convention
and structure the secret type to have your domain name before the name, separated
by a `/`. For example: `cloud-hosting.example.net/cloud-api-credentials`.
-->
<p>如果你要定义一种公开使用的 Secret 类型，请遵守 Secret 类型的约定和结构，
在类型名签名添加域名，并用 <code>/</code> 隔开。
例如：<code>cloud-hosting.example.net/cloud-api-credentials</code>。</p>
<!--
### Opaque secrets

`Opaque` is the default Secret type if omitted from a Secret configuration file.
When you create a Secret using `kubectl`, you will use the `generic`
subcommand to indicate an `Opaque` Secret type. For example, the following
command creates an empty Secret of type `Opaque`.
-->
<h3 id="opaque-secret">Opaque Secret</h3>
<p>当 Secret 配置文件中未作显式设定时，默认的 Secret 类型是 <code>Opaque</code>。
当你使用 <code>kubectl</code> 来创建一个 Secret 时，你会使用 <code>generic</code> 子命令来标明
要创建的是一个 <code>Opaque</code> 类型 Secret。
例如，下面的命令会创建一个空的 <code>Opaque</code> 类型 Secret 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic empty-secret
kubectl get secret empty-secret
</code></pre></div><!--
The output looks like:
-->
<p>输出类似于</p>
<pre><code>NAME           TYPE     DATA   AGE
empty-secret   Opaque   0      2m6s
</code></pre><!--
The `DATA` column shows the number of data items stored in the Secret.
In this case, `0` means we have created an empty Secret.
-->
<p><code>DATA</code> 列显示 Secret 中保存的数据条目个数。
在这个例子种，<code>0</code> 意味着我们刚刚创建了一个空的 Secret。</p>
<!--
###  Service account token Secrets

A `kubernetes.io/service-account-token` type of Secret is used to store a
token that identifies a
<a class='glossary-tooltip' title='为在 Pod 中运行的进程提供标识。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-service-account/' target='_blank' aria-label='service account'>service account</a>.
When using this Secret type, you need to ensure that the
`kubernetes.io/service-account.name` annotation is set to an existing
service account name. A Kubernetes
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> fills in some
other fields such as the `kubernetes.io/service-account.uid` annotation and the
`token` key in the `data` field, which is  set to contain an authentication
token.

The following example configuration declares a service account token Secret:
-->
<h3 id="service-account-token-secrets">服务账号令牌 Secret </h3>
<p>类型为 <code>kubernetes.io/service-account-token</code> 的 Secret 用来存放标识某
<a class='glossary-tooltip' title='为在 Pod 中运行的进程提供标识。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-service-account/' target='_blank' aria-label='服务账号'>服务账号</a>的令牌。
使用这种 Secret 类型时，你需要确保对象的注解 <code>kubernetes.io/service-account-name</code>
被设置为某个已有的服务账号名称。某个 Kubernetes
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>会填写 Secret
的其它字段，例如 <code>kubernetes.io/service-account.uid</code> 注解以及 <code>data</code> 字段中的
<code>token</code> 键值，使之包含实际的令牌内容。</p>
<p>下面的配置实例声明了一个服务账号令牌 Secret：</p>
<!--
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-sa-sample
  annotations:
    kubernetes.io/service-account.name: "sa-name"
type: kubernetes.io/service-account-token
data:
  # You can include additional key value pairs as you do with Opaque Secrets
  extra: YmFyCg==
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-sa-sample<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/service-account.name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;sa-name&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>kubernetes.io/service-account-token<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 你可以像 Opaque Secret 一样在这里添加额外的键/值偶对</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">extra</span>:<span style="color:#bbb"> </span>YmFyCg==<span style="color:#bbb">
</span></code></pre></div><!--
When creating a `Pod`, Kubernetes automatically creates a service account
Secret and automatically modifies your Pod to use this Secret. The service account
token Secret contains credentials for accessing the API.

The automatic creation and use of API credentials can be disabled or
overridden if desired. However, if all you need to do is securely access the
API server, this is the recommended workflow.
-->
<p>Kubernetes 在创建 Pod 时会自动创建一个服务账号 Secret 并自动修改你的 Pod
以使用该 Secret。该服务账号令牌 Secret 中包含了访问 Kubernetes API
所需要的凭据。</p>
<p>如果需要，可以禁止或者重载这种自动创建并使用 API 凭据的操作。
不过，如果你仅仅是希望能够安全地访问 API 服务器，这是建议的工作方式。</p>
<!--
See the [ServiceAccount](/docs/tasks/configure-pod-container/configure-service-account/)
documentation for more information on how service accounts work.
You can also check the `automountServiceAccountToken` field and the
`serviceAccountName` field of the
[`Pod`](/docs/reference/generated/kubernetes-api/v1.23/#secret-v1-core)
for information on referencing service account from Pods.
-->
<p>参考 <a href="/zh/docs/tasks/configure-pod-container/configure-service-account/">ServiceAccount</a>
文档了解服务账号的工作原理。你也可以查看
<a href="/docs/reference/generated/kubernetes-api/v1.23/#pod-v1-core"><code>Pod</code></a>
资源中的 <code>automountServiceAccountToken</code> 和 <code>serviceAccountName</code> 字段文档，
进一步了解从 Pod 中引用服务账号。</p>
<!--
### Docker config Secrets

You can use one of the following `type` values to create a Secret to
store the credentials for accessing a Docker registry for images.
-->
<h3 id="docker-config-secrets">Docker 配置 Secret </h3>
<p>你可以使用下面两种 <code>type</code> 值之一来创建 Secret，用以存放访问 Docker 仓库
来下载镜像的凭据。</p>
<ul>
<li><code>kubernetes.io/dockercfg</code></li>
<li><code>kubernetes.io/dockerconfigjson</code></li>
</ul>
<!--
The `kubernetes.io/dockercfg` type is reserved to store a serialized
`~/.dockercfg` which is the legacy format for configuring Docker command line.
When using this Secret type, you have to ensure the Secret `data` field
contains a `.dockercfg` key whose value is content of a `~/.dockercfg` file
encoded in the base64 format.
-->
<p><code>kubernetes.io/dockercfg</code> 是一种保留类型，用来存放 <code>~/.dockercfg</code> 文件的序列化形式。
该文件是配置 Docker 命令行的一种老旧形式。使用此 Secret 类型时，你需要确保
Secret 的 <code>data</code> 字段中包含名为 <code>.dockercfg</code> 的主键，其对应键值是用 base64
编码的某 <code>~/.dockercfg</code> 文件的内容。</p>
<!--
The `kubernetes/dockerconfigjson` type is designed for storing a serialized
JSON that follows the same format rules as the `~/.docker/config.json` file
which is a new format for `~/.dockercfg`.
When using this Secret type, the `data` field of the Secret object must
contain a `.dockerconfigjson` key, in which the content for the
`~/.docker/config.json` file is provided as a base64 encoded string.

Below is an example for a `kubernetes.io/dockercfg` type of Secret:
-->
<p>类型 <code>kubernetes.io/dockerconfigjson</code> 被设计用来保存 JSON 数据的序列化形式，
该 JSON 也遵从 <code>~/.docker/config.json</code> 文件的格式规则，而后者是 <code>~/.dockercfg</code>
的新版本格式。使用此 Secret 类型时，Secret 对象的 <code>data</code> 字段必须包含
<code>.dockerconfigjson</code> 键，其键值为 base64 编码的字符串包含 <code>~/.docker/config.json</code>
文件的内容。</p>
<p>下面是一个 <code>kubernetes.io/dockercfg</code> 类型 Secret 的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-dockercfg<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>kubernetes.io/dockercfg<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">.dockercfg</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span><span style="color:#b44">&#34;&lt;base64 encoded ~/.dockercfg file&gt;&#34;</span><span style="color:#bbb">
</span></code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you do not want to perform the base64 encoding, you can choose to use the
`stringData` field instead.
-->
<p>如果你不希望执行 base64 编码转换，可以使用 <code>stringData</code> 字段代替。
</div>
<!--
When you create these types of Secrets using a manifest, the API
server checks whether the expected key does exists in the `data` field, and
it verifies if the value provided can be parsed as a valid JSON. The API
server doesn't validate if the JSON actually is a Docker config file.

When you do not have a Docker config file, or you want to use `kubectl`
to create a Docker registry Secret, you can do:
-->
<p>当你使用清单文件来创建这两类 Secret 时，API 服务器会检查 <code>data</code> 字段中是否
存在所期望的主键，并且验证其中所提供的键值是否是合法的 JSON 数据。
不过，API 服务器不会检查 JSON 数据本身是否是一个合法的 Docker 配置文件内容。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret docker-registry secret-tiger-docker <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --docker-email<span style="color:#666">=</span>tiger@acme.example <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --docker-username<span style="color:#666">=</span>tiger <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --docker-password<span style="color:#666">=</span>pass113 <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --docker-server<span style="color:#666">=</span>my-registry.example:5000
</code></pre></div><!--
This command creates a Secret of type `kubernetes.io/dockerconfigjson`.
If you dump the `.data.dockerconfigjson` field from the new Secret and then
decode it from base64:
-->
<p>上面的命令创建一个类型为 <code>kubernetes.io/dockerconfigjson</code> 的 Secret。
如果你对 <code>.data.dockerconfigjson</code> 内容进行转储并执行 base64 解码：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;auths&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;my-registry.example:5000&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;username&#34;</span>: <span style="color:#b44">&#34;tiger&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;password&#34;</span>: <span style="color:#b44">&#34;pass113&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;email&#34;</span>: <span style="color:#b44">&#34;tiger@acme.com&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;auth&#34;</span>: <span style="color:#b44">&#34;dGlnZXI6cGFzczExMw==&#34;</span>
    }
  }
}
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The `auth` value there is base64 encoded; it is obscured but not secret.
Anyone who can read that Secret can learn the registry access bearer token.
-->
<p><code>auths</code> 值是 base64 编码的，其内容被屏蔽但未被加密。
任何能够读取该 Secret 的人都可以了解镜像库的访问令牌。
</div>
<!--
### Basic authentication Secret

The `kubernetes.io/basic-auth` type is provided for storing credentials needed
for basic authentication. When using this Secret type, the `data` field of the
Secret must contain the following two keys:

- `username`: the user name for authentication;
- `password`: the password or token for authentication.
-->
<h3 id="basic-authentication-secret">基本身份认证 Secret </h3>
<p><code>kubernetes.io/basic-auth</code> 类型用来存放用于基本身份认证所需的凭据信息。
使用这种 Secret 类型时，Secret 的 <code>data</code> 字段必须包含以下两个键：</p>
<ul>
<li><code>username</code>: 用于身份认证的用户名；</li>
<li><code>password</code>: 用于身份认证的密码或令牌。</li>
</ul>
<!--
Both values for the above two keys are base64 encoded strings. You can, of
course, provide the clear text content using the `stringData` for Secret
creation.

The following YAML is an example config for a basic authentication Secret:
-->
<p>以上两个键的键值都是 base64 编码的字符串。
当然你也可以在创建 Secret 时使用 <code>stringData</code> 字段来提供明文形式的内容。
下面的 YAML 是基本身份认证 Secret 的一个示例清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-basic-auth<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>kubernetes.io/basic-auth<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">stringData</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>admin     <span style="color:#bbb"> </span><span style="color:#080;font-style:italic">#  kubernetes.io/basic-auth 类型的必需字段</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>t0p-Secret<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># kubernetes.io/basic-auth 类型的必需字段</span><span style="color:#bbb">
</span></code></pre></div><!--
The basic authentication Secret type is provided only for convenience.
You can create an `Opaque` for credentials used for basic authentication.
However, using the defined and public Secret type (`kubernetes.io/basic-auth`) helps other
people to understand the purpose of your Secret, and sets a convention for what key names
to expect.
The Kubernetes API verifies that the required keys are set for a Secret
of this type.
-->
<p>提供基本身份认证类型的 Secret 仅仅是出于方便性考虑。
你也可以使用 <code>Opaque</code> 类型来保存用于基本身份认证的凭据。
不过，使用预定义的、公开的 Secret 类型（<code>kubernetes.io/basic-auth</code>）
有助于帮助其他用户理解 Secret 的目的，并且对其中存在的主键形成一种约定。
API 服务器会检查 Secret 配置中是否提供了所需要的主键。</p>
<!--
### SSH authentication secrets

The builtin type `kubernetes.io/ssh-auth` is provided for storing data used in
SSH authentication. When using this Secret type, you will have to specify a
`ssh-privatekey` key-value pair in the `data` (or `stringData`) field.
as the SSH credential to use.

The following manifest is an example of a Secret used for SSH public/private
key authentication:
-->
<h3 id="ssh-authentication-secrets">SSH 身份认证 Secret</h3>
<p>Kubernetes 所提供的内置类型 <code>kubernetes.io/ssh-auth</code> 用来存放 SSH 身份认证中
所需要的凭据。使用这种 Secret 类型时，你就必须在其 <code>data</code> （或 <code>stringData</code>）
字段中提供一个 <code>ssh-privatekey</code> 键值对，作为要使用的 SSH 凭据。</p>
<p>下面的清单是一个 SSH 公钥/私钥身份认证的 Secret 示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-ssh-auth<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>kubernetes.io/ssh-auth<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 此例中的实际数据被截断</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ssh-privatekey</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">     </span><span style="color:#bbb">     </span>MIIEpQIBAAKCAQEAulqb/Y ...<span style="color:#bbb">
</span></code></pre></div><!--
The SSH authentication Secret type is provided only for user's convenience.
You could instead create an `Opaque` type Secret for credentials used for SSH authentication.
However, using the defined and public Secret type (`kubernetes.io/ssh-auth`) helps other
people to understand the purpose of your Secret, and sets a convention for what key names
to expect.
and the API server does verify if the required keys are provided in a Secret
configuration.
-->
<p>提供 SSH 身份认证类型的 Secret 仅仅是出于用户方便性考虑。
你也可以使用 <code>Opaque</code> 类型来保存用于 SSH 身份认证的凭据。
不过，使用预定义的、公开的 Secret 类型（<code>kubernetes.io/ssh-auth</code>）
有助于其他人理解你的 Secret 的用途，也可以就其中包含的主键名形成约定。
API 服务器确实会检查 Secret 配置中是否提供了所需要的主键。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
SSH private keys do not establish trusted communication between an SSH client and
host server on their own. A secondary means of establishing trust is needed to
mitigate "man in the middle" attacks, such as a `known_hosts` file added to a
ConfigMap.
-->
<p>SSH 私钥自身无法建立 SSH 客户端与服务器端之间的可信连接。
需要其它方式来建立这种信任关系，以缓解“中间人（Man In The Middle）”
攻击，例如向 ConfigMap 中添加一个 <code>known_hosts</code> 文件。
</div>

<!--
### TLS secrets

Kubernetes provides a builtin Secret type `kubernetes.io/tls` for storing
a certificate and its associated key that are typically used for TLS.

One common use for TLS secrets is to configure encryption in transit for
an [Ingress](/docs/concepts/services-networking/ingress/), but you can also use it
with other resources or directly in your workload.
When using this type of Secret, the `tls.key` and the `tls.crt` key must be provided
in the `data` (or `stringData`) field of the Secret configuration, although the API
server doesn't actually validate the values for each key.

The following YAML contains an example config for a TLS Secret:
-->
<h3 id="tls-secret">TLS Secret</h3>
<p>Kubernetes 提供一种内置的 <code>kubernetes.io/tls</code> Secret 类型，用来存放 TLS
场合通常要使用的证书及其相关密钥。
TLS Secret 的一种典型用法是为 <a href="/zh/docs/concepts/services-networking/ingress/">Ingress</a>
资源配置传输过程中的数据加密，不过也可以用于其他资源或者直接在负载中使用。
当使用此类型的 Secret 时，Secret 配置中的 <code>data</code> （或 <code>stringData</code>）字段必须包含
<code>tls.key</code> 和 <code>tls.crt</code> 主键，尽管 API 服务器实际上并不会对每个键的取值作进一步的合法性检查。</p>
<p>下面的 YAML 包含一个 TLS Secret 的配置示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-tls<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>kubernetes.io/tls<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 此例中的数据被截断</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tls.crt</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>MIIC2DCCAcCgAwIBAgIBATANBgkqh ...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tls.key</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...<span style="color:#bbb">
</span></code></pre></div><!--
The TLS Secret type is provided for user's convenience. You can create an `Opaque`
for credentials used for TLS server and/or client. However, using the builtin Secret
type helps ensure the consistency of Secret format in your project; the API server
does verify if the required keys are provided in a Secret configuration.

When creating a TLS Secret using `kubectl`, you can use the `tls` subcommand
as shown in the following example:
-->
<p>提供 TLS 类型的 Secret 仅仅是出于用户方便性考虑。
你也可以使用 <code>Opaque</code> 类型来保存用于 TLS 服务器与/或客户端的凭据。
不过，使用内置的 Secret 类型的有助于对凭据格式进行归一化处理，并且
API 服务器确实会检查 Secret 配置中是否提供了所需要的主键。</p>
<p>当使用 <code>kubectl</code> 来创建 TLS Secret 时，你可以像下面的例子一样使用 <code>tls</code>
子命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret tls my-tls-secret <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --cert<span style="color:#666">=</span>path/to/cert/file <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --key<span style="color:#666">=</span>path/to/key/file
</code></pre></div><!--
The public/private key pair must exist beforehand. The public key certificate
for `--cert` must be DER format as per
[Section 5.1 of RFC 7468](https://datatracker.ietf.org/doc/html/rfc7468#section-5.1),
and must match the given private key for `--key` (PKCS #8 in DER format;
[Section 11 of RFC 7468](https://datatracker.ietf.org/doc/html/rfc7468#section-11)).
-->
<p>这里的公钥/私钥对都必须事先已存在。用于 <code>--cert</code> 的公钥证书必须是
<a href="https://datatracker.ietf.org/doc/html/rfc7468#section-5.1">RFC 7468 中 5.1 节</a>
中所规定的 DER 格式，且与 <code>--key</code> 所给定的私钥匹配。
私钥必须是 DER 格式的 PKCS #8
（参见 <a href="https://datatracker.ietf.org/doc/html/rfc7468#section-11">RFC 7468 第 11节</a>）。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
A kubernetes.io/tls Secret stores the Base64-encoded DER data for keys and
certificates. If you're familiar with PEM format for private keys and for certificates,
the base64 data are the same as that format except that you omit
the initial and the last lines that are used in PEM.
-->
<p>类型为 <code>kubernetes.io/tls</code> 的 Secret 中包含密钥和证书的 DER 数据，以 Base64 格式编码。
如果你熟悉私钥和证书的 PEM 格式，base64 与该格式相同，只是你需要略过 PEM
数据中所包含的第一行和最后一行。</p>
<!--
For example, for a certificate, you do **not** include `--------BEGIN CERTIFICATE-----`
and `-------END CERTIFICATE----`.
-->
<p>例如，对于证书而言，你 <strong>不要</strong> 包含 <code>--------BEGIN CERTIFICATE-----</code>
和 <code>-------END CERTIFICATE----</code> 这两行。</p>

</div>
<!--
### Bootstrap token Secrets

A bootstrap token Secret can be created by explicitly specifying the Secret
`type` to `bootstrap.kubernetes.io/token`. This type of Secret is designed for
tokens used during the node bootstrap process. It stores tokens used to sign
well known ConfigMaps.
-->
<h3 id="bootstrap-token-secrets">启动引导令牌 Secret </h3>
<p>通过将 Secret 的 <code>type</code> 设置为 <code>bootstrap.kubernetes.io/token</code> 可以创建
启动引导令牌类型的 Secret。这种类型的 Secret 被设计用来支持节点的启动引导过程。
其中包含用来为周知的 ConfigMap 签名的令牌。</p>
<!--
A bootstrap token Secret is usually created in the `kube-system` namespace and
named in the form `bootstrap-token-<token-id>` where `<token-id>` is a 6 character
string of the token ID.

As a Kubernetes manifest, a bootstrap token Secret might look like the
following:
-->
<p>启动引导令牌 Secret 通常创建于 <code>kube-system</code> 名字空间内，并以
<code>bootstrap-token-&lt;令牌 ID&gt;</code> 的形式命名；其中 <code>&lt;令牌 ID&gt;</code> 是一个由 6 个字符组成
的字符串，用作令牌的标识。</p>
<p>以 Kubernetes 清单文件的形式，某启动引导令牌 Secret 可能看起来像下面这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>bootstrap-token-5emitj<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>bootstrap.kubernetes.io/token<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">auth-extra-groups</span>:<span style="color:#bbb"> </span>c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">expiration</span>:<span style="color:#bbb"> </span>MjAyMC0wOS0xM1QwNDozOToxMFo=<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">token-id</span>:<span style="color:#bbb"> </span>NWVtaXRq<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">token-secret</span>:<span style="color:#bbb"> </span>a3E0Z2lodnN6emduMXAwcg==<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">usage-bootstrap-authentication</span>:<span style="color:#bbb"> </span>dHJ1ZQ==<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">usage-bootstrap-signing</span>:<span style="color:#bbb"> </span>dHJ1ZQ==<span style="color:#bbb">
</span></code></pre></div><!--
A bootstrap type has the following keys specified under `data`:

- `token-id`: A random 6 character string as the token identifier. Required.
- `token-secret`: A random 16 character string as the actual token secret. Required.
- `description1`: A human-readable string that describes what the token is
  used for. Optional.
- `expiration`: An absolute UTC time using RFC3339 specifying when the token
  should be expired. Optional.
- `usage-bootstrap-<usage>`: A boolean flag indicating additional usage for
  the bootstrap token.
- `auth-extra-groups`: A comma-separated list of group names that will be
  authenticated as in addition to the `system:bootstrappers` group.
-->
<p>启动引导令牌类型的 Secret 会在 <code>data</code> 字段中包含如下主键：</p>
<ul>
<li><code>token-id</code>：由 6 个随机字符组成的字符串，作为令牌的标识符。必需。</li>
<li><code>token-secret</code>：由 16 个随机字符组成的字符串，包含实际的令牌机密。必需。</li>
<li><code>description</code>：供用户阅读的字符串，描述令牌的用途。可选。</li>
<li><code>expiration</code>：一个使用 RFC3339 来编码的 UTC 绝对时间，给出令牌要过期的时间。可选。</li>
<li><code>usage-bootstrap-&lt;usage&gt;</code>：布尔类型的标志，用来标明启动引导令牌的其他用途。</li>
<li><code>auth-extra-groups</code>：用逗号分隔的组名列表，身份认证时除被认证为
<code>system:bootstrappers</code> 组之外，还会被添加到所列的用户组中。</li>
</ul>
<!--
The above YAML may look confusing because the values are all in base64 encoded
strings. In fact, you can create an identical Secret using the following YAML:

```yaml
apiVersion: v1
kind: Secret
metadata:
  # Note how the Secret is named
  name: bootstrap-token-5emitj
  # A bootstrap token Secret usually resides in the kube-system namespace
  namespace: kube-system
type: bootstrap.kubernetes.io/token
stringData:
  auth-extra-groups: "system:bootstrappers:kubeadm:default-node-token"
  expiration: "2020-09-13T04:39:10Z"
  # This token ID is used in the name
  token-id: "5emitj"
  token-secret: "kq4gihvszzgn1p0r"
  # This token can be used for authentication
  usage-bootstrap-authentication: "true"
  # and it can be used for signing
  usage-bootstrap-signing: "true"
```
-->
<p>上面的 YAML 文件可能看起来令人费解，因为其中的数值均为 base64 编码的字符串。
实际上，你完全可以使用下面的 YAML 来创建一个一模一样的 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 注意 Secret 的命名方式</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>bootstrap-token-5emitj<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 启动引导令牌 Secret 通常位于 kube-system 名字空间</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>bootstrap.kubernetes.io/token<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">stringData</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">auth-extra-groups</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;system:bootstrappers:kubeadm:default-node-token&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">expiration</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2020-09-13T04:39:10Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 此令牌 ID 被用于生成 Secret 名称</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">token-id</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5emitj&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">token-secret</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kq4gihvszzgn1p0r&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 此令牌还可用于 authentication （身份认证）</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">usage-bootstrap-authentication</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 且可用于 signing （证书签名）</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">usage-bootstrap-signing</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
## Immutable Secrets {#secret-immutable}
-->
<h2 id="secret-immutable">不可更改的 Secret</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>


<!--
Kubernetes lets you mark specific Secrets (and ConfigMaps) as _immutable_.
Preventing changes to the data of an existing Secret has the following benefits:

- protects you from accidental (or unwanted) updates that could cause applications outages
- (for clusters that extensively use Secrets - at least tens of thousands of unique Secret
  to Pod mounts), switching to immutable Secrets improves the performance of your cluster
  by significantly reducing load on kube-apiserver. The kubelet does not need to maintain
  a [watch] on any Secrets that are marked as immutable.
-->
<p>Kubernetes 允许你将特定的 Secret（和 ConfigMap）标记为 <strong>不可更改（Immutable）</strong>。
禁止更改现有 Secret 的数据有下列好处：</p>
<ul>
<li>防止意外（或非预期的）更新导致应用程序中断</li>
<li>（对于大量使用 Secret 的集群而言，至少数万个不同的 Secret 供 Pod 挂载），
通过将 Secret 标记为不可变，可以极大降低 kube-apiserver 的负载，提升集群性能。
kubelet 不需要监视那些被标记为不可更改的 Secret。</li>
</ul>
<!--
### Marking a Secret as immutable {#secret-immutable-create}

You can create an immutable Secret by setting the `immutable` field to `true`. For example,
-->
<h3 id="secret-immutable-create">将 Secret 标记为不可更改  </h3>
<p>你可以通过将 Secret 的 <code>immutable</code> 字段设置为 <code>true</code> 创建不可更改的 Secret。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">immutable</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div><!--
You can also update any existing mutable Secret to make it immutable.
-->
<p>你也可以更改现有的 Secret，令其不可更改。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Once a Secret or ConfigMap is marked as immutable, it is _not_ possible to revert this change
nor to mutate the contents of the `data` field. You can only delete and recreate the Secret.
Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreate
these pods.
-->
<p>一旦一个 Secret 或 ConfigMap 被标记为不可更改，撤销此操作或者更改 <code>data</code>
字段的内容都是 <strong>不</strong> 可能的。
只能删除并重新创建这个 Secret。现有的 Pod 将维持对已删除 Secret 的挂载点 --
建议重新创建这些 Pod。
</div>
<!--
## Information security for Secrets

Although ConfigMap and Secret work similarly, Kubernetes applies some additional
protection for Secret objects.
-->
<h2 id="secret-的信息安全问题">Secret 的信息安全问题</h2>
<p>尽管 ConfigMap 和 Secret 的工作方式类似，但 Kubernetes 对 Secret 有一些额外的保护。</p>
<!--
Secrets often hold values that span a spectrum of importance, many of which can
cause escalations within Kubernetes (e.g. service account tokens) and to
external systems. Even if an individual app can reason about the power of the
Secrets it expects to interact with, other apps within the same namespace can
render those assumptions invalid.
-->
<p>Secret 通常保存重要性各异的数值，其中很多都可能会导致 Kubernetes 中
（例如，服务账号令牌）或对外部系统的特权提升。
即使某些个别应用能够推导它期望使用的 Secret 的能力，
同一名字空间中的其他应用可能会让这种假定不成立。</p>
<!--
A Secret is only sent to a node if a Pod on that node requires it.
For mounting secrets into Pods, the kubelet stores a copy of the data into a `tmpfs`
so that the confidential data is not written to durable storage.
Once the Pod that depends on the Secret is deleted, the kubelet deletes its local copy
of the confidential data from the Secret.
-->
<p>只有当某个节点上的 Pod 需要某 Secret 时，对应的 Secret 才会被发送到该节点上。
如果将 Secret 挂载到 Pod 中，kubelet 会将数据的副本保存在在 <code>tmpfs</code> 中，
这样机密的数据不会被写入到持久性存储中。
一旦依赖于该 Secret 的 Pod 被删除，kubelet 会删除来自于该 Secret 的机密数据的本地副本。</p>
<!--
There may be several containers in a Pod. By default, containers you define
only have access to the default ServiceAccount and its related Secret.
You must explicitly define environment variables or map a volume into a
container in order to provide access to any other Secret.
-->
<p>同一个 Pod 中可能包含多个容器。默认情况下，你所定义的容器只能访问默认 ServiceAccount
及其相关 Secret。你必须显式地定义环境变量或者将卷映射到容器中，才能为容器提供对其他
Secret 的访问。</p>
<!--
There may be Secrets for several Pods on the same node. However, only the
Secrets that a Pod requests are potentially visible within its containers.
Therefore, one Pod does not have access to the Secrets of another Pod.
-->
<p>针对同一节点上的多个 Pod 可能有多个 Secret。不过，只有某个 Pod 所请求的 Secret
才有可能对 Pod 中的容器可见。因此，一个 Pod 不会获得访问其他 Pod 的 Secret 的权限。</p>
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
Any privileged containers on a node are liable to have access to all Secrets used
on that node.
-->
<p>节点上的所有特权容器都可能访问到该节点上使用的所有 Secret。
</div>


<!--
### Security recommendations for developers

- Applications still need to protect the value of confidential information after reading it
  from an environment variable or volume. For example, your application must avoid logging
  the secret data in the clear or transmitting it to an untrusted party.
- If you are defining multiple containers in a Pod, and only one of those
  containers needs access to a Secret, define the volume mount or environment
  variable configuration so that the other containers do not have access to that
  Secret.
-->
<h3 id="针对开发人员的安全性建议">针对开发人员的安全性建议</h3>
<ul>
<li>应用在从环境变量或卷中读取了机密信息内容之后仍要对其进行保护。例如，
你的应用应该避免用明文的方式将 Secret 数据写入日志，或者将其传递给不可信的第三方。</li>
<li>如果你在一个 Pod 中定义了多个容器，而只有一个容器需要访问某 Secret，
定义卷挂载或环境变量配置时，应确保其他容器无法访问该 Secret。</li>
</ul>
<!--
- If you configure a Secret through a <a class='glossary-tooltip' title='一个或多个 Kubernetes API 对象的序列化规范。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-manifest' target='_blank' aria-label='manifest'>manifest</a>,
  with the secret data encoded as base64, sharing this file or checking it in to a
  source repository means the secret is available to everyone who can read the manifest.
  Base64 encoding is _not_ an encryption method, it provides no additional confidentiality
  over plain text.
-->
<ul>
<li>如果你通过<a class='glossary-tooltip' title='一个或多个 Kubernetes API 对象的序列化规范。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-manifest' target='_blank' aria-label='清单'>清单</a>来配置某 Secret，
Secret 数据以 Base64 的形式编码，将此文件共享，或者将其检入到某源码仓库，
都意味着 Secret 对于任何可以读取清单的人都是可见的。
Base64 编码 <strong>不是</strong> 一种加密方法，与明文相比没有任何安全性提升。</li>
</ul>
<!--
- When deploying applications that interact with the Secret API, you should
  limit access using
  [authorization policies](/docs/reference/access-authn-authz/authorization/) such as
  [RBAC]( /docs/reference/access-authn-authz/rbac/).
-->
<ul>
<li>部署与 Secret API 交互的应用时，你应该使用 <a href="/zh/docs/reference/access-authn-authz/rbac/">RBAC</a>
这类<a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权策略</a>来限制访问。</li>
</ul>
<!--
- In the Kubernetes API, `watch` and `list` requests for Secrets within a namespace
  are extremely powerful capabilities. Avoid granting this access where feasible, since
  listing Secrets allows the clients to inspect the values of every Secret in that
  namespace.
-->
<ul>
<li>在 Kubernetes API 中，名字空间内对 Secret 对象的 <code>watch</code> 和 <code>list</code> 请求是非常强大的能力。
在可能的时候应该避免授予这类访问权限，因为通过列举 Secret，
客户端能够查看对应名字空间内所有 Secret 的取值。</li>
</ul>
<!--
### Security recommendations for cluster administrators
-->
<h3 id="针对集群管理员的安全性建议">针对集群管理员的安全性建议</h3>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
A user who can create a Pod that uses a Secret can also see the value of that Secret. Even
if cluster policies do not allow a user to read the Secret directly, the same user could
have access to run a Pod that then exposes the Secret.
-->
<p>能够创建使用 Secret 的 Pod 的用户也可以查看该 Secret 的取值。
即使集群策略不允许某用户直接读取 Secret 对象，这一用户仍然可以通过运行一个
Pod 来访问 Secret 的内容。
</div>

<!--
- Reserve the ability to `watch` or `list` all secrets in a cluster (using the Kubernetes
  API), so that only the most privileged, system-level components can perform this action.
- When deploying applications that interact with the Secret API, you should
  limit access using
  [authorization policies](/docs/reference/access-authn-authz/authorization/) such as
  [RBAC]( /docs/reference/access-authn-authz/rbac/).
-->
<ul>
<li>保留（使用 Kubernetes API）对集群中所有 Secret 对象执行 <code>watch</code> 或 <code>list</code> 操作的能力，
这样只有特权级最高、系统级别的组件能够执行这类操作。</li>
<li>在部署需要通过 Secret API 交互的应用时，你应该通过使用
<a href="/zh/docs/reference/access-authn-authz/rbac/">RBAC</a>
这类<a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权策略</a>来限制访问。</li>
</ul>
<!--
- In the API server, objects (including Secrets) are persisted into
  <a class='glossary-tooltip' title='etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/' target='_blank' aria-label='etcd'>etcd</a>; therefore:
  - only allow cluster admistrators to access etcd (this includes read-only access);
  - enable [encryption at rest](/docs/tasks/administer-cluster/encrypt-data/)
    for Secret objects, so that the data of these Secrets are not stored in the clear
    into <a class='glossary-tooltip' title='etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/' target='_blank' aria-label='etcd'>etcd</a>;
  - consider wiping / shredding the durable storage used by etcd once it is
    no longer in use;
  - if there are multiple etcd instances, make sure that etcd is
    using SSL/TLS for communication between etcd peers.
-->
<ul>
<li>
<p>在 API 服务器上，对象（包括 Secret）会被持久化到 <a class='glossary-tooltip' title='etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/' target='_blank' aria-label='etcd'>etcd</a> 中；
因此：</p>
<ul>
<li>只应准许集群管理员访问 etcd（包括只读访问）；</li>
<li>为 Secret 对象启用<a href="/zh/docs/tasks/administer-cluster/encrypt-data/">静态加密</a>，
这样这些 Secret 的数据就不会以明文的形式保存到
<a class='glossary-tooltip' title='etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/' target='_blank' aria-label='etcd'>etcd</a> 中；</li>
<li>当 etcd 的持久化存储不再被使用时，请考虑彻底擦除存储介质；</li>
<li>如果存在多个 etcd 实例，请确保 etcd 使用 SSL/TLS 来完成其对等通信。</li>
</ul>
</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
- Learn how to [manage Secret using `kubectl`](/docs/tasks/configmap-secret/managing-secret-using-kubectl/)
- Learn how to [manage Secret using config file](/docs/tasks/configmap-secret/managing-secret-using-config-file/)
- Learn how to [manage Secret using kustomize](/docs/tasks/configmap-secret/managing-secret-using-kustomize/)
- Read the [API reference](/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/) for `Secret`
-->
<ul>
<li>学习如何<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kubectl/">使用 <code>kubectl</code> 管理 Secret</a></li>
<li>学习如何<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-config-file/">使用配置文件管理 Secret</a></li>
<li>学习如何<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kustomize/">使用 kustomize 管理 Secret</a></li>
<li>阅读 <a href="/zh/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/">API 参考</a>了解 <code>Secret</code></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-436057b96151ecb8a4a9a9f456b5d0fc">7.4 - 为 Pod 和容器管理资源</h1>
    
	<!--
title: Resource Management for Pods and Containers
content_type: concept
weight: 40
feature:
  title: Automatic binpacking
  description: >
    Automatically places containers based on their resource requirements and other constraints, while not sacrificing availability.
    Mix critical and best-effort workloads in order to drive up utilization and save even more resources.
-->
<!-- overview -->
<!--
When you specify a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>, you can optionally specify how
much of each resource a <a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='container'>container</a> needs.
The most common resources to specify are CPU and memory (RAM); there are others.

When you specify the resource _request_ for Containers in a Pod, the
<a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='kube-scheduler'>kube-scheduler</a> uses this
information to decide which node to place the Pod on. When you specify a resource _limit_
for a Container, the kubelet enforces those limits so that the running container is not
allowed to use more of that resource than the limit you set. The kubelet also reserves
at least the _request_ amount of that system resource specifically for that container
to use.
-->
<p>当你定义 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 时可以选择性地为每个
<a class='glossary-tooltip' title='容器是可移植、可执行的轻量级的镜像，镜像中包含软件及其相关依赖。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank' aria-label='容器'>容器</a>设定所需要的资源数量。
最常见的可设定资源是 CPU 和内存（RAM）大小；此外还有其他类型的资源。</p>
<p>当你为 Pod 中的 Container 指定了资源 <strong>请求</strong> 时，
<a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='kube-scheduler'>kube-scheduler</a>
就利用该信息决定将 Pod 调度到哪个节点上。
当你还为 Container 指定了资源 <strong>约束</strong> 时，kubelet 就可以确保运行的容器不会使用超出所设约束的资源。
kubelet 还会为容器预留所 <strong>请求</strong> 数量的系统资源，供其使用。</p>
<!-- body -->
<!--
## Requests and limits

If the node where a Pod is running has enough of a resource available, it's possible (and
allowed) for a container to use more resource than its `request` for that resource specifies.
However, a container is not allowed to use more than its resource `limit`.

For example, if you set a `memory` request of 256 MiB for a container, and that container is in
a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use
more RAM.
-->
<h2 id="requests-and-limits">请求和约束 </h2>
<p>如果 Pod 运行所在的节点具有足够的可用资源，容器可能（且可以）使用超出对应资源
<code>request</code> 属性所设置的资源量。不过，容器不可以使用超出其资源 <code>limit</code>
属性所设置的资源量。</p>
<p>例如，如果你将容器的 <code>memory</code> 的请求量设置为 256 MiB，而该容器所处的 Pod
被调度到一个具有 8 GiB 内存的节点上，并且该节点上没有其他 Pods
运行，那么该容器就可以尝试使用更多的内存。</p>
<!--
If you set a `memory` limit of 4GiB for that container, the kubelet (and
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>) enforce the limit.
The runtime prevents the container from using more than the configured resource limit. For example:
when a process in the container tries to consume more than the allowed amount of memory,
the system kernel terminates the process that attempted the allocation, with an out of memory
(OOM) error.

Limits can be implemented either reactively (the system intervenes once it sees a violation)
or by enforcement (the system prevents the container from ever exceeding the limit). Different
runtimes can have different ways to implement the same restrictions.
-->
<p>如果你将某容器的 <code>memory</code> 约束设置为 4 GiB，kubelet （和
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>）
就会确保该约束生效。
容器运行时会禁止容器使用超出所设置资源约束的资源。
例如：当容器中进程尝试使用超出所允许内存量的资源时，系统内核会将尝试申请内存的进程终止，
并引发内存不足（OOM）错误。</p>
<p>约束值可以以被动方式来实现（系统会在发现违例时进行干预），或者通过强制生效的方式实现
（系统会避免容器用量超出约束值）。不同的容器运行时采用不同方式来实现相同的限制。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If a container specifies its own memory limit, but does not specify a memory request, Kubernetes
automatically assigns a memory request that matches the limit. Similarly, if a container specifies its own
CPU limit, but does not specify a CPU request, Kubernetes automatically assigns a CPU request that matches
the limit.
-->
<p>如果某容器设置了自己的内存限制但未设置内存请求，Kubernetes
自动为其设置与内存限制相匹配的请求值。类似的，如果某 Container 设置了
CPU 限制值但未设置 CPU 请求值，则 Kubernetes 自动为其设置 CPU
请求并使之与 CPU 限制值匹配。
</div>
<!--
## Resource types

*CPU* and *memory* are each a *resource type*. A resource type has a base unit.
CPU represents compute processing and is specified in units of [Kubernetes CPUs](#meaning-of-cpu).
Memory is specified in units of bytes.
For Linux workloads, you can specify _huge page_ resources.
Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory
that are much larger than the default page size.

For example, on a system where the default page size is 4KiB, you could specify a limit,
`hugepages-2Mi: 80Mi`. If the container tries allocating over 40 2MiB huge pages (a
total of 80 MiB), that allocation fails.
-->
<h2 id="resource-types">资源类型 </h2>
<p><em>CPU</em> 和 <em>内存</em> 都是 <em>资源类型</em>。每种资源类型具有其基本单位。
CPU 表达的是计算处理能力，其单位是 <a href="#meaning-of-cpu">Kubernetes CPUs</a>。
内存的单位是字节。
对于 Linux 负载，则可以指定巨页（Huge Page）资源。
巨页是 Linux 特有的功能，节点内核在其中分配的内存块比默认页大小大得多。</p>
<p>例如，在默认页面大小为 4KiB 的系统上，你可以指定约束 <code>hugepages-2Mi: 80Mi</code>。
如果容器尝试分配 40 个 2MiB 大小的巨页（总共 80 MiB ），则分配请求会失败。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
You cannot overcommit `hugepages-*` resources.
This is different from the `memory` and `cpu` resources.
-->
<p>你不能过量使用 <code>hugepages- * </code>资源。
这与 <code>memory</code> 和 <code>cpu</code> 资源不同。
</div>
<!--
CPU and memory are collectively referred to as *compute resources*, or *resources*. Compute
resources are measurable quantities that can be requested, allocated, and
consumed. They are distinct from
[API resources](/docs/concepts/overview/kubernetes-api/). API resources, such as Pods and
[Services](/docs/concepts/services-networking/service/) are objects that can be read and modified
through the Kubernetes API server.
-->
<p>CPU 和内存统称为“计算资源”，或简称为“资源”。
计算资源的数量是可测量的，可以被请求、被分配、被消耗。
它们与 <a href="/zh/docs/concepts/overview/kubernetes-api/">API 资源</a> 不同。
API 资源（如 Pod 和 <a href="/zh/docs/concepts/services-networking/service/">Service</a>）是可通过
Kubernetes API 服务器读取和修改的对象。</p>
<!--
## Resource requests and limits of Pod and container

For each container, you can specify resource limits and requests,
including the following:
-->
<h2 id="pod-和-容器的资源请求和约束">Pod 和 容器的资源请求和约束</h2>
<p>针对每个容器，你都可以指定其资源约束和请求，包括如下选项：</p>
<ul>
<li><code>spec.containers[].resources.limits.cpu</code></li>
<li><code>spec.containers[].resources.limits.memory</code></li>
<li><code>spec.containers[].resources.limits.hugepages-&lt;size&gt;</code></li>
<li><code>spec.containers[].resources.requests.cpu</code></li>
<li><code>spec.containers[].resources.requests.memory</code></li>
<li><code>spec.containers[].resources.requests.hugepages-&lt;size&gt;</code></li>
</ul>
<!--
Although you can only specify requests and limits for individual containers,
it is also useful to think about the overall resource requests and limits for
a Pod.
A
For a particular resource, a *Pod resource request/limit* is the sum of the
resource requests/limits of that type for each container in the Pod.
-->
<p>尽管你只能逐个容器地指定请求和限制值，考虑 Pod 的总体资源请求和约束也是有用的。
对特定资源而言，Pod 的资源请求/约束值是 Pod 中各容器对该类型资源的请求/约束值的总和。</p>
<!--
## Resource units in Kubernetes

### CPU resource units {#meaning-of-cpu}

Limits and requests for CPU resources are measured in *cpu* units.
In Kubernetes, 1 CPU unit is equivalent to **1 physical CPU core**,
or **1 virtual core**,  depending on whether the node is a physical host
or a virtual machine running inside a physical machine.
-->
<h2 id="resource-units-in-kubernetes">Kubernetes 中的资源单位 </h2>
<h3 id="meaning-of-cpu">CPU 资源单位   </h3>
<p>CPU 资源的约束和请求以 “cpu” 为单位。
在 Kubernetes 中，一个 CPU 等于<strong>1 个物理 CPU 核</strong> 或者 <strong>一个虚拟核</strong>，
取决于节点是一台物理主机还是运行在某物理主机上的虚拟机。</p>
<!--
Fractional requests are allowed. When you define a container with
`spec.containers[].resources.requests.cpu` set to `0.5`, you are requesting half
as much CPU time compared to if you asked for `1.0` CPU.
For CPU resource units, the [quantity](/docs/reference/kubernetes-api/common-definitions/quantity/) expression `0.1` is equivalent to the
expression `100m`, which can be read as "one hundred millicpu". Some people say
"one hundred millicores", and this is understood to mean the same thing.
-->
<p>你也可以表达带小数 CPU 的请求。
当你定义一个容器，将其 <code>spec.containers[].resources.requests.cpu</code> 设置为 0.5 时，
你所请求的 CPU 是你请求 <code>1.0</code> CPU 时的一半。
对于 CPU 资源单位，<a href="/docs/reference/kubernetes-api/common-definitions/quantity/">数量</a>
表达式 <code>0.1</code> 等价于表达式 <code>100m</code>，可以看作 “100 millicpu”。
有些人说成是“一百毫核”，其实说的是同样的事情。</p>
<!--
CPU resource is always specified as an absolute amount of resource, never as a relative amount. For example,
`500m` CPU represents the roughly same amount of computing power whether that container
runs on a single-core, dual-core, or 48-core machine.
-->
<p>CPU 资源总是设置为资源的绝对数量而非相对数量值。
例如，无论容器运行在单核、双核或者 48-核的机器上，<code>500m</code> CPU 表示的是大约相同的计算能力。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Kubernetes doesn't allow you to specify CPU resources with a precision finer than
`1m`. Because of this, it's useful to specify CPU units less than `1.0` or `1000m` using
the milliCPU form; for example, `5m` rather than `0.005`.
-->
<p>Kubernetes 不允许设置精度小于 <code>1m</code> 的 CPU 资源。
因此，当 CPU 单位小于 <code>1</code> 或 <code>1000m</code> 时，使用毫核的形式是有用的；
例如 <code>5m</code> 而不是 <code>0.005</code>。
</div>
<!--
### Memory resource units {#meaning-of-memory}

Limits and requests for `memory` are measured in bytes. You can express memory as
a plain integer or as a fixed-point number using one of these
[quantity](/docs/reference/kubernetes-api/common-definitions/quantity/) suffixes:
E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:
-->
<h2 id="meaning-of-memory">内存资源单位     </h2>
<p><code>memory</code> 的约束和请求以字节为单位。
你可以使用普通的证书，或者带有以下
<a href="/docs/reference/kubernetes-api/common-definitions/quantity/">数量</a>后缀
的定点数字来表示内存：E、P、T、G、M、k。
你也可以使用对应的 2 的幂数：Ei、Pi、Ti、Gi、Mi、Ki。
例如，以下表达式所代表的是大致相同的值：</p>
<pre><code>128974848、129e6、129M、128974848000m、123Mi
</code></pre><!--
Pay attention to the case of the suffixes. If you request `400m` of memory, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (`400Mi`)
or 400 megabytes (`400M`).
-->
<p>请注意后缀的大小写。如果你请求 <code>400m</code> 内存，实际上请求的是 0.4 字节。
如果有人这样设定资源请求或限制，可能他的实际想法是申请 400 兆字节（<code>400Mi</code>）
或者 400M 字节。</p>
<!--
## Container resources example {#example-1}

The following Pod has two containers. Both containers are defined with a request for
0.25 CPU
and 64MiB (2<sup>26</sup> bytes) of memory. Each container has a limit of 0.5
CPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128
MiB of memory, and a limit of 1 CPU and 256MiB of memory.
-->
<h2 id="example-1">容器资源示例    </h2>
<p>以下 Pod 有两个容器。每个容器的请求为 0.25 CPU 和 64MiB（2<sup>26</sup> 字节）内存，
每个容器的资源约束为 0.5 CPU 和 128MiB 内存。
你可以认为该 Pod 的资源请求为 0.5 CPU 和 128 MiB 内存，资源限制为 1 CPU 和 256MiB 内存。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>images.my-company.example/app:v4<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;64Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;250m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;128Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log-aggregator<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>images.my-company.example/log-aggregator:v6<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;64Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;250m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;128Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
## How Pods with resource requests are scheduled

When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum capacity for each of the resource types: the
amount of CPU and memory it can provide for Pods. The scheduler ensures that,
for each resource type, the sum of the resource requests of the scheduled
containers is less than the capacity of the node.
Note that although actual memory
or CPU resource usage on nodes is very low, the scheduler still refuses to place
a Pod on a node if the capacity check fails. This protects against a resource
shortage on a node when resource usage later increases, for example, during a
daily peak in request rate.
-->
<h2 id="带资源请求的-pod-如何调度">带资源请求的 Pod 如何调度</h2>
<p>当你创建一个 Pod 时，Kubernetes 调度程序将为 Pod 选择一个节点。
每个节点对每种资源类型都有一个容量上限：可为 Pod 提供的 CPU 和内存量。
调度程序确保对于每种资源类型，所调度的容器的资源请求的总和小于节点的容量。
请注意，尽管节点上的实际内存或 CPU 资源使用量非常低，如果容量检查失败，
调度程序仍会拒绝在该节点上放置 Pod。
当稍后节点上资源用量增加，例如到达请求率的每日峰值区间时，节点上也不会出现资源不足的问题。</p>
<!--
## How Kubernetes applies resource requests and limits {#how-pods-with-resource-limits-are-run}

When the kubelet starts a container of a Pod, the kubelet passes that container's
requests and limits for memory and CPU to the container runtime.

On Linux, the container runtime typically configures
kernel <a class='glossary-tooltip' title='一组具有可选资源隔离、审计和限制的 Linux 进程。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cgroup' target='_blank' aria-label='cgroups'>cgroups</a> that apply and enforce the
limits you defined.
-->
<h2 id="how-pods-with-resource-limits-are-run">Kubernetes 应用资源请求与约束的方式</h2>
<p>当 kubelet 启动 Pod 中的容器时，它会将容器的 CPU 和内存请求与约束信息传递给容器运行时。</p>
<p>在 Linux 系统上，容器运行时通常会配置内核
<a class='glossary-tooltip' title='一组具有可选资源隔离、审计和限制的 Linux 进程。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cgroup' target='_blank' aria-label='CGroups'>CGroups</a>，负责应用并实施所定义的请求。</p>
<!--
- The CPU limit defines a hard ceiling on how much CPU time that the container can use.
  During each scheduling interval (time slice), the Linux kernel checks to see if this
  limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.
-->
<ul>
<li>CPU 约束值定义的是容器可使用的 CPU 时间的硬性上限。
在每个调度周期（时间片）期间，Linux 内核检查是否已经超出该约束值；
内核会在允许该 cgroup 恢复执行之前会等待。</li>
</ul>
<!--
- The CPU request typically defines a weighting. If several different containers (cgroups)
  want to run on a contended system, workloads with larger CPU requests are allocated more
  CPU time than workloads with small requests.
-->
<ul>
<li>CPU 请求值定义的是一个权重值。如果若干不同的容器（CGroups）需要在一个共享的系统上竞争运行，
CPU 请求值大的负载会获得比请求值小的负载更多的 CPU 时间。</li>
</ul>
<!--
- The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses
  cgroups v2, the container runtime might use the memory request as a hint to set
  `memory.min` and `memory.low`.
-->
<ul>
<li>内存请求值主要用于（Kubernetes）Pod 调度期间。在一个启用了 CGroup v2 的节点上，
容器运行时可能会使用内存请求值作为设置 <code>memory.min</code> 和 <code>memory.low</code> 的提示值。</li>
</ul>
<!--
- The memory limit defines a memory limit for that cgroup. If the container tries to
  allocate more memory than this limit, the Linux kernel out-of-memory subsystem activates
  and, typically, intervenes by stopping one of the processes in the container that tried
  to allocate memory. If that process is the container's PID 1, and the container is marked
  as restartable, Kubernetes restarts the container.
-->
<ul>
<li>内存约束值定义的是 CGroup 的内存约束。如果容器尝试分配的内存量超出约束值，
则 Linux 内核的内存不足处理子系统会被激活，并停止尝试分配内存的容器中的某个进程。
如果该进程在容器中 PID 为 1，而容器被标记为可重新启动，则 Kubernetes
会重新启动该容器。</li>
</ul>
<!--
- The memory limit for the Pod or container can also apply to pages in memory backed
  volumes, such as an `emptyDir`. The kubelet tracks `tmpfs` emptyDir volumes as container
  memory use, rather than as local ephemeral storage.
-->
<ul>
<li>Pod 或容器的内存约束值也适用于通过内存供应的卷，例如 <code>emptyDir</code> 卷。
kubelet 会跟踪 <code>tmpfs</code> 形式的 emptyDir 卷用量，将其作为容器的内存用量，
而不是临时存储用量。</li>
</ul>
<!--
If a container exceeds its memory request, and the node that it runs on becomes short of
memory overall, it is likely that the Pod the container belongs to will be
<a class='glossary-tooltip' title='终止节点上一个或多个 Pod 的过程。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/' target='_blank' aria-label='evicted'>evicted</a>.

A container might or might not be allowed to exceed its CPU limit for extended periods of time.
However, container runtimes don't terminate Pods or containers for excessive CPU usage.

To determine whether a container cannot be scheduled or is being killed due to resource limits,
see the [Troubleshooting](#troubleshooting) section.
-->
<p>如果某容器内存用量超过其内存请求值并且所在节点内存不足时，容器所处的 Pod
可能被<a class='glossary-tooltip' title='终止节点上一个或多个 Pod 的过程。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/' target='_blank' aria-label='逐出'>逐出</a>.</p>
<p>每个容器可能被允许也可能不被允许使用超过其 CPU 约束的处理时间。
但是，容器运行时不会由于 CPU 使用率过高而杀死 Pod 或容器。</p>
<p>要确定某容器是否会由于资源约束而无法调度或被杀死，请参阅<a href="#troubleshooting">疑难解答</a>节。</p>
<!--
## Monitoring compute & memory resource usage

The kubelet reports the resource usage of a Pod as part of the Pod
[`status`](/docs/concepts/overview/working-with-objects/kubernetes-objects/#object-spec-and-status).

If optional [tools for monitoring](/docs/tasks/debug/debug-cluster/resource-usage-monitoring/)
are available in your cluster, then Pod resource usage can be retrieved either
from the [Metrics API](/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-api)
directly or from your monitoring tools.
-->
<h2 id="监控计算和内存资源用量">监控计算和内存资源用量</h2>
<p>kubelet 会将 Pod 的资源使用情况作为 Pod
<a href="/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/#object-spec-and-status"><code>status</code></a>
的一部分来报告的。</p>
<p>如果为集群配置了可选的<a href="/zh/docs/tasks/debug/debug-cluster/resource-usage-monitoring/">监控工具</a>，
则可以直接从<a href="/zh/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-api">指标 API</a>
或者监控工具获得 Pod 的资源使用情况。</p>
<!--
## Local ephemeral storage

Nodes have local ephemeral storage, backed by
locally-attached writeable devices or, sometimes, by RAM.
"Ephemeral" means that there is no long-term guarantee about durability.

Pods use ephemeral local storage for scratch space, caching, and for logs.
The kubelet can provide scratch space to Pods using local ephemeral storage to
mount [`emptyDir`](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir)
 <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='volumes'>volumes</a> into containers.
-->
<h2 id="local-ephemeral-storage">本地临时存储  </h2>
<!-- feature gate LocalStorageCapacityIsolation -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.10 [beta]</code>
</div>


<p>节点通常还可以具有本地的临时性存储，由本地挂接的可写入设备或者有时也用 RAM
来提供支持。
“临时（Ephemeral）”意味着对所存储的数据不提供长期可用性的保证。</p>
<p>Pods 通常可以使用临时性本地存储来实现缓冲区、保存日志等功能。
kubelet 可以为使用本地临时存储的 Pods 提供这种存储空间，允许后者使用
<a href="/zh/docs/concepts/storage/volumes/#emptydir"><code>emptyDir</code></a> 类型的
<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>将其挂载到容器中。</p>
<!--
The kubelet also uses this kind of storage to hold
[node-level container logs](/docs/concepts/cluster-administration/logging/#logging-at-the-node-level),
container images, and the writable layers of running containers.

If a node fails, the data in its ephemeral storage can be lost.
Your applications cannot expect any performance SLAs (disk IOPS for example)
from local ephemeral storage.

As a beta feature, Kubernetes lets you track, reserve and limit the amount
of ephemeral local storage a Pod can consume.
-->
<p>kubelet 也使用此类存储来保存
<a href="/zh/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">节点层面的容器日志</a>，
容器镜像文件、以及运行中容器的可写入层。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 如果节点失效，存储在临时性存储中的数据会丢失。
你的应用不能对本地临时性存储的性能 SLA（例如磁盘 IOPS）作任何假定。
</div>

<p>作为一种 beta 阶段功能特性，Kubernetes 允许你跟踪、预留和限制 Pod
可消耗的临时性本地存储数量。</p>
<!--
### Configurations for local ephemeral storage

Kubernetes supports two ways to configure local ephemeral storage on a node:

In this configuration, you place all different kinds of ephemeral local data
(`emptyDir` volumes, writeable layers, container images, logs) into one filesystem.
The most effective way to configure the kubelet means dedicating this filesystem
to Kubernetes (kubelet) data.

The kubelet also writes
[node-level container logs](/docs/concepts/cluster-administration/logging/#logging-at-the-node-level)
and treats these similarly to ephemeral local storage.
-->
<h3 id="本地临时性存储的配置">本地临时性存储的配置</h3>
<p>Kubernetes 有两种方式支持节点上配置本地临时性存储：</p>
<ul class="nav nav-tabs" id="local-storage-configurations" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#local-storage-configurations-0" role="tab" aria-controls="local-storage-configurations-0" aria-selected="true">单一文件系统</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#local-storage-configurations-1" role="tab" aria-controls="local-storage-configurations-1">双文件系统</a></li></ul>
<div class="tab-content" id="local-storage-configurations"><div id="local-storage-configurations-0" class="tab-pane show active" role="tabpanel" aria-labelledby="local-storage-configurations-0">

<p><p>采用这种配置时，你会把所有类型的临时性本地数据（包括 <code>emptyDir</code>
卷、可写入容器层、容器镜像、日志等）放到同一个文件系统中。
作为最有效的 kubelet 配置方式，这意味着该文件系统是专门提供给 Kubernetes
（kubelet）来保存数据的。</p>
<p>kubelet 也会生成
<a href="/zh/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">节点层面的容器日志</a>，
并按临时性本地存储的方式对待之。</p>
<!--
The kubelet writes logs to files inside its configured log directory (`/var/log`
by default); and has a base directory for other locally stored data
(`/var/lib/kubelet` by default).

Typically, both `/var/lib/kubelet` and `/var/log` are on the system root filesystem,
and the kubelet is designed with that layout in mind.

Your node can have as many other filesystems, not used for Kubernetes,
as you like.
-->
<p>kubelet 会将日志写入到所配置的日志目录（默认为 <code>/var/log</code>）下的文件中；
还会针对其他本地存储的数据使用同一个基础目录（默认为 <code>/var/lib/kubelet</code>）。</p>
<p>通常，<code>/var/lib/kubelet</code> 和 <code>/var/log</code> 都是在系统的根文件系统中。kubelet
的设计也考虑到这一点。</p>
<p>你的集群节点当然可以包含其他的、并非用于 Kubernetes 的很多文件系统。</p>
</div>
  <div id="local-storage-configurations-1" class="tab-pane" role="tabpanel" aria-labelledby="local-storage-configurations-1">

<p><p>你使用节点上的某个文件系统来保存运行 Pods 时产生的临时性数据：日志和
<code>emptyDir</code> 卷等。你可以使用这个文件系统来保存其他数据（例如：与 Kubernetes
无关的其他系统日志）；这个文件系统还可以是根文件系统。</p>
<p>kubelet 也将
<a href="/zh/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">节点层面的容器日志</a>
写入到第一个文件系统中，并按临时性本地存储的方式对待之。</p>
<p>同时你使用另一个由不同逻辑存储设备支持的文件系统。在这种配置下，你会告诉
kubelet 将容器镜像层和可写层保存到这第二个文件系统上的某个目录中。</p>
<p>第一个文件系统中不包含任何镜像层和可写层数据。</p>
<p>当然，你的集群节点上还可以有很多其他与 Kubernetes 没有关联的文件系统。</p>
</div></div>

<!--
The kubelet can measure how much local storage it is using. It does this provided
that:

- the `LocalStorageCapacityIsolation`
  [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
  is enabled (the feature is on by default), and
- you have set up the node using one of the supported configurations
  for local ephemeral storage.

If you have a different configuration, then the kubelet does not apply resource
limits for ephemeral local storage.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> The kubelet tracks <code>tmpfs</code> emptyDir volumes as container memory use, rather
than as local ephemeral storage.
</div>
-->
<p>kubelet 能够度量其本地存储的用量。实现度量机制的前提是：</p>
<ul>
<li><code>LocalStorageCapacityIsolation</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
被启用（默认状态），并且</li>
<li>你已经对节点进行了配置，使之使用所支持的本地临时性储存配置方式之一</li>
</ul>
<p>如果你的节点配置不同于以上预期，kubelet 就无法对临时性本地存储的资源约束实施限制。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> kubelet 会将 <code>tmpfs</code> emptyDir 卷的用量当作容器内存用量，而不是本地临时性存储来统计。
</div>
<!--
### Setting requests and limits for local ephemeral storage

You can use `ephemeral-storage` for managing local ephemeral storage. Each
container of a Pod can specify either or both of the following:

* `spec.containers[].resources.limits.ephemeral-storage`
* `spec.containers[].resources.requests.ephemeral-storage`

Limits and requests for `ephemeral-storage` are measured in quantities.
You can express storage as a plain integer or as a fixed-point number using one of these suffixes:
E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:
-->
<h3 id="为本地临时性存储设置请求和约束值">为本地临时性存储设置请求和约束值</h3>
<p>你可以使用 <code>ephemeral-storage</code> 来管理本地临时性存储。
Pod 中的每个容器可以设置以下属性：</p>
<ul>
<li><code>spec.containers[].resources.limits.ephemeral-storage</code></li>
<li><code>spec.containers[].resources.requests.ephemeral-storage</code></li>
</ul>
<p><code>ephemeral-storage</code> 的请求和约束值是按量纲计量的。你可以使用一般整数或者定点数字
加上下面的后缀来表达存储量：E、P、T、G、M、K。
你也可以使用对应的 2 的幂级数来表达：Ei、Pi、Ti、Gi、Mi、Ki。
例如，下面的表达式所表达的大致是同一个值：</p>
<ul>
<li><code>128974848</code></li>
<li><code>129e6</code></li>
<li><code>129M</code></li>
<li><code>123Mi</code></li>
</ul>
<!--
In the following example, the Pod has two containers. Each container has a request of
2GiB of local ephemeral storage. Each container has a limit of 4GiB of local ephemeral
storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and a
limit of 8GiB of local ephemeral storage.
-->
<p>在下面的例子中，Pod 包含两个容器。每个容器请求 2 GiB 大小的本地临时性存储。
每个容器都设置了 4 GiB 作为其本地临时性存储的约束值。
因此，整个 Pod 的本地临时性存储请求是 4 GiB，且其本地临时性存储的约束为 8 GiB。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>images.my-company.example/app:v4<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;4Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ephemeral<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/tmp&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log-aggregator<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>images.my-company.example/log-aggregator:v6<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;4Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ephemeral<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/tmp&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ephemeral<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div><!--
### How Pods with ephemeral-storage requests are scheduled

When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods.
For more information, see
[Node Allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable).

The scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.
-->
<h3 id="带临时性存储的-pods-的调度行为">带临时性存储的 Pods 的调度行为</h3>
<p>当你创建一个 Pod 时，Kubernetes 调度器会为 Pod 选择一个节点来运行之。
每个节点都有一个本地临时性存储的上限，是其可提供给 Pods 使用的总量。
欲了解更多信息，可参考
<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">节点可分配资源</a>
节。</p>
<p>调度器会确保所调度的容器的资源请求总和不会超出节点的资源容量。</p>
<!--
### Ephemeral storage consumption management {#resource-emphemeralstorage-consumption}

If the kubelet is managing local ephemeral storage as a resource, then the
kubelet measures storage use in:

- `emptyDir` volumes, except _tmpfs_ `emptyDir` volumes
- directories holding node-level logs
- writeable container layers

If a Pod is using more ephemeral storage than you allow it to, the kubelet
sets an eviction signal that triggers Pod eviction.

For container-level isolation, if a container's writable layer and log
usage exceeds its storage limit, the kubelet marks the Pod for eviction.

For pod-level isolation the kubelet works out an overall Pod storage limit by
summing the limits for the containers in that Pod. In this case, if the sum of
the local ephemeral storage usage from all containers and also the Pod's `emptyDir`
volumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod
for eviction.
-->
<h3 id="resource-emphemeralstorage-consumption">临时性存储消耗的管理</h3>
<p>如果 kubelet 将本地临时性存储作为资源来管理，则 kubelet 会度量以下各处的存储用量：</p>
<ul>
<li><code>emptyDir</code> 卷，除了 <em>tmpfs</em> <code>emptyDir</code> 卷</li>
<li>保存节点层面日志的目录</li>
<li>可写入的容器镜像层</li>
</ul>
<p>如果某 Pod 的临时存储用量超出了你所允许的范围，kubelet
会向其发出逐出（eviction）信号，触发该 Pod 被逐出所在节点。</p>
<p>就容器层面的隔离而言，如果某容器的可写入镜像层和日志用量超出其存储约束，
kubelet 也会将所在的 Pod 标记为逐出候选。</p>
<p>就 Pod 层面的隔离而言，kubelet 会将 Pod 中所有容器的约束值相加，得到 Pod
存储约束的总值。如果所有容器的本地临时性存储用量总和加上 Pod 的 <code>emptyDir</code>
卷的用量超出 Pod 存储约束值，kubelet 也会将该 Pod 标记为逐出候选。</p>
<!--
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p>If the kubelet is not measuring local ephemeral storage, then a Pod
that exceeds its local storage limit will not be evicted for breaching
local storage resource limits.</p>
<p>However, if the filesystem space for writeable container layers, node-level logs,
or <code>emptyDir</code> volumes falls low, the node
<a class='glossary-tooltip' title='污点是一种一个核心对象，包含三个必需的属性：key、value 和 effect。 污点会阻止在节点或节点组上调度 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='taints'>taints</a> itself as short on local storage
and this taint triggers eviction for any Pods that don't specifically tolerate the taint.</p>
<p>See the supported <a href="#configurations-for-local-ephemeral-storage">configurations</a>
for ephemeral local storage.</p>

</div>

-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <p>如果 kubelet 没有度量本地临时性存储的用量，即使 Pod
的本地存储用量超出其约束值也不会被逐出。</p>
<p>不过，如果用于可写入容器镜像层、节点层面日志或者 <code>emptyDir</code> 卷的文件系统中可用空间太少，
节点会为自身设置本地存储不足的<a class='glossary-tooltip' title='污点是一种一个核心对象，包含三个必需的属性：key、value 和 effect。 污点会阻止在节点或节点组上调度 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank' aria-label='污点'>污点</a> 标签。
这一污点会触发对那些无法容忍该污点的 Pods 的逐出操作。</p>
<p>关于临时性本地存储的配置信息，请参考<a href="#configurations-for-local-ephemeral-storage">这里</a></p>

</div>

<!--
The kubelet supports different ways to measure Pod storage use:

The kubelet performs regular, scheduled checks that scan each
`emptyDir` volume, container log directory, and writeable container layer.

The scan measures how much space is used.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>In this mode, the kubelet does not track open file descriptors
for deleted files.</p>
<p>If you (or a container) create a file inside an <code>emptyDir</code> volume,
something then opens that file, and you delete the file while it is
still open, then the inode for the deleted file stays until you close
that file but the kubelet does not categorize the space as in use.</p>

</div>
-->
<p>kubelet 支持使用不同方式来度量 Pod 的存储用量：</p>
<ul class="nav nav-tabs" id="resource-emphemeralstorage-measurement" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#resource-emphemeralstorage-measurement-0" role="tab" aria-controls="resource-emphemeralstorage-measurement-0" aria-selected="true">周期性扫描</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#resource-emphemeralstorage-measurement-1" role="tab" aria-controls="resource-emphemeralstorage-measurement-1">文件系统项目配额</a></li></ul>
<div class="tab-content" id="resource-emphemeralstorage-measurement"><div id="resource-emphemeralstorage-measurement-0" class="tab-pane show active" role="tabpanel" aria-labelledby="resource-emphemeralstorage-measurement-0">

<p><p>kubelet 按预定周期执行扫描操作，检查 <code>emptyDir</code> 卷、容器日志目录以及可写入容器镜像层。</p>
<p>这一扫描会度量存储空间用量。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>在这种模式下，kubelet 并不检查已删除文件所对应的、仍处于打开状态的文件描述符。</p>
<p>如果你（或者容器）在 <code>emptyDir</code> 卷中创建了一个文件，写入一些内容之后再次打开
该文件并执行了删除操作，所删除文件对应的 inode 仍然存在，直到你关闭该文件为止。
kubelet 不会将该文件所占用的空间视为已使用空间。</p>
</div>
</div>
  <div id="resource-emphemeralstorage-measurement-1" class="tab-pane" role="tabpanel" aria-labelledby="resource-emphemeralstorage-measurement-1">

<p><div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.15 [alpha]</code>
</div>
<p>项目配额（Project Quota）是一个操作系统层的功能特性，用来管理文件系统中的存储用量。
在 Kubernetes 中，你可以启用项目配额以监视存储用量。
你需要确保节点上为 <code>emptyDir</code> 提供存储的文件系统支持项目配额。
例如，XFS 和 ext4fs  文件系统都支持项目配额。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 项目配额可以帮你监视存储用量，但无法对存储约束执行限制。
</div>
<!--
Kubernetes uses project IDs starting from `1048576`. The IDs in use are
registered in `/etc/projects` and `/etc/projid`. If project IDs in
this range are used for other purposes on the system, those project
IDs must be registered in `/etc/projects` and `/etc/projid` so that
Kubernetes does not use them.

Quotas are faster and more accurate than directory scanning. When a
directory is assigned to a project, all files created under a
directory are created in that project, and the kernel merely has to
keep track of how many blocks are in use by files in that project.
If a file is created and deleted, but has an open file descriptor,
it continues to consume space. Quota tracking records that space accurately
whereas directory scans overlook the storage used by deleted files.
-->
<p>Kubernetes 所使用的项目 ID 始于 <code>1048576</code>。
所使用的 IDs 会注册在 <code>/etc/projects</code> 和 <code>/etc/projid</code> 文件中。
如果该范围中的项目 ID 已经在系统中被用于其他目的，则已占用的项目 IDs
也必须注册到 <code>/etc/projects</code> 和 <code>/etc/projid</code> 中，这样 Kubernetes
才不会使用它们。</p>
<p>配额方式与目录扫描方式相比速度更快，结果更精确。当某个目录被分配给某个项目时，
该目录下所创建的所有文件都属于该项目，内核只需要跟踪该项目中的文件所使用的存储块个数。
如果某文件被创建后又被删除，但对应文件描述符仍处于打开状态，
该文件会继续耗用存储空间。配额跟踪技术能够精确第记录对应存储空间的状态，
而目录扫描方式会忽略被删除文件所占用的空间。</p>
<!--
If you want to use project quotas, you should:

* Enable the `LocalStorageCapacityIsolationFSQuotaMonitoring=true`
  [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
  using the `featureGates` field in the
  [kubelet configuration](/docs/reference/config-api/kubelet-config.v1beta1/)
  or the `--feature-gates` command line flag.

* Ensure that the root filesystem (or optional runtime filesystem)
  has project quotas enabled. All XFS filesystems support project quotas.
  For ext4 filesystems, you need to enable the project quota tracking feature
  while the filesystem is not mounted.
  ```bash
  # For ext4, with /dev/block-device not mounted
  sudo tune2fs -O project -Q prjquota /dev/block-device
  ```

* Ensure that the root filesystem (or optional runtime filesystem) is
  mounted with project quotas enabled. For both XFS and ext4fs, the
  mount option is named `prjquota`.
-->
<p>如果你希望使用项目配额，你需要：</p>
<ul>
<li>
<p>在 <a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/">kubelet 配置</a>中使用 <code>featureGates</code> 字段 或者使用 <code>--feature-gates</code> 命令行参数
启用 <code>LocalStorageCapacityIsolationFSQuotaMonitoring=true</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a> 。</p>
</li>
<li>
<p>确保根文件系统（或者可选的运行时文件系统）启用了项目配额。所有 XFS
文件系统都支持项目配额。
对 extf 文件系统而言，你需要在文件系统尚未被挂载时启用项目配额跟踪特性：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#080;font-style:italic"># 对 ext4 而言，在 /dev/block-device 尚未被挂载时执行下面操作</span>
sudo tune2fs -O project -Q prjquota /dev/block-device
</code></pre></div></li>
<li>
<p>确保根文件系统（或者可选的运行时文件系统）在挂载时项目配额特性是被启用了的。
对于 XFS 和 ext4fs 而言，对应的挂载选项称作 <code>prjquota</code>。</p>
</li>
</ul>
</div></div>

<!--
## Extended resources

Extended resources are fully-qualified resource names outside the
`kubernetes.io` domain. They allow cluster operators to advertise and users to
consume the non-Kubernetes-built-in resources.

There are two steps required to use Extended Resources. First, the cluster
operator must advertise an Extended Resource. Second, users must request the
Extended Resource in Pods.
-->
<h2 id="extended-resources">扩展资源（Extended Resources）  </h2>
<p>扩展资源是 <code>kubernetes.io</code> 域名之外的标准资源名称。
它们使得集群管理员能够颁布非 Kubernetes 内置资源，而用户可以使用他们。</p>
<p>使用扩展资源需要两个步骤。首先，集群管理员必须颁布扩展资源。
其次，用户必须在 Pod 中请求扩展资源。</p>
<!--
### Managing extended resources

#### Node-level extended resources

Node-level extended resources are tied to nodes.

##### Device plugin managed resources

See [Device
Plugin](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
for how to advertise device plugin managed resources on each node.
-->
<h3 id="managing-extended-resources">管理扩展资源  </h3>
<h4 id="node-level-extended-resources">节点级扩展资源    </h4>
<p>节点级扩展资源绑定到节点。</p>
<h5 id="device-plugin-managed-resources">设备插件管理的资源  </h5>
<p>有关如何颁布在各节点上由设备插件所管理的资源，请参阅
<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">设备插件</a>。</p>
<!--
##### Other resources

To advertise a new node-level extended resource, the cluster operator can
submit a `PATCH` HTTP request to the API server to specify the available
quantity in the `status.capacity` for a node in the cluster. After this
operation, the node's `status.capacity` will include a new resource. The
`status.allocatable` field is updated automatically with the new resource
asynchronously by the kubelet.
-->
<h5 id="other-resources">其他资源  </h5>
<p>为了颁布新的节点级扩展资源，集群操作员可以向 API 服务器提交 <code>PATCH</code> HTTP 请求，
以在集群中节点的 <code>status.capacity</code> 中为其配置可用数量。
完成此操作后，节点的 <code>status.capacity</code> 字段中将包含新资源。
kubelet 会异步地对 <code>status.allocatable</code> 字段执行自动更新操作，使之包含新资源。</p>
<!--
Because the scheduler uses the node `status.allocatable` value when
evaluating Pod fitness, the shceduler only takes account of the new value after
the asynchronous update. There may be a short delay between patching the
node capacity with a new resource and the time when the first Pod that requests
the resource to be scheduled on that node.
-->
<p>由于调度器在评估 Pod 是否适合在某节点上执行时会使用节点的 <code>status.allocatable</code> 值，
调度器只会考虑异步更新之后的新值。
在更新节点容量使之包含新资源之后和请求该资源的第一个 Pod 被调度到该节点之间，
可能会有短暂的延迟。</p>
<!--
**Example:**

Here is an example showing how to use `curl` to form an HTTP request that
advertises five "example.com/foo" resources on node `k8s-node-1` whose master
is `k8s-master`.
-->
<p><strong>示例：</strong></p>
<p>这是一个示例，显示了如何使用 <code>curl</code> 构造 HTTP 请求，公告主节点为 <code>k8s-master</code>
的节点 <code>k8s-node-1</code> 上存在五个 <code>example.com/foo</code> 资源。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl --header <span style="color:#b44">&#34;Content-Type: application/json-patch+json&#34;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--request PATCH <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--data <span style="color:#b44">&#39;[{&#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1foo&#34;, &#34;value&#34;: &#34;5&#34;}]&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
</code></pre></div><!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> In the preceding request, <code>~1</code> is the encoding for the character <code>/</code>
in the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在前面的请求中，<code>~1</code> 是在 patch 路径中对字符 <code>/</code> 的编码。
JSON-Patch 中的操作路径的值被视为 JSON-Pointer 类型。
有关更多详细信息，请参见
<a href="https://tools.ietf.org/html/rfc6901#section-3">IETF RFC 6901 第 3 节</a>。
</div>
<!--
#### Cluster-level extended resources

Cluster-level extended resources are not tied to nodes. They are usually managed
by scheduler extenders, which handle the resource consumption and resource quota.

You can specify the extended resources that are handled by scheduler extenders
in [scheduler policy configuration](/docs/reference/config-api/kube-scheduler-config.v1beta3/)
-->
<h4 id="cluster-level-extended-resources">集群层面的扩展资源  </h4>
<p>集群层面的扩展资源并不绑定到具体节点。
它们通常由调度器扩展程序（Scheduler Extenders）管理，这些程序处理资源消耗和资源配额。</p>
<p>你可以在<a href="/zh/docs/reference/config-api/kube-scheduler-config.v1beta3/">调度器策略配置</a>
中指定由调度器扩展程序处理的扩展资源。</p>
<!--
**Example:**

The following configuration for a scheduler policy indicates that the
cluster-level extended resource "example.com/foo" is handled by the scheduler
extender.

- The scheduler sends a Pod to the scheduler extender only if the Pod requests
  "example.com/foo".
- The `ignoredByScheduler` field specifies that the scheduler does not check
  the "example.com/foo" resource in its `PodFitsResources` predicate.
-->
<p><strong>示例：</strong></p>
<p>下面的调度器策略配置标明集群层扩展资源 &quot;example.com/foo&quot; 由调度器扩展程序处理。</p>
<ul>
<li>仅当 Pod 请求 &quot;example.com/foo&quot; 时，调度器才会将 Pod 发送到调度器扩展程序。</li>
<li><code>ignoredByScheduler</code> 字段指定调度器不要在其 <code>PodFitsResources</code> 断言中检查
&quot;example.com/foo&quot; 资源。</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Policy&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;v1&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;extenders&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;urlPrefix&#34;</span>:<span style="color:#b44">&#34;&lt;extender-endpoint&gt;&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;bindVerb&#34;</span>: <span style="color:#b44">&#34;bind&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;managedResources&#34;</span>: [
        {
          <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;example.com/foo&#34;</span>,
          <span style="color:#008000;font-weight:bold">&#34;ignoredByScheduler&#34;</span>: <span style="color:#a2f;font-weight:bold">true</span>
        }
      ]
    }
  ]
}
</code></pre></div><!--
### Consuming extended resources

Users can consume extended resources in Pod specs like CPU and memory.
The scheduler takes care of the resource accounting so that no more than the
available amount is simultaneously allocated to Pods.

The API server restricts quantities of extended resources to whole numbers.
Examples of _valid_ quantities are `3`, `3000m` and `3Ki`. Examples of
_invalid_ quantities are `0.5` and `1500m`.
-->
<h3 id="consuming-extended-resources">使用扩展资源 </h3>
<p>就像 CPU 和内存一样，用户可以在 Pod 的规约中使用扩展资源。
调度器负责资源的核算，确保同时分配给 Pod 的资源总量不会超过可用数量。</p>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Extended resources replace Opaque Integer Resources.
Users can use any domain name prefix other than <code>kubernetes.io</code> which is reserved.
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 扩展资源取代了非透明整数资源（Opaque Integer Resources，OIR）。
用户可以使用 <code>kubernetes.io</code> （保留）以外的任何域名前缀。
</div>
<!--
To consume an extended resource in a Pod, include the resource name as a key
in the `spec.containers[].resources.limits` map in the container spec.

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Extended resources cannot be overcommitted, so request and limit
must be equal if both are present in a container spec.
</div>
-->
<p>要在 Pod 中使用扩展资源，请在容器规范的 <code>spec.containers[].resources.limits</code>
映射中包含资源名称作为键。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 扩展资源不能过量使用，因此如果容器规范中同时存在请求和约束，则它们的取值必须相同。
</div>
<!--
A Pod is scheduled only if all of the resource requests are satisfied, including
CPU, memory and any extended resources. The Pod remains in the `PENDING` state
as long as the resource request cannot be satisfied.

**Example:**

The Pod below requests 2 CPUs and 1 "example.com/foo" (an extended resource).
-->
<p>仅当所有资源请求（包括 CPU、内存和任何扩展资源）都被满足时，Pod 才能被调度。
在资源请求无法满足时，Pod 会保持在 <code>PENDING</code> 状态。</p>
<p><strong>示例：</strong></p>
<p>下面的 Pod 请求 2 个 CPU 和 1 个 &quot;example.com/foo&quot;（扩展资源）。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>myimage<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/foo</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/foo</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></code></pre></div><!--
## PID limiting

Process ID (PID) limits allow for the configuration of a kubelet
to limit the number of PIDs that a given Pod can consume. See
[PID Limiting](/docs/concepts/policy/pid-limiting/) for information.
-->
<h2 id="pid-limiting">PID 限制  </h2>
<p>进程 ID（PID）限制允许对 kubelet 进行配置，以限制给定 Pod 可以消耗的 PID 数量。
有关信息，请参见 <a href="/zh/docs/concepts/policy/pid-limiting/">PID 限制</a>。</p>
<!--
## Troubleshooting

### My Pods are pending with event message `FailedScheduling`

If the scheduler cannot find any node where a Pod can fit, the Pod remains
unscheduled until a place can be found. An
[Event](/docs/reference/kubernetes-api/cluster-resources/event-v1/) is produced
each time the scheduler fails to find a place for the Pod, You can use `kubectl`
to view the events for a Pod; for example:
-->
<h2 id="疑难解答">疑难解答</h2>
<h3 id="我的-pod-处于悬决状态且事件信息显示-failedscheduling">我的 Pod 处于悬决状态且事件信息显示 <code>FailedScheduling</code></h3>
<p>如果调度器找不到该 Pod 可以匹配的任何节点，则该 Pod 将保持未被调度状态，
直到找到一个可以被调度到的位置。每当调度器找不到 Pod 可以调度的地方时，
会产生一个 <a href="/docs/reference/kubernetes-api/cluster-resources/event-v1/">Event</a>。
你可以使用 <code>kubectl</code> 来查看 Pod 的事件；例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod frontend | grep -A <span style="color:#666">9999999999</span> Events
</code></pre></div><pre><code>Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu
</code></pre><!--
In the preceding example, the Pod named "frontend" fails to be scheduled due to
insufficient CPU resource on any node. Similar error messages can also suggest
failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod
is pending with a message of this type, there are several things to try:

- Add more nodes to the cluster.
- Terminate unneeded Pods to make room for pending Pods.
- Check that the Pod is not larger than all the nodes. For example, if all the
  nodes have a capacity of `cpu: 1`, then a Pod with a request of `cpu: 1.1` will
  never be scheduled.
- Check for node taints. If most of your nodes are tainted, and the new Pod does
  not tolerate that taint, the scheduler only considers placements onto the
  remaining nodes that don't have that taint.

You can check node capacities and amounts allocated with the
`kubectl describe nodes` command. For example:
-->
<p>在上述示例中，由于节点上的 CPU 资源不足，名为 “frontend” 的 Pod 无法被调度。
由于内存不足（PodExceedsFreeMemory）而导致失败时，也有类似的错误消息。
一般来说，如果 Pod 处于悬决状态且有这种类型的消息时，你可以尝试如下几件事情：</p>
<ul>
<li>向集群添加更多节点。</li>
<li>终止不需要的 Pod，为悬决的 Pod 腾出空间。</li>
<li>检查 Pod 所需的资源是否超出所有节点的资源容量。例如，如果所有节点的容量都是<code>cpu：1</code>，
那么一个请求为 <code>cpu: 1.1</code> 的 Pod 永远不会被调度。</li>
<li>检查节点上的污点设置。如果集群中节点上存在污点，而新的 Pod 不能容忍污点，
调度器只会考虑将 Pod 调度到不带有该污点的节点上。</li>
</ul>
<p>你可以使用 <code>kubectl describe nodes</code> 命令检查节点容量和已分配的资源数量。 例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe nodes e2e-test-node-pool-4lw4
</code></pre></div><pre><code>Name:            e2e-test-node-pool-4lw4
[ ... 这里忽略了若干行以便阅读 ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... 这里忽略了若干行以便阅读 ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (12%)        1070Mi (14%)
</code></pre><!--
In the preceding output, you can see that if a Pod requests more than 1120m
CPUs or 6.23Gi of memory, it will not fit on the node.

By looking at the "Pods" section, you can see which Pods are taking up space on
the node.
-->
<p>在上面的输出中，你可以看到如果 Pod 请求超过 1120m CPU 或者 6.23Gi 内存，节点将无法满足。</p>
<p>通过查看 &quot;Pods&quot; 部分，你将看到哪些 Pod 占用了节点上的资源。</p>
<!--
The amount of resources available to Pods is less than the node capacity, because
system daemons use a portion of the available resources. Within the Kubernetes API,
each Node has a `.status.allocatable` field
(see [NodeStatus](/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeStatus)
for details).
-->
<p>Pods 可用的资源量低于节点的资源总量，因为系统守护进程也会使用一部分可用资源。
在 Kubernetes API 中，每个 Node 都有一个 <code>.status.allocatable</code> 字段
（详情参见 <a href="/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeStatus">NodeStatus</a>）。</p>
<!--
The `.status.allocatable` field describes the amount of resources that are available
to Pods on that node (for example: 15 virtual CPUs and 7538 MiB of memory).
For more information on node allocatable resources in Kubernetes, see
[Reserve Compute Resources for System Daemons](/docs/tasks/administer-cluster/reserve-compute-resources/).
-->
<p>字段 <code>.status.allocatable</code> 描述节点上可以用于 Pod 的资源总量（例如：15 个虚拟
CPU、7538 MiB 内存）。关于 Kubernetes 中节点可分配资源的信息，可参阅
<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/">为系统守护进程预留计算资源</a>。</p>
<!--
You can configure [resource quotas](/docs/concepts/policy/resource-quotas/)
to limit the total amount of resources that a namespace can consume.
Kubernetes enforces quotas for objects in particular namespace when there is a
ResourceQuota in that namespace.
For example, if you assign specific namespaces to different teams, you
can add ResourceQuotas into those namespaces. Setting resource quotas helps to
prevent one team from using so much of any resource that this over-use affects other teams.

You should also consider what access you grant to that namespace:
**full** write access to a namespace allows someone with that access to remove any
resource, include a configured ResourceQuota.
-->
<p>你可以配置<a href="/zh/docs/concepts/policy/resource-quotas/">资源配额</a>功能特性以限制每个名字空间可以使用的资源总量。
当某名字空间中存在 ResourceQuota 时，Kubernetes 会在该名字空间中的对象强制实施配额。
例如，如果你为不同的团队分配名字空间，你可以为这些名字空间添加 ResourceQuota。
设置资源配额有助于防止一个团队占用太多资源，以至于这种占用会影响其他团队。</p>
<p>你还需要考虑为这些名字空间设置授权访问：
为名字空间提供 <strong>全部</strong> 的写权限时，具有合适权限的人可能删除所有资源，
包括所配置的 ResourceQuota。</p>
<!--
### My Container is terminated

Your container might get terminated because it is resource-starved. To check
whether a Container is being killed because it is hitting a resource limit, call
`kubectl describe pod` on the Pod of interest:
-->
<h3 id="我的容器被终止了">我的容器被终止了</h3>
<p>你的容器可能因为资源紧张而被终止。要查看容器是否因为遇到资源限制而被杀死，
请针对相关的 Pod 执行 <code>kubectl describe pod</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod simmemleak-hra99
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre><code>Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Containers:
  simmemleak:
    Image:  saadali/simmemleak
    Limits:
      cpu:                      100m
      memory:                   50Mi
    State:                      Running
      Started:                  Tue, 07 Jul 2015 12:54:41 -0700
    Last Termination State:     Terminated
      Exit Code:                1
      Started:                  Fri, 07 Jul 2015 12:54:30 -0700
      Finished:                 Fri, 07 Jul 2015 12:54:33 -0700
    Ready:                      False
    Restart Count:              5
Conditions:
  Type      Status
  Ready     False
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Normal  Pulled     41s   kubelet            Container image &quot;saadali/simmemleak:latest&quot; already present on machine
  Normal  Created    41s   kubelet            Created container simmemleak
  Normal  Started    40s   kubelet            Started container simmemleak
  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod
</code></pre><!--
In the preceding example, the `Restart Count:  5` indicates that the `simmemleak`
Container in the Pod was terminated and restarted five times (so far).
The `OOMKilled` reason shows that the container tried to use more memory than its limit.
-->
<p>在上面的例子中，<code>Restart Count: 5</code> 意味着 Pod 中的 <code>simmemleak</code>
容器被终止并且（到目前为止）重启了五次。
原因 <code>OOMKilled</code> 显示容器尝试使用超出其限制的内存量。</p>
<!--
Your next step might be to check the application code for a memory leak. If you
find that the application is behaving how you expect, consider setting a higher
memory limit (and possibly request) for that container.
-->
<p>你接下来要做的或许是检查应用代码，看看是否存在内存泄露。
如果你发现应用的行为与你所预期的相同，则可以考虑为该容器设置一个更高的内存约束
（也可能需要设置请求值）。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Get hands-on experience [assigning Memory resources to containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/).
* Get hands-on experience [assigning CPU resources to containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/).
* Read how the API reference defines a [container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container)
  and its [resource requirements](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#resources)
* Read about [project quotas](https://xfs.org/docs/xfsdocs-xml-dev/XFS_User_Guide/tmp/en-US/html/xfs-quotas.html) in XFS
* Read more about the [kube-scheduler configuration reference (v1beta3)](/docs/reference/config-api/kube-scheduler-config.v1beta3/)
-->
<ul>
<li>获取<a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">分配内存资源给容器和 Pod </a> 的实践经验</li>
<li>获取<a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">分配 CPU 资源给容器和 Pod </a> 的实践经验</li>
<li>阅读 API 参考中 <a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container">Container</a>
和其<a href="/docs/reference/kubernetes-api/workload-resources/pod-v1/#resources">资源请求</a>定义。</li>
<li>阅读 XFS 中<a href="https://xfs.org/docs/xfsdocs-xml-dev/XFS_User_Guide/tmp/en-US/html/xfs-quotas.html">配额</a>的文档</li>
<li>进一步阅读 <a href="/zh/docs/reference/config-api/kube-scheduler-config.v1beta3/">kube-scheduler 配置参考 (v1beta3)</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ab6d20f33ad930a67ee7ef57bff6c75e">7.5 - 使用 kubeconfig 文件组织集群访问</h1>
    
	<!--
title: Organizing Cluster Access Using kubeconfig Files
content_type: concept
weight: 60
-->
<!-- overview -->
<!--
Use kubeconfig files to organize information about clusters, users, namespaces, and
authentication mechanisms. The `kubectl` command-line tool uses kubeconfig files to
find the information it needs to choose a cluster and communicate with the API server
of a cluster.
-->
<p>使用 kubeconfig 文件来组织有关集群、用户、命名空间和身份认证机制的信息。<code>kubectl</code> 命令行工具使用 kubeconfig 文件来查找选择集群所需的信息，并与集群的 API 服务器进行通信。</p>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> A file that is used to configure access to clusters is called
a <em>kubeconfig file</em>. This is a generic way of referring to configuration files.
It does not mean that there is a file named <code>kubeconfig</code>.
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 用于配置集群访问的文件称为 <em>kubeconfig 文件</em>。这是引用配置文件的通用方法。这并不意味着有一个名为 <code>kubeconfig</code> 的文件
</div>
<!--
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig file could result in malicious code execution or file exposure.
If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script.
</div>


-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 只使用来源可靠的 kubeconfig 文件。使用特制的 kubeconfig 文件可能会导致恶意代码执行或文件暴露。
如果必须使用不受信任的 kubeconfig 文件，请首先像检查 shell 脚本一样仔细检查它。
</div>


<!--
By default, `kubectl` looks for a file named `config` in the `$HOME/.kube` directory.
You can specify other kubeconfig files by setting the `KUBECONFIG` environment
variable or by setting the
[`-kubeconfig`](/docs/reference/generated/kubectl/kubectl/) flag.
-->
<p>默认情况下，<code>kubectl</code> 在 <code>$HOME/.kube</code> 目录下查找名为 <code>config</code> 的文件。
您可以通过设置 <code>KUBECONFIG</code> 环境变量或者设置
<a href="/docs/reference/generated/kubectl/kubectl/"><code>--kubeconfig</code></a>参数来指定其他 kubeconfig 文件。</p>
<!--
For step-by-step instructions on creating and specifying kubeconfig files, see
[Configure Access to Multiple Clusters](/docs/tasks/access-application-cluster/configure-access-multiple-clusters).
-->
<p>有关创建和指定 kubeconfig 文件的分步说明，请参阅
<a href="/zh/docs/tasks/access-application-cluster/configure-access-multiple-clusters">配置对多集群的访问</a>。</p>
<!-- body -->
<!--
## Supporting multiple clusters, users, and authentication mechanisms
-->
<h2 id="支持多集群-用户和身份认证机制">支持多集群、用户和身份认证机制</h2>
<!--
Suppose you have several clusters, and your users and components authenticate
in a variety of ways. For example:
-->
<p>假设您有多个集群，并且您的用户和组件以多种方式进行身份认证。比如：</p>
<!--
- A running kubelet might authenticate using certificates.
- A user might authenticate using tokens.
- Administrators might have sets of certificates that they provide to individual users.
-->
<ul>
<li>正在运行的 kubelet 可能使用证书在进行认证。</li>
<li>用户可能通过令牌进行认证。</li>
<li>管理员可能拥有多个证书集合提供给各用户。</li>
</ul>
<!--
With kubeconfig files, you can organize your clusters, users, and namespaces.
You can also define contexts to quickly and easily switch between
clusters and namespaces.
-->
<p>使用 kubeconfig 文件，您可以组织集群、用户和命名空间。您还可以定义上下文，以便在集群和命名空间之间快速轻松地切换。</p>
<!--
## Context
-->
<h2 id="上下文-context">上下文（Context）</h2>
<!--
A *context* element in a kubeconfig file is used to group access parameters
under a convenient name. Each context has three parameters: cluster, namespace, and user.
By default, the `kubectl` command-line tool uses parameters from
the *current context* to communicate with the cluster.
-->
<p>通过 kubeconfig 文件中的 <em>context</em> 元素，使用简便的名称来对访问参数进行分组。每个上下文都有三个参数：cluster、namespace 和 user。默认情况下，<code>kubectl</code> 命令行工具使用 <em>当前上下文</em> 中的参数与集群进行通信。</p>
<!--
To choose the current context:
-->
<p>选择当前上下文</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config use-context
</code></pre></div><!--
## The KUBECONFIG environment variable
-->
<h2 id="kubeconfig-环境变量">KUBECONFIG 环境变量</h2>
<!--
The `KUBECONFIG` environment variable holds a list of kubeconfig files.
For Linux and Mac, the list is colon-delimited. For Windows, the list
is semicolon-delimited. The `KUBECONFIG` environment variable is not
required. If the `KUBECONFIG` environment variable doesn't exist,
`kubectl` uses the default kubeconfig file, `$HOME/.kube/config`.
-->
<p><code>KUBECONFIG</code> 环境变量包含一个 kubeconfig 文件列表。
对于 Linux 和 Mac，列表以冒号分隔。对于 Windows，列表以分号分隔。
<code>KUBECONFIG</code> 环境变量不是必要的。
如果 <code>KUBECONFIG</code> 环境变量不存在，<code>kubectl</code> 使用默认的 kubeconfig 文件，<code>$HOME/.kube/config</code>。</p>
<!--
If the `KUBECONFIG` environment variable does exist, `kubectl` uses
an effective configuration that is the result of merging the files
listed in the `KUBECONFIG` environment variable.
-->
<p>如果 <code>KUBECONFIG</code> 环境变量存在，<code>kubectl</code> 使用 <code>KUBECONFIG</code> 环境变量中列举的文件合并后的有效配置。</p>
<!--
## Merging kubeconfig files
-->
<h2 id="合并-kubeconfig-文件">合并 kubeconfig 文件</h2>
<!--
To see your configuration, enter this command:
-->
<p>要查看配置，输入以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div><!--
As described previously, the output might be from a single kubeconfig file,
or it might be the result of merging several kubeconfig files.
-->
<p>如前所述，输出可能来自 kubeconfig 文件，也可能是合并多个 kubeconfig 文件的结果。</p>
<!--
Here are the rules that `kubectl` uses when it merges kubeconfig files:
-->
<p>以下是 <code>kubectl</code> 在合并 kubeconfig 文件时使用的规则。</p>
<!--
1. If the `-kubeconfig` flag is set, use only the specified file. Do not merge.
   Only one instance of this flag is allowed.

   Otherwise, if the `KUBECONFIG` environment variable is set, use it as a
   list of files that should be merged.
   Merge the files listed in the `KUBECONFIG` environment variable
   according to these rules:

   * Ignore empty filenames.
   * Produce errors for files with content that cannot be deserialized.
   * The first file to set a particular value or map key wins.
   * Never change the value or map key.
     Example: Preserve the context of the first file to set `current-context`.
     Example: If two files specify a `red-user`, use only values from the first file's `red-user`.
     Even if the second file has non-conflicting entries under `red-user`, discard them.
-->
<ol>
<li>
<p>如果设置了 <code>--kubeconfig</code> 参数，则仅使用指定的文件。不进行合并。此参数只能使用一次。</p>
<p>否则，如果设置了 <code>KUBECONFIG</code> 环境变量，将它用作应合并的文件列表。根据以下规则合并 <code>KUBECONFIG</code> 环境变量中列出的文件：</p>
<ul>
<li>忽略空文件名。</li>
<li>对于内容无法反序列化的文件，产生错误信息。</li>
<li>第一个设置特定值或者映射键的文件将生效。</li>
<li>永远不会更改值或者映射键。示例：保留第一个文件的上下文以设置 <code>current-context</code>。示例：如果两个文件都指定了 <code>red-user</code>，则仅使用第一个文件的 <code>red-user</code> 中的值。即使第二个文件在 <code>red-user</code> 下有非冲突条目，也要丢弃它们。</li>
</ul>
</li>
</ol>
<!--
   For an example of setting the `KUBECONFIG` environment variable, see
   [Setting the KUBECONFIG environment variable](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable).
-->
<p>有关设置 <code>KUBECONFIG</code> 环境变量的示例，请参阅
<a href="/zh/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable">设置 KUBECONFIG 环境变量</a>。</p>
<!--
   Otherwise, use the default kubeconfig file, `$HOME/.kube/config`, with no merging.
-->
<p>否则，使用默认的 kubeconfig 文件， <code>$HOME/.kube/config</code>，不进行合并。</p>
<!--
1. Determine the context to use based on the first hit in this chain:

    1. Use the `-context` command-line flag if it exists.
    2. Use the `current-context` from the merged kubeconfig files.
-->
<ol>
<li>
<p>根据此链中的第一个匹配确定要使用的上下文。</p>
<ol>
<li>如果存在，使用 <code>--context</code> 命令行参数。</li>
<li>使用合并的 kubeconfig 文件中的 <code>current-context</code>。</li>
</ol>
</li>
</ol>
<!--
   An empty context is allowed at this point.
-->
<p>这种场景下允许空上下文。</p>
<!--
1. Determine the cluster and user. At this point, there might or might not be a context.
   Determine the cluster and user based on the first hit in this chain,
   which is run twice: once for user and once for cluster:

   1. Use a command-line flag if it exists: `--user` or `--cluster`.
   2. If the context is non-empty, take the user or cluster from the context.
-->
<ol>
<li>
<p>确定集群和用户。此时，可能有也可能没有上下文。根据此链中的第一个匹配确定集群和用户，这将运行两次：一次用于用户，一次用于集群。</p>
<ol>
<li>如果存在，使用命令行参数：<code>--user</code> 或者 <code>--cluster</code>。</li>
<li>如果上下文非空，从上下文中获取用户或集群。</li>
</ol>
</li>
</ol>
<!--
   The user and cluster can be empty at this point.
-->
<p>这种场景下用户和集群可以为空。</p>
<!--
1. Determine the actual cluster information to use. At this point, there might or
   might not be cluster information.
   Build each piece of the cluster information based on this chain; the first hit wins:

   1. Use command line flags if they exist: `--server`, `--certificate-authority`, `--insecure-skip-tls-verify`.
   2. If any cluster information attributes exist from the merged kubeconfig files, use them.
   3. If there is no server location, fail.
-->
<ol>
<li>
<p>确定要使用的实际集群信息。此时，可能有也可能没有集群信息。基于此链构建每个集群信息；第一个匹配项会被采用：</p>
<ol>
<li>如果存在：<code>--server</code>、<code>--certificate-authority</code> 和 <code>--insecure-skip-tls-verify</code>，使用命令行参数。</li>
<li>如果合并的 kubeconfig 文件中存在集群信息属性，则使用它们。</li>
<li>如果没有 server 配置，则配置无效。</li>
</ol>
</li>
</ol>
<!--
2. Determine the actual user information to use. Build user information using the same
   rules as cluster information, except allow only one authentication
   technique per user:

   1. Use command line flags if they exist: `--client-certificate`, `--client-key`, `--username`, `--password`, `--token`.
   2. Use the `user` fields from the merged kubeconfig files.
   3. If there are two conflicting techniques, fail.
-->
<ol start="2">
<li>
<p>确定要使用的实际用户信息。使用与集群信息相同的规则构建用户信息，但每个用户只允许一种身份认证技术：</p>
<ol>
<li>如果存在：<code>--client-certificate</code>、<code>--client-key</code>、<code>--username</code>、<code>--password</code> 和 <code>--token</code>，使用命令行参数。</li>
<li>使用合并的 kubeconfig 文件中的 <code>user</code> 字段。</li>
<li>如果存在两种冲突技术，则配置无效。</li>
</ol>
</li>
</ol>
<!--
3. For any information still missing, use default values and potentially
   prompt for authentication information.
-->
<ol start="3">
<li>对于仍然缺失的任何信息，使用其对应的默认值，并可能提示输入身份认证信息。</li>
</ol>
<!--
## File references
-->
<h2 id="文件引用">文件引用</h2>
<!--
File and path references in a kubeconfig file are relative to the location of the kubeconfig file.
File references on the command line are relative to the current working directory.
In `$HOME/.kube/config`, relative paths are stored relatively, and absolute paths
are stored absolutely.
-->
<p>kubeconfig 文件中的文件和路径引用是相对于 kubeconfig 文件的位置。
命令行上的文件引用是相对于当前工作目录的。
在 <code>$HOME/.kube/config</code> 中，相对路径按相对路径存储，绝对路径按绝对路径存储。</p>
<!--
## Proxy

You can configure `kubectl` to use proxy by setting `proxy-url` in the kubeconfig file, like:
-->
<h2 id="代理">代理</h2>
<p>你可以在 <code>kubeconfig</code> 文件中设置 <code>proxy-url</code> 来为 <code>kubectl</code> 使用代理，例如:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">proxy-url</span>:<span style="color:#bbb"> </span>https://proxy.host:3128<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">clusters</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">users</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">contexts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
* [Configure Access to Multiple Clusters](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/)
* [`kubectl config`](/docs/reference/generated/kubectl/kubectl-commands#config)
--->
<ul>
<li><a href="/zh/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">配置对多集群的访问</a></li>
<li><a href="/docs/reference/generated/kubectl/kubectl-commands#config"><code>kubectl config</code></a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-712cb3c03ff14a39e5a83a6d9b71d203">8 - 安全</h1>
    <div class="lead">确保云原生工作负载安全的一组概念。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-04eeb110d75afc8acb2cf7a3db743985">8.1 - 云原生安全概述</h1>
    <div class="lead">在云原生安全的背景下思考 Kubernetes 安全模型。</div>
	<!-- overview -->
<!--
This overview defines a model for thinking about Kubernetes security in the context of Cloud Native security.
-->
<p>本概述定义了一个模型，用于在 Cloud Native 安全性上下文中考虑 Kubernetes 安全性。</p>
<!--
This container security model provides suggestions, not proven information security policies.
-->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> 此容器安全模型只提供建议，而不是经过验证的信息安全策略。
</div>


<!-- body -->
<!--
## The 4C's of Cloud Native security

You can think about security in layers. The 4C's of Cloud Native security are Cloud,
Clusters, Containers, and Code.
-->
<h2 id="云原生安全的-4-个-c">云原生安全的 4 个 C</h2>
<p>你可以分层去考虑安全性，云原生安全的 4 个 C 分别是云（Cloud）、集群（Cluster）、容器（Container）和代码（Code）。</p>
<!--
This layered approach augments the [defense in depth](https://en.wikipedia.org/wiki/Defense_in_depth_(computing))
computing approach to security, which is widely regarded as a best practice for securing
software systems.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 这种分层方法增强了<a href="https://en.wikipedia.org/wiki/Defense_in_depth_(computing)">深度防护方法</a>在安全性方面的
防御能力，该方法被广泛认为是保护软件系统的最佳实践。
</div>

<figure>
    <img src="/images/docs/4c.png"/> <figcaption>
            <h4>云原生安全的 4C</h4>
        </figcaption>
</figure>

<!--
Each layer of the Cloud Native security model builds upon the next outermost layer.
The Code layer benefits from strong base (Cloud, Cluster, Container) security layers.
You cannot safeguard against poor security standards in the base layers by addressing
security at the Code level.
-->
<p>云原生安全模型的每一层都是基于下一个最外层，代码层受益于强大的基础安全层（云、集群、容器）。你无法通过在代码层解决安全问题来为基础层中糟糕的安全标准提供保护。</p>
<h2 id="云">云</h2>
<!--
In many ways, the Cloud (or co-located servers, or the corporate datacenter) is the
[trusted computing base](https://en.wikipedia.org/wiki/Trusted_computing_base)
of a Kubernetes cluster. If the Cloud layer is vulnerable (or
configured in a vulnerable way) then there is no guarantee that the components built
on top of this base are secure. Each cloud provider makes security recommendations
for running workloads securely in their environment.
-->
<p>在许多方面，云（或者位于同一位置的服务器，或者是公司数据中心）是 Kubernetes 集群中的
<a href="https://en.wikipedia.org/wiki/Trusted_computing_base">可信计算基</a>。
如果云层容易受到攻击（或者被配置成了易受攻击的方式），就不能保证在此基础之上构建的组件是安全的。
每个云提供商都会提出安全建议，以在其环境中安全地运行工作负载。</p>
<!--
### Cloud provider security

If you are running a Kubernetes cluster on your own hardware or a different cloud provider,
consult your documentation for security best practices.
Here are links to some of the popular cloud providers' security documentation:
-->
<h3 id="云提供商安全性">云提供商安全性</h3>
<p>如果您是在您自己的硬件或者其他不同的云提供商上运行 Kubernetes 集群，
请查阅相关文档来获取最好的安全实践。</p>
<p>下面是一些比较流行的云提供商的安全性文档链接：</p>





<table><caption style="display: none;">云提供商安全</caption>
<thead>
<tr>
<th>IaaS 提供商</th>
<th>链接</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alibaba Cloud</td>
<td><a href="https://www.alibabacloud.com/trust-center">https://www.alibabacloud.com/trust-center</a></td>
</tr>
<tr>
<td>Amazon Web Services</td>
<td><a href="https://aws.amazon.com/security/">https://aws.amazon.com/security/</a></td>
</tr>
<tr>
<td>Google Cloud Platform</td>
<td><a href="https://cloud.google.com/security/">https://cloud.google.com/security/</a></td>
</tr>
<tr>
<td>IBM Cloud</td>
<td><a href="https://www.ibm.com/cloud/security">https://www.ibm.com/cloud/security</a></td>
</tr>
<tr>
<td>Microsoft Azure</td>
<td><a href="https://docs.microsoft.com/en-us/azure/security/azure-security">https://docs.microsoft.com/en-us/azure/security/azure-security</a></td>
</tr>
<tr>
<td>Oracle Cloud Infrastructure</td>
<td><a href="https://www.oracle.com/security/">https://www.oracle.com/security/</a></td>
</tr>
<tr>
<td>VMWare VSphere</td>
<td><a href="https://www.vmware.com/security/hardening-guides.html">https://www.vmware.com/security/hardening-guides.html</a></td>
</tr>
</tbody>
</table>

<!--
### Infrastructure security {#infrastructure-security}

Suggestions for securing your infrastructure in a Kubernetes cluster:






<table><caption style="display: none;">Infrastructure security</caption>
<thead>
<tr>
<th>Area of Concern for Kubernetes Infrastructure</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Network access to API Server (Control plane)</td>
<td>All access to the Kubernetes control plane is not allowed publicly on the internet and is controlled by network access control lists restricted to the set of IP addresses needed to administer the cluster.</td>
</tr>
<tr>
<td>Network access to Nodes (nodes)</td>
<td>Nodes should be configured to <em>only</em> accept connections (via network access control lists) from the control plane on the specified ports, and accept connections for services in Kubernetes of type NodePort and LoadBalancer. If possible, these nodes should not be exposed on the public internet entirely.</td>
</tr>
<tr>
<td>Kubernetes access to Cloud Provider API</td>
<td>Each cloud provider needs to grant a different set of permissions to the Kubernetes control plane and nodes. It is best to provide the cluster with cloud provider access that follows the <a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">principle of least privilege</a> for the resources it needs to administer. The <a href="https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md#iam-roles">Kops documentation</a> provides information about IAM policies and roles.</td>
</tr>
<tr>
<td>Access to etcd</td>
<td>Access to etcd (the datastore of Kubernetes) should be limited to the control plane only. Depending on your configuration, you should attempt to use etcd over TLS. More information can be found in the <a href="https://github.com/etcd-io/etcd/tree/master/Documentation">etcd documentation</a>.</td>
</tr>
<tr>
<td>etcd Encryption</td>
<td>Wherever possible it's a good practice to encrypt all drives at rest, and since etcd holds the state of the entire cluster (including Secrets) its disk should especially be encrypted at rest.</td>
</tr>
</tbody>
</table>

-->
<h3 id="infrastructure-security">基础设施安全</h3>
<p>关于在 Kubernetes 集群中保护你的基础设施的建议：</p>





<table><caption style="display: none;">基础设施安全</caption>
<thead>
<tr>
<th>Kubetnetes 基础架构关注领域</th>
<th>建议</th>
</tr>
</thead>
<tbody>
<tr>
<td>通过网络访问 API 服务（控制平面）</td>
<td>所有对 Kubernetes 控制平面的访问不允许在 Internet 上公开，同时应由网络访问控制列表控制，该列表包含管理集群所需的 IP 地址集。</td>
</tr>
<tr>
<td>通过网络访问 Node（节点）</td>
<td>节点应配置为 <em>仅能</em> 从控制平面上通过指定端口来接受（通过网络访问控制列表）连接，以及接受 NodePort 和 LoadBalancer 类型的 Kubernetes 服务连接。如果可能的话，这些节点不应完全暴露在公共互联网上。</td>
</tr>
<tr>
<td>Kubernetes 访问云提供商的 API</td>
<td>每个云提供商都需要向 Kubernetes 控制平面和节点授予不同的权限集。为集群提供云提供商访问权限时，最好遵循对需要管理的资源的<a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">最小特权原则</a>。<a href="https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md#iam-roles">Kops 文档</a>提供有关 IAM 策略和角色的信息。</td>
</tr>
<tr>
<td>访问 etcd</td>
<td>对 etcd（Kubernetes 的数据存储）的访问应仅限于控制平面。根据配置情况，你应该尝试通过 TLS 来使用 etcd。更多信息可以在 <a href="https://github.com/etcd-io/etcd/tree/master/Documentation">etcd 文档</a>中找到。</td>
</tr>
<tr>
<td>etcd 加密</td>
<td>在所有可能的情况下，最好对所有驱动器进行静态数据加密，并且由于 etcd 拥有整个集群的状态（包括机密信息），因此其磁盘更应该进行静态数据加密。</td>
</tr>
</tbody>
</table>

<!--
## Cluster

There are two areas of concern for securing Kubernetes:

* Securing the cluster components that are configurable
* Securing the applications which run in the cluster
-->
<h2 id="集群">集群</h2>
<p>保护 Kubernetes 有两个方面需要注意：</p>
<ul>
<li>保护可配置的集群组件</li>
<li>保护在集群中运行的应用程序</li>
</ul>
<!--
### Components of the Cluster {#cluster-components}

If you want to protect your cluster from accidental or malicious access and adopt
good information practices, read and follow the advice about
[securing your cluster](/docs/tasks/administer-cluster/securing-a-cluster/).
-->
<h3 id="cluster-components">集群组件</h3>
<p>如果想要保护集群免受意外或恶意的访问，采取良好的信息管理实践，请阅读并遵循有关<a href="/zh/docs/tasks/administer-cluster/securing-a-cluster/">保护集群</a>的建议。</p>
<!--
### Components in the cluster (your application) {#cluster-applications}

Depending on the attack surface of your application, you may want to focus on specific
aspects of security. For example: If you are running a service (Service A) that is critical
in a chain of other resources and a separate workload (Service B) which is
vulnerable to a resource exhaustion attack, then the risk of compromising Service A
is high if you do not limit the resources of Service B. The following table lists
areas of security concerns and recommendations for securing workloads running in Kubernetes:

Area of Concern for Workload Security | Recommendation |
------------------------------ | --------------------- |
RBAC Authorization (Access to the Kubernetes API) | https://kubernetes.io/docs/reference/access-authn-authz/rbac/
Authentication | https://kubernetes.io/docs/concepts/security/controlling-access/
Application secrets management (and encrypting them in etcd at rest) | https://kubernetes.io/docs/concepts/configuration/secret/ <br> https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
Ensuring that pods meet defined Pod Security Standards | https://kubernetes.io/docs/concepts/security/pod-security-standards/#policy-instantiation
Quality of Service (and Cluster resource management) | https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/
Network Policies | https://kubernetes.io/docs/concepts/services-networking/network-policies/
TLS for Kubernetes Ingress | https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
-->
<h3 id="cluster-applications">集群中的组件（您的应用）</h3>
<p>根据您的应用程序的受攻击面，您可能需要关注安全性的特定面，比如：
如果您正在运行中的一个服务（A 服务）在其他资源链中很重要，并且所运行的另一工作负载（服务 B）
容易受到资源枯竭的攻击，则如果你不限制服务 B 的资源的话，损害服务 A 的风险就会很高。
下表列出了安全性关注的领域和建议，用以保护 Kubernetes 中运行的工作负载：</p>
<table>
<thead>
<tr>
<th>工作负载安全性关注领域</th>
<th>建议</th>
</tr>
</thead>
<tbody>
<tr>
<td>RBAC 授权(访问 Kubernetes API)</td>
<td><a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/">https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/</a></td>
</tr>
<tr>
<td>认证方式</td>
<td><a href="https://kubernetes.io/zh/docs/concepts/security/controlling-access/">https://kubernetes.io/zh/docs/concepts/security/controlling-access/</a></td>
</tr>
<tr>
<td>应用程序 Secret 管理 (并在 etcd 中对其进行静态数据加密)</td>
<td><a href="https://kubernetes.io/zh/docs/concepts/configuration/secret/">https://kubernetes.io/zh/docs/concepts/configuration/secret/</a> <br> <a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/encrypt-data/">https://kubernetes.io/zh/docs/tasks/administer-cluster/encrypt-data/</a></td>
</tr>
<tr>
<td>确保 Pod 符合定义的 Pod 安全标准</td>
<td><a href="https://kubernetes.io/zh/docs/concepts/security/pod-security-standards/#policy-instantiation">https://kubernetes.io/zh/docs/concepts/security/pod-security-standards/#policy-instantiation</a></td>
</tr>
<tr>
<td>服务质量（和集群资源管理）</td>
<td><a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/">https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/</a></td>
</tr>
<tr>
<td>网络策略</td>
<td><a href="https://kubernetes.io/zh/docs/concepts/services-networking/network-policies/">https://kubernetes.io/zh/docs/concepts/services-networking/network-policies/</a></td>
</tr>
<tr>
<td>Kubernetes Ingress 的 TLS 支持</td>
<td><a href="https://kubernetes.io/zh/docs/concepts/services-networking/ingress/#tls">https://kubernetes.io/zh/docs/concepts/services-networking/ingress/#tls</a></td>
</tr>
</tbody>
</table>
<!--
## Container

Container security is outside the scope of this guide. Here are general recommendations and
links to explore this topic:

Area of Concern for Containers | Recommendation |
------------------------------ | -------------- |
Container Vulnerability Scanning and OS Dependency Security | As part of an image build step, you should scan your containers for known vulnerabilities.
Image Signing and Enforcement | Sign container images to maintain a system of trust for the content of your containers.
Disallow privileged users | When constructing containers, consult your documentation for how to create users inside of the containers that have the least level of operating system privilege necessary in order to carry out the goal of the container.
Use container runtime with stronger isolation | Select [container runtime classes](/docs/concepts/containers/runtime-class/) that provide stronger isolation
-->
<h2 id="容器">容器</h2>
<p>容器安全性不在本指南的探讨范围内。下面是一些探索此主题的建议和连接：</p>
<table>
<thead>
<tr>
<th>容器关注领域</th>
<th>建议</th>
</tr>
</thead>
<tbody>
<tr>
<td>容器漏洞扫描和操作系统依赖安全性</td>
<td>作为镜像构建的一部分，您应该扫描您的容器里的已知漏洞。</td>
</tr>
<tr>
<td>镜像签名和执行</td>
<td>对容器镜像进行签名，以维护对容器内容的信任。</td>
</tr>
<tr>
<td>禁止特权用户</td>
<td>构建容器时，请查阅文档以了解如何在具有最低操作系统特权级别的容器内部创建用户，以实现容器的目标。</td>
</tr>
<tr>
<td>使用带有较强隔离能力的容器运行时</td>
<td>选择提供较强隔离能力的<a href="/zh/docs/concepts/containers/runtime-class/">容器运行时类</a>。</td>
</tr>
</tbody>
</table>
<!--
## Code

Application code is one of the primary attack surfaces over which you have the most control.
While securing application code is outside of the Kubernetes security topic, here
are recommendations to protect application code:
-->
<h2 id="代码">代码</h2>
<p>应用程序代码是您最能够控制的主要攻击面之一，虽然保护应用程序代码不在 Kubernetes 安全主题范围内，但以下是保护应用程序代码的建议：</p>
<!--
### Code security






<table><caption style="display: none;">Code security</caption>
<thead>
<tr>
<th>Area of Concern for Code</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Access over TLS only</td>
<td>If your code needs to communicate by TCP, perform a TLS handshake with the client ahead of time. With the exception of a few cases, encrypt everything in transit. Going one step further, it's a good idea to encrypt network traffic between services. This can be done through a process known as mutual TLS authentication or <a href="https://en.wikipedia.org/wiki/Mutual_authentication">mTLS</a> which performs a two sided verification of communication between two certificate holding services.</td>
</tr>
<tr>
<td>Limiting port ranges of communication</td>
<td>This recommendation may be a bit self-explanatory, but wherever possible you should only expose the ports on your service that are absolutely essential for communication or metric gathering.</td>
</tr>
<tr>
<td>3rd Party Dependency Security</td>
<td>It is a good practice to regularly scan your application's third party libraries for known security vulnerabilities. Each programming language has a tool for performing this check automatically.</td>
</tr>
<tr>
<td>Static Code Analysis</td>
<td>Most languages provide a way for a snippet of code to be analyzed for any potentially unsafe coding practices. Whenever possible you should perform checks using automated tooling that can scan codebases for common security errors. Some of the tools can be found at: <a href="https://owasp.org/www-community/Source_Code_Analysis_Tools">https://owasp.org/www-community/Source_Code_Analysis_Tools</a></td>
</tr>
<tr>
<td>Dynamic probing attacks</td>
<td>There are a few automated tools that you can run against your service to try some of the well known service attacks. These include SQL injection, CSRF, and XSS. One of the most popular dynamic analysis tools is the <a href="https://owasp.org/www-project-zap/">OWASP Zed Attack proxy</a> tool.</td>
</tr>
</tbody>
</table>

-->
<h3 id="代码安全性">代码安全性</h3>





<table><caption style="display: none;">代码安全</caption>
<thead>
<tr>
<th>代码关注领域</th>
<th>建议</th>
</tr>
</thead>
<tbody>
<tr>
<td>仅通过 TLS 访问</td>
<td>如果您的代码需要通过 TCP 通信，请提前与客户端执行 TLS 握手。除少数情况外，请加密传输中的所有内容。更进一步，加密服务之间的网络流量是一个好主意。这可以通过被称为双向 TLS 或 <a href="https://en.wikipedia.org/wiki/Mutual_authentication">mTLS</a> 的过程来完成，该过程对两个证书持有服务之间的通信执行双向验证。</td>
</tr>
<tr>
<td>限制通信端口范围</td>
<td>此建议可能有点不言自明，但是在任何可能的情况下，你都只应公开服务上对于通信或度量收集绝对必要的端口。</td>
</tr>
<tr>
<td>第三方依赖性安全</td>
<td>最好定期扫描应用程序的第三方库以了解已知的安全漏洞。每种编程语言都有一个自动执行此检查的工具。</td>
</tr>
<tr>
<td>静态代码分析</td>
<td>大多数语言都提供给了一种方法，来分析代码段中是否存在潜在的不安全的编码实践。只要有可能，你都应该使用自动工具执行检查，该工具可以扫描代码库以查找常见的安全错误，一些工具可以在以下连接中找到：https://owasp.org/www-community/Source_Code_Analysis_Tools</td>
</tr>
<tr>
<td>动态探测攻击</td>
<td>您可以对服务运行一些自动化工具，来尝试一些众所周知的服务攻击。这些攻击包括 SQL 注入、CSRF 和 XSS。<a href="https://owasp.org/www-project-zap/">OWASP Zed Attack</a> 代理工具是最受欢迎的动态分析工具之一。</td>
</tr>
</tbody>
</table>

<h2 id="what-s-next">What's next</h2>
<!--
Learn about related Kubernetes security topics:

* [Pod security standards](/docs/concepts/security/pod-security-standards/)
* [Network policies for Pods](/docs/concepts/services-networking/network-policies/)
* [Controlling Access to the Kubernetes API](/docs/concepts/security/controlling-access)
* [Securing your cluster](/docs/tasks/administer-cluster/securing-a-cluster/)
* [Data encryption in transit](/docs/tasks/tls/managing-tls-in-a-cluster/) for the control plane
* [Data encryption at rest](/docs/tasks/administer-cluster/encrypt-data/)
* [Secrets in Kubernetes](/docs/concepts/configuration/secret/)
* [Runtime class](/docs/concepts/containers/runtime-class)
-->
<p>学习了解相关的 Kubernetes 安全主题：</p>
<ul>
<li><a href="/zh/docs/concepts/security/pod-security-standards/">Pod 安全标准</a></li>
<li><a href="/zh/docs/concepts/services-networking/network-policies/">Pod 的网络策略</a></li>
<li><a href="/zh/docs/concepts/security/controlling-access/">控制对 Kubernetes API 的访问</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/securing-a-cluster/">保护您的集群</a></li>
<li>为控制面<a href="/zh/docs/tasks/tls/managing-tls-in-a-cluster/">加密通信中的数据</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/encrypt-data/">加密静止状态的数据</a></li>
<li><a href="/zh/docs/concepts/configuration/secret/">Kubernetes 中的 Secret</a></li>
<li><a href="/zh/docs/concepts/containers/runtime-class">运行时类</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1fb24c1dd155f43849da490a74c4b8c5">8.2 - Pod 安全性标准</h1>
    
	<!--
reviewers:
- tallclair
title: Pod Security Standards
content_type: concept
weight: 10
-->
<!-- overview -->
<!--
The Pod Security Standards define three different _policies_ to broadly cover the security
spectrum. These policies are _cumulative_ and range from highly-permissive to highly-restrictive.
This guide outlines the requirements of each policy.

| Profile | Description |
| ------ | ----------- |
| <strong style="white-space: nowrap">Privileged</strong> | Unrestricted policy, providing the widest possible level of permissions. This policy allows for known privilege escalations. |
| <strong style="white-space: nowrap">Baseline</strong> | Minimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) Pod configuration. |
| <strong style="white-space: nowrap">Restricted</strong> | Heavily restricted policy, following current Pod hardening best practices. |
-->
<p>Pod 安全性标准定义了三种不同的 <em>策略（Policy）</em>，以广泛覆盖安全应用场景。
这些策略是 <em>渐进式的（Cumulative）</em>，安全级别从高度宽松至高度受限。
本指南概述了每个策略的要求。</p>
<table>
<thead>
<tr>
<th>Profile</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong style="white-space: nowrap">Privileged</strong></td>
<td>不受限制的策略，提供最大可能范围的权限许可。此策略允许已知的特权提升。</td>
</tr>
<tr>
<td><strong style="white-space: nowrap">Baseline</strong></td>
<td>限制性最弱的策略，禁止已知的策略提升。允许使用默认的（规定最少）Pod 配置。</td>
</tr>
<tr>
<td><strong style="white-space: nowrap">Restricted</strong></td>
<td>限制性非常强的策略，遵循当前的保护 Pod 的最佳实践。</td>
</tr>
</tbody>
</table>
<!-- body -->
<!--
## Profile Details

### Privileged
-->
<h2 id="profile-details">Profile 细节   </h2>
<h3 id="privileged">Privileged</h3>
<!--
**The _Privileged_ policy is purposely-open, and entirely unrestricted.** This type of policy is
typically aimed at system- and infrastructure-level workloads managed by privileged, trusted users.

The privileged policy is defined by an absence of restrictions. For allow-by-default enforcement
mechanisms (such as gatekeeper), the privileged profile may be an absence of applied constraints
rather than an instantiated policy. In contrast, for a deny-by-default mechanism (such as Pod
Security Policy) the privileged policy should enable all controls (disable all restrictions).
-->
<p><strong><em>Privileged</em> 策略是有目的地开放且完全无限制的策略。</strong>
此类策略通常针对由特权较高、受信任的用户所管理的系统级或基础设施级负载。</p>
<p>Privileged 策略定义中限制较少。对于默认允许（Allow-by-default）实施机制（例如 gatekeeper），
Privileged 框架可能意味着不应用任何约束而不是实施某策略实例。
与此不同，对于默认拒绝（Deny-by-default）实施机制（如 Pod 安全策略）而言，
Privileged 策略应该默认允许所有控制（即，禁止所有限制）。</p>
<h3 id="baseline">Baseline</h3>
<!--
**The _Baseline_ policy is aimed at ease of adoption for common containerized workloads while
preventing known privilege escalations.** This policy is targeted at application operators and
developers of non-critical applications. The following listed controls should be
enforced/disallowed:
-->
<p><strong><em>Baseline</em> 策略的目标是便于常见的容器化应用采用，同时禁止已知的特权提升。</strong>
此策略针对的是应用运维人员和非关键性应用的开发人员。
下面列举的控制应该被实施（禁止）：</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In this table, wildcards (`*`) indicate all elements in a list. For example,
`spec.containers[*].securityContext` refers to the Security Context object for _all defined
containers_. If any of the listed containers fails to meet the requirements, the entire pod will
fail validation.
-->
<p>在下述表格中，通配符（<code>*</code>）意味着一个列表中的所有元素。
例如 <code>spec.containers[*].securityContext</code> 表示 <em>所定义的所有容器</em> 的安全性上下文对象。
如果所列出的任一容器不能满足要求，整个 Pod 将无法通过校验。
</div>
<table>
	<!-- caption style="display:none">Baseline policy specification</caption -->
	<caption style="display:none">Baseline 策略规范</caption>
	<tbody>
		<tr>
			<td>控制（Control）</td>
			<td>策略（Policy）</td>
		</tr>
    <tr>
			<!-- <td style="white-space: nowrap">HostProcess</td> -->
      <td style="white-space: nowrap">HostProcess</td>
      <!-- <td>
				<p>Windows pods offer the ability to run <a href="/docs/tasks/configure-pod-container/create-hostprocess-pod">HostProcess containers</a> which enables privileged access to the Windows node. Privileged access to the host is disallowed in the baseline policy. HostProcess pods are an <strong>alpha</strong> feature as of Kubernetes <strong>v1.22</strong>.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.securityContext.windowsOptions.hostProcess</code></li>
					<li><code>spec.containers[*].securityContext.windowsOptions.hostProcess</code></li>
					<li><code>spec.initContainers[*].securityContext.windowsOptions.hostProcess</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.windowsOptions.hostProcess</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li><code>false</code></li>
				</ul>
			</td> -->
			<td>
				<p>Windows Pod 提供了运行
         <a href="/zh/docs/tasks/configure-pod-container/create-hostprocess-pod">HostProcess 容器</a> 的能力，
         这使得对 Windows 节点的特权访问成为可能。 
         基线策略中对宿主的特权访问是被禁止的。 
         HostProcess Pod 是 Kubernetes <strong>v1.22</strong> 版本的
          <strong>alpha</strong> 特性。</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.securityContext.windowsOptions.hostProcess</code></li>
					<li><code>spec.containers[*].securityContext.windowsOptions.hostProcess</code></li>
					<li><code>spec.initContainers[*].securityContext.windowsOptions.hostProcess</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.windowsOptions.hostProcess</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/nil</li>
					<li><code>false</code></li>
				</ul>
			</td>
		</tr>
		<tr>
			<!-- <td style="white-space: nowrap">Host Namespaces</td> -->
			<td style="white-space: nowrap">宿主名字空间</td>
			<!-- 
      <td>
				<p>Sharing the host namespaces must be disallowed.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.hostNetwork</code></li>
					<li><code>spec.hostPID</code></li>
					<li><code>spec.hostIPC</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li><code>false</code></li>
				</ul>
			</td>
      -->
			<td>
        <p>必须禁止共享宿主名字空间。</p>
        <p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.hostNetwork</code></li>
					<li><code>spec.hostPID</code></li>
					<li><code>spec.hostIPC</code></li>
				</ul>
        <p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/nil</li>
					<li><code>false</code></li>
				</ul>
			</td>
		</tr>
		<tr>
			<!-- <td style="white-space: nowrap">Privileged Containers</td> -->
			<td style="white-space: nowrap">特权容器</td>
			<!-- <td>
				<p>Privileged Pods disable most security mechanisms and must be disallowed.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.privileged</code></li>
					<li><code>spec.initContainers[*].securityContext.privileged</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.privileged</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li><code>false</code></li>
				</ul>
			</td> -->
			<td>
        <p>特权 Pod 关闭了大多数安全性机制，必须被禁止。</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.privileged</code></li>
					<li><code>spec.initContainers[*].securityContext.privileged</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.privileged</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/nil</li>
					<li><code>false</code></li>
				</ul>
			</td>
		</tr>
		<tr>
			<!-- <td style="white-space: nowrap">Capabilities</td> -->
			<td style="white-space: nowrap">权能</td>
			<!-- <td>
				<p>Adding additional capabilities beyond those listed below must be disallowed.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.capabilities.add</code></li>
					<li><code>spec.initContainers[*].securityContext.capabilities.add</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li><code>AUDIT_WRITE</code></li>
					<li><code>CHOWN</code></li>
					<li><code>DAC_OVERRIDE</code></li>
					<li><code>FOWNER</code></li>
					<li><code>FSETID</code></li>
					<li><code>KILL</code></li>
					<li><code>MKNOD</code></li>
					<li><code>NET_BIND_SERVICE</code></li>
					<li><code>SETFCAP</code></li>
					<li><code>SETGID</code></li>
					<li><code>SETPCAP</code></li>
					<li><code>SETUID</code></li>
					<li><code>SYS_CHROOT</code></li>
				</ul>
			</td> -->
			<td>
        <p>必须禁止添加除下列字段之外的权能。</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.capabilities.add</code></li>
					<li><code>spec.initContainers[*].securityContext.capabilities.add</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li><code>AUDIT_WRITE</code></li>
					<li><code>CHOWN</code></li>
					<li><code>DAC_OVERRIDE</code></li>
					<li><code>FOWNER</code></li>
					<li><code>FSETID</code></li>
					<li><code>KILL</code></li>
					<li><code>MKNOD</code></li>
					<li><code>NET_BIND_SERVICE</code></li>
					<li><code>SETFCAP</code></li>
					<li><code>SETGID</code></li>
					<li><code>SETPCAP</code></li>
					<li><code>SETUID</code></li>
					<li><code>SYS_CHROOT</code></li>
				</ul>
			</td>
		</tr>
		<tr>
			<!-- <td style="white-space: nowrap">HostPath Volumes</td>-->
			<td style="white-space: nowrap">HostPath 卷</td>
			<!-- <td>
				<p>HostPath volumes must be forbidden.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.volumes[*].hostPath</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
				</ul>
			</td> -->
			<td>
        <p>必须禁止 HostPath 卷。</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.volumes[*].hostPath</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/nil</li>
				</ul>
			</td>
		</tr>
		<tr>
			<!-- <td style="white-space: nowrap">Host Ports</td> -->
			<td style="white-space: nowrap">宿主端口</td>
			<!-- <td>
				<p>HostPorts should be disallowed, or at minimum restricted to a known list.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.containers[*].ports[*].hostPort</code></li>
					<li><code>spec.initContainers[*].ports[*].hostPort</code></li>
					<li><code>spec.ephemeralContainers[*].ports[*].hostPort</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li>Known list</li>
					<li><code>0</code></li>
				</ul>
			</td>-->
			<td>
        <p>应禁止使用宿主端口，或者至少限定为已知列表。</p>
        <p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.containers[*].ports[*].hostPort</code></li>
					<li><code>spec.initContainers[*].ports[*].hostPort</code></li>
					<li><code>spec.ephemeralContainers[*].ports[*].hostPort</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/nil</li>
					<li>已知列表</li>
					<li><code>0</code></li>
				</ul>
			</td>
		</tr>
		<tr>
			<!-- <td style="white-space: nowrap">AppArmor</td> -->
			<td style="white-space: nowrap">AppArmor</td>
			<!-- <td>
				<p>On supported hosts, the <code>runtime/default</code> AppArmor profile is applied by default. The baseline policy should prevent overriding or disabling the default AppArmor profile, or restrict overrides to an allowed set of profiles.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>metadata.annotations["container.apparmor.security.beta.kubernetes.io/*"]</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li><code>runtime/default</code></li>
					<li><code>localhost/*</code></li>
				</ul>
			</td> -->
			<td>
        <p>在受支持的主机上，默认使用 <code>runtime/default</code> AppArmor Profile。
				基线策略应避免覆盖或者禁用默认策略，以及限制覆盖一些 Profile 集合的权限。</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>metadata.annotations["container.apparmor.security.beta.kubernetes.io/*"]</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/nil</li>
					<li><code>runtime/default</code></li>
					<li><code>localhost/*</code></li>
				</ul>
			</td>
		</tr>
		<tr>
			<!-- <td style="white-space: nowrap">SELinux</td> -->
			<td style="white-space: nowrap">SELinux</td>
			<!-- <td>
				<p>Setting the SELinux type is restricted, and setting a custom SELinux user or role option is forbidden.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.securityContext.seLinuxOptions.type</code></li>
					<li><code>spec.containers[*].securityContext.seLinuxOptions.type</code></li>
					<li><code>spec.initContainers[*].securityContext.seLinuxOptions.type</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.type</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/""</li>
					<li><code>container_t</code></li>
					<li><code>container_init_t</code></li>
					<li><code>container_kvm_t</code></li>
				</ul>
				<hr />
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.securityContext.seLinuxOptions.user</code></li>
					<li><code>spec.containers[*].securityContext.seLinuxOptions.user</code></li>
					<li><code>spec.initContainers[*].securityContext.seLinuxOptions.user</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.user</code></li>
					<li><code>spec.securityContext.seLinuxOptions.role</code></li>
					<li><code>spec.containers[*].securityContext.seLinuxOptions.role</code></li>
					<li><code>spec.initContainers[*].securityContext.seLinuxOptions.role</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.role</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/""</li>
				</ul>
			</td>-->
			<td>
        <p>设置 SELinux 类型的操作是被限制的，设置自定义的 SELinux 用户或角色选项是被禁止的。</p>
        <p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.securityContext.seLinuxOptions.type</code></li>
					<li><code>spec.containers[*].securityContext.seLinuxOptions.type</code></li>
					<li><code>spec.initContainers[*].securityContext.seLinuxOptions.type</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.type</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/""</li>
					<li><code>container_t</code></li>
					<li><code>container_init_t</code></li>
					<li><code>container_kvm_t</code></li>
				</ul>
				<hr />
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.securityContext.seLinuxOptions.user</code></li>
					<li><code>spec.containers[*].securityContext.seLinuxOptions.user</code></li>
					<li><code>spec.initContainers[*].securityContext.seLinuxOptions.user</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.user</code></li>
					<li><code>spec.securityContext.seLinuxOptions.role</code></li>
					<li><code>spec.containers[*].securityContext.seLinuxOptions.role</code></li>
					<li><code>spec.initContainers[*].securityContext.seLinuxOptions.role</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.role</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/""</li>
				</ul>
			</td>
		</tr>
		<tr>
			<!-- <td style="white-space: nowrap"><code>/proc</code> Mount Type</td> -->
			<td style="white-space: nowrap"><code>/proc</code> 挂载类型</td>
			<!-- <td>
				<p>The default <code>/proc</code> masks are set up to reduce attack surface, and should be required.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.procMount</code></li>
					<li><code>spec.initContainers[*].securityContext.procMount</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.procMount</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li><code>Default</code></li>
				</ul>
			</td> -->
			<td>
        <p>要求使用默认的 <code>/proc</code> 掩码以减小攻击面。</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.procMount</code></li>
					<li><code>spec.initContainers[*].securityContext.procMount</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.procMount</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/nil</li>
					<li><code>Default</code></li>
				</ul>
			</td>
		</tr>
    <tr>
  			<td>Seccomp</td>
  			<!-- <td>
  				<p>Seccomp profile must not be explicitly set to <code>Unconfined</code>.</p>
  				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.securityContext.seccompProfile.type</code></li>
					<li><code>spec.containers[*].securityContext.seccompProfile.type</code></li>
					<li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li><code>RuntimeDefault</code></li>
					<li><code>Localhost</code></li>
				</ul>
  			</td> -->
        <td>
  				<p>Seccomp Profile 禁止被显式设置为 <code>Unconfined</code>。</p>
  				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.securityContext.seccompProfile.type</code></li>
					<li><code>spec.containers[*].securityContext.seccompProfile.type</code></li>
					<li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/nil</li>
					<li><code>RuntimeDefault</code></li>
					<li><code>Localhost</code></li>
				</ul>
  			</td>
  		</tr>
		<tr>
			<td>Sysctls</td>
			<!-- <td>
				<p>Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed "safe" subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.securityContext.sysctls[*].name</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li><code>kernel.shm_rmid_forced</code></li>
					<li><code>net.ipv4.ip_local_port_range</code></li>
					<li><code>net.ipv4.ip_unprivileged_port_start</code></li>
					<li><code>net.ipv4.tcp_syncookies</code></li>
					<li><code>net.ipv4.ping_group_range</code></li>
				</ul>
			</td> -->
			<td>
        <p>Sysctls 可以禁用安全机制或影响宿主上所有容器，因此除了若干“安全”的子集之外，应该被禁止。
				如果某 sysctl 是受容器或 Pod 的名字空间限制，且与节点上其他 Pod 或进程相隔离，可认为是安全的。</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.securityContext.sysctls[*].name</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/nil</li>
					<li><code>kernel.shm_rmid_forced</code></li>
					<li><code>net.ipv4.ip_local_port_range</code></li>
					<li><code>net.ipv4.ip_unprivileged_port_start</code></li>
					<li><code>net.ipv4.tcp_syncookies</code></li>
					<li><code>net.ipv4.ping_group_range</code></li>
				</ul>
			</td>
		</tr>
	</tbody>
</table>
<h3 id="restricted">Restricted</h3>
<!--
**The _Restricted_ policy is aimed at enforcing current Pod hardening best practices, at the
expense of some compatibility.** It is targeted at operators and developers of security-critical
applications, as well as lower-trust users. The following listed controls should be
enforced/disallowed:

In this table, wildcards (`*`) indicate all elements in a list. For example, 
`spec.containers[*].securityContext` refers to the Security Context object for _all defined
containers_. If any of the listed containers fails to meet the requirements, the entire pod will
fail validation.
-->
<p><strong><em>Restricted</em> 策略旨在实施当前保护 Pod 的最佳实践，尽管这样作可能会牺牲一些兼容性。</strong>
该类策略主要针对运维人员和安全性很重要的应用的开发人员，以及不太被信任的用户。
下面列举的控制需要被实施（禁止）：</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在下述表格中，通配符（<code>*</code>）意味着一个列表中的所有元素。
例如 <code>spec.containers[*].securityContext</code> 表示 <em>所定义的所有容器</em> 的安全性上下文对象。
如果所列出的任一容器不能满足要求，整个 Pod 将无法通过校验。
</div>
<table>
	<!-- caption style="display:none">Restricted policy specification</caption -->
	<caption style="display:none">Restricted 策略规范</caption>
	<tbody>
		<tr>
			<!-- td><strong>Control</strong></td -->
			<td width="30%"><strong>控制（Control）</strong></td>
			<!-- td><strong>Policy</strong></td -->
			<td><strong>策略（Policy）</strong></td>
		</tr>
		<tr>
			<!-- <td colspan="2"><em>Everything from the baseline profile.</em></td> -->
			<td colspan="2"><em>基线策略的所有要求。</em></td>
		</tr>
		<tr>
      <!-- <td style="white-space: nowrap">Volume Types</td>
			<td>
				<p>In addition to restricting HostPath volumes, the restricted policy limits usage of non-core volume types to those defined through PersistentVolumes.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.volumes[*]</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				Every item in the <code>spec.volumes[*]</code> list must set one of the following fields to a non-null value:
				<ul>
					<li><code>spec.volumes[*].configMap</code></li>
					<li><code>spec.volumes[*].csi</code></li>
					<li><code>spec.volumes[*].downwardAPI</code></li>
					<li><code>spec.volumes[*].emptyDir</code></li>
					<li><code>spec.volumes[*].ephemeral</code></li>
					<li><code>spec.volumes[*].persistentVolumeClaim</code></li>
					<li><code>spec.volumes[*].projected</code></li>
					<li><code>spec.volumes[*].secret</code></li>
				</ul>
			</td> -->
			<td>卷类型</td>
			<td>
        <p>除了限制 HostPath 卷之外，此类策略还限制可以通过 PersistentVolumes 定义的非核心卷类型。</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.volumes[*]</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<code>spec.volumes[*]</code> 列表中的每个条目必须将下面字段之一设置为非空值：
				<ul>
					<li><code>spec.volumes[*].configMap</code></li>
					<li><code>spec.volumes[*].csi</code></li>
					<li><code>spec.volumes[*].downwardAPI</code></li>
					<li><code>spec.volumes[*].emptyDir</code></li>
					<li><code>spec.volumes[*].ephemeral</code></li>
					<li><code>spec.volumes[*].persistentVolumeClaim</code></li>
					<li><code>spec.volumes[*].projected</code></li>
					<li><code>spec.volumes[*].secret</code></li>
				</ul>
			</td>
		</tr>
		<tr>
      <!-- <td style="white-space: nowrap">Privilege Escalation (v1.8+)</td>
			<td>
				<p>Privilege escalation (such as via set-user-ID or set-group-ID file mode) should not be allowed.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.allowPrivilegeEscalation</code></li>
					<li><code>spec.initContainers[*].securityContext.allowPrivilegeEscalation</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.allowPrivilegeEscalation</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li><code>false</code></li>
				</ul>
			</td> -->
			<td style="white-space: nowrap">特权提升（v1.8+）</td>
			<td>
        <p>禁止（通过 SetUID 或 SetGID 文件模式）获得特权提升。</p>
				<br>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.allowPrivilegeEscalation</code></li>
					<li><code>spec.initContainers[*].securityContext.allowPrivilegeEscalation</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.allowPrivilegeEscalation</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li><code>false</code></li>
				</ul>
			</td>
		</tr>
		<tr>
      <!-- <td style="white-space: nowrap">Running as Non-root</td> -->
			<td style="white-space: nowrap">以非 root 账号运行 </td>
      <!-- <td>
				<p>Containers must be required to run as non-root users.</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.securityContext.runAsNonRoot</code></li>
					<li><code>spec.containers[*].securityContext.runAsNonRoot</code></li>
					<li><code>spec.initContainers[*].securityContext.runAsNonRoot</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.runAsNonRoot</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li><code>true</code></li>
				</ul>
				<small>
					The container fields may be undefined/<code>nil</code> if the pod-level
					<code>spec.securityContext.runAsNonRoot</code> is set to <code>true</code>.
				</small>
			</td> -->
			<td>
        <p>必须要求容器以非 root 用户运行。</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.securityContext.runAsNonRoot</code></li>
					<li><code>spec.containers[*].securityContext.runAsNonRoot</code></li>
					<li><code>spec.initContainers[*].securityContext.runAsNonRoot</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.runAsNonRoot</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li><code>true</code></li>
				</ul>
				<small>
					如果 Pod 级别 <code>spec.securityContext.runAsNonRoot</code> 设置为 
          <code>true</code>，则允许容器组的安全上下文字段设置为 未定义/<code>nil</code>。
				</small>
			</td>
		</tr>
		<tr>
			<!-- <td style="white-space: nowrap">Running as Non-root user (v1.23+)</td> -->
			<td style="white-space: nowrap">非 root 用户（v1.23+）</td>
			<td>
				<!--
				<p>Containers must not set <tt>runAsUser</tt> to 0</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.securityContext.runAsUser</code></li>
					<li><code>spec.containers[*].securityContext.runAsUser</code></li>
					<li><code>spec.initContainers[*].securityContext.runAsUser</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.runAsUser</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>any non-zero value</li>
					<li><code>undefined/null</code></li>
				</ul>
			</td> -->
				<p>Containers 不可以将 <tt>runAsUser</tt> 设置为 0</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.securityContext.runAsUser</code></li>
					<li><code>spec.containers[*].securityContext.runAsUser</code></li>
					<li><code>spec.initContainers[*].securityContext.runAsUser</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.runAsUser</code></li>
				</ul>
				<p><strong>允许的字段</strong></p>
				<ul>
					<li>any non-zero value</li>
					<li><code>未定义/空值</code></li>
				</ul>
			</td>
		</tr>
		<tr>
			<td style="white-space: nowrap">Seccomp (v1.19+)</td>
			<td>
				<!-- <td>
  				<p>Seccomp profile must be explicitly set to one of the allowed values. Both the <code>Unconfined</code> profile and the <em>absence</em> of a profile are prohibited.</p>
  				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.securityContext.seccompProfile.type</code></li>
					<li><code>spec.containers[*].securityContext.seccompProfile.type</code></li>
					<li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li><code>RuntimeDefault</code></li>
					<li><code>Localhost</code></li>
				</ul>
				<small>
					The container fields may be undefined/<code>nil</code> if the pod-level
					<code>spec.securityContext.seccompProfile.type</code> field is set appropriately.
					Conversely, the pod-level field may be undefined/<code>nil</code> if _all_ container-
					level fields are set.
				</small>
  			</td> -->
        <p>Seccomp Profile 必须被显式设置成一个允许的值。禁止使用 <code>Unconfined</code> 
        Profile 或者指定 <em>不存在的</em> Profile。</p>
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.securityContext.seccompProfile.type</code></li>
					<li><code>spec.containers[*].securityContext.seccompProfile.type</code></li>
					<li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li><code>RuntimeDefault</code></li>
					<li><code>Localhost</code></li>
				</ul>
				<small>
					如果 Pod 级别的 <code>spec.securityContext.seccompProfile.type</code> 
          已设置得当，容器级别的安全上下文字段可以为 未定义/<code>nil</code>。
          反过来说，如果 _所有的_ 容器级别的安全上下文字段已设置，则 Pod 级别的字段可为 未定义/<code>nil</code>。 
				</small>
			</td>
		</tr>
    <tr>
			<!-- <td style="white-space: nowrap">Capabilities (v1.22+)</td> -->
      <td style="white-space: nowrap">权能（v1.22+）</td>
      <!-- <td>
				<p>
					Containers must drop <code>ALL</code> capabilities, and are only permitted to add back
					the <code>NET_BIND_SERVICE</code> capability.
				</p>
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.capabilities.drop</code></li>
					<li><code>spec.initContainers[*].securityContext.capabilities.drop</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.capabilities.drop</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Any list of capabilities that includes <code>ALL</code></li>
				</ul>
				<hr />
				<p><strong>Restricted Fields</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.capabilities.add</code></li>
					<li><code>spec.initContainers[*].securityContext.capabilities.add</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li>
				</ul>
				<p><strong>Allowed Values</strong></p>
				<ul>
					<li>Undefined/nil</li>
					<li><code>NET_BIND_SERVICE</code></li>
				</ul>
			</td> -->
			<td>
        <p>
					容器组必须弃用 <code>ALL</code> 权能，并且只允许添加 <code>NET_BIND_SERVICE</code> 权能。
				</p>
        <p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.capabilities.drop</code></li>
					<li><code>spec.initContainers[*].securityContext.capabilities.drop</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.capabilities.drop</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>包含 <code>ALL</code> 的任何一种权能列表。</li>
				</ul>
				<hr />
				<p><strong>限制的字段</strong></p>
				<ul>
					<li><code>spec.containers[*].securityContext.capabilities.add</code></li>
					<li><code>spec.initContainers[*].securityContext.capabilities.add</code></li>
					<li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li>
				</ul>
				<p><strong>允许的值</strong></p>
				<ul>
					<li>未定义/nil</li>
					<li><code>NET_BIND_SERVICE</code></li>
				</ul>
			</td>
		</tr>
	</tbody>
</table>
<!--
## Policy Instantiation

Decoupling policy definition from policy instantiation allows for a common understanding and
consistent language of policies across clusters, independent of the underlying enforcement
mechanism.

As mechanisms mature, they will be defined below on a per-policy basis. The methods of enforcement
of individual policies are not defined here.
-->
<h2 id="policy-instantiation">策略实例化  </h2>
<p>将策略定义从策略实例中解耦出来有助于形成跨集群的策略理解和语言陈述，
以免绑定到特定的下层实施机制。</p>
<p>随着相关机制的成熟，这些机制会按策略分别定义在下面。特定策略的实施方法不在这里定义。</p>
<p><a href="/zh/docs/concepts/security/pod-security-admission/"><strong>Pod 安全性准入控制器</strong></a></p>
<ul>
<li>






<a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/security/podsecurity-privileged.yaml" download="security/podsecurity-privileged.yaml">Privileged 名字空间</a>
</li>
<li>






<a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/security/podsecurity-baseline.yaml" download="security/podsecurity-baseline.yaml">Baseline 名字空间</a>
</li>
<li>






<a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/security/podsecurity-restricted.yaml" download="security/podsecurity-restricted.yaml">Restricted 名字空间</a>
</li>
</ul>
<p><a href="/zh/docs/concepts/security/pod-security-policy/"><strong>PodSecurityPolicy</strong></a> （已弃用）</p>
<ul>
<li>






<a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/policy/privileged-psp.yaml" download="policy/privileged-psp.yaml">Privileged</a>
</li>
<li>






<a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/policy/baseline-psp.yaml" download="policy/baseline-psp.yaml">Baseline</a>
</li>
<li>






<a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/policy/restricted-psp.yaml" download="policy/restricted-psp.yaml">Restricted</a>
</li>
</ul>
<!--
## FAQ

### Why isn't there a profile between privileged and baseline?
-->
<h2 id="faq">常见问题   </h2>
<h3 id="为什么不存在介于-privileged-和-baseline-之间的策略类型">为什么不存在介于 Privileged 和 Baseline 之间的策略类型</h3>
<!--
The three profiles defined here have a clear linear progression from most secure (restricted) to least
secure (privileged), and cover a broad set of workloads. Privileges required above the baseline
policy are typically very application specific, so we do not offer a standard profile in this
niche. This is not to say that the privileged profile should always be used in this case, but that
policies in this space need to be defined on a case-by-case basis.

SIG Auth may reconsider this position in the future, should a clear need for other profiles arise.
-->
<p>这里定义的三种策略框架有一个明晰的线性递进关系，从最安全（Restricted）到最不安全，
并且覆盖了很大范围的工作负载。特权要求超出 Baseline 策略者通常是特定于应用的需求，
所以我们没有在这个范围内提供标准框架。
这并不意味着在这样的情形下仍然只能使用 Privileged 框架，只是说处于这个范围的
策略需要因地制宜地定义。</p>
<p>SIG Auth 可能会在将来考虑这个范围的框架，前提是有对其他框架的需求。</p>
<!--
### What's the difference between a security policy and a security context?

[Security Contexts](/docs/tasks/configure-pod-container/security-context/) configure Pods and
Containers at runtime. Security contexts are defined as part of the Pod and container specifications
in the Pod manifest, and represent parameters to the container runtime.
-->
<h3 id="安全策略与安全上下文的区别是什么">安全策略与安全上下文的区别是什么？</h3>
<p><a href="/zh/docs/tasks/configure-pod-container/security-context/">安全上下文</a>在运行时配置 Pod
和容器。安全上下文是在 Pod 清单中作为 Pod 和容器规约的一部分来定义的，所代表的是
传递给容器运行时的参数。</p>
<!--
Security profiles are control plane mechanisms to enforce specific settings in the Security Context,
as well as other related parameters outside the Security Context. As of July 2021, 
[Pod Security Policies](/docs/concepts/security/pod-security-policy/) are deprecated in favor of the
built-in [Pod Security Admission Controller](/docs/concepts/security/pod-security-admission/). 

Other alternatives for enforcing security profiles are being developed in the Kubernetes
ecosystem, such as: 
- [OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper).
- [Kubewarden](https://github.com/kubewarden).
- [Kyverno](https://kyverno.io/policies/pod-security/).
-->
<p>安全策略则是控制面用来对安全上下文以及安全性上下文之外的参数实施某种设置的机制。
在 2020 年 7 月，
<a href="/zh/docs/concepts/security/pod-security-policy/">Pod 安全性策略</a>已被废弃，
取而代之的是内置的 <a href="/zh/docs/concepts/security/pod-security-admission/">Pod 安全性准入控制器</a>。</p>
<p>Kubernetes 生态系统中还在开发一些其他的替代方案，例如</p>
<ul>
<li><a href="https://github.com/open-policy-agent/gatekeeper">OPA Gatekeeper</a>。</li>
<li><a href="https://github.com/kubewarden">Kubewarden</a>。</li>
<li><a href="https://kyverno.io/policies/pod-security/">Kyverno</a>。</li>
</ul>
<!--
### What profiles should I apply to my Windows Pods?

Windows in Kubernetes has some limitations and differentiators from standard Linux-based
workloads. Specifically, the Pod SecurityContext fields [have no effect on
Windows](/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#v1-podsecuritycontext). As
such, no standardized Pod Security profiles currently exists.
-->
<h3 id="我应该为我的-windows-pod-实施哪种框架">我应该为我的 Windows Pod 实施哪种框架？</h3>
<p>Kubernetes 中的 Windows 负载与标准的基于 Linux 的负载相比有一些局限性和区别。
尤其是 Pod SecurityContext 字段
<a href="/zh/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#v1-podsecuritycontext">对 Windows 不起作用</a>。
因此，目前没有对应的标准 Pod 安全性框架。</p>
<!-- 
If you apply the restricted profile for a Windows pod, this **may** have an impact on the pod
at runtime. The restricted profile requires enforcing Linux-specific restrictions (such as seccomp
profile, and disallowing privilege escalation). If the kubelet and / or its container runtime ignore
these Linux-specific values, then the Windows pod should still work normally within the restricted
profile. However, the lack of enforcement means that there is no additional restriction, for Pods
that use Windows containers, compared to the baseline profile.

The use of the HostProcess flag to create a HostProcess pod should only be done in alignment with the privileged policy. Creation of a Windows HostProcess pod is blocked under the baseline and restricted policies, so any HostProcess pod should be considered privileged.
-->
<p>如果你为一个 Windows Pod 应用了 Restricted 策略，<strong>可能会</strong> 对该 Pod 的运行时产生影响。
Restricted 策略需要强制执行 Linux 特有的限制（如 seccomp Profile，并且禁止特权提升）。
如果 kubelet 和/或其容器运行时忽略了 Linux 特有的值，那么应该不影响 Windows Pod 正常工作。
然而，对于使用 Windows 容器的 Pod 来说，缺乏强制执行意味着相比于 Restricted 策略，没有任何额外的限制。</p>
<p>你应该只在 Privileged 策略下使用 HostProcess 标志来创建 HostProcess Pod。
在 Baseline 和 Restricted 策略下，创建 Windows HostProcess Pod 是被禁止的，
因此任何 HostProcess Pod 都应该被认为是有特权的。</p>
<!--
### What about sandboxed Pods?

There is not currently an API standard that controls whether a Pod is considered sandboxed or
not. Sandbox Pods may be identified by the use of a sandboxed runtime (such as gVisor or Kata
Containers), but there is no standard definition of what a sandboxed runtime is.

The protections necessary for sandboxed workloads can differ from others. For example, the need to
restrict privileged permissions is lessened when the workload is isolated from the underlying
kernel. This allows for workloads requiring heightened permissions to still be isolated.

Additionally, the protection of sandboxed workloads is highly dependent on the method of
sandboxing. As such, no single recommended policy is recommended for all sandboxed workloads.
-->
<h3 id="沙箱-sandboxed-pod-怎么处理">沙箱（Sandboxed） Pod 怎么处理？</h3>
<p>现在还没有 API 标准来控制 Pod 是否被视作沙箱化 Pod。
沙箱 Pod 可以通过其是否使用沙箱化运行时（如 gVisor 或 Kata Container）来辨别，不过
目前还没有关于什么是沙箱化运行时的标准定义。</p>
<p>沙箱化负载所需要的保护可能彼此各不相同。例如，当负载与下层内核直接隔离开来时，
限制特权化操作的许可就不那么重要。这使得那些需要更多许可权限的负载仍能被有效隔离。</p>
<p>此外，沙箱化负载的保护高度依赖于沙箱化的实现方法。
因此，现在还没有针对所有沙箱化负载的建议策略。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bc9934fccfeaf880eec6ea79025c0381">8.3 - Pod 安全性准入</h1>
    <div class="lead">对 Pod 安全性准入控制器的概述，Pod 安全性准入控制器可以实施 Pod 安全性标准。</div>
	<!--
reviewers:
- tallclair
- liggitt
title: Pod Security Admission
description: >
  An overview of the Pod Security Admission Controller, which can enforce the Pod Security
  Standards.
content_type: concept
weight: 20
min-kubernetes-server-version: v1.22
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!--
The Kubernetes [Pod Security Standards](/docs/concepts/security/pod-security-standards/) define
different isolation levels for Pods. These standards let you define how you want to restrict the
behavior of pods in a clear, consistent fashion.
-->
<p>Kubernetes <a href="/zh/docs/concepts/security/pod-security-standards/">Pod 安全性标准（Security Standards）</a>
为 Pod 定义不同的隔离级别。这些标准能够让你以一种清晰、一致的方式定义如何限制 Pod 行为。</p>
<!--
As a Beta feature, Kubernetes offers a built-in _Pod Security_ <a class='glossary-tooltip' title='在对象持久化之前拦截 Kubernetes Api 服务器请求的一段代码' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/access-authn-authz/admission-controllers/' target='_blank' aria-label='admission controller'>admission controller</a>, the successor
to [PodSecurityPolicies](/docs/concepts/security/pod-security-policy/). Pod security restrictions
are applied at the <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespace'>namespace</a> level when pods
are created.
-->
<p>作为一项 Beta 功能特性，Kubernetes 提供一种内置的 <em>Pod 安全性</em>
<a class='glossary-tooltip' title='在对象持久化之前拦截 Kubernetes Api 服务器请求的一段代码' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/access-authn-authz/admission-controllers/' target='_blank' aria-label='准入控制器'>准入控制器</a>，
作为 <a href="/zh/docs/concepts/security/pod-security-policy/">PodSecurityPolicies</a>
特性的后继演化版本。Pod 安全性限制是在 Pod 被创建时在
<a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>层面实施的。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The PodSecurityPolicy API is deprecated and will be 
[removed](/docs/reference/using-api/deprecation-guide/#v1-25) from Kubernetes in v1.25.
-->
<p>PodSecurityPolicy API 已经被废弃，会在 Kubernetes v1.25 发行版中
<a href="/zh/docs/reference/using-api/deprecation-guide/#v1-25">移除</a>。
</div>
<!-- body -->
<!--
## Enabling the `PodSecurity` admission plugin

In v1.23, the `PodSecurity` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
is a Beta feature and is enabled by default.

In v1.22, the `PodSecurity` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
is an Alpha feature and must be enabled in `kube-apiserver` in order to use the built-in admission plugin.
-->
<h2 id="enabling-the-podsecurity-admission-plugin">启用 <code>PodSecurity</code> 准入插件  </h2>
<p>在 v1.23 中，<code>PodSecurity</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
是一项 Beta 功能特性，默认被启用。</p>
<p>在 v1.22 中，<code>PodSecurity</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
是一项 Alpha 功能特性，必须在 <code>kube-apiserver</code> 上启用才能使用内置的准入插件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">--feature-gates<span style="color:#666">=</span><span style="color:#b44">&#34;...,PodSecurity=true&#34;</span>
</code></pre></div><!--
## Alternative: installing the `PodSecurity` admission webhook {#webhook}

For environments where the built-in `PodSecurity` admission plugin cannot be used,
either because the cluster is older than v1.22, or the `PodSecurity` feature cannot be enabled,
the `PodSecurity` admission logic is also available as a Beta [validating admission webhook](https://git.k8s.io/pod-security-admission/webhook).
-->
<h2 id="webhook">替代方案：安装 <code>PodSecurity</code> 准入 Webhook  </h2>
<p>对于无法应用内置 <code>PodSecurity</code> 准入插件的环境，无论是因为集群版本低于 v1.22，
或者 <code>PodSecurity</code> 特性无法被启用，都可以使用 Beta 版本的
<a href="https://git.k8s.io/pod-security-admission/webhook">验证性准入 Webhook</a>。
来使用 <code>PodSecurity</code> 准入逻辑。</p>
<!--
A pre-built container image, certificate generation scripts, and example manifests
are available at [https://git.k8s.io/pod-security-admission/webhook](https://git.k8s.io/pod-security-admission/webhook).

To install:
-->
<p>在 <a href="https://git.k8s.io/pod-security-admission/webhook">https://git.k8s.io/pod-security-admission/webhook</a>
上可以找到一个预先构建的容器镜像、证书生成脚本以及一些示例性质的清单。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">git clone git@github.com:kubernetes/pod-security-admission.git
<span style="color:#a2f">cd</span> pod-security-admission/webhook
make certs
kubectl apply -k .
</code></pre></div><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The generated certificate is valid for 2 years. Before it expires,
regenerate the certificate or remove the webhook in favor of the built-in admission plugin.
-->
<p>所生成的证书合法期限为 2 年。在证书过期之前，
需要重新生成证书或者去掉 Webhook 以使用内置的准入查件。
</div>
<!--
## Pod Security levels
-->
<h2 id="pod-security-levels">Pod 安全性级别  </h2>
<!--
Pod Security admission places requirements on a Pod's [Security
Context](/docs/tasks/configure-pod-container/security-context/) and other related fields according
to the three levels defined by the [Pod Security
Standards](/docs/concepts/security/pod-security-standards): `privileged`, `baseline`, and
`restricted`. Refer to the [Pod Security Standards](/docs/concepts/security/pod-security-standards)
page for an in-depth look at those requirements.
-->
<p>Pod 安全性准入插件对 Pod 的<a href="/zh/docs/tasks/configure-pod-container/security-context/">安全性上下文</a>
有一定的要求，并且依据 <a href="/zh/docs/concepts/security/pod-security-standards">Pod 安全性标准</a>
所定义的三个级别（<code>privileged</code>、<code>baseline</code> 和 <code>restricted</code>）对其他字段也有要求。
关于这些需求的更进一步讨论，请参阅
<a href="/zh/docs/concepts/security/pod-security-standards/">Pod 安全性标准</a>页面。</p>
<!--
## Pod Security Admission labels for namespaces

Once the feature is enabled or the webhook is installed, you can configure namespaces to define the admission
control mode you want to use for pod security in each namespace. Kubernetes defines a set of 
<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='labels'>labels</a> that you can set to define which of the 
predefined Pod Security Standard levels you want to use for a namespace. The label you select
defines what action the <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>
takes if a potential violation is detected:
-->
<h2 id="为名字空间设置-pod-安全性准入控制标签">为名字空间设置 Pod 安全性准入控制标签</h2>
<p>一旦特性被启用或者安装了 Webhook，你可以配置名字空间以定义每个名字空间中
Pod 安全性准入控制模式。
Kubernetes 定义了一组<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>，
你可以设置这些标签来定义某个名字空间上要使用的预定义的 Pod 安全性标准级别。
你所选择的标签定义了检测到潜在违例时，<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a>
要采取什么样的动作。</p>
<!--





<table><caption style="display: none;">Pod Security Admission modes</caption>
<thead>
<tr>
<th style="text-align:left">Mode</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>enforce</strong></td>
<td style="text-align:left">Policy violations will cause the pod to be rejected.</td>
</tr>
<tr>
<td style="text-align:left"><strong>audit</strong></td>
<td style="text-align:left">Policy violations will trigger the addition of an audit annotation to the event recorded in the <a href="/docs/tasks/debug-application-cluster/audit/">audit log</a>, but are otherwise allowed.</td>
</tr>
<tr>
<td style="text-align:left"><strong>warn</strong></td>
<td style="text-align:left">Policy violations will trigger a user-facing warning, but are otherwise allowed.</td>
</tr>
</tbody>
</table>

-->





<table><caption style="display: none;">Pod 安全准入模式</caption>
<thead>
<tr>
<th style="text-align:left">模式</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>enforce</strong></td>
<td style="text-align:left">策略违例会导致 Pod 被拒绝</td>
</tr>
<tr>
<td style="text-align:left"><strong>audit</strong></td>
<td style="text-align:left">策略违例会触发<a href="/zh/docs/tasks/debug-application-cluster/audit/">审计日志</a>中记录新事件时添加审计注解；但是 Pod 仍是被接受的。</td>
</tr>
<tr>
<td style="text-align:left"><strong>warn</strong></td>
<td style="text-align:left">策略违例会触发用户可见的警告信息，但是 Pod 仍是被接受的。</td>
</tr>
</tbody>
</table>

<!--
A namespace can configure any or all modes, or even set a different level for different modes.

For each mode, there are two labels that determine the policy used:
-->
<p>名字空间可以配置任何一种或者所有模式，或者甚至为不同的模式设置不同的级别。</p>
<p>对于每种模式，决定所使用策略的标签有两个：</p>
<!--
# The per-mode level label indicates which policy level to apply for the mode.
#
# MODE must be one of `enforce`, `audit`, or `warn`.
# LEVEL must be one of `privileged`, `baseline`, or `restricted`.
pod-security.kubernetes.io/<MODE>: <LEVEL>

# Optional: per-mode version label that can be used to pin the policy to the
# version that shipped with a given Kubernetes minor version (for example v1.23).
#
# MODE must be one of `enforce`, `audit`, or `warn`.
# VERSION must be a valid Kubernetes minor version, or `latest`.
pod-security.kubernetes.io/<MODE>-version: <VERSION>
-->
<pre><code># 针对模式的级别标签用来标示针对该模式所应用的策略级别
#
# MODE 必须是 `enforce`、`audit` 或 `warn` 之一
# LEVEL 必须是 `privileged`、baseline` 或 `restricted` 之一
pod-security.kubernetes.io/&lt;MODE&gt;: &lt;LEVEL&gt;

# 可选：针对每个模式版本的版本标签可以将策略锁定到
# 给定 Kubernetes 小版本号所附带的版本（例如 v1.23）
#
# MODE 必须是 `enforce`、`audit` 或 `warn` 之一
# VERSION 必须是一个合法的 Kubernetes 小版本号或者 `latest`
pod-security.kubernetes.io/&lt;MODE&gt;-version: &lt;VERSION&gt;
</code></pre><!--
Check out [Enforce Pod Security Standards with Namespace Labels](/docs/tasks/configure-pod-container/enforce-standards-namespace-labels) to see example usage.
-->
<p>关于用法示例，可参阅
<a href="/zh/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/">使用名字空间标签来强制实施 Pod 安全标准</a>。</p>
<!--
## Workload resources and Pod templates

Pods are often created indirectly, by creating a [workload
object](/docs/concepts/workloads/controllers/) such as a <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a> or <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a>. The workload object defines a
_Pod template_ and a <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> for the
workload resource creates Pods based on that template. To help catch violations early, both the
audit and warning modes are applied to the workload resources. However, enforce mode is **not**
applied to workload resources, only to the resulting pod objects.
-->
<h2 id="workload-resources-and-pod-templates">负载资源和 Pod 模板   </h2>
<p>Pod 通常是通过创建 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a> 或
<a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Job'>Job</a> 这类<a href="/zh/docs/concepts/workloads/controllers/">工作负载对象</a>
来间接创建的。工作负载对象为工作负载资源定义一个 <em>Pod 模板</em> 和一个对应的
负责基于该模板来创建 Pod 的<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>。
为了尽早地捕获违例状况，<code>audit</code> 和 <code>warn</code> 模式都应用到负载资源。
不过，<code>enforce</code> 模式并 <strong>不</strong> 应用到工作负载资源，仅应用到所生成的 Pod 对象上。</p>
<!--
## Exemptions

You can define _exemptions_ from pod security enforcement in order to allow the creation of pods that
would have otherwise been prohibited due to the policy associated with a given namespace.
Exemptions can be statically configured in the
[Admission Controller configuration](/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller).
-->
<h2 id="exemptions">豁免  </h2>
<p>你可以为 Pod 安全性的实施设置 <em>豁免（Exemptions）</em> 规则，
从而允许创建一些本来会被与给定名字空间相关的策略所禁止的 Pod。
豁免规则可以在<a href="/zh/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller">准入控制器配置</a>
中静态配置。</p>
<!--
Exemptions must be explicitly enumerated. Requests meeting exemption criteria are _ignored_ by the
Admission Controller (all `enforce`, `audit` and `warn` behaviors are skipped). Exemption dimensions include:
-->
<p>豁免规则可以显式枚举。满足豁免标准的请求会被准入控制器 <em>忽略</em>
（所有 <code>enforce</code>、<code>audit</code> 和 <code>warn</code> 行为都会被略过）。
豁免的维度包括：</p>
<!--
- **Usernames:** requests from users with an exempt authenticated (or impersonated) username are
  ignored.
- **RuntimeClassNames:** pods and [workload resources](#workload-resources-and-pod-templates) specifying an exempt runtime class name are
  ignored.
- **Namespaces:** pods and [workload resources](#workload-resources-and-pod-templates) in an exempt namespace are ignored.
-->
<ul>
<li><strong>Username：</strong> 来自用户名已被豁免的、已认证的（或伪装的）的用户的请求会被忽略。</li>
<li><strong>RuntimeClassName：</strong> 指定了已豁免的运行时类名称的 Pod
和<a href="#workload-resources-and-pod-templates">负载资源</a>会被忽略。</li>
<li><strong>Namespace：</strong> 位于被豁免的名字空间中的 Pod 和<a href="#workload-resources-and-pod-templates">负载资源</a>
会被忽略。</li>
</ul>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
Most pods are created by a controller in response to a [workload
resource](#workload-resources-and-pod-templates), meaning that exempting an end user will only
exempt them from enforcement when creating pods directly, but not when creating a workload resource.
Controller service accounts (such as `system:serviceaccount:kube-system:replicaset-controller`)
should generally not be exempted, as doing so would implicitly exempt any user that can create the
corresponding workload resource.
-->
<p>大多数 Pod 是作为对<a href="#workload-resources-and-pod-templates">工作负载资源</a>的响应，
由控制器所创建的，这意味着为某最终用户提供豁免时，只会当该用户直接创建 Pod
时对其实施安全策略的豁免。用户创建工作负载资源时不会被豁免。
控制器服务账号（例如：<code>system:serviceaccount:kube-system:replicaset-controller</code>）
通常不应该被豁免，因为豁免这类服务账号隐含着对所有能够创建对应工作负载资源的用户豁免。
</div>

<!--
Updates to the following pod fields are exempt from policy checks, meaning that if a pod update
request only changes these fields, it will not be denied even if the pod is in violation of the
current policy level:
-->
<p>策略检查时会对以下 Pod 字段的更新操作予以豁免，这意味着如果 Pod
更新请求仅改变这些字段时，即使 Pod 违反了当前的策略级别，请求也不会被拒绝。</p>
<!--
- Any metadata updates **except** changes to the seccomp or AppArmor annotations:
  - `seccomp.security.alpha.kubernetes.io/pod` (deprecated)
  - `container.seccomp.security.alpha.kubernetes.io/*` (deprecated)
  - `container.apparmor.security.beta.kubernetes.io/*`
- Valid updates to `.spec.activeDeadlineSeconds`
- Valid updates to `.spec.tolerations`
-->
<ul>
<li>除了对 seccomp 或 AppArmor 注解之外的所有 meatadata 更新操作：
<ul>
<li><code>seccomp.security.alpha.kubernetes.io/pod</code> （已弃用）</li>
<li><code>container.seccomp.security.alpha.kubernetes.io/*</code> （已弃用）</li>
<li><code>container.apparmor.security.beta.kubernetes.io/*</code></li>
</ul>
</li>
<li>对 <code>.spec.activeDeadlineSeconds</code> 的合法更新</li>
<li>对 <code>.spec.tolerations</code> 的合法更新</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
- [Pod Security Standards](/docs/concepts/security/pod-security-standards)
- [Enforcing Pod Security Standards](/docs/setup/best-practices/enforcing-pod-security-standards)
- [Enforce Pod Security Standards by Configuring the Built-in Admission Controller](/docs/tasks/configure-pod-container/enforce-standards-admission-controller)
- [Enforce Pod Security Standards with Namespace Labels](/docs/tasks/configure-pod-container/enforce-standards-namespace-labels)
- [Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller](/docs/tasks/configure-pod-container/migrate-from-psp)
-->
<ul>
<li><a href="/zh/docs/concepts/security/pod-security-standards/">Pod 安全性标准</a></li>
<li><a href="/zh/docs/setup/best-practices/enforcing-pod-security-standards/">强制实施 Pod 安全性标准</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/enforce-standards-admission-controller/">通过配置内置的准入控制器强制实施 Pod 安全性标准</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/">使用名字空间标签来实施 Pod 安全性标准</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/migrate-from-psp/">从 PodSecurityPolicy 迁移到内置的 PodSecurity 准入控制器</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4d77d1ae4c06aa14f54b385191627881">8.4 - Kubernetes API 访问控制</h1>
    
	<!--
---
reviewers:
- erictune
- lavalamp
title: Controlling Access to the Kubernetes API
content_type: concept
---
-->
<!-- overview -->
<!--
This page provides an overview of controlling access to the Kubernetes API.
-->
<p>本页面概述了对 Kubernetes API 的访问控制。</p>
<!-- body -->
<!--
Users access the [Kubernetes API](/docs/concepts/overview/kubernetes-api/) using `kubectl`,
client libraries, or by making REST requests.  Both human users and
[Kubernetes service accounts](/docs/tasks/configure-pod-container/configure-service-account/) can be
authorized for API access.
When a request reaches the API, it goes through several stages, illustrated in the
following diagram:
-->
<p>用户使用 <code>kubectl</code>、客户端库或构造 REST 请求来访问 <a href="/zh/docs/concepts/overview/kubernetes-api/">Kubernetes API</a>。
人类用户和 <a href="/zh/docs/tasks/configure-pod-container/configure-service-account/">Kubernetes 服务账户</a>都可以被鉴权访问 API。
当请求到达 API 时，它会经历多个阶段，如下图所示：</p>
<p><img src="/images/docs/admin/access-control-overview.svg" alt="Kubernetes API 请求处理步骤示意图"></p>
<!-- ## Transport security -->
<h2 id="transport-security">传输安全</h2>
<!--
In a typical Kubernetes cluster, the API serves on port 443, protected by TLS.
The API server presents a certificate. This certificate may be signed using
a private certificate authority (CA), or based on a public key infrastructure linked
to a generally recognized CA.
-->
<p>在典型的 Kubernetes 集群中，API 服务器在 443 端口上提供服务，受 TLS 保护。
API 服务器出示证书。
该证书可以使用私有证书颁发机构（CA）签名，也可以基于链接到公认的 CA 的公钥基础架构签名。</p>
<!--
If your cluster uses a private certificate authority, you need a copy of that CA
certificate configured into your `~/.kube/config` on the client, so that you can
trust the connection and be confident it was not intercepted.

Your client can present a TLS client certificate at this stage.
-->
<p>如果你的集群使用私有证书颁发机构，你需要在客户端的 <code>~/.kube/config</code> 文件中提供该 CA 证书的副本，
以便你可以信任该连接并确认该连接没有被拦截。</p>
<p>你的客户端可以在此阶段出示 TLS 客户端证书。</p>
<!-- ## Authentication -->
<h2 id="authentication">认证</h2>
<!--
Once TLS is established, the HTTP request moves to the Authentication step.
This is shown as step **1** in the diagram.
The cluster creation script or cluster admin configures the API server to run
one or more Authenticator modules.
Authenticators are described in more detail in
[Authentication](/docs/reference/access-authn-authz/authentication/).
-->
<p>如上图步骤 <strong>1</strong> 所示，建立 TLS 后， HTTP 请求将进入认证（Authentication）步骤。
集群创建脚本或者集群管理员配置 API 服务器，使之运行一个或多个身份认证组件。
身份认证组件在<a href="/zh/docs/reference/access-authn-authz/authentication/">认证</a>节中有更详细的描述。</p>
<!--
The input to the authentication step is the entire HTTP request; however, it typically
examines the headers and/or client certificate.

Authentication modules include client certificates, password, and plain tokens,
bootstrap tokens, and JSON Web Tokens (used for service accounts).

Multiple authentication modules can be specified, in which case each one is tried in sequence,
until one of them succeeds.
-->
<p>认证步骤的输入整个 HTTP 请求；但是，通常组件只检查头部或/和客户端证书。</p>
<p>认证模块包含客户端证书、密码、普通令牌、引导令牌和 JSON Web 令牌（JWT，用于服务账户）。</p>
<p>可以指定多个认证模块，在这种情况下，服务器依次尝试每个验证模块，直到其中一个成功。</p>
<!--
If the request cannot be authenticated, it is rejected with HTTP status code 401.
Otherwise, the user is authenticated as a specific `username`, and the user name
is available to subsequent steps to use in their decisions.  Some authenticators
also provide the group memberships of the user, while other authenticators
do not.

While Kubernetes uses usernames for access control decisions and in request logging,
it does not have a `User` object nor does it store usernames or other information about
users in its API.
-->
<p>如果请求认证不通过，服务器将以 HTTP 状态码 401 拒绝该请求。
反之，该用户被认证为特定的 <code>username</code>，并且该用户名可用于后续步骤以在其决策中使用。
部分验证器还提供用户的组成员身份，其他则不提供。</p>
<!-- ## Authorization -->
<h2 id="authorization">鉴权</h2>
<!--
After the request is authenticated as coming from a specific user, the request must be authorized. This is shown as step **2** in the diagram.

A request must include the username of the requester, the requested action, and the object affected by the action. The request is authorized if an existing policy declares that the user has permissions to complete the requested action.

For example, if Bob has the policy below, then he can read pods only in the namespace `projectCaribou`:
-->
<p>如上图的步骤 <strong>2</strong> 所示，将请求验证为来自特定的用户后，请求必须被鉴权。</p>
<p>请求必须包含请求者的用户名、请求的行为以及受该操作影响的对象。
如果现有策略声明用户有权完成请求的操作，那么该请求被鉴权通过。</p>
<p>例如，如果 Bob 有以下策略，那么他只能在 <code>projectCaribou</code> 名称空间中读取 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;abac.authorization.kubernetes.io/v1beta1&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Policy&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;spec&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;user&#34;</span>: <span style="color:#b44">&#34;bob&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;projectCaribou&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;resource&#34;</span>: <span style="color:#b44">&#34;pods&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;readonly&#34;</span>: <span style="color:#a2f;font-weight:bold">true</span>
    }
}
</code></pre></div><!--
If Bob makes the following request, the request is authorized because he is allowed to read objects in the `projectCaribou` namespace:
-->
<p>如果 Bob 执行以下请求，那么请求会被鉴权，因为允许他读取 <code>projectCaribou</code> 名称空间中的对象。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;authorization.k8s.io/v1beta1&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;SubjectAccessReview&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;spec&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;resourceAttributes&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;projectCaribou&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;verb&#34;</span>: <span style="color:#b44">&#34;get&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;group&#34;</span>: <span style="color:#b44">&#34;unicorn.example.org&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;resource&#34;</span>: <span style="color:#b44">&#34;pods&#34;</span>
    }
  }
}
</code></pre></div><!--
If Bob makes a request to write (`create` or `update`) to the objects in the `projectCaribou` namespace, his authorization is denied.
If Bob makes a request to read (`get`) objects in a different namespace such as `projectFish`, then his authorization is denied.

Kubernetes authorization requires that you use common REST attributes to interact with existing organization-wide or cloud-provider-wide access control systems.
It is important to use REST formatting because these control systems might interact with other APIs besides the Kubernetes API.
-->
<p>如果 Bob 在 <code>projectCaribou</code> 名字空间中请求写（<code>create</code> 或 <code>update</code>）对象，其鉴权请求将被拒绝。
如果 Bob 在诸如 <code>projectFish</code> 这类其它名字空间中请求读取（<code>get</code>）对象，其鉴权也会被拒绝。</p>
<p>Kubernetes 鉴权要求使用公共 REST 属性与现有的组织范围或云提供商范围的访问控制系统进行交互。
使用 REST 格式很重要，因为这些控制系统可能会与 Kubernetes API 之外的 API 交互。</p>
<!--
Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode.
When an administrator creates a cluster, they configure the authorization modules that should be used in the API server.
If more than one authorization modules are configured, Kubernetes checks each module,
and if any module authorizes the request, then the request can proceed.
If all of the modules deny the request, then the request is denied (HTTP status code 403).

To learn more about Kubernetes authorization, including details about creating policies using the supported authorization modules,
see [Authorization](/docs/reference/access-authn-authz/authorization/).
-->
<p>Kubernetes 支持多种鉴权模块，例如 ABAC 模式、RBAC 模式和 Webhook 模式等。
管理员创建集群时，他们配置应在 API 服务器中使用的鉴权模块。
如果配置了多个鉴权模块，则 Kubernetes 会检查每个模块，任意一个模块鉴权该请求，请求即可继续；
如果所有模块拒绝了该请求，请求将会被拒绝（HTTP 状态码 403）。</p>
<p>要了解更多有关 Kubernetes 鉴权的更多信息，包括有关使用支持鉴权模块创建策略的详细信息，
请参阅<a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权</a>。</p>
<!-- ## Admission control -->
<h2 id="admission-control">准入控制</h2>
<!--
Admission Control modules are software modules that can modify or reject requests.
In addition to the attributes available to Authorization modules, Admission
Control modules can access the contents of the object that is being created or modified.

Admission controllers act on requests that create, modify, delete, or connect to (proxy) an object.
Admission controllers do not act on requests that merely read objects.
When multiple admission controllers are configured, they are called in order.
-->
<p>准入控制模块是可以修改或拒绝请求的软件模块。
除鉴权模块可用的属性外，准入控制模块还可以访问正在创建或修改的对象的内容。</p>
<p>准入控制器对创建、修改、删除或（通过代理）连接对象的请求进行操作。
准入控制器不会对仅读取对象的请求起作用。
有多个准入控制器被配置时，服务器将依次调用它们。</p>
<!--
This is shown as step **3** in the diagram.

Unlike Authentication and Authorization modules, if any admission controller module
rejects, then the request is immediately rejected.

In addition to rejecting objects, admission controllers can also set complex defaults for
fields.

The available Admission Control modules are described in [Admission Controllers](/docs/reference/access-authn-authz/admission-controllers/).

Once a request passes all admission controllers, it is validated using the validation routines
for the corresponding API object, and then written to the object store (shown as step **4**).
-->
<p>这一操作如上图的步骤 <strong>3</strong> 所示。</p>
<p>与身份认证和鉴权模块不同，如果任何准入控制器模块拒绝某请求，则该请求将立即被拒绝。</p>
<p>除了拒绝对象之外，准入控制器还可以为字段设置复杂的默认值。</p>
<p>可用的准入控制模块在<a href="/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制器</a>中进行了描述。</p>
<p>请求通过所有准入控制器后，将使用检验例程检查对应的 API 对象，然后将其写入对象存储（如步骤 <strong>4</strong> 所示）。</p>
<!--
## Auditing

Kubernetes auditing provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster.
The cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.

For more information, see [Auditing](/docs/tasks/debug-application-cluster/audit/).
-->
<h2 id="auditing">审计</h2>
<p>Kubernetes 审计提供了一套与安全相关的、按时间顺序排列的记录，其中记录了集群中的操作序列。
集群对用户、使用 Kubernetes API 的应用程序以及控制平面本身产生的活动进行审计。</p>
<p>更多信息请参考 <a href="/zh/docs/tasks/debug-application-cluster/audit/">审计</a>.</p>
<!-- ## API server ports and IPs -->
<h2 id="api-server-ports-and-ips">API 服务器端口和 IP</h2>
<!--
The previous discussion applies to requests sent to the secure port of the API server
(the typical case).  The API server can actually serve on 2 ports:

By default, the Kubernetes API server serves HTTP on 2 ports:
-->
<p>前面的讨论适用于发送到 API 服务器的安全端口的请求（典型情况）。 API 服务器实际上可以在 2 个端口上提供服务：</p>
<p>默认情况下，Kubernetes API 服务器在 2 个端口上提供 HTTP 服务：</p>
<!--
  1. `localhost` port:

      - is intended for testing and bootstrap, and for other components of the master node
        (scheduler, controller-manager) to talk to the API
      - no TLS
      - default is port 8080
      - default IP is localhost, change with `--insecure-bind-address` flag.
      - request **bypasses** authentication and authorization modules.
      - request handled by admission control module(s).
      - protected by need to have host access

  2. “Secure port”:

      - use whenever possible
      - uses TLS.  Set cert with `--tls-cert-file` and key with `--tls-private-key-file` flag.
      - default is port 6443, change with `--secure-port` flag.
      - default IP is first non-localhost network interface, change with `--bind-address` flag.
      - request handled by authentication and authorization modules.
      - request handled by admission control module(s).
      - authentication and authorization modules run.
 -->
<ol>
<li>
<p><code>localhost</code> 端口:</p>
<ul>
<li>用于测试和引导，以及主控节点上的其他组件（调度器，控制器管理器）与 API 通信</li>
<li>没有 TLS</li>
<li>默认为端口 8080</li>
<li>默认 IP 为 localhost，使用 <code>--insecure-bind-address</code> 进行更改</li>
<li>请求 <strong>绕过</strong> 身份认证和鉴权模块</li>
<li>由准入控制模块处理的请求</li>
<li>受需要访问主机的保护</li>
</ul>
</li>
<li>
<p>“安全端口”：</p>
<ul>
<li>尽可能使用</li>
<li>使用 TLS。 用 <code>--tls-cert-file</code> 设置证书，用 <code>--tls-private-key-file</code> 设置密钥</li>
<li>默认端口 6443，使用 <code>--secure-port</code> 更改</li>
<li>默认 IP 是第一个非本地网络接口，使用 <code>--bind-address</code> 更改</li>
<li>请求须经身份认证和鉴权组件处理</li>
<li>请求须经准入控制模块处理</li>
<li>身份认证和鉴权模块运行</li>
</ul>
</li>
</ol>
<h2 id="what-s-next">What's next</h2>
<!--
Read more documentation on authentication, authorization and API access control:

- [Authenticating](/docs/reference/access-authn-authz/authentication/)
   - [Authenticating with Bootstrap Tokens](/docs/reference/access-authn-authz/bootstrap-tokens/)
- [Admission Controllers](/docs/reference/access-authn-authz/admission-controllers/)
   - [Dynamic Admission Control](/docs/reference/access-authn-authz/extensible-admission-controllers/)
- [Authorization](/docs/reference/access-authn-authz/authorization/)
   - [Role Based Access Control](/docs/reference/access-authn-authz/rbac/)
   - [Attribute Based Access Control](/docs/reference/access-authn-authz/abac/)
   - [Node Authorization](/docs/reference/access-authn-authz/node/)
   - [Webhook Authorization](/docs/reference/access-authn-authz/webhook/)
- [Certificate Signing Requests](/docs/reference/access-authn-authz/certificate-signing-requests/)
   - including [CSR approval](/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection)
     and [certificate signing](/docs/reference/access-authn-authz/certificate-signing-requests/#signing)
- Service accounts
  - [Developer guide](/docs/tasks/configure-pod-container/configure-service-account/)
  - [Administration](/docs/reference/access-authn-authz/service-accounts-admin/)

You can learn about:
- how Pods can use
  [Secrets](/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials)
  to obtain API credentials.
-->
<p>阅读更多有关身份认证、鉴权和 API 访问控制的文档：</p>
<ul>
<li><a href="/zh/docs/reference/access-authn-authz/authentication/">认证</a>
<ul>
<li><a href="/zh/docs/reference/access-authn-authz/bootstrap-tokens/">使用 Bootstrap 令牌进行身份认证</a></li>
</ul>
</li>
<li><a href="/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制器</a>
<ul>
<li><a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/">动态准入控制</a></li>
</ul>
</li>
<li><a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权</a>
<ul>
<li><a href="/zh/docs/reference/access-authn-authz/rbac/">基于角色的访问控制</a></li>
<li><a href="/zh/docs/reference/access-authn-authz/abac/">基于属性的访问控制</a></li>
<li><a href="/zh/docs/reference/access-authn-authz/node/">节点鉴权</a></li>
<li><a href="/zh/docs/reference/access-authn-authz/webhook/">Webhook 鉴权</a></li>
</ul>
</li>
<li><a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/">证书签名请求</a>
<ul>
<li>包括 <a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection">CSR 认证</a>
和<a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/#signing">证书签名</a></li>
</ul>
</li>
<li>服务账户
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/configure-service-account/">开发者指导</a></li>
<li><a href="/zh/docs/reference/access-authn-authz/service-accounts-admin/">管理</a></li>
</ul>
</li>
</ul>
<p>你可以了解</p>
<ul>
<li>Pod 如何使用
<a href="/zh/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials">Secrets</a>
获取 API 凭证.</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ac9161c6d952925b083ad9602b4e8e7f">9 - 策略</h1>
    <div class="lead">可配置的、可应用到一组资源的策略。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-a935ff8c59eb116b43494255cc67f69a">9.1 - 限制范围</h1>
    
	<!-- overview -->
<!--
By default, containers run with unbounded [compute resources](/docs/concepts/configuration/manage-resources-containers/) on a Kubernetes cluster.
With resource quotas, cluster administrators can restrict resource consumption and creation on a <a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespace'>namespace</a> basis.
Within a namespace, a Pod or Container can consume as much CPU and memory as defined by the namespace's resource quota. There is a concern that one Pod or Container could monopolize all available resources. A LimitRange is a policy to constrain resource allocations (to Pods or Containers) in a namespace.
-->
<p>默认情况下， Kubernetes 集群上的容器运行使用的<a href="/zh/docs/concepts/configuration/manage-resources-containers/">计算资源</a>没有限制。
使用资源配额，集群管理员可以以<a class='glossary-tooltip' title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>为单位，限制其资源的使用与创建。
在命名空间中，一个 Pod 或 Container 最多能够使用命名空间的资源配额所定义的 CPU 和内存用量。
有人担心，一个 Pod 或 Container 会垄断所有可用的资源。
LimitRange 是在命名空间内限制资源分配（给多个 Pod 或 Container）的策略对象。</p>
<!-- body -->
<!--
A _LimitRange_ provides constraints that can:

- Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.
- Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.
- Enforce a ratio between request and limit for a resource in a namespace.
- Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.
-->
<p>一个 <em>LimitRange（限制范围）</em> 对象提供的限制能够做到：</p>
<ul>
<li>在一个命名空间中实施对每个 Pod 或 Container 最小和最大的资源使用量的限制。</li>
<li>在一个命名空间中实施对每个 PersistentVolumeClaim 能申请的最小和最大的存储空间大小的限制。</li>
<li>在一个命名空间中实施对一种资源的申请值和限制值的比值的控制。</li>
<li>设置一个命名空间中对计算资源的默认申请/限制值，并且自动的在运行时注入到多个 Container 中。</li>
</ul>
<!--
## Enabling LimitRange

LimitRange support has been enabled by default since Kubernetes 1.10.

LimitRange support is enabled by default for many Kubernetes distributions.
-->
<h2 id="启用-limitrange">启用 LimitRange</h2>
<p>对 LimitRange 的支持自 Kubernetes 1.10 版本默认启用。</p>
<p>LimitRange 支持在很多 Kubernetes 发行版本中也是默认启用的。</p>
<!--
The name of a LimitRange object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>LimitRange 的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
### Overview of Limit Range

- The administrator creates one `LimitRange` in one namespace.
- Users create resources like Pods, Containers, and PersistentVolumeClaims in the namespace.
- The `LimitRanger` admission controller enforces defaults and limits for all Pods and Containers that do not set compute resource requirements and tracks usage to ensure it does not exceed resource minimum, maximum and ratio defined in any LimitRange present in the namespace.
- If creating or updating a resource (Pod, Container, PersistentVolumeClaim) that violates a LimitRange constraint, the request to the API server will fail with an HTTP status code `403 FORBIDDEN` and a message explaining the constraint that have been violated.
- If a LimitRange is activated in a namespace for compute resources like `cpu` and `memory`, users must specify
  requests or limits for those values. Otherwise, the system may reject Pod creation.
- LimitRange validations occurs only at Pod Admission stage, not on Running Pods.
-->
<h3 id="限制范围总览">限制范围总览</h3>
<ul>
<li>管理员在一个命名空间内创建一个 <code>LimitRange</code> 对象。</li>
<li>用户在命名空间内创建 Pod ，Container 和 PersistentVolumeClaim 等资源。</li>
<li><code>LimitRanger</code> 准入控制器对所有没有设置计算资源需求的 Pod 和 Container 设置默认值与限制值，
并跟踪其使用量以保证没有超出命名空间中存在的任意 LimitRange 对象中的最小、最大资源使用量以及使用量比值。</li>
<li>若创建或更新资源（Pod、 Container、PersistentVolumeClaim）违反了 LimitRange 的约束，
向 API 服务器的请求会失败，并返回 HTTP 状态码 <code>403 FORBIDDEN</code> 与描述哪一项约束被违反的消息。</li>
<li>若命名空间中的 LimitRange 启用了对 <code>cpu</code> 和 <code>memory</code> 的限制，
用户必须指定这些值的需求使用量与限制使用量。否则，系统将会拒绝创建 Pod。</li>
<li>LimitRange 的验证仅在 Pod 准入阶段进行，不对正在运行的 Pod 进行验证。</li>
</ul>
<!--
Examples of policies that could be created using limit range are:

- In a 2 node cluster with a capacity of 8 GiB RAM and 16 cores, constrain Pods in a namespace to request 100m of CPU with a max limit of 500m for CPU and request 200Mi for Memory with a max limit of 600Mi for Memory.
- Define default CPU limit and request to 150m and memory default request to 300Mi for Containers started with no cpu and memory requests in their specs.
-->
<p>能够使用限制范围创建的策略示例有：</p>
<ul>
<li>在一个有两个节点，8 GiB 内存与16个核的集群中，限制一个命名空间的 Pod 申请
100m 单位，最大 500m 单位的 CPU，以及申请 200Mi，最大 600Mi 的内存。</li>
<li>为 spec 中没有 cpu 和内存需求值的 Container 定义默认 CPU 限制值与需求值
150m，内存默认需求值 300Mi。</li>
</ul>
<!--
In the case where the total limits of the namespace is less than the sum of the limits of the Pods/Containers,
there may be contention for resources. In this case, the Containers or Pods will not be created.
-->
<p>在命名空间的总限制值小于 Pod 或 Container 的限制值的总和的情况下，可能会产生资源竞争。
在这种情况下，将不会创建 Container 或 Pod。</p>
<!--
Neither contention nor changes to a LimitRange will affect already created resources.
-->
<p>竞争和对 LimitRange 的改变都不会影响任何已经创建了的资源。</p>
<h2 id="what-s-next">What's next</h2>
<!--
See [LimitRanger design doc](https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md) for more information.
-->
<p>参阅 <a href="https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md">LimitRanger 设计文档</a>获取更多信息。</p>
<!--
For examples on using limits, see:

- See [how to configure minimum and maximum CPU constraints per namespace](/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/).
- See [how to configure minimum and maximum Memory constraints per namespace](/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/).
- See [how to configure default CPU Requests and Limits per namespace](/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/).
- See [how to configure default Memory Requests and Limits per namespace](/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/).
- Check [how to configure minimum and maximum Storage consumption per namespace](/docs/tasks/administer-cluster/limit-storage-consumption/#limitrange-to-limit-requests-for-storage).
- See a [detailed example on configuring quota per namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/).
-->
<p>关于使用限值的例子，可参看</p>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">如何配置每个命名空间最小和最大的 CPU 约束</a>。</li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">如何配置每个命名空间最小和最大的内存约束</a>。</li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">如何配置每个命名空间默认的 CPU 申请值和限制值</a>。</li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">如何配置每个命名空间默认的内存申请值和限制值</a>。</li>
<li><a href="/zh/docs/tasks/administer-cluster/limit-storage-consumption/#limitrange-to-limit-requests-for-storage">如何配置每个命名空间最小和最大存储使用量</a>。</li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">配置每个命名空间的配额的详细例子</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-94ddc6e901c30f256138db11d09f05a3">9.2 - 资源配额</h1>
    
	<!--
reviewers:
- derekwaynecarr
title: Resource Quotas
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
When several users or teams share a cluster with a fixed number of nodes,
there is a concern that one team could use more than its fair share of resources.

Resource quotas are a tool for administrators to address this concern.
-->
<p>当多个用户或团队共享具有固定节点数目的集群时，人们会担心有人使用超过其基于公平原则所分配到的资源量。</p>
<p>资源配额是帮助管理员解决这一问题的工具。</p>
<!-- body -->
<!--
A resource quota, defined by a `ResourceQuota` object, provides constraints that limit
aggregate resource consumption per namespace.  It can limit the quantity of objects that can
be created in a namespace by type, as well as the total amount of compute resources that may
be consumed by resources in that namespace.
-->
<p>资源配额，通过 <code>ResourceQuota</code> 对象来定义，对每个命名空间的资源消耗总量提供限制。
它可以限制命名空间中某种类型的对象的总数目上限，也可以限制命令空间中的 Pod 可以使用的计算资源的总上限。</p>
<!--
Resource quotas work like this:
-->
<p>资源配额的工作方式如下：</p>
<!--
- Different teams work in different namespaces.  Currently this is voluntary, but
  support for making this mandatory via ACLs is planned.
- The administrator creates one ResourceQuota for each namespace.
- Users create resources (pods, services, etc.) in the namespace, and the quota system
  tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.
- If creating or updating a resource violates a quota constraint, the request will fail with HTTP
  status code `403 FORBIDDEN` with a message explaining the constraint that would have been violated.
- If quota is enabled in a namespace for compute resources like `cpu` and `memory`, users must specify
  requests or limits for those values; otherwise, the quota system may reject pod creation.  Hint: Use
  the `LimitRanger` admission controller to force defaults for pods that make no compute resource requirements.
  See the [walkthrough](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/) for an example of how to avoid this problem.
-->
<ul>
<li>
<p>不同的团队可以在不同的命名空间下工作，目前这是非约束性的，在未来的版本中可能会通过
ACL (Access Control List 访问控制列表) 来实现强制性约束。</p>
</li>
<li>
<p>集群管理员可以为每个命名空间创建一个或多个 ResourceQuota 对象。</p>
</li>
<li>
<p>当用户在命名空间下创建资源（如 Pod、Service 等）时，Kubernetes 的配额系统会
跟踪集群的资源使用情况，以确保使用的资源用量不超过 ResourceQuota 中定义的硬性资源限额。</p>
</li>
<li>
<p>如果资源创建或者更新请求违反了配额约束，那么该请求会报错（HTTP 403 FORBIDDEN），
并在消息中给出有可能违反的约束。</p>
</li>
<li>
<p>如果命名空间下的计算资源 （如 <code>cpu</code> 和 <code>memory</code>）的配额被启用，则用户必须为
这些资源设定请求值（request）和约束值（limit），否则配额系统将拒绝 Pod 的创建。
提示: 可使用 <code>LimitRanger</code> 准入控制器来为没有设置计算资源需求的 Pod 设置默认值。</p>
<p>若想避免这类问题，请参考
<a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">演练</a>示例。</p>
</li>
</ul>
<!--
The name of a ResourceQuota object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>ResourceQuota 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
Examples of policies that could be created using namespaces and quotas are:
-->
<p>下面是使用命名空间和配额构建策略的示例：</p>
<!--
- In a cluster with a capacity of 32 GiB RAM, and 16 cores, let team A use 20 GiB and 10 cores,
  let B use 10GiB and 4 cores, and hold 2GiB and 2 cores in reserve for future allocation.
- Limit the "testing" namespace to using 1 core and 1GiB RAM.  Let the "production" namespace
  use any amount.
-->
<ul>
<li>在具有 32 GiB 内存和 16 核 CPU 资源的集群中，允许 A 团队使用 20 GiB 内存 和 10 核的 CPU 资源，
允许 B 团队使用 10 GiB 内存和 4 核的 CPU 资源，并且预留 2 GiB 内存和 2 核的 CPU 资源供将来分配。</li>
<li>限制 &quot;testing&quot; 命名空间使用 1 核 CPU 资源和 1GiB 内存。允许 &quot;production&quot; 命名空间使用任意数量。</li>
</ul>
<!--
In the case where the total capacity of the cluster is less than the sum of the quotas of the namespaces,
there may be contention for resources.  This is handled on a first-come-first-served basis.

Neither contention nor changes to quota will affect already created resources.
-->
<p>在集群容量小于各命名空间配额总和的情况下，可能存在资源竞争。资源竞争时，Kubernetes 系统会遵循先到先得的原则。</p>
<p>不管是资源竞争还是配额的修改，都不会影响已经创建的资源使用对象。</p>
<!--
## Enabling Resource Quota

Resource Quota support is enabled by default for many Kubernetes distributions. It is
enabled when the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a>
`--enable-admission-plugins=` flag has `ResourceQuota` as
one of its arguments.
-->
<h2 id="启用资源配额">启用资源配额</h2>
<p>资源配额的支持在很多 Kubernetes 版本中是默认启用的。
当 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>
的命令行标志 <code>--enable-admission-plugins=</code> 中包含 <code>ResourceQuota</code> 时，
资源配额会被启用。</p>
<!--
A resource quota is enforced in a particular namespace when there is a
ResourceQuota in that namespace.
-->
<p>当命名空间中存在一个 ResourceQuota 对象时，对于该命名空间而言，资源配额就是开启的。</p>
<!--
## Compute Resource Quota

You can limit the total sum of
[compute resources](/docs/concepts/configuration/manage-resources-containers/)
that can be requested in a given namespace.
-->
<h2 id="计算资源配额">计算资源配额</h2>
<p>用户可以对给定命名空间下的可被请求的
<a href="/zh/docs/concepts/configuration/manage-resources-containers/">计算资源</a>
总量进行限制。</p>
<!--
The following resource types are supported:
-->
<p>配额机制所支持的资源类型：</p>
<!--
| Resource Name | Description |
| --------------------- | --------------------------------------------------------- |
| `limits.cpu` | Across all pods in a non-terminal state, the sum of CPU limits cannot exceed this value. |
| `limits.memory` | Across all pods in a non-terminal state, the sum of memory limits cannot exceed this value. |
| `requests.cpu` | Across all pods in a non-terminal state, the sum of CPU requests cannot exceed this value. |
| `requests.memory` | Across all pods in a non-terminal state, the sum of memory requests cannot exceed this value. |
| `hugepages-<size>` | Across all pods in a non-terminal state, the number of huge page requests of the specified size cannot exceed this value. |
| `cpu` | Same as `requests.cpu` |
| `memory` | Same as `requests.memory` |
-->
<table>
<thead>
<tr>
<th>资源名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>limits.cpu</code></td>
<td>所有非终止状态的 Pod，其 CPU 限额总量不能超过该值。</td>
</tr>
<tr>
<td><code>limits.memory</code></td>
<td>所有非终止状态的 Pod，其内存限额总量不能超过该值。</td>
</tr>
<tr>
<td><code>requests.cpu</code></td>
<td>所有非终止状态的 Pod，其 CPU 需求总量不能超过该值。</td>
</tr>
<tr>
<td><code>requests.memory</code></td>
<td>所有非终止状态的 Pod，其内存需求总量不能超过该值。</td>
</tr>
<tr>
<td><code>hugepages-&lt;size&gt;</code></td>
<td>对于所有非终止状态的 Pod，针对指定尺寸的巨页请求总数不能超过此值。</td>
</tr>
<tr>
<td><code>cpu</code></td>
<td>与 <code>requests.cpu</code> 相同。</td>
</tr>
<tr>
<td><code>memory</code></td>
<td>与 <code>requests.memory</code> 相同。</td>
</tr>
</tbody>
</table>
<!--
### Resource Quota For Extended Resources

In addition to the resources mentioned above, in release 1.10, quota support for
[extended resources](/docs/concepts/configuration/manage-compute-resources-container/#extended-resources) is added.
-->
<h3 id="扩展资源的资源配额">扩展资源的资源配额</h3>
<p>除上述资源外，在 Kubernetes 1.10 版本中，还添加了对
<a href="/zh/docs/concepts/configuration/manage-resources-containers/#extended-resources">扩展资源</a>
的支持。</p>
<!--
As overcommit is not allowed for extended resources, it makes no sense to specify both `requests`
and `limits` for the same extended resource in a quota. So for extended resources, only quota items
with prefix `requests.` is allowed for now.
-->
<p>由于扩展资源不可超量分配，因此没有必要在配额中为同一扩展资源同时指定 <code>requests</code> 和 <code>limits</code>。
对于扩展资源而言，目前仅允许使用前缀为 <code>requests.</code> 的配额项。</p>
<!--
Take the GPU resource as an example, if the resource name is `nvidia.com/gpu`, and you want to
limit the total number of GPUs requested in a namespace to 4, you can define a quota as follows:
-->
<p>以 GPU 拓展资源为例，如果资源名称为 <code>nvidia.com/gpu</code>，并且要将命名空间中请求的 GPU
资源总数限制为 4，则可以如下定义配额：</p>
<ul>
<li><code>requests.nvidia.com/gpu: 4</code></li>
</ul>
<!--
See [Viewing and Setting Quotas](#viewing-and-setting-quotas) for more detail information.
-->
<p>有关更多详细信息，请参阅<a href="#viewing-and-setting-quotas">查看和设置配额</a>。</p>
<!--
## Storage Resource Quota

You can limit the total sum of [storage resources](/docs/concepts/storage/persistent-volumes/) that can be requested in a given namespace.

In addition, you can limit consumption of storage resources based on associated storage-class.
-->
<h2 id="存储资源配额">存储资源配额</h2>
<p>用户可以对给定命名空间下的<a href="/zh/docs/concepts/storage/persistent-volumes/">存储资源</a>
总量进行限制。</p>
<p>此外，还可以根据相关的存储类（Storage Class）来限制存储资源的消耗。</p>
<!--
| Resource Name | Description |
| --------------------- | --------------------------------------------------------- |
| `requests.storage` | Across all persistent volume claims, the sum of storage requests cannot exceed this value. |
| `persistentvolumeclaims` | The total number of [PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) that can exist in the namespace. |
| `<storage-class-name>.storageclass.storage.k8s.io/requests.storage` | Across all persistent volume claims associated with the `<storage-class-name>`, the sum of storage requests cannot exceed this value. |
| `<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims` | Across all persistent volume claims associated with the storage-class-name, the total number of [persistent volume claims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) that can exist in the namespace. |
-->
<table>
<thead>
<tr>
<th>资源名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>requests.storage</code></td>
<td>所有 PVC，存储资源的需求总量不能超过该值。</td>
</tr>
<tr>
<td><code>persistentvolumeclaims</code></td>
<td>在该命名空间中所允许的 <a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PVC</a> 总量。</td>
</tr>
<tr>
<td><code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/requests.storage</code></td>
<td>在所有与 <code>&lt;storage-class-name&gt;</code> 相关的持久卷申领中，存储请求的总和不能超过该值。</td>
</tr>
<tr>
<td><code>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/persistentvolumeclaims</code></td>
<td>在与 storage-class-name 相关的所有持久卷申领中，命名空间中可以存在的<a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">持久卷申领</a>总数。</td>
</tr>
</tbody>
</table>
<!--
For example, if an operator wants to quota storage with `gold` storage class separate from `bronze` storage class, the operator can
define a quota as follows:
-->
<p>例如，如果一个操作人员针对 <code>gold</code> 存储类型与 <code>bronze</code> 存储类型设置配额，
操作人员可以定义如下配额：</p>
<ul>
<li><code>gold.storageclass.storage.k8s.io/requests.storage: 500Gi</code></li>
<li><code>bronze.storageclass.storage.k8s.io/requests.storage: 100Gi</code></li>
</ul>
<!--
In release 1.8, quota support for local ephemeral storage is added as an alpha feature:
-->
<p>在 Kubernetes 1.8 版本中，本地临时存储的配额支持已经是 Alpha 功能：</p>
<!--
| Resource Name | Description |
| ------------------------------- |----------------------------------------------------------- |
| `requests.ephemeral-storage` | Across all pods in the namespace, the sum of local ephemeral storage requests cannot exceed this value. |
| `limits.ephemeral-storage` | Across all pods in the namespace, the sum of local ephemeral storage limits cannot exceed this value. |
| `ephemeral-storage` | Same as `requests.ephemeral-storage`. |
-->
<table>
<thead>
<tr>
<th>资源名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>requests.ephemeral-storage</code></td>
<td>在命名空间的所有 Pod 中，本地临时存储请求的总和不能超过此值。</td>
</tr>
<tr>
<td><code>limits.ephemeral-storage</code></td>
<td>在命名空间的所有 Pod 中，本地临时存储限制值的总和不能超过此值。</td>
</tr>
<tr>
<td><code>ephemeral-storage</code></td>
<td>与 <code>requests.ephemeral-storage</code> 相同。</td>
</tr>
</tbody>
</table>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
When using a CRI container runtime, container logs will count against the ephemeral storage quota.
This can result in the unexpected eviction of pods that have exhausted their storage quotas.
Refer to [Logging Architecture](/docs/concepts/cluster-administration/logging/) for details.
-->
<p>如果所使用的是 CRI 容器运行时，容器日志会被计入临时存储配额。
这可能会导致存储配额耗尽的 Pods 被意外地驱逐出节点。
参考<a href="/zh/docs/concepts/cluster-administration/logging/">日志架构</a>
了解详细信息。
</div>
<!--
## Object Count Quota

You can set quota for the total number of certain resources of all standard,
namespaced resource types using the following syntax:

* `count/<resource>.<group>` for resources from non-core groups
* `count/<resource>` for resources from the core group
-->
<h2 id="对象数量配额">对象数量配额</h2>
<p>你可以使用以下语法对所有标准的、命名空间域的资源类型进行配额设置：</p>
<ul>
<li><code>count/&lt;resource&gt;.&lt;group&gt;</code>：用于非核心（core）组的资源</li>
<li><code>count/&lt;resource&gt;</code>：用于核心组的资源</li>
</ul>
<!--
Here is an example set of resources users may want to put under object count quota:
-->
<p>这是用户可能希望利用对象计数配额来管理的一组资源示例。</p>
<ul>
<li><code>count/persistentvolumeclaims</code></li>
<li><code>count/services</code></li>
<li><code>count/secrets</code></li>
<li><code>count/configmaps</code></li>
<li><code>count/replicationcontrollers</code></li>
<li><code>count/deployments.apps</code></li>
<li><code>count/replicasets.apps</code></li>
<li><code>count/statefulsets.apps</code></li>
<li><code>count/jobs.batch</code></li>
<li><code>count/cronjobs.batch</code></li>
</ul>
<!--
The same syntax can be used for custom resources.
For example, to create a quota on a `widgets` custom resource in the `example.com` API group, use `count/widgets.example.com`.
-->
<p>相同语法也可用于自定义资源。
例如，要对 <code>example.com</code> API 组中的自定义资源 <code>widgets</code> 设置配额，请使用
<code>count/widgets.example.com</code>。</p>
<!--
When using `count/*` resource quota, an object is charged against the quota if it exists in server storage.
These types of quotas are useful to protect against exhaustion of storage resources.  For example, you may
want to limit the number of Secrets in a server given their large size. Too many Secrets in a cluster can
actually prevent servers and controllers from starting. You can set a quota for Jobs to protect against
a poorly configured CronJob. CronJobs that create too many Jobs in a namespace can lead to a denial of service.
-->
<p>当使用 <code>count/*</code> 资源配额时，如果对象存在于服务器存储中，则会根据配额管理资源。
这些类型的配额有助于防止存储资源耗尽。例如，用户可能想根据服务器的存储能力来对服务器中
Secret 的数量进行配额限制。
集群中存在过多的 Secret 实际上会导致服务器和控制器无法启动。
用户可以选择对 Job 进行配额管理，以防止配置不当的 CronJob 在某命名空间中创建太多
Job 而导致集群拒绝服务。</p>
<!--
It is possible to do generic object count quota on a limited set of resources.
In addition, it is possible to further constrain quota for particular resources by their type.

The following types are supported:
-->
<p>对有限的一组资源上实施一般性的对象数量配额也是可能的。
此外，还可以进一步按资源的类型设置其配额。</p>
<p>支持以下类型：</p>
<!--
| Resource Name | Description |
| ----------------------------|--------------------------------------------- |
| `configmaps` | The total number of ConfigMaps that can exist in the namespace. |
| `persistentvolumeclaims` | The total number of [PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) that can exist in the namespace. |
| `pods` | The total number of Pods in a non-terminal state that can exist in the namespace.  A pod is in a terminal state if `.status.phase in (Failed, Succeeded)` is true.  |
| `replicationcontrollers` | The total number of ReplicationControllers that can exist in the namespace. |
| `resourcequotas` | The total number of ResourceQuotas that can exist in the namespace. |
| `services` | The total number of Services that can exist in the namespace. |
| `services.loadbalancers` | The total number of Services of type `LoadBalancer` that can exist in the namespace. |
| `services.nodeports` | The total number of Services of type `NodePort` that can exist in the namespace. |
| `secrets` | The total number of Secrets that can exist in the namespace. |
-->
<table>
<thead>
<tr>
<th>资源名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>configmaps</code></td>
<td>在该命名空间中允许存在的 ConfigMap 总数上限。</td>
</tr>
<tr>
<td><code>persistentvolumeclaims</code></td>
<td>在该命名空间中允许存在的 <a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PVC</a> 的总数上限。</td>
</tr>
<tr>
<td><code>pods</code></td>
<td>在该命名空间中允许存在的非终止状态的 Pod 总数上限。Pod 终止状态等价于 Pod 的 <code>.status.phase in (Failed, Succeeded)</code> 为真。</td>
</tr>
<tr>
<td><code>replicationcontrollers</code></td>
<td>在该命名空间中允许存在的 ReplicationController 总数上限。</td>
</tr>
<tr>
<td><code>resourcequotas</code></td>
<td>在该命名空间中允许存在的 ResourceQuota 总数上限。</td>
</tr>
<tr>
<td><code>services</code></td>
<td>在该命名空间中允许存在的 Service 总数上限。</td>
</tr>
<tr>
<td><code>services.loadbalancers</code></td>
<td>在该命名空间中允许存在的 LoadBalancer 类型的 Service 总数上限。</td>
</tr>
<tr>
<td><code>services.nodeports</code></td>
<td>在该命名空间中允许存在的 NodePort 类型的 Service 总数上限。</td>
</tr>
<tr>
<td><code>secrets</code></td>
<td>在该命名空间中允许存在的 Secret 总数上限。</td>
</tr>
</tbody>
</table>
<!--
For example, `pods` quota counts and enforces a maximum on the number of `pods`
created in a single namespace that are not terminal. You might want to set a `pods`
quota on a namespace to avoid the case where a user creates many small pods and
exhausts the cluster's supply of Pod IPs.
-->
<p>例如，<code>pods</code> 配额统计某个命名空间中所创建的、非终止状态的 <code>Pod</code> 个数并确保其不超过某上限值。
用户可能希望在某命名空间中设置 <code>pods</code> 配额，以避免有用户创建很多小的 Pod，
从而耗尽集群所能提供的 Pod IP 地址。</p>
<!--
## Quota Scopes

Each quota can have an associated set of `scopes`. A quota will only measure usage for a resource if it matches
the intersection of enumerated scopes.
-->
<h2 id="quota-scopes">配额作用域  </h2>
<p>每个配额都有一组相关的 <code>scope</code>（作用域），配额只会对作用域内的资源生效。
配额机制仅统计所列举的作用域的交集中的资源用量。</p>
<!--
When a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope.
Resources specified on the quota outside of the allowed set results in a validation error.
-->
<p>当一个作用域被添加到配额中后，它会对作用域相关的资源数量作限制。
如配额中指定了允许（作用域）集合之外的资源，会导致验证错误。</p>
<!--
| Scope | Description |
| ----- | ------------ |
| `Terminating` | Match pods where `.spec.activeDeadlineSeconds >= 0` |
| `NotTerminating` | Match pods where `.spec.activeDeadlineSeconds is nil` |
| `BestEffort` | Match pods that have best effort quality of service. |
| `NotBestEffort` | Match pods that do not have best effort quality of service. |
| `PriorityClass` | Match pods that references the specified [priority class](/docs/concepts/scheduling-eviction/pod-priority-preemption). |
| `CrossNamespacePodAffinity` | Match pods that have cross-namespace pod [(anti)affinity terms](/docs/concepts/scheduling-eviction/assign-pod-node). |
-->
<table>
<thead>
<tr>
<th>作用域</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Terminating</code></td>
<td>匹配所有 <code>spec.activeDeadlineSeconds</code> 不小于 0 的 Pod。</td>
</tr>
<tr>
<td><code>NotTerminating</code></td>
<td>匹配所有 <code>spec.activeDeadlineSeconds</code> 是 nil 的 Pod。</td>
</tr>
<tr>
<td><code>BestEffort</code></td>
<td>匹配所有 Qos 是 BestEffort 的 Pod。</td>
</tr>
<tr>
<td><code>NotBestEffort</code></td>
<td>匹配所有 Qos 不是 BestEffort 的 Pod。</td>
</tr>
<tr>
<td><code>PriorityClass</code></td>
<td>匹配所有引用了所指定的<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption">优先级类</a>的 Pods。</td>
</tr>
<tr>
<td><code>CrossNamespacePodAffinity</code></td>
<td>匹配那些设置了跨名字空间 <a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node">（反）亲和性条件</a>的 Pod。</td>
</tr>
</tbody>
</table>
<!--
The `BestEffort` scope restricts a quota to tracking the following resource:

* `pods`

The `Terminating`, `NotTerminating`, `NotBestEffort` and `PriorityClass`
scopes restrict a quota to tracking the following resources:
-->
<p><code>BestEffort</code> 作用域限制配额跟踪以下资源：</p>
<ul>
<li><code>pods</code></li>
</ul>
<p><code>Terminating</code>、<code>NotTerminating</code>、<code>NotBestEffort</code> 和 <code>PriorityClass</code> 这些作用域限制配额跟踪以下资源：</p>
<ul>
<li><code>pods</code></li>
<li><code>cpu</code></li>
<li><code>memory</code></li>
<li><code>requests.cpu</code></li>
<li><code>requests.memory</code></li>
<li><code>limits.cpu</code></li>
<li><code>limits.memory</code></li>
</ul>
<!--
Note that you cannot specify both the `Terminating` and the `NotTerminating`
scopes in the same quota, and you cannot specify both the `BestEffort` and
`NotBestEffort` scopes in the same quota either.

The `scopeSelector` supports the following values in the `operator` field:
-->
<p>需要注意的是，你不可以在同一个配额对象中同时设置 <code>Terminating</code> 和 <code>NotTerminating</code>
作用域，你也不可以在同一个配额中同时设置 <code>BestEffort</code> 和 <code>NotBestEffort</code>
作用域。</p>
<p><code>scopeSelector</code> 支持在 <code>operator</code> 字段中使用以下值：</p>
<ul>
<li><code>In</code></li>
<li><code>NotIn</code></li>
<li><code>Exists</code></li>
<li><code>DoesNotExist</code></li>
</ul>
<!--
When using one of the following values as the `scopeName` when defining the
`scopeSelector`, the `operator` must be `Exists`. 
-->
<p>定义 <code>scopeSelector</code> 时，如果使用以下值之一作为 <code>scopeName</code> 的值，则对应的
<code>operator</code> 只能是 <code>Exists</code>。</p>
<ul>
<li><code>Terminating</code></li>
<li><code>NotTerminating</code></li>
<li><code>BestEffort</code></li>
<li><code>NotBestEffort</code></li>
</ul>
<!--
If the `operator` is `In` or `NotIn`, the `values` field must have at least
one value. For example:
-->
<p>如果 <code>operator</code> 是 <code>In</code> 或 <code>NotIn</code> 之一，则 <code>values</code> 字段必须至少包含一个值。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- middle<span style="color:#bbb">
</span></code></pre></div><!--
If the `operator` is `Exists` or `DoesNotExist`, the `values field must *NOT* be
specified.
-->
<p>如果 <code>operator</code> 为 <code>Exists</code> 或 <code>DoesNotExist</code>，则<em>不</em>可以设置 <code>values</code> 字段。</p>
<!--
### Resource Quota Per PriorityClass
-->
<h3 id="基于优先级类-priorityclass-来设置资源配额">基于优先级类（PriorityClass）来设置资源配额</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code>
</div>


<!--
Pods can be created at a specific [priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority).
You can control a pod's consumption of system resources based on a pod's priority, by using the `scopeSelector`
field in the quota spec.
-->
<p>Pod 可以创建为特定的<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority">优先级</a>。
通过使用配额规约中的 <code>scopeSelector</code> 字段，用户可以根据 Pod 的优先级控制其系统资源消耗。</p>
<!--
A quota is matched and consumed only if `scopeSelector` in the quota spec selects the pod.
-->
<p>仅当配额规范中的 <code>scopeSelector</code> 字段选择到某 Pod 时，配额机制才会匹配和计量 Pod 的资源消耗。</p>
<!--
When quota is scoped for priority class using `scopeSelector` field, quota object
is restricted to track only following resources:
-->
<p>如果配额对象通过 <code>scopeSelector</code> 字段设置其作用域为优先级类，则配额对象只能
跟踪以下资源：</p>
<ul>
<li><code>pods</code></li>
<li><code>cpu</code></li>
<li><code>memory</code></li>
<li><code>ephemeral-storage</code></li>
<li><code>limits.cpu</code></li>
<li><code>limits.memory</code></li>
<li><code>limits.ephemeral-storage</code></li>
<li><code>requests.cpu</code></li>
<li><code>requests.memory</code></li>
<li><code>requests.ephemeral-storage</code></li>
</ul>
<!--
This example creates a quota object and matches it with pods at specific priorities. The example
works as follows:
-->
<p>本示例创建一个配额对象，并将其与具有特定优先级的 Pod 进行匹配。
该示例的工作方式如下：</p>
<!--
- Pods in the cluster have one of the three priority classes, "low", "medium", "high".
- One quota object is created for each priority.
-->
<ul>
<li>集群中的 Pod 可取三个优先级类之一，即 &quot;low&quot;、&quot;medium&quot;、&quot;high&quot;。</li>
<li>为每个优先级创建一个配额对象。</li>
</ul>
<!-- Save the following YAML to a file `quota.yml`.  -->
<p>将以下 YAML 保存到文件 <code>quota.yml</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>List<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pods-high<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1000&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Gi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">operator </span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;high&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pods-medium<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>20Gi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">operator </span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;medium&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pods-low<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">operator </span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;low&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
Apply the YAML using `kubectl create`.
-->
<p>使用 <code>kubectl create</code> 命令运行以下操作。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./quota.yml
</code></pre></div><pre><code>resourcequota/pods-high created
resourcequota/pods-medium created
resourcequota/pods-low created
</code></pre><!--
Verify that `Used` quota is `0` using `kubectl describe quota`.
-->
<p>使用 <code>kubectl describe quota</code> 操作验证配额的 <code>Used</code> 值为 <code>0</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe quota
</code></pre></div><pre><code>Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     1k
memory      0     200Gi
pods        0     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
</code></pre><!--
Create a pod with priority "high". Save the following YAML to a
file `high-priority-pod.yml`.
-->
<p>创建优先级为 &quot;high&quot; 的 Pod。
将以下 YAML 保存到文件 <code>high-priority-pod.yml</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>high-priority<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>high-priority<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>ubuntu<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/bin/sh&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;while true; do echo hello; sleep 10;done&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">priorityClassName</span>:<span style="color:#bbb"> </span>high<span style="color:#bbb">
</span></code></pre></div><!--
Apply it with `kubectl create`.
-->
<p>使用 <code>kubectl create</code> 运行以下操作。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./high-priority-pod.yml
</code></pre></div><!--
Verify that "Used" stats for "high" priority quota, `pods-high`, has changed and that
the other two quotas are unchanged.
-->
<p>确认 &quot;high&quot; 优先级配额 <code>pods-high</code> 的 &quot;Used&quot; 统计信息已更改，并且其他两个配额未更改。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe quota
</code></pre></div><pre><code>Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         500m  1k
memory      10Gi  200Gi
pods        1     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
</code></pre><!--
### Cross-namespace Pod Affinity Quota
-->
<h3 id="cross-namespace-pod-affinity-quota">跨名字空间的 Pod 亲和性配额  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
Operators can use `CrossNamespacePodAffinity` quota scope to limit which namespaces are allowed to
have pods with affinity terms that cross namespaces. Specifically, it controls which pods are allowed
to set `namespaces` or `namespaceSelector` fields in pod affinity terms.
-->
<p>集群运维人员可以使用 <code>CrossNamespacePodAffinity</code> 配额作用域来
限制哪个名字空间中可以存在包含跨名字空间亲和性规则的 Pod。
更为具体一点，此作用域用来配置哪些 Pod 可以在其 Pod 亲和性规则
中设置 <code>namespaces</code> 或 <code>namespaceSelector</code> 字段。</p>
<!--
Preventing users from using cross-namespace affinity terms might be desired since a pod
with anti-affinity constraints can block pods from all other namespaces 
from getting scheduled in a failure domain. 
-->
<p>禁止用户使用跨名字空间的亲和性规则可能是一种被需要的能力，因为带有
反亲和性约束的 Pod 可能会阻止所有其他名字空间的 Pod 被调度到某失效域中。</p>
<!--
Using this scope operators can prevent certain namespaces (`foo-ns` in the example below) 
from having pods that use cross-namespace pod affinity by creating a resource quota object in
that namespace with `CrossNamespaceAffinity` scope and hard limit of 0:
-->
<p>使用此作用域操作符可以避免某些名字空间（例如下面例子中的 <code>foo-ns</code>）运行
特别的 Pod，这类 Pod 使用跨名字空间的 Pod 亲和性约束，在该名字空间中创建
了作用域为 <code>CrossNamespaceAffinity</code> 的、硬性约束为 0 的资源配额对象。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>disable-cross-namespace-affinity<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>foo-ns<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>CrossNamespaceAffinity<span style="color:#bbb">
</span></code></pre></div><!--
If operators want to disallow using `namespaces` and `namespaceSelector` by default, and 
only allow it for specific namespaces, they could configure `CrossNamespaceAffinity` 
as a limited resource by setting the kube-apiserver flag -admission-control-config-file
to the path of the following configuration file:
-->
<p>如果集群运维人员希望默认禁止使用 <code>namespaces</code> 和 <code>namespaceSelector</code>，而
仅仅允许在特定名字空间中这样做，他们可以将 <code>CrossNamespaceAffinity</code> 作为一个
被约束的资源。方法是为 <code>kube-apiserver</code> 设置标志
<code>--admission-control-config-file</code>，使之指向如下的配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>AdmissionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">plugins</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;ResourceQuota&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">configuration</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuotaConfiguration<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">limitedResources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>pods<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchScopes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>CrossNamespaceAffinity<span style="color:#bbb">
</span></code></pre></div><!--
With the above configuration, pods can use `namespaces` and `namespaceSelector` in pod affinity only
if the namespace where they are created have a resource quota object with 
`CrossNamespaceAffinity` scope and a hard limit greater than or equal to the number of pods using those fields.
-->
<p>基于上面的配置，只有名字空间中包含作用域为 <code>CrossNamespaceAffinity</code> 且
硬性约束大于或等于使用 <code>namespaces</code> 和 <code>namespaceSelector</code> 字段的 Pods
个数时，才可以在该名字空间中继续创建在其 Pod 亲和性规则中设置 <code>namespaces</code>
或 <code>namespaceSelector</code> 的新 Pod。</p>
<!--
This feature is beta and enabled by default. You can disable it using the
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`PodAffinityNamespaceSelector` in both kube-apiserver and kube-scheduler.
-->
<p>此功能特性处于 Beta 阶段，默认被禁用。你可以通过为 kube-apiserver 和
kube-scheduler 设置
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>PodAffinityNamespaceSelector</code> 来启用此特性。</p>
<!--
## Requests compared to Limits {#requests-vs-limits}

When allocating compute resources, each container may specify a request and a limit value for either CPU or memory.
The quota can be configured to quota either value.
-->
<h2 id="requests-vs-limits">请求与限制的比较  </h2>
<p>分配计算资源时，每个容器可以为 CPU 或内存指定请求和约束。
配额可以针对二者之一进行设置。</p>
<!--
If the quota has a value specified for `requests.cpu` or `requests.memory`, then it requires that every incoming
container makes an explicit request for those resources.  If the quota has a value specified for `limits.cpu` or `limits.memory`,
then it requires that every incoming container specifies an explicit limit for those resources.
-->
<p>如果配额中指定了 <code>requests.cpu</code> 或 <code>requests.memory</code> 的值，则它要求每个容器都显式给出对这些资源的请求。
同理，如果配额中指定了 <code>limits.cpu</code> 或 <code>limits.memory</code> 的值，那么它要求每个容器都显式设定对应资源的限制。</p>
<!--
## Viewing and Setting Quotas

Kubectl supports creating, updating, and viewing quotas:
-->
<h2 id="viewing-and-setting-quotas">查看和设置配额</h2>
<p>Kubectl 支持创建、更新和查看配额：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace myspace
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt; compute-resources.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: ResourceQuota
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: compute-resources
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  hard:
</span><span style="color:#b44">    requests.cpu: &#34;1&#34;
</span><span style="color:#b44">    requests.memory: 1Gi
</span><span style="color:#b44">    limits.cpu: &#34;2&#34;
</span><span style="color:#b44">    limits.memory: 2Gi
</span><span style="color:#b44">    requests.nvidia.com/gpu: 4
</span><span style="color:#b44">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./compute-resources.yaml --namespace<span style="color:#666">=</span>myspace
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt; object-counts.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: ResourceQuota
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: object-counts
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  hard:
</span><span style="color:#b44">    configmaps: &#34;10&#34;
</span><span style="color:#b44">    persistentvolumeclaims: &#34;4&#34;
</span><span style="color:#b44">    pods: &#34;4&#34;
</span><span style="color:#b44">    replicationcontrollers: &#34;20&#34;
</span><span style="color:#b44">    secrets: &#34;10&#34;
</span><span style="color:#b44">    services: &#34;10&#34;
</span><span style="color:#b44">    services.loadbalancers: &#34;2&#34;
</span><span style="color:#b44">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./object-counts.yaml --namespace<span style="color:#666">=</span>myspace
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get quota --namespace<span style="color:#666">=</span>myspace
</code></pre></div><pre><code class="language-none" data-lang="none">NAME                    AGE
compute-resources       30s
object-counts           32s
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe quota compute-resources --namespace<span style="color:#666">=</span>myspace
</code></pre></div><pre><code class="language-none" data-lang="none">Name:                    compute-resources
Namespace:               myspace
Resource                 Used  Hard
--------                 ----  ----
limits.cpu               0     2
limits.memory            0     2Gi
requests.cpu             0     1
requests.memory          0     1Gi
requests.nvidia.com/gpu  0     4
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe quota object-counts --namespace<span style="color:#666">=</span>myspace
</code></pre></div><pre><code class="language-none" data-lang="none">Name:                   object-counts
Namespace:              myspace
Resource                Used    Hard
--------                ----    ----
configmaps              0       10
persistentvolumeclaims  0       4
pods                    0       4
replicationcontrollers  0       20
secrets                 1       10
services                0       10
services.loadbalancers  0       2
</code></pre><!--
Kubectl also supports object count quota for all standard namespaced resources
using the syntax `count/<resource>.<group>`:
-->
<p>kubectl 还使用语法 <code>count/&lt;resource&gt;.&lt;group&gt;</code> 支持所有标准的、命名空间域的资源的对象计数配额：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace myspace
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create quota <span style="color:#a2f">test</span> --hard<span style="color:#666">=</span>count/deployments.apps<span style="color:#666">=</span>2,count/replicasets.apps<span style="color:#666">=</span>4,count/pods<span style="color:#666">=</span>3,count/secrets<span style="color:#666">=</span><span style="color:#666">4</span> --namespace<span style="color:#666">=</span>myspace
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment nginx --image<span style="color:#666">=</span>nginx --namespace<span style="color:#666">=</span>myspace --replicas<span style="color:#666">=</span><span style="color:#666">2</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe quota --namespace<span style="color:#666">=</span>myspace
</code></pre></div><pre><code>Name:                         test
Namespace:                    myspace
Resource                      Used  Hard
--------                      ----  ----
count/deployments.apps        1     2
count/pods                    2     3
count/replicasets.apps        1     4
count/secrets                 1     4
</code></pre><!--
## Quota and Cluster Capacity

ResourceQuotas are independent of the cluster capacity. They are
expressed in absolute units.  So, if you add nodes to your cluster, this does *not*
automatically give each namespace the ability to consume more resources.
-->
<h2 id="quota-and-cluster-capacity">配额和集群容量  </h2>
<p>ResourceQuota 与集群资源总量是完全独立的。它们通过绝对的单位来配置。
所以，为集群添加节点时，资源配额<em>不会</em>自动赋予每个命名空间消耗更多资源的能力。</p>
<!--
Sometimes more complex policies may be desired, such as:

- Proportionally divide total cluster resources among several teams.
- Allow each tenant to grow resource usage as needed, but have a generous
  limit to prevent accidental resource exhaustion.
- Detect demand from one namespace, add nodes, and increase quota.
-->
<p>有时可能需要资源配额支持更复杂的策略，比如：</p>
<ul>
<li>在几个团队中按比例划分总的集群资源。</li>
<li>允许每个租户根据需要增加资源使用量，但要有足够的限制以防止资源意外耗尽。</li>
<li>探测某个命名空间的需求，添加物理节点并扩大资源配额值。</li>
</ul>
<!--
Such policies could be implemented using `ResourceQuotas` as building blocks, by
writing a "controller" that watches the quota usage and adjusts the quota
hard limits of each namespace according to other signals.
-->
<p>这些策略可以通过将资源配额作为一个组成模块、手动编写一个控制器来监控资源使用情况，
并结合其他信号调整命名空间上的硬性资源配额来实现。</p>
<!--
Note that resource quota divides up aggregate cluster resources, but it creates no
restrictions around nodes: pods from several namespaces may run on the same node.
-->
<p>注意：资源配额对集群资源总体进行划分，但它对节点没有限制：来自不同命名空间的 Pod 可能在同一节点上运行。</p>
<!--
## Limit Priority Class consumption by default

It may be desired that pods at a particular priority, eg. "cluster-services",
should be allowed in a namespace, if and only if, a matching quota object exists.
-->
<h2 id="默认情况下限制特定优先级的资源消耗">默认情况下限制特定优先级的资源消耗</h2>
<p>有时候可能希望当且仅当某名字空间中存在匹配的配额对象时，才可以创建特定优先级
（例如 &quot;cluster-services&quot;）的 Pod。</p>
<!--
With this mechanism, operators will be able to restrict usage of certain high
priority classes to a limited number of namespaces and not every namespace
will be able to consume these priority classes by default.
-->
<p>通过这种机制，操作人员能够将限制某些高优先级类仅出现在有限数量的命名空间中，
而并非每个命名空间默认情况下都能够使用这些优先级类。</p>
<!--
To enforce this, kube-apiserver flag `-admission-control-config-file` should be
used to pass path to the following configuration file:
-->
<p>要实现此目的，应设置 kube-apiserver 的标志 <code>--admission-control-config-file</code>
指向如下配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>AdmissionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">plugins</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;ResourceQuota&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">configuration</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuotaConfiguration<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">limitedResources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>pods<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">matchScopes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;cluster-services&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
Then, create a resource quota object in the `kube-system` namespace:
-->
<p>现在在 <code>kube-system</code> 名字空间中创建一个资源配额对象：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/policy/priority-class-resourcequota.yaml" download="policy/priority-class-resourcequota.yaml"><code>policy/priority-class-resourcequota.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('policy-priority-class-resourcequota-yaml')" title="Copy policy/priority-class-resourcequota.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="policy-priority-class-resourcequota-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pods-cluster-services<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scopeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">operator </span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">scopeName</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;cluster-services&#34;</span>]</code></pre></div>
    </div>
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system
</code></pre></div><pre><code class="language-none" data-lang="none">resourcequota/pods-cluster-services created
</code></pre><!--
In this case, a pod creation will be allowed if:

1.  the Pod's `priorityClassName` is not specified.
1.  the Pod's `priorityClassName` is specified to a value other than `cluster-services`.
1.  the Pod's `priorityClassName` is set to `cluster-services`, it is to be created
   in the `kube-system` namespace, and it has passed the resource quota check.
-->
<p>在这里，当以下条件满足时可以创建 Pod：</p>
<ol>
<li>Pod 未设置 <code>priorityClassName</code></li>
<li>Pod 的 <code>priorityClassName</code> 设置值不是 <code>cluster-services</code></li>
<li>Pod 的 <code>priorityClassName</code> 设置值为 <code>cluster-services</code>，它将被创建于
<code>kube-system</code> 名字空间中，并且它已经通过了资源配额检查。</li>
</ol>
<!--
A Pod creation request is rejected if its `priorityClassName` is set to `cluster-services`
and it is to be created in a namespace other than `kube-system`.
-->
<p>如果 Pod 的 <code>priorityClassName</code> 设置为 <code>cluster-services</code>，但要被创建到
<code>kube-system</code> 之外的别的名字空间，则 Pod 创建请求也被拒绝。</p>
<h2 id="what-s-next">What's next</h2>
<!--
- See [ResourceQuota design doc](https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_resource_quota.md) for more information.
- See a [detailed example for how to use resource quota](/docs/tasks/administer-cluster/quota-api-object/).
- Read [Quota support for priority class design doc](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/pod-priority-resourcequota.md).
- See [LimitedResources](https://github.com/kubernetes/kubernetes/pull/36765)
-->
<ul>
<li>查看<a href="https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_resource_quota.md">资源配额设计文档</a></li>
<li>查看<a href="/zh/docs/tasks/administer-cluster/quota-api-object/">如何使用资源配额的详细示例</a>。</li>
<li>阅读<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/pod-priority-resourcequota.md">优先级类配额支持的设计文档</a>。
了解更多信息。</li>
<li>参阅 <a href="https://github.com/kubernetes/kubernetes/pull/36765">LimitedResources</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7352434db5f5954d2f7656b46fe5a324">9.3 - 进程 ID 约束与预留</h1>
    
	<!--
reviewers:
- derekwaynecarr
title: Process ID Limits And Reservations
content_type: concept
weight: 40
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>


<!--
Kubernetes allow you to limit the number of process IDs (PIDs) that a
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> can use.
You can also reserve a number of allocatable PIDs for each <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='node'>node</a>
for use by the operating system and daemons (rather than by Pods).
-->
<p>Kubernetes 允许你限制一个 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 中可以使用的
进程 ID（PID）数目。你也可以为每个 <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>
预留一定数量的可分配的 PID，供操作系统和守护进程（而非 Pod）使用。</p>
<!-- body -->
<!--
Process IDs (PIDs) are a fundamental resource on nodes. It is trivial to hit the
task limit without hitting any other resource limits, which can then cause
instability to a host machine.
-->
<p>进程 ID（PID）是节点上的一种基础资源。很容易就会在尚未超出其它资源约束的时候就
已经触及任务个数上限，进而导致宿主机器不稳定。</p>
<!--
Cluster administrators require mechanisms to ensure that Pods running in the
cluster cannot induce PID exhaustion that prevents host daemons (such as the
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> or
<a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a>,
and potentially also the container runtime) from running.
In addition, it is important to ensure that PIDs are limited among Pods in order
to ensure they have limited impact on other workloads on the same node.
-->
<p>集群管理员需要一定的机制来确保集群中运行的 Pod 不会导致 PID 资源枯竭，甚而
造成宿主机上的守护进程（例如
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 或者
<a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a>
乃至包括容器运行时本身）无法正常运行。
此外，确保 Pod 中 PID 的个数受限对于保证其不会影响到同一节点上其它负载也很重要。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
On certain Linux installations, the operating system sets the PIDs limit to a low default,
such as `32768`. Consider raising the value of `/proc/sys/kernel/pid_max`.
-->
<p>在某些 Linux 安装环境中，操作系统会将 PID 约束设置为一个较低的默认值，例如
<code>32768</code>。这时可以考虑提升 <code>/proc/sys/kernel/pid_max</code> 的设置值。
</div>
<!--
You can configure a kubelet to limit the number of PIDs a given Pod can consume.
For example, if your node's host OS is set to use a maximum of `262144` PIDs and
expect to host less than `250` Pods, one can give each Pod a budget of `1000`
PIDs to prevent using up that node's overall number of available PIDs. If the
admin wants to overcommit PIDs similar to CPU or memory, they may do so as well
with some additional risks. Either way, a single Pod will not be able to bring
the whole machine down. This kind of resource limiting helps to prevent simple
fork bombs from affecting operation of an entire cluster.
-->
<p>你可以配置 kubelet 限制给定 Pod 能够使用的 PID 个数。
例如，如果你的节点上的宿主操作系统被设置为最多可使用 <code>262144</code> 个 PID，同时预期
节点上会运行的 Pod 个数不会超过 <code>250</code>，那么你可以为每个 Pod 设置 <code>1000</code> 个 PID
的预算，避免耗尽该节点上可用 PID 的总量。
如果管理员系统像 CPU 或内存那样允许对 PID 进行过量分配（Overcommit），他们也可以
这样做，只是会有一些额外的风险。不管怎样，任何一个 Pod 都不可以将整个机器的运行
状态破坏。这类资源限制有助于避免简单的派生炸弹（Fork
Bomb）影响到整个集群的运行。</p>
<!--
Per-Pod PID limiting allows administrators to protect one Pod from another, but
does not ensure that all Pods scheduled onto that host are unable to impact the node overall.
Per-Pod limiting also does not protect the node agents themselves from PID exhaustion.

You can also reserve an amount of PIDs for node overhead, separate from the
allocation to Pods. This is similar to how you can reserve CPU, memory, or other
resources for use by the operating system and other facilities outside of Pods
and their containers.
-->
<p>在 Pod 级别设置 PID 限制使得管理员能够保护 Pod 之间不会互相伤害，不过无法
确保所有调度到该宿主机器上的所有 Pod 都不会影响到节点整体。
Pod 级别的限制也无法保护节点代理任务自身不会受到 PID 耗尽的影响。</p>
<p>你也可以预留一定量的 PID，作为节点的额外开销，与分配给 Pod 的 PID 集合独立。
这有点类似于在给操作系统和其它设施预留 CPU、内存或其它资源时所做的操作，
这些任务都在 Pod 及其所包含的容器之外运行。</p>
<!--
PID limiting is a an important sibling to [compute
resource](/docs/concepts/configuration/manage-resources-containers/) requests
and limits. However, you specify it in a different way: rather than defining a
Pod's resource limit in the `.spec` for a Pod, you configure the limit as a
setting on the kubelet. Pod-defined PID limits are not currently supported.
-->
<p>PID 限制是与<a href="/zh/docs/concepts/configuration/manage-resources-containers/">计算资源</a>
请求和限制相辅相成的一种机制。不过，你需要用一种不同的方式来设置这一限制：
你需要将其设置到 kubelet 上而不是在 Pod 的 <code>.spec</code> 中为 Pod 设置资源限制。
目前还不支持在 Pod 级别设置 PID 限制。</p>
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> <!--
This means that the limit that applies to a Pod may be different depending on
where the Pod is scheduled. To make things simple, it's easiest if all Nodes use
the same PID resource limits and reservations.
-->
<p>这意味着，施加在 Pod 之上的限制值可能因为 Pod 运行所在的节点不同而有差别。
为了简化系统，最简单的方法是为所有节点设置相同的 PID 资源限制和预留值。
</div>

<!--
## Node PID limits

Kubernetes allows you to reserve a number of process IDs for the system use. To
configure the reservation, use the parameter `pid=<number>` in the
`--system-reserved` and `--kube-reserved` command line options to the kubelet.
The value you specified declares that the specified number of process IDs will
be reserved for the system as a whole and for Kubernetes system daemons
respectively.
-->
<h2 id="node-pid-limits">节点级别 PID 限制  </h2>
<p>Kubernetes 允许你为系统预留一定量的进程 ID。为了配置预留数量，你可以使用
kubelet 的 <code>--system-reserved</code> 和 <code>--kube-reserved</code> 命令行选项中的参数
<code>pid=&lt;number&gt;</code>。你所设置的参数值分别用来声明为整个系统和 Kubernetes 系统
守护进程所保留的进程 ID 数目。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Before Kubernetes version 1.20, PID resource limiting with Node-level
reservations required enabling the [feature
gate](/docs/reference/command-line-tools-reference/feature-gates/)
`SupportNodePidsLimit` to work.
-->
<p>在 Kubernetes 1.20 版本之前，在节点级别通过 PID 资源限制预留 PID 的能力
需要启用<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>SupportNodePidsLimit</code> 才行。
</div>
<!--
## Pod PID limits

Kubernetes allows you to limit the number of processes running in a Pod. You
specify this limit at the node level, rather than configuring it as a resource
limit for a particular Pod. Each Node can have a different PID limit.  
To configure the limit, you can specify the command line parameter `--pod-max-pids`
to the kubelet, or set `PodPidsLimit` in the kubelet
[configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).
-->
<h2 id="pod-pid-limits">Pod 级别 PID 限制  </h2>
<p>Kubernetes 允许你限制 Pod 中运行的进程个数。你可以在节点级别设置这一限制，
而不是为特定的 Pod 来将其设置为资源限制。
每个节点都可以有不同的 PID 限制设置。
要设置限制值，你可以设置 kubelet 的命令行参数 <code>--pod-max-pids</code>，或者
在 kubelet 的<a href="/zh/docs/tasks/administer-cluster/kubelet-config-file/">配置文件</a>
中设置 <code>PodPidsLimit</code>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Before Kubernetes version 1.20, PID resource limiting for Pods required enabling
the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`SupportPodPidsLimit` to work.
-->
<p>在 Kubernetes 1.20 版本之前，为 Pod 设置 PID 资源限制的能力需要启用
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>SupportNodePidsLimit</code> 才行。
</div>
<!--
## PID based eviction

You can configure kubelet to start terminating a Pod when it is misbehaving and consuming abnormal amount of resources.
This feature is called eviction. You can
[Configure Out of Resource Handling](/docs/concepts/scheduling-eviction/node-pressure-eviction/)
for various eviction signals.
Use `pid.available` eviction signal to configure the threshold for number of PIDs used by Pod.
You can set soft and hard eviction policies.
However, even with the hard eviction policy, if the number of PIDs growing very fast,
node can still get into unstable state by hitting the node PIDs limit.
Eviction signal value is calculated periodically and does NOT enforce the limit.
-->
<h2 id="pid-based-eviction">基于 PID 的驱逐   </h2>
<p>你可以配置 kubelet 使之在 Pod 行为不正常或者消耗不正常数量资源的时候将其终止。
这一特性称作驱逐。你可以针对不同的驱逐信号
<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/">配置资源不足的处理</a>。
使用 <code>pid.available</code> 驱逐信号来配置 Pod 使用的 PID 个数的阈值。
你可以设置硬性的和软性的驱逐策略。不过，即使使用硬性的驱逐策略，
如果 PID 个数增长过快，节点仍然可能因为触及节点 PID 限制而进入一种不稳定状态。
驱逐信号的取值是周期性计算的，而不是一直能够强制实施约束。</p>
<!--
PID limiting - per Pod and per Node sets the hard limit.
Once the limit is hit, workload will start experiencing failures when trying to get a new PID.
It may or may not lead to rescheduling of a Pod,
depending on how workload reacts on these failures and how liveleness and readiness
probes are configured for the Pod. However, if limits were set correctly,
you can guarantee that other Pods workload and system processes will not run out of PIDs
when one Pod is misbehaving.
-->
<p>Pod 级别和节点级别的 PID 限制会设置硬性限制。
一旦触及限制值，工作负载会在尝试获得新的 PID 时开始遇到问题。
这可能会也可能不会导致 Pod 被重新调度，取决于工作负载如何应对这类失败
以及 Pod 的存活性和就绪态探测是如何配置的。
可是，如果限制值被正确设置，你可以确保其它 Pod 负载和系统进程不会因为某个
Pod 行为不正常而没有 PID 可用。</p>
<h2 id="what-s-next">What's next</h2>
<!--
- Refer to the [PID Limiting enhancement document](https://github.com/kubernetes/enhancements/blob/097b4d8276bc9564e56adf72505d43ce9bc5e9e8/keps/sig-node/20190129-pid-limiting.md) for more information.
- For historical context, read
  [Process ID Limiting for Stability Improvements in Kubernetes 1.14](/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/).
- Read [Managing Resources for Containers](/docs/concepts/configuration/manage-resources-containers/).
- Learn how to [Configure Out of Resource Handling](/docs/concepts/scheduling-eviction/node-pressure-eviction/).
-->
<ul>
<li>参阅 <a href="https://github.com/kubernetes/enhancements/blob/097b4d8276bc9564e56adf72505d43ce9bc5e9e8/keps/sig-node/20190129-pid-limiting.md">PID 约束改进文档</a>
以了解更多信息。</li>
<li>关于历史背景，请阅读
<a href="/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/">Kubernetes 1.14 中限制进程 ID 以提升稳定性</a>
的博文。</li>
<li>请阅读<a href="/zh/docs/concepts/configuration/manage-resources-containers/">为容器管理资源</a>。</li>
<li>学习如何<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/">配置资源不足情况的处理</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b528c4464c030f3f044124b38d778f04">9.4 - 节点资源管理器</h1>
    
	<!-- 
---
reviewers:
- derekwaynecarr
- klueska
title: Node Resource Managers 
content_type: concept
weight: 50
---
-->
<!-- overview -->
<!-- 
In order to support latency-critical and high-throughput workloads, Kubernetes offers a suite of Resource Managers. The managers aim to co-ordinate and optimise node's resources alignment for pods configured with a specific requirement for CPUs, devices, and memory (hugepages) resources. 
-->
<p>Kubernetes 提供了一组资源管理器，用于支持延迟敏感的、高吞吐量的工作负载。
资源管理器的目标是协调和优化节点资源，以支持对 CPU、设备和内存（巨页）等资源有特殊需求的 Pod。</p>
<!-- body -->
<!-- 
The main manager, the Topology Manager, is a Kubelet component that co-ordinates the overall resource management process through its [policy](/docs/tasks/administer-cluster/topology-manager/).

The configuration of individual managers is elaborated in dedicated documents:
-->
<p>主管理器，也叫拓扑管理器（Topology Manager），是一个 Kubelet 组件，
它通过<a href="/zh/docs/tasks/administer-cluster/topology-manager/">策略</a>，
协调全局的资源管理过程。</p>
<p>各个管理器的配置方式会在专项文档中详细阐述：</p>
<!-- 
- [CPU Manager Policies](/docs/tasks/administer-cluster/cpu-management-policies/)
- [Device Manager](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager)
- [Memory Manager Policies](/docs/tasks/administer-cluster/memory-manager/)
-->
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/cpu-management-policies/">CPU 管理器策略</a></li>
<li><a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager">设备管理器</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/memory-manager/">内存管理器策略</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c21d05f31057c5bcd2ebdd01f4e62a0e">10 - 调度，抢占和驱逐</h1>
    <div class="lead">在Kubernetes中，调度 (scheduling) 指的是确保 Pods 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pods 以便高优先级的 Pods 可以 调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pods 失效的过程。</div>
	<!--
title: "Scheduling, Preemption and Eviction"
weight: 90
content_type: concept
description: >
  In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes
  so that the kubelet can run them. Preemption is the process of terminating
  Pods with lower Priority so that Pods with higher Priority can schedule on
  Nodes. Eviction is the process of proactively terminating one or more Pods on
  resource-starved Nodes.
no_list: true
-->
<!--
In Kubernetes, scheduling refers to making sure that <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>
are matched to <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='Nodes'>Nodes</a> so that the
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> can run them. Preemption
is the process of terminating Pods with lower <a class='glossary-tooltip' title='Pod 优先级表示一个 Pod 相对于其他 Pod 的重要性。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority' target='_blank' aria-label='Priority'>Priority</a>
so that Pods with higher Priority can schedule on Nodes. Eviction is the process
of terminating one or more Pods on Nodes.
-->
<!-- ## Scheduling -->
<h2 id="调度">调度</h2>
<ul>
<li><a href="/zh/docs/concepts/scheduling-eviction/kube-scheduler/">Kubernetes 调度器</a></li>
<li><a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">将 Pods 指派到节点</a></li>
<li><a href="/zh/docs/concepts/scheduling-eviction/pod-overhead/">Pod 开销</a></li>
<li><a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">污点和容忍</a></li>
<li><a href="/zh/docs/concepts/scheduling-eviction/scheduling-framework">调度框架</a></li>
<li><a href="/zh/docs/concepts/scheduling-eviction/scheduler-perf-tuning/">调度器的性能调试</a></li>
<li><a href="/zh/docs/concepts/scheduling-eviction/resource-bin-packing/">扩展资源的资源装箱</a></li>
</ul>
<!-- ## Pod Disruption -->
<h2 id="pod-干扰">Pod 干扰</h2>
<ul>
<li><a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod 优先级和抢占</a></li>
<li><a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">节点压力驱逐</a></li>
<li><a href="/zh/docs/concepts/scheduling-eviction/api-eviction/">API发起的驱逐</a></li>
</ul>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-598f36d691ab197f9d995784574b0a12">10.1 - Kubernetes 调度器</h1>
    
	<!--
title: Kubernetes Scheduler
content_type: concept
weight: 10
-->
<!-- overview -->
<!--
In Kubernetes, _scheduling_ refers to making sure that <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>
are matched to <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='Nodes'>Nodes</a> so that
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='Kubelet'>Kubelet</a> can run them.
-->
<p>在 Kubernetes 中，<em>调度</em> 是指将 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 放置到合适的
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='Node'>Node</a> 上，然后对应 Node 上的
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='Kubelet'>Kubelet</a> 才能够运行这些 pod。</p>
<!-- body -->
<!--
## Scheduling overview {#scheduling}
-->
<h2 id="scheduling">调度概览</h2>
<!--
A scheduler watches for newly created Pods that have no Node assigned. For
every Pod that the scheduler discovers, the scheduler becomes responsible
for finding the best Node for that Pod to run on. The scheduler reaches
this placement decision taking into account the scheduling principles
described below.
-->
<p>调度器通过 kubernetes 的监测（Watch）机制来发现集群中新创建且尚未被调度到 Node 上的 Pod。
调度器会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上来运行。
调度器会依据下文的调度原则来做出调度选择。</p>
<!--
If you want to understand why Pods are placed onto a particular Node,
or if you're planning to implement a custom scheduler yourself, this
page will help you learn about scheduling.
-->
<p>如果你想要理解 Pod 为什么会被调度到特定的 Node 上，或者你想要尝试实现
一个自定义的调度器，这篇文章将帮助你了解调度。</p>
<!--
## kube-scheduler
-->
<h2 id="kube-scheduler">kube-scheduler</h2>
<!--
[kube-scheduler](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)
is the default scheduler for Kubernetes and runs as part of the
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>.
kube-scheduler is designed so that, if you want and need to, you can
write your own scheduling component and use that instead.
-->
<p><a href="/zh/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler</a>
是 Kubernetes 集群的默认调度器，并且是集群
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a> 的一部分。
如果你真的希望或者有这方面的需求，kube-scheduler 在设计上是允许
你自己写一个调度组件并替换原有的 kube-scheduler。</p>
<!--
For every newly created pods or other unscheduled pods, kube-scheduler
selects a optimal node for them to run on.  However, every container in
pods has different requirements for resources and every pod also has
different requirements. Therefore, existing nodes need to be filtered
according to the specific scheduling requirements.
-->
<p>对每一个新创建的 Pod 或者是未被调度的 Pod，kube-scheduler 会选择一个最优的
Node 去运行这个 Pod。然而，Pod 内的每一个容器对资源都有不同的需求，而且
Pod 本身也有不同的资源需求。因此，Pod 在被调度到 Node 上之前，
根据这些特定的资源调度需求，需要对集群中的 Node 进行一次过滤。</p>
<!--
In a cluster, Nodes that meet the scheduling requirements for a Pod
are called _feasible_ nodes. If none of the nodes are suitable, the pod
remains unscheduled until the scheduler is able to place it.
-->
<p>在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 <em>可调度节点</em>。
如果没有任何一个 Node 能满足 Pod 的资源请求，那么这个 Pod 将一直停留在
未调度状态直到调度器能够找到合适的 Node。</p>
<!--
The scheduler finds feasible Nodes for a Pod and then runs a set of
functions to score the feasible Nodes and picks a Node with the highest
score among the feasible ones to run the Pod. The scheduler then notifies
the API server about this decision in a process called _binding_.
-->
<p>调度器先在集群中找到一个 Pod 的所有可调度节点，然后根据一系列函数对这些可调度节点打分，
选出其中得分最高的 Node 来运行 Pod。之后，调度器将这个调度决定通知给
kube-apiserver，这个过程叫做 <em>绑定</em>。</p>
<!--
Factors that need to be taken into account for scheduling decisions include
individual and collective resource requirements, hardware / software /
policy constraints, affinity and anti-affinity specifications, data
locality, inter-workload interference, and so on.
-->
<p>在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件/软件/策略限制、
亲和以及反亲和要求、数据局域性、负载间的干扰等等。</p>
<!--
## Scheduling with kube-scheduler {#kube-scheduler-implementation}
-->
<h2 id="kube-scheduler-implementation">kube-scheduler 调度流程</h2>
<!--
kube-scheduler selects a node for the pod in a 2-step operation:

1. Filtering
2. Scoring
-->
<p>kube-scheduler 给一个 pod 做调度选择包含两个步骤：</p>
<ol>
<li>过滤</li>
<li>打分</li>
</ol>
<!--
The _filtering_ step finds the set of Nodes where it's feasible to
schedule the Pod. For example, the PodFitsResources filter checks whether a
candidate Node has enough available resource to meet a Pod's specific
resource requests. After this step, the node list contains any suitable
Nodes; often, there will be more than one. If the list is empty, that
Pod isn't (yet) schedulable.
-->
<p>过滤阶段会将所有满足 Pod 调度需求的 Node 选出来。
例如，PodFitsResources 过滤函数会检查候选 Node 的可用资源能否满足 Pod 的资源请求。
在过滤之后，得出一个 Node 列表，里面包含了所有可调度节点；通常情况下，
这个 Node 列表包含不止一个 Node。如果这个列表是空的，代表这个 Pod 不可调度。</p>
<!--
In the _scoring_ step, the scheduler ranks the remaining nodes to choose
the most suitable Pod placement. The scheduler assigns a score to each Node
that survived filtering, basing this score on the active scoring rules.
-->
<p>在打分阶段，调度器会为 Pod 从所有可调度节点中选取一个最合适的 Node。
根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。</p>
<!--
Finally, kube-scheduler assigns the Pod to the Node with the highest ranking.
If there is more than one node with equal scores, kube-scheduler selects
one of these at random.
-->
<p>最后，kube-scheduler 会将 Pod 调度到得分最高的 Node 上。
如果存在多个得分最高的 Node，kube-scheduler 会从中随机选取一个。</p>
<!--
There are two supported ways to configure the filtering and scoring behavior
of the scheduler:
-->
<p>支持以下两种方式配置调度器的过滤和打分行为：</p>
<!--
1. [Scheduling Policies](/docs/reference/scheduling/policies) allow you to
  configure _Predicates_ for filtering and _Priorities_ for scoring.
1. [Scheduling Profiles](/docs/reference/scheduling/config/#profiles) allow you to
  configure Plugins that implement different scheduling stages, including:
  `QueueSort`, `Filter`, `Score`, `Bind`, `Reserve`, `Permit`, and others. You
  can also configure the kube-scheduler to run different profiles.
 -->
<ol>
<li><a href="/zh/docs/reference/scheduling/policies">调度策略</a> 允许你配置过滤的 <em>断言(Predicates)</em>
和打分的 <em>优先级(Priorities)</em> 。</li>
<li><a href="/zh/docs/reference/scheduling/config/#profiles">调度配置</a> 允许你配置实现不同调度阶段的插件，
包括：<code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code> 等等。
你也可以配置 kube-scheduler 运行不同的配置文件。</li>
</ol>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about [scheduler performance tuning](/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)
* Read about [Pod topology spread constraints](/docs/concepts/workloads/pods/pod-topology-spread-constraints/)
* Read the [reference documentation](/docs/reference/command-line-tools-reference/kube-scheduler/) for kube-scheduler
* Read the [kube-scheduler config (v1beta3)](/docs/reference/config-api/kube-scheduler-config.v1beta3/) reference
* Learn about [configuring multiple schedulers](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)
* Learn about [topology management policies](/docs/tasks/administer-cluster/topology-manager/)
* Learn about [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/)
-->
<ul>
<li>阅读关于 <a href="/zh/docs/concepts/scheduling-eviction/scheduler-perf-tuning/">调度器性能调优</a></li>
<li>阅读关于 <a href="/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/">Pod 拓扑分布约束</a></li>
<li>阅读关于 kube-scheduler 的 <a href="/zh/docs/reference/command-line-tools-reference/kube-scheduler/">参考文档</a></li>
<li>阅读 <a href="/zh/docs/reference/config-api/kube-scheduler-config.v1beta3/">kube-scheduler 配置参考 (v1beta3)</a></li>
<li>了解关于 <a href="/zh/docs/tasks/extend-kubernetes/configure-multiple-schedulers/">配置多个调度器</a> 的方式</li>
<li>了解关于 <a href="/zh/docs/tasks/administer-cluster/topology-manager/">拓扑结构管理策略</a></li>
<li>了解关于 <a href="/zh/docs/concepts/scheduling-eviction/pod-overhead/">Pod 额外开销</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-21169f516071aea5d16734a4c27789a5">10.2 - 将 Pod 指派给节点</h1>
    
	<!--
reviewers:
- davidopp
- kevin-wangzefeng
- bsalamat
title: Assigning Pods to Nodes
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
You can constrain a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> so that it can only run on particular set of
<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='Node(s)'>Node(s)</a>.
There are several ways to do this, and the recommended approaches all use
[label selectors](/docs/concepts/overview/working-with-objects/labels/) to facilitate the selection.
Generally such constraints are unnecessary, as the scheduler will automatically do a reasonable placement
(e.g. spread your pods across nodes so as not place the pod on a node with insufficient free resources, etc.)
However, there are some circumstances where you may want to control which node
the Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it, or to co-locate pods from two different
services that communicate a lot into the same availability zone.
-->
<p>你可以约束一个 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
只能在特定的<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>上运行。
有几种方法可以实现这点，推荐的方法都是用
<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签选择算符</a>来进行选择。
通常这样的约束不是必须的，因为调度器将自动进行合理的放置（比如，将 Pod 分散到节点上，
而不是将 Pod 放置在可用资源不足的节点上等等）。但在某些情况下，你可能需要进一步控制
Pod 被部署到的节点。例如，确保 Pod 最终落在连接了 SSD 的机器上，
或者将来自两个不同的服务且有大量通信的 Pods 被放置在同一个可用区。</p>
<!-- body -->
<!--
You can use any of the following methods to choose where Kubernetes schedules
specific Pods: 

* [nodeSelector](#nodeselector) field matching against [node labels](#built-in-node-labels)
* [Affinity and anti-affinity](#affinity-and-anti-affinity)
* [nodeName](#nodename) field
-->
<p>你可以使用下列方法中的任何一种来选择 Kubernetes 对特定 Pod 的调度：</p>
<ul>
<li>与<a href="#built-in-node-labels">节点标签</a>匹配的 <a href="#nodeSelector">nodeSelector</a></li>
<li><a href="#affinity-and-anti-affinity">亲和性与反亲和性</a></li>
<li><a href="#nodename">nodeName</a> 字段</li>
</ul>
<!--
## Node labels {#built-in-node-labels}

Like many other Kubernetes objects, nodes have
[labels](/docs/concepts/overview/working-with-objects/labels/). You can [attach labels manually](/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node).
Kubernetes also populates a standard set of labels on all nodes in a cluster. See [Well-Known Labels, Annotations and Taints](/docs/reference/labels-annotations-taints/)
for a list of common node labels.
-->
<h2 id="built-in-node-labels">节点标签    </h2>
<p>与很多其他 Kubernetes 对象类似，节点也有<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签</a>。
你可以<a href="/zh/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node">手动地添加标签</a>。
Kubernetes 也会为集群中所有节点添加一些标准的标签。
参见<a href="/zh/docs/reference/labels-annotations-taints/">常用的标签、注解和污点</a>以了解常见的节点标签。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The value of these labels is cloud provider specific and is not guaranteed to be reliable.
For example, the value of `kubernetes.io/hostname` may be the same as the node name in some environments
and a different value in other environments.
-->
<p>这些标签的取值是取决于云提供商的，并且是无法在可靠性上给出承诺的。
例如，<code>kubernetes.io/hostname</code> 的取值在某些环境中可能与节点名称相同，
而在其他环境中会取不同的值。
</div>
<!--
### Node isolation/restriction

Adding labels to nodes allows you to target Pods for scheduling on specific
nodes or groups of nodes. You can use this functionality to ensure that specific
Pods only run on nodes with certain isolation, security, or regulatory
properties. 
-->
<h2 id="node-isolation-restriction">节点隔离/限制 </h2>
<p>通过为节点添加标签，你可以准备让 Pod 调度到特定节点或节点组上。
你可以使用这个功能来确保特定的 Pod 只能运行在具有一定隔离性，安全性或监管属性的节点上。</p>
<!--
If you use labels for node isolation, choose label keys that the <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>
cannot modify. This prevents a compromised node from setting those labels on
itself so that the scheduler schedules workloads onto the compromised node.
-->
<p>如果使用标签来实现节点隔离，建议选择节点上的
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>
无法修改的标签键。
这可以防止受感染的节点在自身上设置这些标签，进而影响调度器将工作负载调度到受感染的节点。</p>
<!--
The [`NodeRestriction` admission plugin](/docs/reference/access-authn-authz/admission-controllers/#noderestriction)
prevents the kubelet from setting or modifying labels with a
`node-restriction.kubernetes.io/` prefix. 

To make use of that label prefix for node isolation:
-->
<p><a href="/zh/docs/reference/access-authn-authz/admission-controllers/#noderestriction"><code>NodeRestriction</code> 准入插件</a>防止
kubelet 使用 <code>node-restriction.kubernetes.io/</code> 前缀设置或修改标签。</p>
<p>要使用该标签前缀进行节点隔离：</p>
<!--
1. Ensure you are using the [Node authorizer](/docs/reference/access-authn-authz/node/) and have _enabled_ the `NodeRestriction` admission plugin.
2. Add labels with the `node-restriction.kubernetes.io/` prefix to your nodes, and use those labels in your [node selectors](#nodeselector).
   For example, `example.com.node-restriction.kubernetes.io/fips=true` or `example.com.node-restriction.kubernetes.io/pci-dss=true`.
-->
<ol>
<li>确保你在使用<a href="/zh/docs/reference/access-authn-authz/node/">节点鉴权</a>机制并且已经启用了
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction 准入插件</a>。</li>
<li>将带有 <code>node-restriction.kubernetes.io/</code> 前缀的标签添加到 Node 对象，
然后在<a href="#nodeSelector">节点选择器</a>中使用这些标签。
例如，<code>example.com.node-restriction.kubernetes.io/fips=true</code> 或
<code>example.com.node-restriction.kubernetes.io/pci-dss=true</code>。</li>
</ol>
<h2 id="nodeselector">nodeSelector</h2>
<!--
`nodeSelector` is the simplest recommended form of node selection constraint.
You can add the `nodeSelector` field to your Pod specification and specify the
[node labels](#built-in-node-labels) you want the target node to have.
Kubernetes only schedules the Pod onto nodes that have each of the labels you
specify. 
-->
<p><code>nodeSelector</code> 是节点选择约束的最简单推荐形式。你可以将 <code>nodeSelector</code> 字段添加到
Pod 的规约中设置你希望目标节点所具有的<a href="#built-in-node-labels">节点标签</a>。
Kubernetes 只会将 Pod 调度到拥有你所指定的每个标签的节点上。</p>
<!--
See [Assign Pods to Nodes](/docs/tasks/configure-pod-container/assign-pods-nodes) for more
information.
-->
<p>进一步的信息可参见<a href="/zh/docs/tasks/configure-pod-container/assign-pods-nodes">将 Pod 指派给节点</a>。</p>
<!--
## Affinity and anti-affinity

`nodeSelector` is the simplest way to constrain Pods to nodes with specific
labels. Affinity and anti-affinity expands the types of constraints you can
define. Some of the benefits of affinity and anti-affinity include:
-->
<h2 id="affinity-and-anti-affinity">亲和性与反亲和性 </h2>
<p><code>nodeSelector</code> 提供了一种最简单的方法来将 Pod 约束到具有特定标签的节点上。
亲和性和反亲和性扩展了你可以定义的约束类型。使用亲和性与反亲和性的一些好处有：</p>
<!--
* The affinity/anti-affinity language is more expressive. `nodeSelector` only
  selects nodes with all the specified labels. Affinity/anti-affinity gives you
  more control over the selection logic.
* You can indicate that a rule is *soft* or *preferred*, so that the scheduler
  still schedules the Pod even if it can't find a matching node.
* You can constrain a Pod using labels on other Pods running on the node (or other topological domain),
  instead of just node labels, which allows you to define rules for which Pods
  can be co-located on a node.
-->
<ul>
<li>亲和性、反亲和性语言的表达能力更强。<code>nodeSelector</code> 只能选择拥有所有指定标签的节点。
亲和性、反亲和性为你提供对选择逻辑的更强控制能力。</li>
<li>你可以标明某规则是“软需求”或者“偏好”，这样调度器在无法找到匹配节点时仍然调度该 Pod。</li>
<li>你可以使用节点上（或其他拓扑域中）运行的其他 Pod 的标签来实施调度约束，
而不是只能使用节点本身的标签。这个能力让你能够定义规则允许哪些 Pod 可以被放置在一起。</li>
</ul>
<!--
### Node affinity

Node affinity is conceptually similar to `nodeSelector`, allowing you to constrain which nodes your
Pod can be scheduled on based on node labels. There are two types of node
affinity:
-->
<h3 id="node-affinity">节点亲和性  </h3>
<p>节点亲和性概念上类似于 <code>nodeSelector</code>，
它使你可以根据节点上的标签来约束 Pod 可以调度到哪些节点上。
节点亲和性有两种：</p>
<!--
* `requiredDuringSchedulingIgnoredDuringExecution`: The scheduler can't
  schedule the Pod unless the rule is met. This functions like `nodeSelector`,
  but with a more expressive syntax.
* `preferredDuringSchedulingIgnoredDuringExecution`: The scheduler tries to
  find a node that meets the rule. If a matching node is not available, the
  scheduler still schedules the Pod.
-->
<ul>
<li><code>requiredDuringSchedulingIgnoredDuringExecution</code>：
调度器只有在规则被满足的时候才能执行调度。此功能类似于 <code>nodeSelector</code>，
但其语法表达能力更强。</li>
<li><code>preferredDuringSchedulingIgnoredDuringExecution</code>：
调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod。</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
In the preceding types, `IgnoredDuringExecution` means that if the node labels
change after Kubernetes schedules the Pod, the Pod continues to run.
-->
<p>在上述类型中，<code>IgnoredDuringExecution</code> 意味着如果节点标签在 Kubernetes
调度 Pod 时发生了变更，Pod 仍将继续运行。
</div>
<!--
You can specify node affinities using the `.spec.affinity.nodeAffinity` field in
your Pod spec.

For example, consider the following Pod spec:
-->
<p>你可以使用 Pod 规约中的 <code>.spec.affinity.nodeAffinity</code> 字段来设置节点亲和性。
例如，考虑下面的 Pod 规约：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-with-node-affinity.yaml" download="pods/pod-with-node-affinity.yaml"><code>pods/pod-with-node-affinity.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-with-node-affinity-yaml')" title="Copy pods/pod-with-node-affinity.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-with-node-affinity-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>with-node-affinity<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>kubernetes.io/os<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- linux<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">preferredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">preference</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>another-node-label-key<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- another-node-label-value<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>with-node-affinity<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:2.0</code></pre></div>
    </div>
</div>


<!--
In this example, the following rules apply:

* The node *must* have a label with the key `kubernetes.io/os` and
  the value `linux`.
* The node *preferably* has a label with the key `another-node-label-key` and
  the value `another-node-label-value`.
-->
<p>在这一示例中，所应用的规则如下：</p>
<ul>
<li>节点必须包含键名为 <code>kubernetes.io/os</code> 的标签，并且其取值为 <code>linux</code>。</li>
<li>节点 <strong>最好</strong> 具有键名为 <code>another-node-label-key</code> 且取值为
<code>another-node-label-value</code> 的标签。</li>
</ul>
<!--
You can use the `operator` field to specify a logical operator for Kubernetes to use when
interpreting the rules. You can use `In`, `NotIn`, `Exists`, `DoesNotExist`,
`Gt` and `Lt`.
-->
<p>你可以使用 <code>operator</code> 字段来为 Kubernetes 设置在解释规则时要使用的逻辑操作符。
你可以使用 <code>In</code>、<code>NotIn</code>、<code>Exists</code>、<code>DoesNotExist</code>、<code>Gt</code> 和 <code>Lt</code> 之一作为操作符。</p>
<!--
`NotIn` and `DoesNotExist` allow you to define node anti-affinity behavior.
Alternatively, you can use [node taints](/docs/concepts/scheduling-eviction/taint-and-toleration/) 
to repel Pods from specific nodes.
-->
<p><code>NotIn</code> 和 <code>DoesNotExist</code> 可用来实现节点反亲和性行为。
你也可以使用<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">节点污点</a>
将 Pod 从特定节点上驱逐。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you specify both `nodeSelector` and `nodeAffinity`, *both* must be satisfied
for the Pod to be scheduled onto a node.
-->
<p>如果你同时指定了 <code>nodeSelector</code> 和 <code>nodeAffinity</code>，<strong>两者</strong> 必须都要满足，
才能将 Pod 调度到候选节点上。</p>
<!--
If you specify multiple `nodeSelectorTerms` associated with `nodeAffinity`
types, then the Pod can be scheduled onto a node if one of the specified `nodeSelectorTerms` can be
satisfied.
-->
<p>如果你指定了多个与 <code>nodeAffinity</code> 类型关联的 <code>nodeSelectorTerms</code>，
只要其中一个 <code>nodeSelectorTerms</code> 满足的话，Pod 就可以被调度到节点上。</p>
<!--
If you specify multiple `matchExpressions` associated with a single `nodeSelectorTerms`,
then the Pod can be scheduled onto a node only if all the `matchExpressions` are
satisfied. 
-->
<p>如果你指定了多个与同一 <code>nodeSelectorTerms</code> 关联的 <code>matchExpressions</code>，
则只有当所有 <code>matchExpressions</code> 都满足时 Pod 才可以被调度到节点上。</p>

</div>
<!--
See [Assign Pods to Nodes using Node Affinity](/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/)
for more information.
-->
<p>参阅<a href="/zh/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/">使用节点亲和性来为 Pod 指派节点</a>，
以了解进一步的信息。</p>
<!--
#### Node affinity weight

You can specify a `weight` between 1 and 100 for each instance of the
`preferredDuringSchedulingIgnoredDuringExecution` affinity type. When the
scheduler finds nodes that meet all the other scheduling requirements of the Pod, the
scheduler iterates through every preferred rule that the node satisfies and adds the
value of the `weight` for that expression to a sum.
-->
<h4 id="node-affinity-weight">节点亲和性权重  </h4>
<p>你可以为 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 亲和性类型的每个实例设置
<code>weight</code> 字段，其取值范围是 1 到 100。
当调度器找到能够满足 Pod 的其他调度请求的节点时，调度器会遍历节点满足的所有的偏好性规则，
并将对应表达式的 <code>weight</code> 值加和。</p>
<!--
The final sum is added to the score of other priority functions for the node.
Nodes with the highest total score are prioritized when the scheduler makes a
scheduling decision for the Pod.

For example, consider the following Pod spec: 
-->
<p>最终的加和值会添加到该节点的其他优先级函数的评分之上。
在调度器为 Pod 作出调度决定时，总分最高的节点的优先级也最高。</p>
<p>例如，考虑下面的 Pod 规约：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-with-affinity-anti-affinity.yaml" download="pods/pod-with-affinity-anti-affinity.yaml"><code>pods/pod-with-affinity-anti-affinity.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-with-affinity-anti-affinity-yaml')" title="Copy pods/pod-with-affinity-anti-affinity.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-with-affinity-anti-affinity-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>with-affinity-anti-affinity<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>kubernetes.io/os<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- linux<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">preferredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">preference</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>label-1<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- key-1<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">50</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">preference</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>label-2<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- key-2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>with-node-affinity<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:2.0</code></pre></div>
    </div>
</div>


<!--
If there are two possible nodes that match the
`requiredDuringSchedulingIgnoredDuringExecution` rule, one with the
`label-1:key-1` label and another with the `label-2:key-2` label, the scheduler
considers the `weight` of each node and adds the weight to the other scores for
that node, and schedules the Pod onto the node with the highest final score.
-->
<p>如果存在两个候选节点，都满足 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 规则，
其中一个节点具有标签 <code>label-1:key-1</code>，另一个节点具有标签 <code>label-2:key-2</code>，
调度器会考察各个节点的 <code>weight</code> 取值，并将该权重值添加到节点的其他得分值之上，</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
If you want Kubernetes to successfully schedule the Pods in this example, you
must have existing nodes with the `kubernetes.io/os=linux` label.
-->
<p>如果你希望 Kubernetes 能够成功地调度此例中的 Pod，你必须拥有打了
<code>kubernetes.io/os=linux</code> 标签的节点。
</div>
<!--
#### Node affinity per scheduling profile
-->
<h4 id="node-affinity-per-scheduling-profile">逐个调度方案中设置节点亲和性   </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code>
</div>


<!--
When configuring multiple [scheduling profiles](/docs/reference/scheduling/config/#multiple-profiles), you can associate
a profile with a Node affinity, which is useful if a profile only applies to a specific set of nodes.
To do so, add an `addedAffinity` to the `args` field  of the [`NodeAffinity` plugin](/docs/reference/scheduling/config/#scheduling-plugins)
in the [scheduler configuration](/docs/reference/scheduling/config/). For example:
-->
<p>在配置多个<a href="/zh/docs/reference/scheduling/config/#multiple-profiles">调度方案</a>时，
你可以将某个方案与节点亲和性关联起来，如果某个调度方案仅适用于某组特殊的节点时，
这样做是很有用的。
要实现这点，可以在<a href="/zh/docs/reference/scheduling/config/">调度器配置</a>中为
<a href="/zh/docs/reference/scheduling/config/#scheduling-plugins"><code>NodeAffinity</code> 插件</a>的
<code>args</code> 字段添加 <code>addedAffinity</code>。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">profiles</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">schedulerName</span>:<span style="color:#bbb"> </span>default-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">schedulerName</span>:<span style="color:#bbb"> </span>foo-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pluginConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>NodeAffinity<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">addedAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- <span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>scheduler-profile<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span>- foo<span style="color:#bbb">
</span></code></pre></div><!--
The `addedAffinity` is applied to all Pods that set `.spec.schedulerName` to `foo-scheduler`, in addition to the
NodeAffinity specified in the PodSpec.
That is, in order to match the Pod, nodes need to satisfy `addedAffinity` and
the Pod's `.spec.NodeAffinity`.
-->
<p>这里的 <code>addedAffinity</code> 除遵从 Pod 规约中设置的节点亲和性之外，还
适用于将 <code>.spec.schedulerName</code> 设置为 <code>foo-scheduler</code>。
换言之，为了匹配 Pod，节点需要满足 <code>addedAffinity</code> 和 Pod 的 <code>.spec.NodeAffinity</code>。</p>
<!--
Since the `addedAffinity` is not visible to end users, its behavior might be
unexpected to them. Use node labels that have a clear correlation to the
scheduler profile name.
-->
<p>由于 <code>addedAffinity</code> 对最终用户不可见，其行为可能对用户而言是出乎意料的。
应该使用与调度方案名称有明确关联的节点标签。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The DaemonSet controller, which [creates Pods for DaemonSets](/docs/concepts/workloads/controllers/daemonset/#scheduled-by-default-scheduler),
does not support scheduling profiles. When the DaemonSet controller creates
Pods, the default Kubernetes scheduler places those Pods and honors any
`nodeAffinity` rules in the DaemonSet controller.
-->
<p>DaemonSet 控制器<a href="/zh/docs/concepts/workloads/controllers/daemonset/#scheduled-by-default-scheduler">为 DaemonSet 创建 Pods</a>，
但该控制器不理会调度方案。
DaemonSet 控制器创建 Pod 时，默认的 Kubernetes 调度器负责放置 Pod，
并遵从 DaemonSet 控制器中奢侈的 <code>nodeAffinity</code> 规则。
</div>
<!--
### Inter-pod affinity and anti-affinity

Inter-pod affinity and anti-affinity allow you to constrain which nodes your
Pods can be scheduled on based on the labels of **Pods** already running on that
node, instead of the node labels.
-->
<h3 id="inter-pod-affinity-and-anti-affinity">pod 间亲和性与反亲和性 </h3>
<p>Pod 间亲和性与反亲和性使你可以基于已经在节点上运行的 <strong>Pod</strong> 的标签来约束
Pod 可以调度到的节点，而不是基于节点上的标签。</p>
<!--
Inter-pod affinity and anti-affinity rules take the form "this
Pod should (or, in the case of anti-affinity, should not) run in an X if that X
is already running one or more Pods that meet rule Y", where X is a topology
domain like node, rack, cloud provider zone or region, or similar and Y is the
rule Kubernetes tries to satisfy.
-->
<p>Pod 间亲和性与反亲和性的规则格式为“如果 X 上已经运行了一个或多个满足规则 Y 的 Pod，
则这个 Pod 应该（或者在反亲和性的情况下不应该）运行在 X 上”。
这里的 X 可以是节点、机架、云提供商可用区或地理区域或类似的拓扑域，
Y 则是 Kubernetes 尝试满足的规则。</p>
<!--
You express these rules (Y) as [label selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors)
with an optional associated list of namespaces. Pods are namespaced objects in
Kubernetes, so Pod labels also implicitly have namespaces. Any label selectors
for Pod labels should specify the namespaces in which Kubernetes should look for those
labels.
-->
<p>你通过<a href="/zh/docs/concepts/overview/working-with-objects/labels/#label-selectors">标签选择算符</a>
的形式来表达规则（Y），并可根据需要指定选关联的名字空间列表。
Pod 在 Kubernetes 中是名字空间作用域的对象，因此 Pod 的标签也隐式地具有名字空间属性。
针对 Pod 标签的所有标签选择算符都要指定名字空间，Kubernetes
会在指定的名字空间内寻找标签。</p>
<!--
You express the topology domain (X) using a `topologyKey`, which is the key for
the node label that the system uses to denote the domain. For examples, see
[Well-Known Labels, Annotations and Taints](/docs/reference/labels-annotations-taints/).
-->
<p>你会通过 <code>topologyKey</code> 来表达拓扑域（X）的概念，其取值是系统用来标示域的节点标签键。
相关示例可参见<a href="/zh/docs/reference/labels-annotations-taints/">常用标签、注解和污点</a>。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Inter-pod affinity and anti-affinity require substantial amount of
processing which can slow down scheduling in large clusters significantly. We do
not recommend using them in clusters larger than several hundred nodes.
-->
<p>Pod 间亲和性和反亲和性都需要相当的计算量，因此会在大规模集群中显著降低调度速度。
我们不建议在包含数百个节点的集群中使用这类设置。
</div>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Pod anti-affinity requires nodes to be consistently labelled, in other words,
every node in the cluster must have an appropriate label matching `topologyKey`.
If some or all nodes are missing the specified `topologyKey` label, it can lead
to unintended behavior.
-->
<p>Pod 反亲和性需要节点上存在一致性的标签。换言之，
集群中每个节点都必须拥有与 <code>topologyKey</code> 匹配的标签。
如果某些或者所有节点上不存在所指定的 <code>topologyKey</code> 标签，调度行为可能与预期的不同。
</div>
<!--
#### Types of inter-pod affinity and anti-affinity

Similar to [node affinity](#node-affinity) are two types of Pod affinity and
anti-affinity as follows:
-->
<h4 id="pod-间亲和性与反亲和性的类型">Pod 间亲和性与反亲和性的类型</h4>
<p>与<a href="#node-affinity">节点亲和性</a>类似，Pod 的亲和性与反亲和性也有两种类型：</p>
<ul>
<li><code>requiredDuringSchedulingIgnoredDuringExecution</code></li>
<li><code>preferredDuringSchedulingIgnoredDuringExecution</code></li>
</ul>
<!--
For example, you could use
`requiredDuringSchedulingIgnoredDuringExecution` affinity to tell the scheduler to
co-locate Pods of two services in the same cloud provider zone because they
communicate with each other a lot. Similarly, you could use
`preferredDuringSchedulingIgnoredDuringExecution` anti-affinity to spread Pods
from a service across multiple cloud provider zones.
-->
<p>例如，你可以使用 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 亲和性来告诉调度器，
将两个服务的 Pod 放到同一个云提供商可用区内，因为它们彼此之间通信非常频繁。
类似地，你可以使用 <code>preferredDuringSchedulingIgnoredDuringExecution</code>
反亲和性来将同一服务的多个 Pod 分布到多个云提供商可用区中。</p>
<!--
To use inter-pod affinity, use the `affinity.podAffinity` field in the Pod spec.
For inter-pod anti-affinity, use the `affinity.podAntiAffinity` field in the Pod
spec.
-->
<p>要使用 Pod 间亲和性，可以使用 Pod 规约中的 <code>.affinity.podAffinity</code> 字段。
对于 Pod 间反亲和性，可以使用 Pod 规约中的 <code>.affinity.podAntiAffinity</code> 字段。</p>
<!--
#### Pod affinity example {#an-example-of-a-pod-that-uses-pod-affinity}

Consider the following Pod spec:
-->
<h4 id="an-example-of-a-pod-that-uses-pod-affinity">Pod 亲和性示例  </h4>
<p>考虑下面的 Pod 规约：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-with-pod-affinity.yaml" download="pods/pod-with-pod-affinity.yaml"><code>pods/pod-with-pod-affinity.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-with-pod-affinity-yaml')" title="Copy pods/pod-with-pod-affinity.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-with-pod-affinity-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>with-pod-affinity<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">podAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>security<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- S1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">podAntiAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">preferredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">podAffinityTerm</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>security<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- S2<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span>topology.kubernetes.io/zone<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>with-pod-affinity<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:2.0<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
This example defines one Pod affinity rule and one Pod anti-affinity rule. The
Pod affinity rule uses the "hard"
`requiredDuringSchedulingIgnoredDuringExecution`, while the anti-affinity rule
uses the "soft" `preferredDuringSchedulingIgnoredDuringExecution`.
-->
<p>本示例定义了一条 Pod 亲和性规则和一条 Pod 反亲和性规则。Pod 亲和性规则配置为
<code>requiredDuringSchedulingIgnoredDuringExecution</code>，而 Pod 反亲和性配置为
<code>preferredDuringSchedulingIgnoredDuringExecution</code>。</p>
<!--
The affinity rule says that the scheduler can only schedule a Pod onto a node if
the node is in the same zone as one or more existing Pods with the label
`security=S1`. More precisely, the scheduler must place the Pod on a node that has the
`topology.kubernetes.io/zone=V` label, as long as there is at least one node in
that zone that currently has one or more Pods with the Pod label `security=S1`. 
-->
<p>亲和性规则表示，仅当节点和至少一个已运行且有 <code>security=S1</code> 的标签的
Pod 处于同一区域时，才可以将该 Pod 调度到节点上。
更确切的说，调度器必须将 Pod 调度到具有 <code>topology.kubernetes.io/zone=V</code>
标签的节点上，并且集群中至少有一个位于该可用区的节点上运行着带有
<code>security=S1</code> 标签的 Pod。</p>
<!--
The anti-affinity rule says that the scheduler should try to avoid scheduling
the Pod onto a node that is in the same zone as one or more Pods with the label
`security=S2`. More precisely, the scheduler should try to avoid placing the Pod on a node that has the
`topology.kubernetes.io/zone=R` label if there are other nodes in the
same zone currently running Pods with the `Security=S2` Pod label.
-->
<p>反亲和性规则表示，如果节点处于 Pod 所在的同一可用区且至少一个 Pod 具有
<code>security=S2</code> 标签，则该 Pod 不应被调度到该节点上。
更确切地说， 如果同一可用区中存在其他运行着带有 <code>security=S2</code> 标签的 Pod 节点，
并且节点具有标签 <code>topology.kubernetes.io/zone=R</code>，Pod 不能被调度到该节点上。</p>
<!--
See the
[design doc](https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md)
for many more examples of Pod affinity and anti-affinity.
-->
<p>查阅<a href="https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md">设计文档</a>
以了解 Pod 亲和性与反亲和性的更多示例。</p>
<!--
You can use the `In`, `NotIn`, `Exists` and `DoesNotExist` values in the
`operator` field for Pod affinity and anti-affinity.

In principle, the `topologyKey` can be any allowed label key with the following
exceptions for performance and security reasons:
-->
<p>你可以针对 Pod 间亲和性与反亲和性为其 <code>operator</code> 字段使用 <code>In</code>、<code>NotIn</code>、<code>Exists</code>、
<code>DoesNotExist</code> 等值。</p>
<p>原则上，<code>topologyKey</code> 可以是任何合法的标签键。出于性能和安全原因，<code>topologyKey</code>
有一些限制：</p>
<!--
* For Pod affinity and anti-affinity, an empty `topologyKey` field is not allowed in both
  `requiredDuringSchedulingIgnoredDuringExecution`
  and `preferredDuringSchedulingIgnoredDuringExecution`.
* For `requiredDuringSchedulingIgnoredDuringExecution` Pod anti-affinity rules,
  the admission controller `LimitPodHardAntiAffinityTopology` limits
  `topologyKey` to `kubernetes.io/hostname`. You can modify or disable the
  admission controller if you want to allow custom topologies.
-->
<ul>
<li>对于 Pod 亲和性而言，在 <code>requiredDuringSchedulingIgnoredDuringExecution</code>
和 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 中，<code>topologyKey</code>
不允许为空。</li>
<li>对于 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 要求的 Pod 反亲和性，
准入控制器 <code>LimitPodHardAntiAffinityTopology</code> 要求 <code>topologyKey</code> 只能是
<code>kubernetes.io/hostname</code>。如果你希望使用其他定制拓扑逻辑，
你可以更改准入控制器或者禁用之。</li>
</ul>
<!--
In addition to `labelSelector` and `topologyKey`, you can optionally specify a list
of namespaces which the `labelSelector` should match against using the
`namespaces` field at the same level as `labelSelector` and `topologyKey`.
If omitted or empty, `namespaces` defaults to the namespace of the Pod where the
affinity/anti-affinity definition appears.
-->
<p>除了 <code>labelSelector</code> 和 <code>topologyKey</code>，你也可以指定 <code>labelSelector</code>
要匹配的命名空间列表，方法是在 <code>labelSelector</code> 和 <code>topologyKey</code>
所在层同一层次上设置  <code>namespaces</code>。
如果 <code>namespaces</code> 被忽略或者为空，则默认为 Pod 亲和性/反亲和性的定义所在的命名空间。</p>
<!--
#### Namespace selector
-->
<h4 id="namespace-selector">名字空间选择算符 </h4>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>


<!--
You can also select matching namespaces using `namespaceSelector`, which is a label query over the set of namespaces.
The affinity term is applied to namespaces selected by both `namespaceSelector` and the `namespaces` field.
Note that an empty `namespaceSelector` ({}) matches all namespaces, while a null or empty `namespaces` list and 
null `namespaceSelector` matches the namespace of the Pod where the rule is defined.
-->
<p>用户也可以使用 <code>namespaceSelector</code> 选择匹配的名字空间，<code>namespaceSelector</code>
是对名字空间集合进行标签查询的机制。
亲和性条件会应用到 <code>namespaceSelector</code> 所选择的名字空间和 <code>namespaces</code> 字段中
所列举的名字空间之上。
注意，空的 <code>namespaceSelector</code>（<code>{}</code>）会匹配所有名字空间，而 null 或者空的
<code>namespaces</code> 列表以及 null 值 <code>namespaceSelector</code> 意味着“当前 Pod 的名字空间”。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
This feature is beta and enabled by default. You can disable it via the
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`PodAffinityNamespaceSelector` in both kube-apiserver and kube-scheduler.
-->
<p>此功能特性是 Beta 版本的，默认是被启用的。你可以通过针对 kube-apiserver 和
kube-scheduler 设置<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>PodAffinityNamespaceSelector</code> 来禁用此特性。
</div>
<!--
#### More practical use-cases

Inter-pod affinity and anti-affinity can be even more useful when they are used with higher
level collections such as ReplicaSets, StatefulSets, Deployments, etc.  These
rules allow you to configure that a set of workloads should
be co-located in the same defined topology, eg., the same node.
-->
<h4 id="更实际的用例">更实际的用例</h4>
<p>Pod 间亲和性与反亲和性在与更高级别的集合（例如 ReplicaSet、StatefulSet、
Deployment 等）一起使用时，它们可能更加有用。
这些规则使得你可以配置一组工作负载，使其位于相同定义拓扑（例如，节点）中。</p>
<!--
In the following example Deployment for the redis cache, the replicas get the label `app=store`. The
`podAntiAffinity` rule tells the scheduler to avoid placing multiple replicas
with the `app=store` label on a single node. This creates each cache in a
separate node.
-->
<p>在下面的 Redis 缓存 Deployment 示例中，副本上设置了标签 <code>app=store</code>。
<code>podAntiAffinity</code> 规则告诉调度器避免将多个带有 <code>app=store</code> 标签的副本部署到同一节点上。
因此，每个独立节点上会创建一个缓存实例。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis-cache<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>store<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>store<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">podAntiAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span>- store<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis-server<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis:3.2-alpine<span style="color:#bbb">
</span></code></pre></div><!--
The following Deployment for the web servers creates replicas with the label `app=web-store`. The
Pod affinity rule tells the scheduler to place each replica on a node that has a
Pod with the label `app=store`. The Pod anti-affinity rule tells the scheduler
to avoid placing multiple `app=web-store` servers on a single node.
-->
<p>下面的 Deployment 用来提供 Web 服务器服务，会创建带有标签 <code>app=web-store</code> 的副本。
Pod 亲和性规则告诉调度器将副本放到运行有标签包含 <code>app=store</code> Pod 的节点上。
Pod 反亲和性规则告诉调度器不要在同一节点上放置多个 <code>app=web-store</code> 的服务器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web-server<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>web-store<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>web-store<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">podAntiAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span>- web-store<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">podAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span>- store<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">topologyKey</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;kubernetes.io/hostname&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web-app<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.16-alpine<span style="color:#bbb">
</span></code></pre></div><!--
Creating the two preceding Deployments results in the following cluster layout,
where each web server is co-located with a cache, on three separate nodes.
-->
<p>创建前面两个 Deployment 会产生如下的集群布局，每个 Web 服务器与一个缓存实例并置，
并分别运行在三个独立的节点上。</p>
<table>
<thead>
<tr>
<th style="text-align:center">node-1</th>
<th style="text-align:center">node-2</th>
<th style="text-align:center">node-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>webserver-1</em></td>
<td style="text-align:center"><em>webserver-2</em></td>
<td style="text-align:center"><em>webserver-3</em></td>
</tr>
<tr>
<td style="text-align:center"><em>cache-1</em></td>
<td style="text-align:center"><em>cache-2</em></td>
<td style="text-align:center"><em>cache-3</em></td>
</tr>
</tbody>
</table>
<!--
See the [ZooKeeper tutorial](/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure)
for an example of a StatefulSet configured with anti-affinity for high
availability, using the same technique as this example.
-->
<p>参阅 <a href="/zh/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure">ZooKeeper 教程</a>
了解一个 StatefulSet 的示例，该 StatefulSet 配置了反亲和性以实现高可用，
所使用的是与此例相同的技术。</p>
<!--
## nodeName

`nodeName` is a more direct form of node selection than affinity or
`nodeSelector`. `nodeName` is a field in the Pod spec. If the `nodeName` field
is not empty, the scheduler ignores the Pod and the kubelet on the named node
tries to place the Pod on that node. Using `nodeName` overrules using
`nodeSelector` or affinity and anti-affinity rules.
-->
<h2 id="nodename">nodeName</h2>
<p><code>nodeName</code> 是比亲和性或者 <code>nodeSelector</code> 更为直接的形式。<code>nodeName</code> 是 Pod
规约中的一个字段。如果 <code>nodeName</code> 字段不为空，调度器会忽略该 Pod，
而指定节点上的 kubelet 会尝试将 Pod 放到该节点上。
使用 <code>nodeName</code> 规则的优先级会高于使用 <code>nodeSelector</code> 或亲和性与非亲和性的规则。</p>
<!--
Some of the limitations of using `nodeName` to select nodes are:

- If the named node does not exist, the Pod will not run, and in
  some cases may be automatically deleted.
- If the named node does not have the resources to accommodate the
  Pod, the Pod will fail and its reason will indicate why,
  for example OutOfmemory or OutOfcpu.
- Node names in cloud environments are not always predictable or stable.
-->
<p>使用 <code>nodeName</code> 来选择节点的方式有一些局限性：</p>
<ul>
<li>如果所指代的节点不存在，则 Pod 无法运行，而且在某些情况下可能会被自动删除。</li>
<li>如果所指代的节点无法提供用来运行 Pod 所需的资源，Pod 会失败，
而其失败原因中会给出是否因为内存或 CPU 不足而造成无法运行。</li>
<li>在云环境中的节点名称并不总是可预测的，也不总是稳定的。</li>
</ul>
<!--
Here is an example of a Pod spec using the `nodeName` field:
-->
<p>下面是一个使用 <code>nodeName</code> 字段的 Pod 规约示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeName</span>:<span style="color:#bbb"> </span>kube-01<span style="color:#bbb">
</span></code></pre></div><!--
The above Pod will only run on the node `kube-01`.
-->
<p>上面的 Pod 只能运行在节点 <code>kube-01</code> 之上。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read more about [taints and tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration/) .
* Read the design docs for [node affinity](https://git.k8s.io/community/contributors/design-proposals/scheduling/nodeaffinity.md)
  and for [inter-pod affinity/anti-affinity](https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md).
* Learn about how the [topology manager](/docs/tasks/administer-cluster/topology-manager/) takes part in node-level
  resource allocation decisions. 
* Learn how to use [nodeSelector](/docs/tasks/configure-pod-container/assign-pods-nodes/).
* Learn how to use [affinity and anti-affinity](/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/).
-->
<ul>
<li>进一步阅读<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">污点与容忍度</a>文档。</li>
<li>阅读<a href="https://git.k8s.io/community/contributors/design-proposals/scheduling/nodeaffinity.md">节点亲和性</a>
和<a href="https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md">Pod 间亲和性与反亲和性</a>
的设计文档。</li>
<li>了解<a href="/zh/docs/tasks/administer-cluster/topology-manager/">拓扑管理器</a>如何参与节点层面资源分配决定。</li>
<li>了解如何使用 <a href="/zh/docs/tasks/configure-pod-container/assign-pods-nodes/">nodeSelector</a>。</li>
<li>了解如何使用<a href="/zh/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/">亲和性和反亲和性</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-da22fe2278df236f71efbe672f392677">10.3 - Pod 开销</h1>
    
	<!--
---
reviewers:
- dchen1107
- egernst
- tallclair
title: Pod Overhead
content_type: concept
weight: 30
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<!--
When you run a Pod on a Node, the Pod itself takes an amount of system resources. These
resources are additional to the resources needed to run the container(s) inside the Pod.
_Pod Overhead_ is a feature for accounting for the resources consumed by the Pod infrastructure
on top of the container requests & limits.
-->
<p>在节点上运行 Pod 时，Pod 本身占用大量系统资源。这些是运行 Pod 内容器所需资源之外的资源。
<em>POD 开销</em> 是一个特性，用于计算 Pod 基础设施在容器请求和限制之上消耗的资源。</p>
<!-- body -->
<!--
In Kubernetes, the Pod's overhead is set at
[admission](/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks)
time according to the overhead associated with the Pod's
[RuntimeClass](/docs/concepts/containers/runtime-class/).
-->
<p>在 Kubernetes 中，Pod 的开销是根据与 Pod 的 <a href="/zh/docs/concepts/containers/runtime-class/">RuntimeClass</a>
相关联的开销在<a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">准入</a>时设置的。</p>
<!--
When Pod Overhead is enabled, the overhead is considered in addition to the sum of container
resource requests when scheduling a Pod. Similarly,the kubelet will include the Pod overhead when sizing
the Pod cgroup, and when carrying out Pod eviction ranking.
-->
<p>如果启用了 Pod Overhead，在调度 Pod 时，除了考虑容器资源请求的总和外，还要考虑 Pod 开销。
类似地，kubelet 将在确定 Pod cgroups 的大小和执行 Pod 驱逐排序时也会考虑 Pod 开销。</p>
<!--
## Enabling Pod Overhead {#set-up}
-->
<h2 id="set-up">启用 Pod 开销</h2>
<!--
You need to make sure that the `PodOverhead`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled (it is on by default as of 1.18)
across your cluster, and a `RuntimeClass` is utilized which defines the `overhead` field.
-->
<p>你需要确保在集群中启用了 <code>PodOverhead</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
（在 1.18 默认是开启的），以及一个定义了 <code>overhead</code> 字段的 <code>RuntimeClass</code>。</p>
<!--
## Usage example
-->
<h2 id="使用示例">使用示例</h2>
<!--
To use the PodOverhead feature, you need a RuntimeClass that defines the `overhead` field. As
an example, you could use the following RuntimeClass definition with a virtualizing container runtime
that uses around 120MiB per Pod for the virtual machine and the guest OS:
-->
<p>要使用 PodOverhead 特性，需要一个定义了 <code>overhead</code> 字段的 RuntimeClass。
作为例子，下面的 RuntimeClass 定义中包含一个虚拟化所用的容器运行时，
RuntimeClass 如下，其中每个 Pod 大约使用 120MiB 用来运行虚拟机和寄宿操作系统：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>RuntimeClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>node.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kata-fc<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">handler</span>:<span style="color:#bbb"> </span>kata-fc<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">overhead</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">podFixed</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;120Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;250m&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
Workloads which are created which specify the `kata-fc` RuntimeClass handler will take the memory and
cpu overheads into account for resource quota calculations, node scheduling, as well as Pod cgroup sizing.

Consider running the given example workload, test-pod:
-->
<p>通过指定 <code>kata-fc</code> RuntimeClass 处理程序创建的工作负载会将内存和 CPU
开销计入资源配额计算、节点调度以及 Pod cgroup 尺寸确定。</p>
<p>假设我们运行下面给出的工作负载示例 test-pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">runtimeClassName</span>:<span style="color:#bbb"> </span>kata-fc<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>busybox-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">stdin</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tty</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>500m<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>1500m<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span></code></pre></div><!--
At admission time the RuntimeClass [admission controller](/docs/reference/access-authn-authz/admission-controllers/)
updates the workload's PodSpec to include the `overhead` as described in the RuntimeClass. If the PodSpec already has this field defined,
the Pod will be rejected. In the given example, since only the RuntimeClass name is specified, the admission controller mutates the Pod
to include an `overhead`.
-->
<p>在准入阶段 RuntimeClass <a href="/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制器</a>
更新工作负载的 PodSpec 以包含
RuntimeClass 中定义的 <code>overhead</code>。如果 PodSpec 中已定义该字段，该 Pod 将会被拒绝。
在这个例子中，由于只指定了 RuntimeClass 名称，所以准入控制器更新了 Pod，使之包含 <code>overhead</code>。</p>
<!--
After the RuntimeClass admission controller, you can check the updated PodSpec:
-->
<p>在 RuntimeClass 准入控制器之后，可以检验一下已更新的 PodSpec:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get pod test-pod -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.spec.overhead}&#39;</span>
</code></pre></div><!--
The output is:
-->
<p>输出：</p>
<pre><code>map[cpu:250m memory:120Mi]
</code></pre><!--
If a ResourceQuota is defined, the sum of container requests as well as the
`overhead` field are counted.
 -->
<p>如果定义了 ResourceQuata, 则容器请求的总量以及 <code>overhead</code> 字段都将计算在内。</p>
<!--
When the kube-scheduler is deciding which node should run a new Pod, the scheduler considers that Pod's
`overhead` as well as the sum of container requests for that Pod. For this example, the scheduler adds the
requests and the overhead, then looks for a node that has 2.25 CPU and 320 MiB of memory available.
-->
<p>当 kube-scheduler 决定在哪一个节点调度运行新的 Pod 时，调度器会兼顾该 Pod 的
<code>overhead</code> 以及该 Pod 的容器请求总量。在这个示例中，调度器将资源请求和开销相加，
然后寻找具备 2.25 CPU 和 320 MiB 内存可用的节点。</p>
<!--
Once a Pod is scheduled to a node, the kubelet on that node creates a new <a class='glossary-tooltip' title='一组具有可选资源隔离、审计和限制的 Linux 进程。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cgroup' target='_blank' aria-label='cgroup'>cgroup</a>
for the Pod. It is within this pod that the underlying container runtime will create containers. -->
<p>一旦 Pod 被调度到了某个节点， 该节点上的 kubelet 将为该 Pod 新建一个
<a class='glossary-tooltip' title='一组具有可选资源隔离、审计和限制的 Linux 进程。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cgroup' target='_blank' aria-label='cgroup'>cgroup</a>。 底层容器运行时将在这个
Pod 中创建容器。</p>
<!--
If the resource has a limit defined for each container (Guaranteed QoS or Burstable QoS with limits defined),
the kubelet will set an upper limit for the pod cgroup associated with that resource (cpu.cfs_quota_us for CPU
and memory.limit_in_bytes memory). This upper limit is based on the sum of the container limits plus the `overhead`
defined in the PodSpec.
-->
<p>如果该资源对每一个容器都定义了一个限制（定义了限制值的 Guaranteed QoS 或者
Burstable QoS），kubelet 会为与该资源（CPU 的 <code>cpu.cfs_quota_us</code> 以及内存的
<code>memory.limit_in_bytes</code>）
相关的 Pod cgroup 设定一个上限。该上限基于 PodSpec 中定义的容器限制总量与 <code>overhead</code> 之和。</p>
<!--
For CPU, if the Pod is Guaranteed or Burstable QoS, the kubelet will set `cpu.shares` based on the sum of container
requests plus the `overhead` defined in the PodSpec.
-->
<p>对于 CPU，如果 Pod 的 QoS 是 Guaranteed 或者 Burstable，kubelet 会基于容器请求总量与
PodSpec 中定义的 <code>overhead</code> 之和设置 <code>cpu.shares</code>。</p>
<!--
Looking at our example, verify the container requests for the workload:
-->
<p>请看这个例子，验证工作负载的容器请求：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get pod test-pod -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.spec.containers[*].resources.limits}&#39;</span>
</code></pre></div><!--
The total container requests are 2000m CPU and 200MiB of memory:
-->
<p>容器请求总计 2000m CPU 和 200MiB 内存：</p>
<pre><code>map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]
</code></pre><!--
Check this against what is observed by the node:
 -->
<p>对照从节点观察到的情况来检查一下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl describe node | grep test-pod -B2
</code></pre></div><!--
The output shows 2250m CPU and 320MiB of memory are requested, which includes PodOverhead:
 -->
<p>该输出显示请求了 2250m CPU 以及 320MiB 内存，包含了 PodOverhead 在内：</p>
<pre><code>  Namespace                   Name                CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------                   ----                ------------  ----------   ---------------  -------------  ---
  default                     test-pod            2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m
</code></pre><!--
## Verify Pod cgroup limits
-->
<h2 id="验证-pod-cgroup-限制">验证 Pod cgroup 限制</h2>
<!--
Check the Pod's memory cgroups on the node where the workload is running. In the following example, [`crictl`](https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md)
is used on the node, which provides a CLI for CRI-compatible container runtimes. This is an
advanced example to show PodOverhead behavior, and it is not expected that users should need to check
cgroups directly on the node.

First, on the particular node, determine the Pod identifier:
-->
<p>在工作负载所运行的节点上检查 Pod 的内存 cgroups。在接下来的例子中，
将在该节点上使用具备 CRI 兼容的容器运行时命令行工具
<a href="https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md"><code>crictl</code></a>。
这是一个显示 PodOverhead 行为的高级示例， 预计用户不需要直接在节点上检查 cgroups。
首先在特定的节点上确定该 Pod 的标识符：</p>
<!--
```bash
# Run this on the node where the Pod is scheduled
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#080;font-style:italic"># 在该 Pod 被调度到的节点上执行如下命令：</span>
<span style="color:#b8860b">POD_ID</span><span style="color:#666">=</span><span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>sudo crictl pods --name test-pod -q<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>
</code></pre></div><!--
From this, you can determine the cgroup path for the Pod:
 -->
<p>可以依此判断该 Pod 的 cgroup 路径：</p>
<!--
```bash
# Run this on the node where the Pod is scheduled
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#080;font-style:italic"># 在该 Pod 被调度到的节点上执行如下命令：</span>
sudo crictl inspectp -o<span style="color:#666">=</span>json <span style="color:#b8860b">$POD_ID</span> | grep cgroupsPath
</code></pre></div><!--
The resulting cgroup path includes the Pod's `pause` container. The Pod level cgroup is one directory above.
-->
<p>执行结果的 cgroup 路径中包含了该 Pod 的 <code>pause</code> 容器。Pod 级别的 cgroup 在即上一层目录。</p>
<pre><code>        &quot;cgroupsPath&quot;: &quot;/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a&quot;
</code></pre><!--
In this specific case, the pod cgroup path is `kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2`. Verify the Pod level cgroup setting for memory:
 -->
<p>在这个例子中，该 Pod 的 cgroup 路径是 <code>kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2</code>。
验证内存的 Pod 级别 cgroup 设置：</p>
<!--
```bash
# Run this on the node where the Pod is scheduled.
# Also, change the name of the cgroup to match the cgroup allocated for your pod.
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#080;font-style:italic"># 在该 Pod 被调度到的节点上执行这个命令。</span>
<span style="color:#080;font-style:italic"># 另外，修改 cgroup 的名称以匹配为该 Pod 分配的 cgroup。</span>
 cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes
</code></pre></div><!--
This is 320 MiB, as expected:
-->
<p>和预期的一样，这一数值为 320 MiB。</p>
<pre><code>335544320
</code></pre><!--
### Observability
-->
<h3 id="可观察性">可观察性</h3>
<!--
A `kube_pod_overhead` metric is available in [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)
to help identify when PodOverhead is being utilized and to help observe stability of workloads
running with a defined Overhead. This functionality is not available in the 1.9 release of
kube-state-metrics, but is expected in a following release. Users will need to build kube-state-metrics
from source in the meantime.
-->
<p>在 <a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a> 中可以通过
<code>kube_pod_overhead</code> 指标来协助确定何时使用 PodOverhead
以及协助观察以一个既定开销运行的工作负载的稳定性。
该特性在 kube-state-metrics 的 1.9 发行版本中不可用，不过预计将在后续版本中发布。
在此之前，用户需要从源代码构建 kube-state-metrics。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* [RuntimeClass](/docs/concepts/containers/runtime-class/)
* [PodOverhead Design](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead)
-->
<ul>
<li><a href="/zh/docs/concepts/containers/runtime-class/">RuntimeClass</a></li>
<li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead">PodOverhead 设计</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ede4960b56a3529ee0bfe7c8fe2d09a5">10.4 - 污点和容忍度</h1>
    
	<!-- overview -->
<!--
[_Node affinity_](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
is a property of <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> that *attracts* them to
a set of <a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='nodes'>nodes</a> (either as a preference or a
hard requirement). _Taints_ are the opposite -- they allow a node to repel a set of pods.
-->
<p><a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity"><em>节点亲和性</em></a>
是 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 的一种属性，它使 Pod
被吸引到一类特定的<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>
（这可能出于一种偏好，也可能是硬性要求）。
<em>污点</em>（Taint）则相反——它使节点能够排斥一类特定的 Pod。</p>
<!--
_Tolerations_ are applied to pods, and allow (but do not require) the pods to schedule
onto nodes with matching taints.

Taints and tolerations work together to ensure that pods are not scheduled
onto inappropriate nodes. One or more taints are applied to a node; this
marks that the node should not accept any pods that do not tolerate the taints.
-->
<p>容忍度（Toleration）是应用于 Pod 上的，允许（但并不要求）Pod
调度到带有与之匹配的污点的节点上。</p>
<p>污点和容忍度（Toleration）相互配合，可以用来避免 Pod 被分配到不合适的节点上。
每个节点上都可以应用一个或多个污点，这表示对于那些不能容忍这些污点的 Pod，是不会被该节点接受的。</p>
<!-- body -->
<!--
## Concepts
-->
<h2 id="概念">概念</h2>
<!--
You add a taint to a node using [kubectl taint](/docs/reference/generated/kubectl/kubectl-commands#taint).
For example,
-->
<p>您可以使用命令 <a href="/docs/reference/generated/kubectl/kubectl-commands#taint">kubectl taint</a> 给节点增加一个污点。比如，</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl taint nodes node1 <span style="color:#b8860b">key1</span><span style="color:#666">=</span>value1:NoSchedule
</code></pre></div><!--
places a taint on node `node1`. The taint has key `key1`, value `value1`, and taint effect `NoSchedule`.
This means that no pod will be able to schedule onto `node1` unless it has a matching toleration.

```shell
kubectl taint nodes node1 key:NoSchedule
```

To remove the taint added by the command above, you can run:
```shell
kubectl taint nodes node1 key1=value1:NoSchedule-
```
-->
<p>给节点 <code>node1</code> 增加一个污点，它的键名是 <code>key1</code>，键值是 <code>value1</code>，效果是 <code>NoSchedule</code>。
这表示只有拥有和这个污点相匹配的容忍度的 Pod 才能够被分配到 <code>node1</code> 这个节点。</p>
<p>若要移除上述命令所添加的污点，你可以执行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl taint nodes node1 <span style="color:#b8860b">key1</span><span style="color:#666">=</span>value1:NoSchedule-
</code></pre></div><!--
You specify a toleration for a pod in the PodSpec. Both of the following tolerations "match" the
taint created by the `kubectl taint` line above, and thus a pod with either toleration would be able
to schedule onto `node1`:
-->
<p>您可以在 PodSpec 中定义 Pod 的容忍度。
下面两个容忍度均与上面例子中使用 <code>kubectl taint</code> 命令创建的污点相匹配，
因此如果一个 Pod 拥有其中的任何一个容忍度都能够被分配到 <code>node1</code> ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;key1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Equal&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;value1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;NoSchedule&#34;</span><span style="color:#bbb">
</span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;key1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Exists&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;NoSchedule&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
Here’s an example of a pod that uses tolerations:
-->
<p>这里是一个使用了容忍度的 Pod：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-with-toleration.yaml" download="pods/pod-with-toleration.yaml"><code>pods/pod-with-toleration.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-with-toleration-yaml')" title="Copy pods/pod-with-toleration.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-with-toleration-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;example-key&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Exists&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;NoSchedule&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
The default value for `operator` is `Equal`.
-->
<p><code>operator</code> 的默认值是 <code>Equal</code>。</p>
<!--
A toleration "matches" a taint if the keys are the same and the effects are the same, and:

* the `operator` is `Exists` (in which case no `value` should be specified), or
* the `operator` is `Equal` and the `value`s are equal
-->
<p>一个容忍度和一个污点相“匹配”是指它们有一样的键名和效果，并且：</p>
<ul>
<li>如果 <code>operator</code> 是 <code>Exists</code> （此时容忍度不能指定 <code>value</code>），或者</li>
<li>如果 <code>operator</code> 是 <code>Equal</code> ，则它们的 <code>value</code> 应该相等</li>
</ul>
<!--
There are two special cases:

An empty `key` with operator `Exists` matches all keys, values and effects which means this
will tolerate everything.

An empty `effect` matches all effects with key `key1`.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>存在两种特殊情况：</p>
<p>如果一个容忍度的 <code>key</code> 为空且 operator 为 <code>Exists</code>，
表示这个容忍度与任意的 key 、value 和 effect 都匹配，即这个容忍度能容忍任意 taint。</p>
<p>如果 <code>effect</code> 为空，则可以与所有键名 <code>key1</code> 的效果相匹配。</p>

</div>
<!--
The above example used `effect` of `NoSchedule`. Alternatively, you can use `effect` of `PreferNoSchedule`.
This is a "preference" or "soft" version of `NoSchedule` - the system will *try* to avoid placing a
pod that does not tolerate the taint on the node, but it is not required. The third kind of `effect` is
`NoExecute`, described later.
-->
<p>上述例子中 <code>effect</code> 使用的值为 <code>NoSchedule</code>，您也可以使用另外一个值 <code>PreferNoSchedule</code>。
这是“优化”或“软”版本的 <code>NoSchedule</code> —— 系统会 <em>尽量</em> 避免将 Pod 调度到存在其不能容忍污点的节点上，
但这不是强制的。<code>effect</code> 的值还可以设置为 <code>NoExecute</code>，下文会详细描述这个值。</p>
<!--
You can put multiple taints on the same node and multiple tolerations on the same pod.
The way Kubernetes processes multiple taints and tolerations is like a filter: start
with all of a node's taints, then ignore the ones for which the pod has a matching toleration; the
remaining un-ignored taints have the indicated effects on the pod. In particular,
-->
<p>您可以给一个节点添加多个污点，也可以给一个 Pod 添加多个容忍度设置。
Kubernetes 处理多个污点和容忍度的过程就像一个过滤器：从一个节点的所有污点开始遍历，
过滤掉那些 Pod 中存在与之相匹配的容忍度的污点。余下未被过滤的污点的 effect 值决定了
Pod 是否会被分配到该节点，特别是以下情况：</p>
<!--

* if there is at least one un-ignored taint with effect `NoSchedule` then Kubernetes will not schedule
the pod onto that node
* if there is no un-ignored taint with effect `NoSchedule` but there is at least one un-ignored taint with
effect `PreferNoSchedule` then Kubernetes will *try* to not schedule the pod onto the node
* if there is at least one un-ignored taint with effect `NoExecute` then the pod will be evicted from
the node (if it is already running on the node), and will not be
scheduled onto the node (if it is not yet running on the node).
-->
<ul>
<li>如果未被过滤的污点中存在至少一个 effect 值为 <code>NoSchedule</code> 的污点，
则 Kubernetes 不会将 Pod 分配到该节点。</li>
<li>如果未被过滤的污点中不存在 effect 值为 <code>NoSchedule</code> 的污点，
但是存在 effect 值为 <code>PreferNoSchedule</code> 的污点，
则 Kubernetes 会 <em>尝试</em> 不将 Pod 分配到该节点。</li>
<li>如果未被过滤的污点中存在至少一个 effect 值为 <code>NoExecute</code> 的污点，
则 Kubernetes 不会将 Pod 分配到该节点（如果 Pod 还未在节点上运行），
或者将 Pod 从该节点驱逐（如果 Pod 已经在节点上运行）。</li>
</ul>
<!--
For example, imagine you taint a node like this
-->
<p>例如，假设您给一个节点添加了如下污点</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl taint nodes node1 <span style="color:#b8860b">key1</span><span style="color:#666">=</span>value1:NoSchedule
kubectl taint nodes node1 <span style="color:#b8860b">key1</span><span style="color:#666">=</span>value1:NoExecute
kubectl taint nodes node1 <span style="color:#b8860b">key2</span><span style="color:#666">=</span>value2:NoSchedule
</code></pre></div><!--
And a pod has two tolerations:
-->
<p>假定有一个 Pod，它有两个容忍度：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;key1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Equal&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;value1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;NoSchedule&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;key1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Equal&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;value1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;NoExecute&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
In this case, the pod will not be able to schedule onto the node, because there is no
toleration matching the third taint. But it will be able to continue running if it is
already running on the node when the taint is added, because the third taint is the only
one of the three that is not tolerated by the pod.
-->
<p>在这种情况下，上述 Pod 不会被分配到上述节点，因为其没有容忍度和第三个污点相匹配。
但是如果在给节点添加上述污点之前，该 Pod 已经在上述节点运行，
那么它还可以继续运行在该节点上，因为第三个污点是三个污点中唯一不能被这个 Pod 容忍的。</p>
<!--
Normally, if a taint with effect `NoExecute` is added to a node, then any pods that do
not tolerate the taint will be evicted immediately, and any pods that do tolerate the
taint will never be evicted. However, a toleration with `NoExecute` effect can specify
an optional `tolerationSeconds` field that dictates how long the pod will stay bound
to the node after the taint is added. For example,
-->
<p>通常情况下，如果给一个节点添加了一个 effect 值为 <code>NoExecute</code> 的污点，
则任何不能忍受这个污点的 Pod 都会马上被驱逐，
任何可以忍受这个污点的 Pod 都不会被驱逐。
但是，如果 Pod 存在一个 effect 值为 <code>NoExecute</code> 的容忍度指定了可选属性
<code>tolerationSeconds</code> 的值，则表示在给节点添加了上述污点之后，
Pod 还能继续在节点上运行的时间。例如，</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;key1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Equal&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;value1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;NoExecute&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tolerationSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">3600</span><span style="color:#bbb">
</span></code></pre></div><!--
means that if this pod is running and a matching taint is added to the node, then
the pod will stay bound to the node for 3600 seconds, and then be evicted. If the
taint is removed before that time, the pod will not be evicted.
-->
<p>这表示如果这个 Pod 正在运行，同时一个匹配的污点被添加到其所在的节点，
那么 Pod 还将继续在节点上运行 3600 秒，然后被驱逐。
如果在此之前上述污点被删除了，则 Pod 不会被驱逐。</p>
<!--
## Example Use Cases
-->
<h2 id="使用例子">使用例子</h2>
<!--
Taints and tolerations are a flexible way to steer pods *away* from nodes or evict
pods that shouldn't be running. A few of the use cases are
-->
<p>通过污点和容忍度，可以灵活地让 Pod <em>避开</em> 某些节点或者将 Pod 从某些节点驱逐。下面是几个使用例子：</p>
<!--
* **Dedicated Nodes**: If you want to dedicate a set of nodes for exclusive use by
a particular set of users, you can add a taint to those nodes (say,
`kubectl taint nodes nodename dedicated=groupName:NoSchedule`) and then add a corresponding
toleration to their pods (this would be done most easily by writing a custom
[admission controller](/docs/reference/access-authn-authz/admission-controllers/)).
The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as
well as any other nodes in the cluster. If you want to dedicate the nodes to them *and*
ensure they *only* use the dedicated nodes, then you should additionally add a label similar
to the taint to the same set of nodes (e.g. `dedicated=groupName`), and the admission
controller should additionally add a node affinity to require that the pods can only schedule
onto nodes labeled with `dedicated=groupName`.
-->
<ul>
<li><strong>专用节点</strong>：如果您想将某些节点专门分配给特定的一组用户使用，您可以给这些节点添加一个污点（即，
<code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>），
然后给这组用户的 Pod 添加一个相对应的 toleration（通过编写一个自定义的
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制器</a>，很容易就能做到）。
拥有上述容忍度的 Pod 就能够被分配到上述专用节点，同时也能够被分配到集群中的其它节点。
如果您希望这些 Pod 只能被分配到上述专用节点，那么您还需要给这些专用节点另外添加一个和上述
污点类似的 label （例如：<code>dedicated=groupName</code>），同时 还要在上述准入控制器中给 Pod
增加节点亲和性要求上述 Pod 只能被分配到添加了 <code>dedicated=groupName</code> 标签的节点上。</li>
</ul>
<!--
* **Nodes with Special Hardware**: In a cluster where a small subset of nodes have specialized
hardware (for example GPUs), it is desirable to keep pods that don't need the specialized
hardware off of those nodes, thus leaving room for later-arriving pods that do need the
specialized hardware. This can be done by tainting the nodes that have the specialized
hardware (e.g. `kubectl taint nodes nodename special=true:NoSchedule` or
`kubectl taint nodes nodename special=true:PreferNoSchedule`) and adding a corresponding
toleration to pods that use the special hardware. As in the dedicated nodes use case,
it is probably easiest to apply the tolerations using a custom
[admission controller](/docs/reference/access-authn-authz/admission-controllers/).
For example, it is recommended to use [Extended
Resources](/docs/concepts/configuration/manage-compute-resources-container/#extended-resources)
to represent the special hardware, taint your special hardware nodes with the
extended resource name and run the
[ExtendedResourceToleration](/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration)
admission controller. Now, because the nodes are tainted, no pods without the
toleration will schedule on them. But when you submit a pod that requests the
extended resource, the `ExtendedResourceToleration` admission controller will
automatically add the correct toleration to the pod and that pod will schedule
on the special hardware nodes. This will make sure that these special hardware
nodes are dedicated for pods requesting such hardware and you don't have to
manually add tolerations to your pods.
-->
<ul>
<li><strong>配备了特殊硬件的节点</strong>：在部分节点配备了特殊硬件（比如 GPU）的集群中，
我们希望不需要这类硬件的 Pod 不要被分配到这些特殊节点，以便为后继需要这类硬件的 Pod 保留资源。
要达到这个目的，可以先给配备了特殊硬件的节点添加 taint
（例如 <code>kubectl taint nodes nodename special=true:NoSchedule</code> 或
<code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>)，
然后给使用了这类特殊硬件的 Pod 添加一个相匹配的 toleration。
和专用节点的例子类似，添加这个容忍度的最简单的方法是使用自定义
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制器</a>。
比如，我们推荐使用<a href="/zh/docs/concepts/configuration/manage-resources-containers/#extended-resources">扩展资源</a>
来表示特殊硬件，给配置了特殊硬件的节点添加污点时包含扩展资源名称，
然后运行一个 <a href="/zh/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration">ExtendedResourceToleration</a>
准入控制器。此时，因为节点已经被设置污点了，没有对应容忍度的 Pod
不会被调度到这些节点。但当你创建一个使用了扩展资源的 Pod 时，
<code>ExtendedResourceToleration</code> 准入控制器会自动给 Pod 加上正确的容忍度，
这样 Pod 就会被自动调度到这些配置了特殊硬件件的节点上。
这样就能够确保这些配置了特殊硬件的节点专门用于运行需要使用这些硬件的 Pod，
并且您无需手动给这些 Pod 添加容忍度。</li>
</ul>
<!--
* **Taint based Evictions**: A per-pod-configurable eviction behavior
when there are node problems, which is described in the next section.
-->
<ul>
<li><strong>基于污点的驱逐</strong>: 这是在每个 Pod 中配置的在节点出现问题时的驱逐行为，接下来的章节会描述这个特性。</li>
</ul>
<!--
## Taint based Evictions
-->
<h2 id="taint-based-evictions">基于污点的驱逐 </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>


<!--
The `NoExecute` taint effect, which affects pods that are already
running on the node as follows

 * pods that do not tolerate the taint are evicted immediately
 * pods that tolerate the taint without specifying `tolerationSeconds` in
   their toleration specification remain bound forever
 * pods that tolerate the taint with a specified `tolerationSeconds` remain
   bound for the specified amount of time
-->
<p>前文提到过污点的 effect 值 <code>NoExecute</code>会影响已经在节点上运行的 Pod</p>
<ul>
<li>如果 Pod 不能忍受 effect 值为 <code>NoExecute</code> 的污点，那么 Pod 将马上被驱逐</li>
<li>如果 Pod 能够忍受 effect 值为 <code>NoExecute</code> 的污点，但是在容忍度定义中没有指定
<code>tolerationSeconds</code>，则 Pod 还会一直在这个节点上运行。</li>
<li>如果 Pod 能够忍受 effect 值为 <code>NoExecute</code> 的污点，而且指定了 <code>tolerationSeconds</code>，
则 Pod 还能在这个节点上继续运行这个指定的时间长度。</li>
</ul>
<!--
The node controller automatically taints a node when certain conditions are
true. The following taints are built in:

 * `node.kubernetes.io/not-ready`: Node is not ready. This corresponds to
   the NodeCondition `Ready` being "`False`".
 * `node.kubernetes.io/unreachable`: Node is unreachable from the node
   controller. This corresponds to the NodeCondition `Ready` being "`Unknown`".
 * `node.kubernetes.io/memory-pressure`: Node has memory pressure.
 * `node.kubernetes.io/disk-pressure`: Node has disk pressure.
 * `node.kubernetes.io/pid-pressure`: Node has PID pressure.
 * `node.kubernetes.io/network-unavailable`: Node's network is unavailable.
 * `node.kubernetes.io/unschedulable`: Node is unschedulable.
 * `node.cloudprovider.kubernetes.io/uninitialized`: When the kubelet is started
    with "external" cloud provider, this taint is set on a node to mark it
    as unusable. After a controller from the cloud-controller-manager initializes
    this node, the kubelet removes this taint.
  -->
<p>当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：</p>
<ul>
<li><code>node.kubernetes.io/not-ready</code>：节点未准备好。这相当于节点状态 <code>Ready</code> 的值为 &quot;<code>False</code>&quot;。</li>
<li><code>node.kubernetes.io/unreachable</code>：节点控制器访问不到节点. 这相当于节点状态 <code>Ready</code> 的值为 &quot;<code>Unknown</code>&quot;。</li>
<li><code>node.kubernetes.io/memory-pressure</code>：节点存在内存压力。</li>
<li><code>node.kubernetes.io/disk-pressure</code>：节点存在磁盘压力。</li>
<li><code>node.kubernetes.io/pid-pressure</code>: 节点的 PID 压力。</li>
<li><code>node.kubernetes.io/network-unavailable</code>：节点网络不可用。</li>
<li><code>node.kubernetes.io/unschedulable</code>: 节点不可调度。</li>
<li><code>node.cloudprovider.kubernetes.io/uninitialized</code>：如果 kubelet 启动时指定了一个 &quot;外部&quot; 云平台驱动，
它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager
的一个控制器初始化这个节点后，kubelet 将删除这个污点。</li>
</ul>
<!--
In case a node is to be evicted, the node controller or the kubelet adds relevant taints
with `NoExecute` effect. If the fault condition returns to normal the kubelet or node
controller can remove the relevant taint(s).
-->
<p>在节点被驱逐时，节点控制器或者 kubelet 会添加带有 <code>NoExecute</code> 效应的相关污点。
如果异常状态恢复正常，kubelet 或节点控制器能够移除相关的污点。</p>
<!--
To maintain the existing [rate limiting](/docs/concepts/architecture/nodes/)
behavior of pod evictions due to node problems, the system actually adds the taints
in a rate-limited way. This prevents massive pod evictions in scenarios such
as the master becoming partitioned from the nodes.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 为了保证由于节点问题引起的 Pod 驱逐
<a href="/zh/docs/concepts/architecture/nodes/">速率限制</a>行为正常，
系统实际上会以限定速率的方式添加污点。在像主控节点与工作节点间通信中断等场景下，
这样做可以避免 Pod 被大量驱逐。
</div>
<!--
This feature, in combination with `tolerationSeconds`, allows a pod
to specify how long it should stay bound to a node that has one or both of these problems.
-->
<p>使用这个功能特性，结合 <code>tolerationSeconds</code>，Pod 就可以指定当节点出现一个
或全部上述问题时还将在这个节点上运行多长的时间。</p>
<!--
For example, an application with a lot of local state might want to stay
bound to node for a long time in the event of network partition, in the hope
that the partition will recover and thus the pod eviction can be avoided.
The toleration the pod would use in that case would look like
-->
<p>比如，一个使用了很多本地状态的应用程序在网络断开时，仍然希望停留在当前节点上运行一段较长的时间，
愿意等待网络恢复以避免被驱逐。在这种情况下，Pod 的容忍度可能是下面这样的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;node.kubernetes.io/unreachable&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Exists&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;NoExecute&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">tolerationSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">6000</span><span style="color:#bbb">
</span></code></pre></div><!--
Note that Kubernetes automatically adds a toleration for
`node.kubernetes.io/not-ready` with `tolerationSeconds=300`
unless the pod configuration provided
by the user already has a toleration for `node.kubernetes.io/not-ready`.
Likewise it adds a toleration for
`node.kubernetes.io/unreachable` with `tolerationSeconds=300`
unless the pod configuration provided
by the user already has a toleration for `node.kubernetes.io/unreachable`.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>Kubernetes 会自动给 Pod 添加一个 key 为 <code>node.kubernetes.io/not-ready</code> 的容忍度
并配置 <code>tolerationSeconds=300</code>，除非用户提供的 Pod 配置中已经已存在了 key 为
<code>node.kubernetes.io/not-ready</code> 的容忍度。</p>
<p>同样，Kubernetes 会给 Pod 添加一个 key 为 <code>node.kubernetes.io/unreachable</code> 的容忍度
并配置 <code>tolerationSeconds=300</code>，除非用户提供的 Pod 配置中已经已存在了 key 为
<code>node.kubernetes.io/unreachable</code> 的容忍度。</p>

</div>
<!--
These automatically-added tolerations mean that Pods remain bound to
Nodes for 5 minutes after one of these problems is detected.
-->
<p>这种自动添加的容忍度意味着在其中一种问题被检测到时 Pod
默认能够继续停留在当前节点运行 5 分钟。</p>
<!--
[DaemonSet](/docs/concepts/workloads/controllers/daemonset/) pods are created with
`NoExecute` tolerations for the following taints with no `tolerationSeconds`:

  * `node.kubernetes.io/unreachable`
  * `node.kubernetes.io/not-ready`

This ensures that DaemonSet pods are never evicted due to these problems.
-->
<p><a href="/zh/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> 中的 Pod 被创建时，
针对以下污点自动添加的 <code>NoExecute</code> 的容忍度将不会指定 <code>tolerationSeconds</code>：</p>
<ul>
<li><code>node.kubernetes.io/unreachable</code></li>
<li><code>node.kubernetes.io/not-ready</code></li>
</ul>
<p>这保证了出现上述问题时 DaemonSet 中的 Pod 永远不会被驱逐。</p>
<!--
## Taint Nodes by Condition
-->
<h2 id="基于节点状态添加污点">基于节点状态添加污点</h2>
<!--
The control plane, using the node <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a>,
automatically creates taints with a `NoSchedule` effect for [node conditions](/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions).

The scheduler checks taints, not node conditions, when it makes scheduling
decisions. This ensures that node conditions don't directly affect scheduling.
For example, if the `DiskPressure` node condition is active, the control plane
adds the `node.kubernetes.io/disk-pressure` taint and does not schedule new pods
onto the affected node. If the `MemoryPressure` node condition is active, the
control plane adds the `node.kubernetes.io/memory-pressure` taint. 
-->
<p>控制平面使用节点<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>自动创建
与<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions">节点状况</a>对应的带有 <code>NoSchedule</code> 效应的污点。</p>
<p>调度器在进行调度时检查污点，而不是检查节点状况。这确保节点状况不会直接影响调度。
例如，如果 <code>DiskPressure</code> 节点状况处于活跃状态，则控制平面
添加 <code>node.kubernetes.io/disk-pressure</code> 污点并且不会调度新的 pod
到受影响的节点。如果 <code>MemoryPressure</code> 节点状况处于活跃状态，则
控制平面添加 <code>node.kubernetes.io/memory-pressure</code> 污点。</p>
<!--
You can ignore node conditions for newly created pods by adding the corresponding
Pod tolerations. The control plane also adds the `node.kubernetes.io/memory-pressure` 
toleration on pods that have a <a class='glossary-tooltip' title='QoS 类（Quality of Service Class）为 Kubernetes 提供了一种将集群中的 Pod 分为几个类并做出有关调度和驱逐决策的方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-qos-class' target='_blank' aria-label='QoS class'>QoS class</a> 
other than `BestEffort`. This is because Kubernetes treats pods in the `Guaranteed` 
or `Burstable` QoS classes (even pods with no memory request set) as if they are
able to cope with memory pressure, while new `BestEffort` pods are not scheduled
onto the affected node. 
-->
<p>对于新创建的 Pod，可以通过添加相应的 Pod 容忍度来忽略节点状况。
控制平面还在具有除 <code>BestEffort</code> 之外的 <a class='glossary-tooltip' title='QoS 类（Quality of Service Class）为 Kubernetes 提供了一种将集群中的 Pod 分为几个类并做出有关调度和驱逐决策的方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-qos-class' target='_blank' aria-label='QoS 类'>QoS 类</a>的 pod 上
添加 <code>node.kubernetes.io/memory-pressure</code> 容忍度。
这是因为 Kubernetes 将 <code>Guaranteed</code> 或 <code>Burstable</code> QoS 类中的 Pod（甚至没有设置内存请求的 Pod）
视为能够应对内存压力，而新创建的 <code>BestEffort</code> Pod 不会被调度到受影响的节点上。</p>
<!--
The DaemonSet controller automatically adds the
following `NoSchedule` tolerations to all daemons, to prevent DaemonSets from
breaking.

  * `node.kubernetes.io/memory-pressure`
  * `node.kubernetes.io/disk-pressure`
  * `node.kubernetes.io/pid-pressure` (1.14 or later)
  * `node.kubernetes.io/unschedulable` (1.10 or later)
  * `node.kubernetes.io/network-unavailable` (*host network only*)
-->
<p>DaemonSet 控制器自动为所有守护进程添加如下 <code>NoSchedule</code> 容忍度以防 DaemonSet 崩溃：</p>
<ul>
<li><code>node.kubernetes.io/memory-pressure</code></li>
<li><code>node.kubernetes.io/disk-pressure</code></li>
<li><code>node.kubernetes.io/pid-pressure</code> (1.14 或更高版本)</li>
<li><code>node.kubernetes.io/unschedulable</code> (1.10 或更高版本)</li>
<li><code>node.kubernetes.io/network-unavailable</code> (<em>只适合主机网络配置</em>)</li>
</ul>
<!--
Adding these tolerations ensures backward compatibility. You can also add
arbitrary tolerations to DaemonSets.
-->
<p>添加上述容忍度确保了向后兼容，您也可以选择自由向 DaemonSet 添加容忍度。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about [Node-pressure Eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/) and how you can configure it
* Read about [Pod Priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/)
-->
<ul>
<li>阅读<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">节点压力驱逐</a>，以及如何配置其行为</li>
<li>阅读 <a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod 优先级</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-60e5a2861609e0848d58ce8bf99c4a31">10.5 - Pod 优先级和抢占</h1>
    
	<!-- 
reviewers:
- davidopp
- wojtek-t
title: Pod Priority and Preemption
content_type: concept
weight: 50
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code>
</div>


<!--  
[Pods](/docs/concepts/workloads/pods/) can have _priority_. Priority indicates the
importance of a Pod relative to other Pods. If a Pod cannot be scheduled, the
scheduler tries to preempt (evict) lower priority Pods to make scheduling of the
pending Pod possible.
-->
<p><a href="/zh/docs/concepts/workloads/pods/">Pod</a> 可以有 <em>优先级</em>。
优先级表示一个 Pod 相对于其他 Pod 的重要性。
如果一个 Pod 无法被调度，调度程序会尝试抢占（驱逐）较低优先级的 Pod，
以使悬决 Pod 可以被调度。</p>
<!-- body -->
<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!-- 
In a cluster where not all users are trusted, a malicious user could create Pods
at the highest possible priorities, causing other Pods to be evicted/not get
scheduled.
An administrator can use ResourceQuota to prevent users from creating pods at
high priorities.

See [limit Priority Class consumption by default](/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default)
for details.
-->
<p>在一个并非所有用户都是可信的集群中，恶意用户可能以最高优先级创建 Pod，
导致其他 Pod 被驱逐或者无法被调度。
管理员可以使用 ResourceQuota 来阻止用户创建高优先级的 Pod。
参见<a href="/zh/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default">默认限制优先级消费</a>。
</div>


<!--  
## How to use priority and preemption

To use priority and preemption:

1.  Add one or more [PriorityClasses](#priorityclass).

1.  Create Pods with[`priorityClassName`](#pod-priority) set to one of the added
    PriorityClasses. Of course you do not need to create the Pods directly;
    normally you would add `priorityClassName` to the Pod template of a
    collection object like a Deployment.

Keep reading for more information about these steps.
-->
<h2 id="如何使用优先级和抢占">如何使用优先级和抢占</h2>
<p>要使用优先级和抢占：</p>
<ol>
<li>
<p>新增一个或多个 <a href="#priorityclass">PriorityClass</a>。</p>
</li>
<li>
<p>创建 Pod，并将其 <a href="#pod-priority"><code>priorityClassName</code></a> 设置为新增的 PriorityClass。
当然你不需要直接创建 Pod；通常，你将会添加 <code>priorityClassName</code> 到集合对象（如 Deployment）
的 Pod 模板中。</p>
</li>
</ol>
<p>继续阅读以获取有关这些步骤的更多信息。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
Kubernetes already ships with two PriorityClasses:
`system-cluster-critical` and `system-node-critical`.
These are common classes and are used to [ensure that critical components are always scheduled first](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).
-->
<p>Kubernetes 已经提供了 2 个 PriorityClass：
<code>system-cluster-critical</code> 和 <code>system-node-critical</code>。
这些是常见的类，用于<a href="/zh/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">确保始终优先调度关键组件</a>。
</div>
<!-- 
## PriorityClass

A PriorityClass is a non-namespaced object that defines a mapping from a
priority class name to the integer value of the priority. The name is specified
in the `name` field of the PriorityClass object's metadata. The value is
specified in the required `value` field. The higher the value, the higher the
priority.
The name of a PriorityClass object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names),
and it cannot be prefixed with `system-`.
-->
<h2 id="priorityclass">PriorityClass</h2>
<p>PriorityClass 是一个无名称空间对象，它定义了从优先级类名称到优先级整数值的映射。
名称在 PriorityClass 对象元数据的 <code>name</code> 字段中指定。
值在必填的 <code>value</code> 字段中指定。值越大，优先级越高。
PriorityClass 对象的名称必须是有效的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>，
并且它不能以 <code>system-</code> 为前缀。</p>
<!--  
A PriorityClass object can have any 32-bit integer value smaller than or equal
to 1 billion. Larger numbers are reserved for critical system Pods that should
not normally be preempted or evicted. A cluster admin should create one
PriorityClass object for each such mapping that they want.

PriorityClass also has two optional fields: `globalDefault` and `description`.
The `globalDefault` field indicates that the value of this PriorityClass should
be used for Pods without a `priorityClassName`. Only one PriorityClass with
`globalDefault` set to true can exist in the system. If there is no
PriorityClass with `globalDefault` set, the priority of Pods with no
`priorityClassName` is zero.

The `description` field is an arbitrary string. It is meant to tell users of the
cluster when they should use this PriorityClass.
-->
<p>PriorityClass 对象可以设置任何小于或等于 10 亿的 32 位整数值。
较大的数字是为通常不应被抢占或驱逐的关键的系统 Pod 所保留的。
集群管理员应该为这类映射分别创建独立的 PriorityClass 对象。</p>
<p>PriorityClass 还有两个可选字段：<code>globalDefault</code> 和 <code>description</code>。
<code>globalDefault</code> 字段表示这个 PriorityClass 的值应该用于没有 <code>priorityClassName</code> 的 Pod。
系统中只能存在一个 <code>globalDefault</code> 设置为 true 的 PriorityClass。
如果不存在设置了 <code>globalDefault</code> 的 PriorityClass，
则没有 <code>priorityClassName</code> 的 Pod 的优先级为零。</p>
<p><code>description</code> 字段是一个任意字符串。
它用来告诉集群用户何时应该使用此 PriorityClass。</p>
<!--  
### Notes about PodPriority and existing clusters

-   If you upgrade an existing cluster without this feature, the priority
    of your existing Pods is effectively zero.

-   Addition of a PriorityClass with `globalDefault` set to `true` does not
    change the priorities of existing Pods. The value of such a PriorityClass is
    used only for Pods created after the PriorityClass is added.

-   If you delete a PriorityClass, existing Pods that use the name of the
    deleted PriorityClass remain unchanged, but you cannot create more Pods that
    use the name of the deleted PriorityClass.
-->
<h3 id="关于-podpriority-和现有集群的注意事项">关于 PodPriority 和现有集群的注意事项</h3>
<ul>
<li>
<p>如果你升级一个已经存在的但尚未使用此特性的集群，该集群中已经存在的 Pod 的优先级等效于零。</p>
</li>
<li>
<p>添加一个将 <code>globalDefault</code> 设置为 <code>true</code> 的 PriorityClass 不会改变现有 Pod 的优先级。
此类 PriorityClass 的值仅用于添加 PriorityClass 后创建的 Pod。</p>
</li>
<li>
<p>如果你删除了某个 PriorityClass 对象，则使用被删除的 PriorityClass 名称的现有 Pod 保持不变，
但是你不能再创建使用已删除的 PriorityClass 名称的 Pod。</p>
</li>
</ul>
<!-- ### Example PriorityClass -->
<h3 id="priorityclass-示例">PriorityClass 示例</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>scheduling.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>high-priority<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#666">1000000</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">globalDefault</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">description</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;此优先级类应仅用于 XYZ 服务 Pod。&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--  
## Non-preempting PriorityClass {#non-preempting-priority-class}






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code>
</div>



Pods with `preemptionPolicy: Never` will be placed in the scheduling queue
ahead of lower-priority pods,
but they cannot preempt other pods.
A non-preempting pod waiting to be scheduled will stay in the scheduling queue,
until sufficient resources are free,
and it can be scheduled.
Non-preempting pods,
like other pods,
are subject to scheduler back-off.
This means that if the scheduler tries these pods and they cannot be scheduled,
they will be retried with lower frequency,
allowing other pods with lower priority to be scheduled before them.

Non-preempting pods may still be preempted by other,
high-priority pods.
-->
<h2 id="non-preempting-priority-class">非抢占式 PriorityClass</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code>
</div>


<p>配置了 <code>preemptionPolicy: Never</code> 的 Pod 将被放置在调度队列中较低优先级 Pod 之前，
但它们不能抢占其他 Pod。等待调度的非抢占式 Pod 将留在调度队列中，直到有足够的可用资源，
它才可以被调度。非抢占式 Pod，像其他 Pod 一样，受调度程序回退的影响。
这意味着如果调度程序尝试这些 Pod 并且无法调度它们，它们将以更低的频率被重试，
从而允许其他优先级较低的 Pod 排在它们之前。</p>
<p>非抢占式 Pod 仍可能被其他高优先级 Pod 抢占。</p>
<!--  
`preemptionPolicy` defaults to `PreemptLowerPriority`,
which will allow pods of that PriorityClass to preempt lower-priority pods
(as is existing default behavior).
If `preemptionPolicy` is set to `Never`,
pods in that PriorityClass will be non-preempting.

An example use case is for data science workloads.
A user may submit a job that they want to be prioritized above other workloads,
but do not wish to discard existing work by preempting running pods.
The high priority job with `preemptionPolicy: Never` will be scheduled
ahead of other queued pods,
as soon as sufficient cluster resources "naturally" become free.
-->
<p><code>preemptionPolicy</code> 默认为 <code>PreemptLowerPriority</code>，
这将允许该 PriorityClass 的 Pod 抢占较低优先级的 Pod（现有默认行为也是如此）。
如果 <code>preemptionPolicy</code> 设置为 <code>Never</code>，则该 PriorityClass 中的 Pod 将是非抢占式的。</p>
<p>数据科学工作负载是一个示例用例。用户可以提交他们希望优先于其他工作负载的作业，
但不希望因为抢占运行中的 Pod 而导致现有工作被丢弃。
设置为 <code>preemptionPolicy: Never</code> 的高优先级作业将在其他排队的 Pod 之前被调度，
只要足够的集群资源“自然地”变得可用。</p>
<!-- ### Example Non-preempting PriorityClass -->
<h3 id="非抢占式-priorityclass-示例">非抢占式 PriorityClass 示例</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>scheduling.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PriorityClass<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>high-priority-nonpreempting<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#666">1000000</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">preemptionPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">globalDefault</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">description</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;This priority class will not cause other pods to be preempted.&#34;</span><span style="color:#bbb">
</span></code></pre></div><!-- 
## Pod priority

After you have one or more PriorityClasses, you can create Pods that specify one
of those PriorityClass names in their specifications. The priority admission
controller uses the `priorityClassName` field and populates the integer value of
the priority. If the priority class is not found, the Pod is rejected.

The following YAML is an example of a Pod configuration that uses the
PriorityClass created in the preceding example. The priority admission
controller checks the specification and resolves the priority of the Pod to
1000000.
-->
<h2 id="pod-priority">Pod 优先级</h2>
<p>在你拥有一个或多个 PriorityClass 对象之后，
你可以创建在其规约中指定这些 PriorityClass 名称之一的 Pod。
优先级准入控制器使用 <code>priorityClassName</code> 字段并填充优先级的整数值。
如果未找到所指定的优先级类，则拒绝 Pod。</p>
<p>以下 YAML 是 Pod 配置的示例，它使用在前面的示例中创建的 PriorityClass。
优先级准入控制器检查 Pod 规约并将其优先级解析为 1000000。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">priorityClassName</span>:<span style="color:#bbb"> </span>high-priority<span style="color:#bbb">
</span></code></pre></div><!--  
### Effect of Pod priority on scheduling order

When Pod priority is enabled, the scheduler orders pending Pods by
their priority and a pending Pod is placed ahead of other pending Pods
with lower priority in the scheduling queue. As a result, the higher
priority Pod may be scheduled sooner than Pods with lower priority if
its scheduling requirements are met. If such Pod cannot be scheduled,
scheduler will continue and tries to schedule other lower priority Pods.
-->
<h3 id="pod-优先级对调度顺序的影响">Pod 优先级对调度顺序的影响</h3>
<p>当启用 Pod 优先级时，调度程序会按优先级对悬决 Pod 进行排序，
并且每个悬决的 Pod 会被放置在调度队列中其他优先级较低的悬决 Pod 之前。
因此，如果满足调度要求，较高优先级的 Pod 可能会比具有较低优先级的 Pod 更早调度。
如果无法调度此类 Pod，调度程序将继续并尝试调度其他较低优先级的 Pod。</p>
<!-- 
## Preemption

When Pods are created, they go to a queue and wait to be scheduled. The
scheduler picks a Pod from the queue and tries to schedule it on a Node. If no
Node is found that satisfies all the specified requirements of the Pod,
preemption logic is triggered for the pending Pod. Let's call the pending Pod P.
Preemption logic tries to find a Node where removal of one or more Pods with
lower priority than P would enable P to be scheduled on that Node. If such a
Node is found, one or more lower priority Pods get evicted from the Node. After
the Pods are gone, P can be scheduled on the Node.
-->
<h2 id="preemption">抢占   </h2>
<p>Pod 被创建后会进入队列等待调度。
调度器从队列中挑选一个 Pod 并尝试将它调度到某个节点上。
如果没有找到满足 Pod 的所指定的所有要求的节点，则触发对悬决 Pod 的抢占逻辑。
让我们将悬决 Pod 称为 P。抢占逻辑试图找到一个节点，
在该节点中删除一个或多个优先级低于 P 的 Pod，则可以将 P 调度到该节点上。
如果找到这样的节点，一个或多个优先级较低的 Pod 会被从节点中驱逐。
被驱逐的 Pod 消失后，P 可以被调度到该节点上。</p>
<!--  
### User exposed information

When Pod P preempts one or more Pods on Node N, `nominatedNodeName` field of Pod
P's status is set to the name of Node N. This field helps scheduler track
resources reserved for Pod P and also gives users information about preemptions
in their clusters.

Please note that Pod P is not necessarily scheduled to the "nominated Node".
After victim Pods are preempted, they get their graceful termination period. If
another node becomes available while scheduler is waiting for the victim Pods to
terminate, scheduler will use the other node to schedule Pod P. As a result
`nominatedNodeName` and `nodeName` of Pod spec are not always the same. Also, if
scheduler preempts Pods on Node N, but then a higher priority Pod than Pod P
arrives, scheduler may give Node N to the new higher priority Pod. In such a
case, scheduler clears `nominatedNodeName` of Pod P. By doing this, scheduler
makes Pod P eligible to preempt Pods on another Node.
-->
<h3 id="用户暴露的信息">用户暴露的信息</h3>
<p>当 Pod P 抢占节点 N 上的一个或多个 Pod 时，
Pod P 状态的 <code>nominatedNodeName</code> 字段被设置为节点 N 的名称。
该字段帮助调度程序跟踪为 Pod P 保留的资源，并为用户提供有关其集群中抢占的信息。</p>
<p>请注意，Pod P 不一定会调度到“被提名的节点（Nominated Node）”。
在 Pod 因抢占而牺牲时，它们将获得体面终止期。
如果调度程序正在等待牺牲者 Pod 终止时另一个节点变得可用，
则调度程序将使用另一个节点来调度 Pod P。
因此，Pod 规约中的 <code>nominatedNodeName</code> 和 <code>nodeName</code> 并不总是相同。
此外，如果调度程序抢占节点 N 上的 Pod，但随后比 Pod P 更高优先级的 Pod 到达，
则调度程序可能会将节点 N 分配给新的更高优先级的 Pod。
在这种情况下，调度程序会清除 Pod P 的 <code>nominatedNodeName</code>。
通过这样做，调度程序使 Pod P 有资格抢占另一个节点上的 Pod。</p>
<!-- 
### Limitations of preemption

#### Graceful termination of preemption victims

When Pods are preempted, the victims get their
[graceful termination period](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination).
They have that much time to finish their work and exit. If they don't, they are
killed. This graceful termination period creates a time gap between the point
that the scheduler preempts Pods and the time when the pending Pod (P) can be
scheduled on the Node (N). In the meantime, the scheduler keeps scheduling other
pending Pods. As victims exit or get terminated, the scheduler tries to schedule
Pods in the pending queue. Therefore, there is usually a time gap between the
point that scheduler preempts victims and the time that Pod P is scheduled. In
order to minimize this gap, one can set graceful termination period of lower
priority Pods to zero or a small number.
-->
<h3 id="抢占的限制">抢占的限制</h3>
<h4 id="被抢占牺牲者的体面终止">被抢占牺牲者的体面终止</h4>
<p>当 Pod 被抢占时，牺牲者会得到他们的
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">体面终止期</a>。
它们可以在体面终止期内完成工作并退出。如果它们不这样做就会被杀死。
这个体面终止期在调度程序抢占 Pod 的时间点和待处理的 Pod (P)
可以在节点 (N) 上调度的时间点之间划分出了一个时间跨度。
同时，调度器会继续调度其他待处理的 Pod。当牺牲者退出或被终止时，
调度程序会尝试在待处理队列中调度 Pod。
因此，调度器抢占牺牲者的时间点与 Pod P 被调度的时间点之间通常存在时间间隔。
为了最小化这个差距，可以将低优先级 Pod 的体面终止时间设置为零或一个小数字。</p>
<!-- 
#### PodDisruptionBudget is supported, but not guaranteed

A [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) (PDB)
allows application owners to limit the number of Pods of a replicated application
that are down simultaneously from voluntary disruptions. Kubernetes supports
PDB when preempting Pods, but respecting PDB is best effort. The scheduler tries
to find victims whose PDB are not violated by preemption, but if no such victims
are found, preemption will still happen, and lower priority Pods will be removed
despite their PDBs being violated.
-->
<h4 id="支持-poddisruptionbudget-但不保证">支持 PodDisruptionBudget，但不保证</h4>
<p><a href="/zh/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>
(PDB) 允许多副本应用程序的所有者限制因自愿性质的干扰而同时终止的 Pod 数量。
Kubernetes 在抢占 Pod 时支持 PDB，但对 PDB 的支持是基于尽力而为原则的。
调度器会尝试寻找不会因被抢占而违反 PDB 的牺牲者，但如果没有找到这样的牺牲者，
抢占仍然会发生，并且即使违反了 PDB 约束也会删除优先级较低的 Pod。</p>
<!-- 
#### Inter-Pod affinity on lower-priority Pods

A Node is considered for preemption only when the answer to this question is
yes: "If all the Pods with lower priority than the pending Pod are removed from
the Node, can the pending Pod be scheduled on the Node?"

<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Preemption does not necessarily remove all lower-priority
Pods. If the pending Pod can be scheduled by removing fewer than all
lower-priority Pods, then only a portion of the lower-priority Pods are removed.
Even so, the answer to the preceding question must be yes. If the answer is no,
the Node is not considered for preemption.
</div>
-->
<h4 id="与低优先级-pod-之间的-pod-间亲和性">与低优先级 Pod 之间的 Pod 间亲和性</h4>
<p>只有当这个问题的答案是肯定的时，才考虑在一个节点上执行抢占操作：
“如果从此节点上删除优先级低于悬决 Pod 的所有 Pod，悬决 Pod 是否可以在该节点上调度？”</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 抢占并不一定会删除所有较低优先级的 Pod。
如果悬决 Pod 可以通过删除少于所有较低优先级的 Pod 来调度，
那么只有一部分较低优先级的 Pod 会被删除。
即便如此，上述问题的答案必须是肯定的。
如果答案是否定的，则不考虑在该节点上执行抢占。
</div>
<!-- 
If a pending Pod has inter-pod <a class='glossary-tooltip' title='调度程序用于确定在何处放置 Pods（亲和性）的规则' data-toggle='tooltip' data-placement='top' href='zh/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity' target='_blank' aria-label='affinity'>affinity</a>
to one or more of the lower-priority Pods on the Node, the inter-Pod affinity
rule cannot be satisfied in the absence of those lower-priority Pods. In this case, 
the scheduler does not preempt any Pods on the Node. Instead, it looks for another
Node. The scheduler might find a suitable Node or it might not. There is no 
guarantee that the pending Pod can be scheduled.

Our recommended solution for this problem is to create inter-Pod affinity only
towards equal or higher priority Pods.
-->
<p>如果悬决 Pod 与节点上的一个或多个较低优先级 Pod 具有 Pod 间<a class='glossary-tooltip' title='调度程序用于确定在何处放置 Pods（亲和性）的规则' data-toggle='tooltip' data-placement='top' href='zh/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity' target='_blank' aria-label='亲和性'>亲和性</a>，
则在没有这些较低优先级 Pod 的情况下，无法满足 Pod 间亲和性规则。
在这种情况下，调度程序不会抢占节点上的任何 Pod。
相反，它寻找另一个节点。调度程序可能会找到合适的节点，
也可能不会。无法保证悬决 Pod 可以被调度。</p>
<p>我们针对此问题推荐的解决方案是仅针对同等或更高优先级的 Pod 设置 Pod 间亲和性。</p>
<!-- 
#### Cross node preemption

Suppose a Node N is being considered for preemption so that a pending Pod P can
be scheduled on N. P might become feasible on N only if a Pod on another Node is
preempted. Here's an example:

*   Pod P is being considered for Node N.
*   Pod Q is running on another Node in the same Zone as Node N.
*   Pod P has Zone-wide anti-affinity with Pod Q (`topologyKey:
    topology.kubernetes.io/zone`).
*   There are no other cases of anti-affinity between Pod P and other Pods in
    the Zone.
*   In order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler
    does not perform cross-node preemption. So, Pod P will be deemed
    unschedulable on Node N.

If Pod Q were removed from its Node, the Pod anti-affinity violation would be
gone, and Pod P could possibly be scheduled on Node N.

We may consider adding cross Node preemption in future versions if there is
enough demand and if we find an algorithm with reasonable performance.
-->
<h4 id="跨节点抢占">跨节点抢占</h4>
<p>假设正在考虑在一个节点 N 上执行抢占，以便可以在 N 上调度待处理的 Pod P。
只有当另一个节点上的 Pod 被抢占时，P 才可能在 N 上变得可行。
下面是一个例子：</p>
<ul>
<li>正在考虑将 Pod P 调度到节点 N 上。</li>
<li>Pod Q 正在与节点 N 位于同一区域的另一个节点上运行。</li>
<li>Pod P 与 Pod Q 具有 Zone 维度的反亲和（<code>topologyKey:topology.kubernetes.io/zone</code>）。</li>
<li>Pod P 与 Zone 中的其他 Pod 之间没有其他反亲和性设置。</li>
<li>为了在节点 N 上调度 Pod P，可以抢占 Pod Q，但调度器不会进行跨节点抢占。
因此，Pod P 将被视为在节点 N 上不可调度。</li>
</ul>
<p>如果将 Pod Q 从所在节点中移除，则不会违反 Pod 间反亲和性约束，
并且 Pod P 可能会被调度到节点 N 上。</p>
<p>如果有足够的需求，并且如果我们找到性能合理的算法，
我们可能会考虑在未来版本中添加跨节点抢占。</p>
<!-- 
## Troubleshooting

Pod priority and pre-emption can have unwanted side effects. Here are some
examples of potential problems and ways to deal with them.
-->
<h2 id="故障排除">故障排除</h2>
<p>Pod 优先级和抢占可能会产生不必要的副作用。以下是一些潜在问题的示例以及处理这些问题的方法。</p>
<!--  
### Pods are preempted unnecessarily

Preemption removes existing Pods from a cluster under resource pressure to make
room for higher priority pending Pods. If you give high priorities to
certain Pods by mistake, these unintentionally high priority Pods may cause
preemption in your cluster. Pod priority is specified by setting the
`priorityClassName` field in the Pod's specification. The integer value for
priority is then resolved and populated to the `priority` field of `podSpec`.

To address the problem, you can change the `priorityClassName` for those Pods
to use lower priority classes, or leave that field empty. An empty
`priorityClassName` is resolved to zero by default.

When a Pod is preempted, there will be events recorded for the preempted Pod.
Preemption should happen only when a cluster does not have enough resources for
a Pod. In such cases, preemption happens only when the priority of the pending
Pod (preemptor) is higher than the victim Pods. Preemption must not happen when
there is no pending Pod, or when the pending Pods have equal or lower priority
than the victims. If preemption happens in such scenarios, please file an issue.
-->
<h3 id="pod-被不必要地抢占">Pod 被不必要地抢占</h3>
<p>抢占在资源压​​力较大时从集群中删除现有 Pod，为更高优先级的悬决 Pod 腾出空间。
如果你错误地为某些 Pod 设置了高优先级，这些无意的高优先级 Pod 可能会导致集群中出现抢占行为。
Pod 优先级是通过设置 Pod 规约中的 <code>priorityClassName</code> 字段来指定的。
优先级的整数值然后被解析并填充到 <code>podSpec</code> 的 <code>priority</code> 字段。</p>
<p>为了解决这个问题，你可以将这些 Pod 的 <code>priorityClassName</code> 更改为使用较低优先级的类，
或者将该字段留空。默认情况下，空的 <code>priorityClassName</code> 解析为零。</p>
<p>当 Pod 被抢占时，集群会为被抢占的 Pod 记录事件。只有当集群没有足够的资源用于 Pod 时，
才会发生抢占。在这种情况下，只有当悬决 Pod（抢占者）的优先级高于受害 Pod 时才会发生抢占。
当没有悬决 Pod，或者悬决 Pod 的优先级等于或低于牺牲者时，不得发生抢占。
如果在这种情况下发生抢占，请提出问题。</p>
<!-- 
### Pods are preempted, but the preemptor is not scheduled

When pods are preempted, they receive their requested graceful termination
period, which is by default 30 seconds. If the victim Pods do not terminate within
this period, they are forcibly terminated. Once all the victims go away, the
preemptor Pod can be scheduled.

While the preemptor Pod is waiting for the victims to go away, a higher priority
Pod may be created that fits on the same Node. In this case, the scheduler will
schedule the higher priority Pod instead of the preemptor.

This is expected behavior: the Pod with the higher priority should take the place
of a Pod with a lower priority.
-->
<h3 id="有-pod-被抢占-但抢占者并没有被调度">有 Pod 被抢占，但抢占者并没有被调度</h3>
<p>当 Pod 被抢占时，它们会收到请求的体面终止期，默认为 30 秒。
如果受害 Pod 在此期限内没有终止，它们将被强制终止。
一旦所有牺牲者都离开，就可以调度抢占者 Pod。</p>
<p>在抢占者 Pod 等待牺牲者离开的同时，可能某个适合同一个节点的更高优先级的 Pod 被创建。
在这种情况下，调度器将调度优先级更高的 Pod 而不是抢占者。</p>
<p>这是预期的行为：具有较高优先级的 Pod 应该取代具有较低优先级的 Pod。</p>
<!-- 
### Higher priority Pods are preempted before lower priority pods

The scheduler tries to find nodes that can run a pending Pod. If no node is
found, the scheduler tries to remove Pods with lower priority from an arbitrary
node in order to make room for the pending pod.
If a node with low priority Pods is not feasible to run the pending Pod, the scheduler
may choose another node with higher priority Pods (compared to the Pods on the
other node) for preemption. The victims must still have lower priority than the
preemptor Pod.

When there are multiple nodes available for preemption, the scheduler tries to
choose the node with a set of Pods with lowest priority. However, if such Pods
have PodDisruptionBudget that would be violated if they are preempted then the
scheduler may choose another node with higher priority Pods.

When multiple nodes exist for preemption and none of the above scenarios apply,
the scheduler chooses a node with the lowest priority.
-->
<h3 id="优先级较高的-pod-在优先级较低的-pod-之前被抢占">优先级较高的 Pod 在优先级较低的 Pod 之前被抢占</h3>
<p>调度程序尝试查找可以运行悬决 Pod 的节点。如果没有找到这样的节点，
调度程序会尝试从任意节点中删除优先级较低的 Pod，以便为悬决 Pod 腾出空间。
如果具有低优先级 Pod 的节点无法运行悬决 Pod，
调度器可能会选择另一个具有更高优先级 Pod 的节点（与其他节点上的 Pod 相比）进行抢占。
牺牲者的优先级必须仍然低于抢占者 Pod。</p>
<p>当有多个节点可供执行抢占操作时，调度器会尝试选择具有一组优先级最低的 Pod 的节点。
但是，如果此类 Pod 具有 PodDisruptionBudget，当它们被抢占时，
则会违反 PodDisruptionBudget，那么调度程序可能会选择另一个具有更高优先级 Pod 的节点。</p>
<p>当存在多个节点抢占且上述场景均不适用时，调度器会选择优先级最低的节点。</p>
<!-- 
## Interactions between Pod priority and quality of service {#interactions-of-pod-priority-and-qos}

Pod priority and <a class='glossary-tooltip' title='QoS 类（Quality of Service Class）为 Kubernetes 提供了一种将集群中的 Pod 分为几个类并做出有关调度和驱逐决策的方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-qos-class' target='_blank' aria-label='QoS class'>QoS class</a>
are two orthogonal features with few interactions and no default restrictions on
setting the priority of a Pod based on its QoS classes. The scheduler's
preemption logic does not consider QoS when choosing preemption targets.
Preemption considers Pod priority and attempts to choose a set of targets with
the lowest priority. Higher-priority Pods are considered for preemption only if
the removal of the lowest priority Pods is not sufficient to allow the scheduler
to schedule the preemptor Pod, or if the lowest priority Pods are protected by
`PodDisruptionBudget`.
-->
<h2 id="interactions-of-pod-priority-and-qos">Pod 优先级和服务质量之间的相互作用</h2>
<p>Pod 优先级和 <a class='glossary-tooltip' title='QoS 类（Quality of Service Class）为 Kubernetes 提供了一种将集群中的 Pod 分为几个类并做出有关调度和驱逐决策的方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-qos-class' target='_blank' aria-label='QoS 类'>QoS 类</a>
是两个正交特征，交互很少，并且对基于 QoS 类设置 Pod 的优先级没有默认限制。
调度器的抢占逻辑在选择抢占目标时不考虑 QoS。
抢占会考虑 Pod 优先级并尝试选择一组优先级最低的目标。
仅当移除优先级最低的 Pod 不足以让调度程序调度抢占式 Pod，
或者最低优先级的 Pod 受 PodDisruptionBudget 保护时，才会考虑优先级较高的 Pod。</p>
<!-- 
The kubelet uses Priority to determine pod order for [node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).
You can use the QoS class to estimate the order in which pods are most likely
to get evicted. The kubelet ranks pods for eviction based on the following factors:

  1. Whether the starved resource usage exceeds requests
  1. Pod Priority
  1. Amount of resource usage relative to requests 

See [evicting end-user pods](/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction)
for more details.

kubelet node-pressure eviction does not evict Pods when their
usage does not exceed their requests. If a Pod with lower priority is not
exceeding its requests, it won't be evicted. Another Pod with higher priority
that exceeds its requests may be evicted.
-->
<p>kubelet 使用优先级来确定
<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">节点压力驱逐</a> Pod 的顺序。
你可以使用 QoS 类来估计 Pod 最有可能被驱逐的顺序。kubelet 根据以下因素对 Pod 进行驱逐排名：</p>
<ol>
<li>对紧俏资源的使用是否超过请求值</li>
<li>Pod 优先级</li>
<li>相对于请求的资源使用量</li>
</ol>
<p>有关更多详细信息，请参阅
<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction">kubelet 驱逐时 Pod 的选择</a>。</p>
<p>当某 Pod 的资源用量未超过其请求时，kubelet 节点压力驱逐不会驱逐该 Pod。
如果优先级较低的 Pod 没有超过其请求，则不会被驱逐。
另一个优先级高于其请求的 Pod 可能会被驱逐。</p>
<h2 id="what-s-next">What's next</h2>
<!-- 
* Read about using ResourceQuotas in connection with PriorityClasses: 
  [limit Priority Class consumption by default](/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default)
* Learn about [Pod Disruption](/docs/concepts/workloads/pods/disruptions/)
* Learn about [API-initiated Eviction](/docs/reference/generated/kubernetes-api/v1.23/)
* Learn about [Node-pressure Eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/)
-->
<ul>
<li>阅读有关将 ResourceQuota 与 PriorityClass 结合使用的信息：
<a href="/zh/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default">默认限制优先级消费</a></li>
<li>了解 <a href="/zh/docs/concepts/workloads/pods/disruptions/">Pod 干扰</a></li>
<li>了解 <a href="/docs/reference/generated/kubernetes-api/v1.23/">API 发起的驱逐</a></li>
<li>了解<a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">节点压力驱逐</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-78e0431b4b7516092662a7c289cbb304">10.6 - 节点压力驱逐</h1>
    
	<!-- 
title: Node-pressure Eviction
content_type: concept
weight: 60 
-->
<p>节点压力驱逐是 <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 主动终止 Pod 以回收节点上资源的过程。</br></p>
<!-- 
The <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> monitors resources 
like CPU, memory, disk space, and filesystem inodes on your cluster's nodes. 
When one or more of these resources reach specific consumption levels, the 
kubelet can proactively fail one or more pods on the node to reclaim resources
and prevent starvation. 

During a node-pressure eviction, the kubelet sets the `PodPhase` for the
selected pods to `Failed`. This terminates the pods. 

Node-pressure eviction is not the same as 
[API-initiated eviction](/docs/reference/generated/kubernetes-api/v1.23/).
-->
<p><a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>
监控集群节点的 CPU、内存、磁盘空间和文件系统的 inode 等资源。
当这些资源中的一个或者多个达到特定的消耗水平，
kubelet 可以主动地使节点上一个或者多个 Pod 失效，以回收资源防止饥饿。</p>
<p>在节点压力驱逐期间，kubelet 将所选 Pod 的 <code>PodPhase</code> 设置为 <code>Failed</code>。这将终止 Pod。</p>
<p>节点压力驱逐不同于 <a href="/docs/reference/generated/kubernetes-api/v1.23/">API 发起的驱逐</a>。</p>
<!-- 
The kubelet does not respect your configured `PodDisruptionBudget` or the pod's
`terminationGracePeriodSeconds`. If you use [soft eviction thresholds](#soft-eviction-thresholds),
the kubelet respects your configured `eviction-max-pod-grace-period`. If you use
[hard eviction thresholds](#hard-eviction-thresholds), it uses a `0s` grace period for termination.

If the pods are managed by a <a class='glossary-tooltip' title='工作负载是在 Kubernetes 上运行的应用程序。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/' target='_blank' aria-label='workload'>workload</a>
resource (such as <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>
or <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>) that
replaces failed pods, the control plane or `kube-controller-manager` creates new 
pods in place of the evicted pods.
-->
<p>kubelet 并不理会你配置的 <code>PodDisruptionBudget</code> 或者是 Pod 的 <code>terminationGracePeriodSeconds</code>。
如果你使用了<a href="#soft-eviction-thresholds">软驱逐条件</a>，kubelet 会考虑你所配置的
<code>eviction-max-pod-grace-period</code>。
如果你使用了<a href="#hard-eviction-thresholds">硬驱逐条件</a>，它使用 <code>0s</code> 宽限期来终止 Pod。</p>
<p>如果 Pod 是由替换失败 Pod 的<a class='glossary-tooltip' title='工作负载是在 Kubernetes 上运行的应用程序。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/' target='_blank' aria-label='工作负载'>工作负载</a>资源
（例如 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>
或者 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>）管理，
则控制平面或 <code>kube-controller-manager</code> 会创建新的 Pod 来代替被驱逐的 Pod。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
The kubelet attempts to [reclaim node-level resources](#reclaim-node-resources)
before it terminates end-user pods. For example, it removes unused container
images when disk resources are starved.
-->
<p>kubelet 在终止最终用户 Pod 之前会尝试<a href="#reclaim-node-resources">回收节点级资源</a>。
例如，它会在磁盘资源不足时删除未使用的容器镜像。
</div>
<!-- 
The kubelet uses various parameters to make eviction decisions, like the following:

  * Eviction signals
  * Eviction thresholds
  * Monitoring intervals
-->
<p>kubelet 使用各种参数来做出驱逐决定，如下所示：</p>
<ul>
<li>驱逐信号</li>
<li>驱逐条件</li>
<li>监控间隔</li>
</ul>
<!-- 
### Eviction signals {#eviction-signals}

Eviction signals are the current state of a particular resource at a specific
point in time. Kubelet uses eviction signals to make eviction decisions by
comparing the signals to eviction thresholds, which are the minimum amount of 
the resource that should be available on the node. 

Kubelet uses the following eviction signals:
-->
<h3 id="eviction-signals">驱逐信号</h3>
<p>驱逐信号是特定资源在特定时间点的当前状态。
kubelet 使用驱逐信号，通过将信号与驱逐条件进行比较来做出驱逐决定，
驱逐条件是节点上应该可用资源的最小量。</p>
<p>kubelet 使用以下驱逐信号：</p>
<table>
<thead>
<tr>
<th>驱逐信号</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>memory.available</code></td>
<td><code>memory.available</code> := <code>node.status.capacity[memory]</code> - <code>node.stats.memory.workingSet</code></td>
</tr>
<tr>
<td><code>nodefs.available</code></td>
<td><code>nodefs.available</code> := <code>node.stats.fs.available</code></td>
</tr>
<tr>
<td><code>nodefs.inodesFree</code></td>
<td><code>nodefs.inodesFree</code> := <code>node.stats.fs.inodesFree</code></td>
</tr>
<tr>
<td><code>imagefs.available</code></td>
<td><code>imagefs.available</code> := <code>node.stats.runtime.imagefs.available</code></td>
</tr>
<tr>
<td><code>imagefs.inodesFree</code></td>
<td><code>imagefs.inodesFree</code> := <code>node.stats.runtime.imagefs.inodesFree</code></td>
</tr>
<tr>
<td><code>pid.available</code></td>
<td><code>pid.available</code> := <code>node.stats.rlimit.maxpid</code> - <code>node.stats.rlimit.curproc</code></td>
</tr>
</tbody>
</table>
<!-- 
In this table, the `Description` column shows how kubelet gets the value of the
signal. Each signal supports either a percentage or a literal value. Kubelet 
calculates the percentage value relative to the total capacity associated with
the signal. 
-->
<p>在上表中，<code>描述</code>列显示了 kubelet 如何获取信号的值。每个信号支持百分比值或者是字面值。
kubelet 计算相对于与信号有关的总量的百分比值。</p>
<!--
The value for `memory.available` is derived from the cgroupfs instead of tools
like `free -m`. This is important because `free -m` does not work in a
container, and if users use the [node
allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable) feature, out of resource decisions
are made local to the end user Pod part of the cgroup hierarchy as well as the
root node. This [script](/examples/admin/resource/memory-available.sh)
reproduces the same set of steps that the kubelet performs to calculate
`memory.available`. The kubelet excludes inactive_file (i.e. # of bytes of
file-backed memory on inactive LRU list) from its calculation as it assumes that
memory is reclaimable under pressure.  
-->
<p><code>memory.available</code> 的值来自 cgroupfs，而不是像 <code>free -m</code> 这样的工具。
这很重要，因为 <code>free -m</code> 在容器中不起作用，如果用户使用
<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">节点可分配资源</a>
这一功能特性，资源不足的判定是基于 CGroup 层次结构中的用户 Pod 所处的局部及 CGroup 根节点作出的。
这个<a href="/zh/examples/admin/resource/memory-available.sh">脚本</a>
重现了 kubelet 为计算 <code>memory.available</code> 而执行的相同步骤。
kubelet 在其计算中排除了 inactive_file（即非活动 LRU 列表上基于文件来虚拟的内存的字节数），
因为它假定在压力下内存是可回收的。</p>
<!--
The kubelet supports the following filesystem partitions:

1. `nodefs`: The node's main filesystem, used for local disk volumes, emptyDir,
   log storage, and more. For example, `nodefs` contains `/var/lib/kubelet/`. 
1. `imagefs`: An optional filesystem that container runtimes use to store container
   images and container writable layers.

Kubelet auto-discovers these filesystems and ignores other filesystems. Kubelet
does not support other configurations. 
-->
<p>kubelet 支持以下文件系统分区：</p>
<ol>
<li><code>nodefs</code>：节点的主要文件系统，用于本地磁盘卷、emptyDir、日志存储等。
例如，<code>nodefs</code> 包含 <code>/var/lib/kubelet/</code>。</li>
<li><code>imagefs</code>：可选文件系统，供容器运行时存储容器镜像和容器可写层。</li>
</ol>
<p>kubelet 会自动发现这些文件系统并忽略其他文件系统。kubelet 不支持其他配置。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
Some kubelet garbage collection features are deprecated in favor of eviction.
For a list of the deprecated features, see [kubelet garbage collection deprecation](/docs/concepts/cluster-administration/kubelet-garbage-collection/#deprecation).
-->
<p>一些 kubelet 垃圾收集功能已被弃用，以支持驱逐。
有关已弃用功能的列表，请参阅
<a href="/zh/docs/concepts/cluster-administration/kubelet-garbage-collection/#deprecation">kubelet 垃圾收集弃用</a>。
</div>
<!-- 
### Eviction thresholds

You can specify custom eviction thresholds for the kubelet to use when it makes
eviction decisions.

Eviction thresholds have the form `[eviction-signal][operator][quantity]`, where:

* `eviction-signal` is the [eviction signal](#eviction-signals) to use.
* `operator` is the [relational operator](https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators)
  you want, such as `<` (less than).
* `quantity` is the eviction threshold amount, such as `1Gi`. The value of `quantity`
  must match the quantity representation used by Kubernetes. You can use either
  literal values or percentages (`%`).
-->
<h3 id="eviction-thresholds">驱逐条件</h3>
<p>你可以为 kubelet 指定自定义驱逐条件，以便在作出驱逐决定时使用。</p>
<p>驱逐条件的形式为 <code>[eviction-signal][operator][quantity]</code>，其中：</p>
<ul>
<li><code>eviction-signal</code> 是要使用的<a href="#eviction-signals">驱逐信号</a>。</li>
<li><code>operator</code> 是你想要的<a href="https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators">关系运算符</a>，
比如 <code>&lt;</code>（小于）。</li>
<li><code>quantity</code> 是驱逐条件数量，例如 <code>1Gi</code>。
<code>quantity</code> 的值必须与 Kubernetes 使用的数量表示相匹配。
你可以使用文字值或百分比（<code>%</code>）。</li>
</ul>
<!--
For example, if a node has `10Gi` of total memory and you want trigger eviction if
the available memory falls below `1Gi`, you can define the eviction threshold as
either `memory.available<10%` or `memory.available<1Gi`. You cannot use both.

You can configure soft and hard eviction thresholds.  
-->
<p>例如，如果一个节点的总内存为 10Gi 并且你希望在可用内存低于 1Gi 时触发驱逐，
则可以将驱逐条件定义为 <code>memory.available&lt;10%</code> 或 <code>memory.available&lt; 1G</code>。
你不能同时使用二者。</p>
<p>你可以配置软和硬驱逐条件。</p>
<!--  
#### Soft eviction thresholds {#soft-eviction-thresholds}

A soft eviction threshold pairs an eviction threshold with a required
administrator-specified grace period. The kubelet does not evict pods until the
grace period is exceeded. The kubelet returns an error on startup if there is no
specified grace period. 
-->
<h4 id="soft-eviction-thresholds">软驱逐条件</h4>
<p>软驱逐条件将驱逐条件与管理员所必须指定的宽限期配对。
在超过宽限期之前，kubelet 不会驱逐 Pod。
如果没有指定的宽限期，kubelet 会在启动时返回错误。</p>
<!-- 
You can specify both a soft eviction threshold grace period and a maximum
allowed pod termination grace period for kubelet to use during evictions. If you
specify a maximum allowed grace period and the soft eviction threshold is met, 
the kubelet uses the lesser of the two grace periods. If you do not specify a
maximum allowed grace period, the kubelet kills evicted pods immediately without
graceful termination.
-->
<p>你可以既指定软驱逐条件宽限期，又指定 Pod 终止宽限期的上限，，给 kubelet 在驱逐期间使用。
如果你指定了宽限期的上限并且 Pod 满足软驱逐阈条件，则 kubelet 将使用两个宽限期中的较小者。
如果你没有指定宽限期上限，kubelet 会立即杀死被驱逐的 Pod，不允许其体面终止。</p>
<!--  
You can use the following flags to configure soft eviction thresholds:

* `eviction-soft`: A set of eviction thresholds like `memory.available<1.5Gi`
  that can trigger pod eviction if held over the specified grace period.
* `eviction-soft-grace-period`: A set of eviction grace periods like `memory.available=1m30s`
  that define how long a soft eviction threshold must hold before triggering a Pod eviction.
* `eviction-max-pod-grace-period`: The maximum allowed grace period (in seconds)
  to use when terminating pods in response to a soft eviction threshold being met.
-->
<p>你可以使用以下标志来配置软驱逐条件：</p>
<ul>
<li><code>eviction-soft</code>：一组驱逐条件，如 <code>memory.available&lt;1.5Gi</code>，
如果驱逐条件持续时长超过指定的宽限期，可以触发 Pod 驱逐。</li>
<li><code>eviction-soft-grace-period</code>：一组驱逐宽限期，
如 <code>memory.available=1m30s</code>，定义软驱逐条件在触发 Pod 驱逐之前必须保持多长时间。</li>
<li><code>eviction-max-pod-grace-period</code>：在满足软驱逐条件而终止 Pod 时使用的最大允许宽限期（以秒为单位）。</li>
</ul>
<!-- 
#### Hard eviction thresholds {#hard-eviction-thresholds}

A hard eviction threshold has no grace period. When a hard eviction threshold is
met, the kubelet kills pods immediately without graceful termination to reclaim
the starved resource.

You can use the `eviction-hard` flag to configure a set of hard eviction 
thresholds like `memory.available<1Gi`. 
-->
<h4 id="hard-eviction-thresholds">硬驱逐条件</h4>
<p>硬驱逐条件没有宽限期。当达到硬驱逐条件时，
kubelet 会立即杀死 pod，而不会正常终止以回收紧缺的资源。</p>
<p>你可以使用 <code>eviction-hard</code> 标志来配置一组硬驱逐条件，
例如 <code>memory.available&lt;1Gi</code>。</p>
<!-- 
The kubelet has the following default hard eviction thresholds:

* `memory.available<100Mi`
* `nodefs.available<10%`
* `imagefs.available<15%`
* `nodefs.inodesFree<5%` (Linux nodes)
-->
<p>kubelet 具有以下默认硬驱逐条件：</p>
<ul>
<li><code>memory.available&lt;100Mi</code></li>
<li><code>nodefs.available&lt;10%</code></li>
<li><code>imagefs.available&lt;15%</code></li>
<li><code>nodefs.inodesFree&lt;5%</code>（Linux 节点）</li>
</ul>
<!--  
### Eviction monitoring interval

The kubelet evaluates eviction thresholds based on its configured `housekeeping-interval`
which defaults to `10s`.
-->
<h3 id="驱逐监测间隔">驱逐监测间隔</h3>
<p>kubelet 根据其配置的 <code>housekeeping-interval</code>（默认为 <code>10s</code>）评估驱逐条件。</p>
<!--
### Node conditions {#node-conditions}

The kubelet reports node conditions to reflect that the node is under pressure
because hard or soft eviction threshold is met, independent of configured grace
periods. 
-->
<h3 id="node-conditions">节点条件</h3>
<p>kubelet 报告节点状况以反映节点处于压力之下，因为满足硬或软驱逐条件，与配置的宽限期无关。</p>
<!--  
The kubelet maps eviction signals to node conditions as follows: 

| Node Condition    | Eviction Signal                                                                       | Description                                                                                                                  |
|-------------------|---------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| `MemoryPressure`  | `memory.available`                                                                    | Available memory on the node has satisfied an eviction threshold                                                             |
| `DiskPressure`    | `nodefs.available`, `nodefs.inodesFree`, `imagefs.available`, or `imagefs.inodesFree` | Available disk space and inodes on either the node's root filesystem or image filesystem has satisfied an eviction threshold |
| `PIDPressure`     | `pid.available`                                                                       | Available processes identifiers on the (Linux) node has fallen below an eviction threshold                                   |

The kubelet updates the node conditions based on the configured 
`--node-status-update-frequency`, which defaults to `10s`.
-->
<p>kubelet 根据下表将驱逐信号映射为节点状况：</p>
<table>
<thead>
<tr>
<th>节点条件</th>
<th>驱逐信号</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MemoryPressure</code></td>
<td><code>memory.available</code></td>
<td>节点上的可用内存已满足驱逐条件</td>
</tr>
<tr>
<td><code>DiskPressure</code></td>
<td><code>nodefs.available</code>、<code>nodefs.inodesFree</code>、<code>imagefs.available</code> 或 <code>imagefs.inodesFree</code></td>
<td>节点的根文件系统或映像文件系统上的可用磁盘空间和 inode 已满足驱逐条件</td>
</tr>
<tr>
<td><code>PIDPressure</code></td>
<td><code>pid.available</code></td>
<td>(Linux) 节点上的可用进程标识符已低于驱逐条件</td>
</tr>
</tbody>
</table>
<p>kubelet 根据配置的 <code>--node-status-update-frequency</code> 更新节点条件，默认为 <code>10s</code>。</p>
<!-- 
#### Node condition oscillation

In some cases, nodes oscillate above and below soft eviction thresholds without
holding for the defined grace periods. This causes the reported node condition
to constantly switch between `true` and `false`, leading to bad eviction decisions.

To protect against oscillation, you can use the `eviction-pressure-transition-period`
flag, which controls how long the kubelet must wait before transitioning a node
condition to a different state. The transition period has a default value of `5m`.
-->
<h4 id="节点条件振荡">节点条件振荡</h4>
<p>在某些情况下，节点在软驱逐条件上下振荡，而没有保持定义的宽限期。
这会导致报告的节点条件在 <code>true</code> 和 <code>false</code> 之间不断切换，从而导致错误的驱逐决策。</p>
<p>为了防止振荡，你可以使用 <code>eviction-pressure-transition-period</code> 标志，
该标志控制 kubelet 在将节点条件转换为不同状态之前必须等待的时间。
过渡期的默认值为 <code>5m</code>。</p>
<!-- 
### Reclaiming node level resources {#reclaim-node-resources}

The kubelet tries to reclaim node-level resources before it evicts end-user pods.

When a `DiskPressure` node condition is reported, the kubelet reclaims node-level
resources based on the filesystems on the node. 
-->
<h3 id="reclaim-node-resources">回收节点级资源</h3>
<p>kubelet 在驱逐最终用户 Pod 之前会先尝试回收节点级资源。</p>
<p>当报告 <code>DiskPressure</code> 节点状况时，kubelet 会根据节点上的文件系统回收节点级资源。</p>
<!--
#### With `imagefs`

If the node has a dedicated `imagefs` filesystem for container runtimes to use,
the kubelet does the following:

  * If the `nodefs` filesystem meets the eviction thresholds, the kubelet garbage collects
    dead pods and containers. 
  * If the `imagefs` filesystem meets the eviction thresholds, the kubelet
    deletes all unused images.
-->
<h4 id="有-imagefs">有 <code>imagefs</code></h4>
<p>如果节点有一个专用的 <code>imagefs</code> 文件系统供容器运行时使用，kubelet 会执行以下操作：</p>
<ul>
<li>如果 <code>nodefs</code> 文件系统满足驱逐条件，kubelet 垃圾收集死亡 Pod 和容器。</li>
<li>如果 <code>imagefs</code> 文件系统满足驱逐条件，kubelet 将删除所有未使用的镜像。</li>
</ul>
<!-- 
#### Without `imagefs`

If the node only has a `nodefs` filesystem that meets eviction thresholds,
the kubelet frees up disk space in the following order:

1. Garbage collect dead pods and containers
1. Delete unused images
-->
<h4 id="没有-imagefs">没有 <code>imagefs</code></h4>
<p>如果节点只有一个满足驱逐条件的 <code>nodefs</code> 文件系统，
kubelet 按以下顺序释放磁盘空间：</p>
<ol>
<li>对死亡的 Pod 和容器进行垃圾收集</li>
<li>删除未使用的镜像</li>
</ol>
<!-- 
### Pod selection for kubelet eviction

If the kubelet's attempts to reclaim node-level resources don't bring the eviction
signal below the threshold, the kubelet begins to evict end-user pods. 

The kubelet uses the following parameters to determine pod eviction order:

1. Whether the pod's resource usage exceeds requests
1. [Pod Priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/)
1. The pod's resource usage relative to requests
-->
<h3 id="kubelet-驱逐时-pod-的选择">kubelet 驱逐时 Pod 的选择</h3>
<p>如果 kubelet 回收节点级资源的尝试没有使驱逐信号低于条件，
则 kubelet 开始驱逐最终用户 Pod。</p>
<p>kubelet 使用以下参数来确定 Pod 驱逐顺序：</p>
<ol>
<li>Pod 的资源使用是否超过其请求</li>
<li><a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod 优先级</a></li>
<li>Pod 相对于请求的资源使用情况</li>
</ol>
<!-- 
As a result, kubelet ranks and evicts pods in the following order:

1. `BestEffort` or `Burstable` pods where the usage exceeds requests. These pods
   are evicted based on their Priority and then by how much their usage level
   exceeds the request.
1. `Guaranteed` pods and `Burstable` pods where the usage is less than requests
   are evicted last, based on their Priority.
-->
<p>因此，kubelet 按以下顺序排列和驱逐 Pod：</p>
<ol>
<li>首先考虑资源使用量超过其请求的 <code>BestEffort</code> 或 <code>Burstable</code> Pod。
这些 Pod 会根据它们的优先级以及它们的资源使用级别超过其请求的程度被逐出。</li>
<li>资源使用量少于请求量的 <code>Guaranteed</code> Pod 和 <code>Burstable</code> Pod 根据其优先级被最后驱逐。</li>
</ol>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
The kubelet does not use the pod's QoS class to determine the eviction order.
You can use the QoS class to estimate the most likely pod eviction order when 
reclaiming resources like memory. QoS does not apply to EphemeralStorage requests,
so the above scenario will not apply if the node is, for example, under `DiskPressure`.
-->
<p>kubelet 不使用 Pod 的 QoS 类来确定驱逐顺序。
在回收内存等资源时，你可以使用 QoS 类来估计最可能的 Pod 驱逐顺序。
QoS 不适用于临时存储（EphemeralStorage）请求，
因此如果节点在 <code>DiskPressure</code> 下，则上述场景将不适用。
</div>
<!-- 
`Guaranteed` pods are guaranteed only when requests and limits are specified for
all the containers and they are equal. These pods will never be evicted because
of another pod's resource consumption. If a system daemon (such as `kubelet`,
and `journald`) is consuming more resources than were reserved via 
`system-reserved` or `kube-reserved` allocations, and the node only has
`Guaranteed` or `Burstable` pods using less resources than requests left on it,
then the kubelet must choose to evict one of these pods to preserve node stability
and to limit the impact of resource starvation on other pods. In this case, it
will choose to evict pods of lowest Priority first.
-->
<p>仅当 <code>Guaranteed</code> Pod 中所有容器都被指定了请求和限制并且二者相等时，才保证 Pod 不被驱逐。
这些 Pod 永远不会因为另一个 Pod 的资源消耗而被驱逐。
如果系统守护进程（例如 <code>kubelet</code> 和 <code>journald</code>）
消耗的资源比通过 <code>system-reserved</code> 或 <code>kube-reserved</code> 分配保留的资源多，
并且该节点只有 <code>Guaranteed</code> 或 <code>Burstable</code> Pod 使用的资源少于其上剩余的请求，
那么 kubelet 必须选择驱逐这些 Pod 中的一个以保持节点稳定性并减少资源匮乏对其他 Pod 的影响。
在这种情况下，它会选择首先驱逐最低优先级的 Pod。</p>
<!--  
When the kubelet evicts pods in response to `inode` or `PID` starvation, it uses
the Priority to determine the eviction order, because `inodes` and `PIDs` have no
requests.

The kubelet sorts pods differently based on whether the node has a dedicated
`imagefs` filesystem:
-->
<p>当 kubelet 因 inode 或 PID 不足而驱逐 pod 时，
它使用优先级来确定驱逐顺序，因为 inode 和 PID 没有请求。</p>
<p>kubelet 根据节点是否具有专用的 <code>imagefs</code> 文件系统对 Pod 进行不同的排序：</p>
<!-- 
#### With `imagefs`

If `nodefs` is triggering evictions, the kubelet sorts pods based on `nodefs`
usage (`local volumes + logs of all containers`).

If `imagefs` is triggering evictions, the kubelet sorts pods based on the
writable layer usage of all containers.

#### Without `imagefs`

If `nodefs` is triggering evictions, the kubelet sorts pods based on their total
disk usage (`local volumes + logs & writable layer of all containers`)
-->
<h4 id="有-imagefs-1">有 <code>imagefs</code></h4>
<p>如果 <code>nodefs</code> 触发驱逐，
kubelet 会根据 <code>nodefs</code> 使用情况（<code>本地卷 + 所有容器的日志</code>）对 Pod 进行排序。</p>
<p>如果 <code>imagefs</code> 触发驱逐，kubelet 会根据所有容器的可写层使用情况对 Pod 进行排序。</p>
<h4 id="没有-imagefs-1">没有 <code>imagefs</code></h4>
<p>如果 <code>nodefs</code> 触发驱逐，
kubelet 会根据磁盘总用量（<code>本地卷 + 日志和所有容器的可写层</code>）对 Pod 进行排序。</p>
<!-- 
### Minimum eviction reclaim

In some cases, pod eviction only reclaims a small amount of the starved resource.
This can lead to the kubelet repeatedly hitting the configured eviction thresholds
and triggering multiple evictions. 
-->
<h3 id="minimum-eviction-reclaim">最小驱逐回收 </h3>
<p>在某些情况下，驱逐 Pod 只会回收少量的紧俏资源。
这可能导致 kubelet 反复达到配置的驱逐条件并触发多次驱逐。</p>
<!-- 
You can use the `--eviction-minimum-reclaim` flag or a [kubelet config file](/docs/tasks/administer-cluster/kubelet-config-file/)
to configure a minimum reclaim amount for each resource. When the kubelet notices
that a resource is starved, it continues to reclaim that resource until it
reclaims the quantity you specify. 

For example, the following configuration sets minimum reclaim amounts: 
-->
<p>你可以使用 <code>--eviction-minimum-reclaim</code> 标志或
<a href="/zh/docs/tasks/administer-cluster/kubelet-config-file/">kubelet 配置文件</a>
为每个资源配置最小回收量。
当 kubelet 注意到某个资源耗尽时，它会继续回收该资源，直到回收到你所指定的数量为止。</p>
<p>例如，以下配置设置最小回收量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">evictionHard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">memory.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodefs.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">imagefs.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">evictionMinimumReclaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">memory.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodefs.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">imagefs.available</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2Gi&#34;</span><span style="color:#bbb">
</span></code></pre></div><!-- 
In this example, if the `nodefs.available` signal meets the eviction threshold,
the kubelet reclaims the resource until the signal reaches the threshold of `1Gi`,
and then continues to reclaim the minimum amount of `500Mi` it until the signal
reaches `1.5Gi`. 

Similarly, the kubelet reclaims the `imagefs` resource until the `imagefs.available`
signal reaches `102Gi`. 

The default `eviction-minimum-reclaim` is `0` for all resources.
-->
<p>在这个例子中，如果 <code>nodefs.available</code> 信号满足驱逐条件，
kubelet 会回收资源，直到信号达到 <code>1Gi</code> 的条件，
然后继续回收至少 <code>500Mi</code> 直到信号达到 <code>1.5Gi</code>。</p>
<p>类似地，kubelet 会回收 <code>imagefs</code> 资源，直到 <code>imagefs.available</code> 信号达到 <code>102Gi</code>。</p>
<p>对于所有资源，默认的 <code>eviction-minimum-reclaim</code> 为 <code>0</code>。</p>
<!-- 
### Node out of memory behavior

If the node experiences an out of memory (OOM) event prior to the kubelet
being able to reclaim memory, the node depends on the [oom_killer](https://lwn.net/Articles/391222/)
to respond.

The kubelet sets an `oom_score_adj` value for each container based on the QoS for the pod.
-->
<h3 id="节点内存不足行为">节点内存不足行为</h3>
<p>如果节点在 kubelet 能够回收内存之前遇到内存不足（OOM）事件，
则节点依赖 <a href="https://lwn.net/Articles/391222/">oom_killer</a> 来响应。</p>
<p>kubelet 根据 Pod 的服务质量（QoS）为每个容器设置一个 <code>oom_score_adj</code> 值。</p>
<table>
<thead>
<tr>
<th>服务质量</th>
<th>oom_score_adj</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Guaranteed</code></td>
<td>-997</td>
</tr>
<tr>
<td><code>BestEffort</code></td>
<td>1000</td>
</tr>
<tr>
<td><code>Burstable</code></td>
<td>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</td>
</tr>
</tbody>
</table>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- 
The kubelet also sets an `oom_score_adj` value of `-997` for containers in Pods that have
`system-node-critical` <a class='glossary-tooltip' title='Pod 优先级表示一个 Pod 相对于其他 Pod 的重要性。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority' target='_blank' aria-label='Priority'>Priority</a>
-->
<p>kubelet 还将具有 <code>system-node-critical</code>
<a class='glossary-tooltip' title='Pod 优先级表示一个 Pod 相对于其他 Pod 的重要性。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority' target='_blank' aria-label='优先级'>优先级</a>
的 Pod 中的容器 <code>oom_score_adj</code> 值设为 <code>-997</code>。
</div>
<!-- 
If the kubelet can't reclaim memory before a node experiences OOM, the
`oom_killer` calculates an `oom_score` based on the percentage of memory it's
using on the node, and then adds the `oom_score_adj` to get an effective `oom_score`
for each container. It then kills the container with the highest score.

This means that containers in low QoS pods that consume a large amount of memory
relative to their scheduling requests are killed first.

Unlike pod eviction, if a container is OOM killed, the `kubelet` can restart it 
based on its `RestartPolicy`.
-->
<p>如果 kubelet 在节点遇到 OOM 之前无法回收内存，
则 <code>oom_killer</code> 根据它在节点上使用的内存百分比计算 <code>oom_score</code>，
然后加上 <code>oom_score_adj</code> 得到每个容器有效的 <code>oom_score</code>。
然后它会杀死得分最高的容器。</p>
<p>这意味着低 QoS Pod 中相对于其调度请求消耗内存较多的容器，将首先被杀死。</p>
<p>与 Pod 驱逐不同，如果容器被 OOM 杀死，
<code>kubelet</code> 可以根据其 <code>RestartPolicy</code> 重新启动它。</p>
<!-- 
### Best practices {#node-pressure-eviction-good-practices}

The following sections describe best practices for eviction configuration.
-->
<h3 id="node-pressure-eviction-good-practices">最佳实践</h3>
<p>以下部分描述了驱逐配置的最佳实践。</p>
<!-- 
#### Schedulable resources and eviction policies

When you configure the kubelet with an eviction policy, you should make sure that
the scheduler will not schedule pods if they will trigger eviction because they
immediately induce memory pressure.
-->
<h4 id="可调度的资源和驱逐策略">可调度的资源和驱逐策略</h4>
<p>当你为 kubelet 配置驱逐策略时，
你应该确保调度程序不会在 Pod 触发驱逐时对其进行调度，因为这类 Pod 会立即引起内存压力。</p>
<!-- 
Consider the following scenario:

* Node memory capacity: `10Gi`
* Operator wants to reserve 10% of memory capacity for system daemons (kernel, `kubelet`, etc.)
* Operator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM.
-->
<p>考虑以下场景：</p>
<ul>
<li>节点内存容量：<code>10Gi</code></li>
<li>操作员希望为系统守护进程（内核、<code>kubelet</code> 等）保留 10% 的内存容量</li>
<li>操作员希望驱逐内存利用率为 95% 的Pod，以减少系统 OOM 的概率。</li>
</ul>
<!-- 
For this to work, the kubelet is launched as follows:
-->
<p>为此，kubelet 启动设置如下：</p>
<pre><code>--eviction-hard=memory.available&lt;500Mi
--system-reserved=memory=1.5Gi
</code></pre><!-- 
In this configuration, the `--system-reserved` flag reserves `1.5Gi` of memory
for the system, which is `10% of the total memory + the eviction threshold amount`. 

The node can reach the eviction threshold if a pod is using more than its request,
or if the system is using more than `1Gi` of memory, which makes the `memory.available`
signal fall below `500Mi` and triggers the threshold. 
-->
<p>在此配置中，<code>--system-reserved</code> 标志为系统预留了 <code>1.5Gi</code> 的内存，
即 <code>总内存的 10% + 驱逐条件量</code>。</p>
<p>如果 Pod 使用的内存超过其请求值或者系统使用的内存超过 <code>1Gi</code>，
则节点可以达到驱逐条件，这使得 <code>memory.available</code> 信号低于 <code>500Mi</code> 并触发条件。</p>
<!-- 
#### DaemonSet

Pod Priority is a major factor in making eviction decisions. If you do not want
the kubelet to evict pods that belong to a `DaemonSet`, give those pods a high
enough `priorityClass` in the pod spec. You can also use a lower `priorityClass`
or the default to only allow `DaemonSet` pods to run when there are enough 
resources.
-->
<h3 id="daemonset">DaemonSet</h3>
<p>Pod 优先级是做出驱逐决定的主要因素。
如果你不希望 kubelet 驱逐属于 <code>DaemonSet</code> 的 Pod，
请在 Pod 规约中为这些 Pod 提供足够高的 <code>priorityClass</code>。
你还可以使用优先级较低的 <code>priorityClass</code> 或默认配置，
仅在有足够资源时才运行 <code>DaemonSet</code> Pod。</p>
<!-- 
### Known issues

The following sections describe known issues related to out of resource handling.
-->
<h3 id="已知问题">已知问题</h3>
<p>以下部分描述了与资源不足处理相关的已知问题。</p>
<!-- 
#### kubelet may not observe memory pressure right away

By default, the kubelet polls `cAdvisor` to collect memory usage stats at a
regular interval. If memory usage increases within that window rapidly, the
kubelet may not observe `MemoryPressure` fast enough, and the `OOMKiller`
will still be invoked. 
-->
<h4 id="kubelet-可能不会立即观察到内存压力">kubelet 可能不会立即观察到内存压力</h4>
<p>默认情况下，kubelet 轮询 <code>cAdvisor</code> 以定期收集内存使用情况统计信息。
如果该轮询时间窗口内内存使用量迅速增加，kubelet 可能无法足够快地观察到 <code>MemoryPressure</code>，
但是 <code>OOMKiller</code> 仍将被调用。</p>
<!-- 
You can use the `--kernel-memcg-notification` flag to enable the `memcg`
notification API on the kubelet to get notified immediately when a threshold
is crossed.

If you are not trying to achieve extreme utilization, but a sensible measure of
overcommit, a viable workaround for this issue is to use the `--kube-reserved`
and `--system-reserved` flags to allocate memory for the system. 
-->
<p>你可以使用 <code>--kernel-memcg-notification</code>
标志在 kubelet 上启用 <code>memcg</code> 通知 API，以便在超过条件时立即收到通知。</p>
<p>如果你不是追求极端利用率，而是要采取合理的过量使用措施，
则解决此问题的可行方法是使用 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 标志为系统分配内存。</p>
<!-- 
#### active_file memory is not considered as available memory

On Linux, the kernel tracks the number of bytes of file-backed memory on active 
LRU list as the `active_file` statistic. The kubelet treats `active_file` memory
areas as not reclaimable. For workloads that make intensive use of block-backed 
local storage, including ephemeral local storage, kernel-level caches of file 
and block data means that many recently accessed cache pages are likely to be 
counted as `active_file`. If enough of these kernel block buffers are on the 
active LRU list, the kubelet is liable to observe this as high resource use and 
taint the node as experiencing memory pressure - triggering pod eviction.
-->
<h4 id="active-file-内存未被视为可用内存">active_file 内存未被视为可用内存</h4>
<p>在 Linux 上，内核跟踪活动 LRU 列表上的基于文件所虚拟的内存字节数作为 <code>active_file</code> 统计信息。
kubelet 将 <code>active_file</code> 内存区域视为不可回收。
对于大量使用块设备形式的本地存储（包括临时本地存储）的工作负载，
文件和块数据的内核级缓存意味着许多最近访问的缓存页面可能被计为 <code>active_file</code>。
如果这些内核块缓冲区中在活动 LRU 列表上有足够多，
kubelet 很容易将其视为资源用量过量并为节点设置内存压力污点，从而触发 Pod 驱逐。</p>
<!-- 
For more more details, see [https://github.com/kubernetes/kubernetes/issues/43916](https://github.com/kubernetes/kubernetes/issues/43916)

You can work around that behavior by setting the memory limit and memory request
the same for containers likely to perform intensive I/O activity. You will need 
to estimate or measure an optimal memory limit value for that container.
-->
<p>更多细节请参见 <a href="https://github.com/kubernetes/kubernetes/issues/43916">https://github.com/kubernetes/kubernetes/issues/43916</a></p>
<p>你可以通过为可能执行 I/O 密集型活动的容器设置相同的内存限制和内存请求来应对该行为。
你将需要估计或测量该容器的最佳内存限制值。</p>
<h2 id="what-s-next">What's next</h2>
<!-- 
* Learn about [API-initiated Eviction](/docs/reference/generated/kubernetes-api/v1.23/)
* Learn about [Pod Priority and Preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/)
* Learn about [PodDisruptionBudgets](/docs/tasks/run-application/configure-pdb/)
* Learn about [Quality of Service](/docs/tasks/configure-pod-container/quality-service-pod/) (QoS)
* Check out the [Eviction API](/docs/reference/generated/kubernetes-api/v1.23/#create-eviction-pod-v1-core)
-->
<ul>
<li>了解 <a href="/docs/reference/generated/kubernetes-api/v1.23/">API 发起的驱逐</a></li>
<li>了解 <a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod 优先级和驱逐</a></li>
<li>了解 <a href="/docs/tasks/run-application/configure-pdb/">PodDisruptionBudgets</a></li>
<li>了解<a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">服务质量</a>（QoS）</li>
<li>查看<a href="/docs/reference/generated/kubernetes-api/v1.23/#create-eviction-pod-v1-core">驱逐 API</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b87723bf81b079042860f0ebd37b0a64">10.7 - API 发起的驱逐</h1>
    
	<!-- 
---
title: API-initiated Eviction
content_type: concept
weight: 70
---
-->
<p>API 发起的驱逐是一个先调用
<a href="/docs/reference/generated/kubernetes-api/v1.23/#create-eviction-pod-v1-core">Eviction API</a>
创建 <code>Eviction</code> 对象，再由该对象体面地中止 Pod 的过程。 </br></p>
<!-- 
You can request eviction by calling the Eviction API directly, or programmatically
using a client of the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a>, like the `kubectl drain` command. This
creates an `Eviction` object, which causes the API server to terminate the Pod.

API-initiated evictions respect your configured [`PodDisruptionBudgets`](/docs/tasks/run-application/configure-pdb/)
and [`terminationGracePeriodSeconds`](/docs/concepts/workloads/pods/pod-lifecycle#pod-termination). 

Using the API to create an Eviction object for a Pod is like performing a
policy-controlled [`DELETE` operation](/docs/reference/kubernetes-api/workload-resources/pod-v1/#delete-delete-a-pod)
on the Pod. 
-->
<p>你可以通过直接调用 Eviction API 发起驱逐，也可以通过编程的方式使用
<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a>的客户端来发起驱逐，
比如 <code>kubectl drain</code> 命令。
此操作创建一个 <code>Eviction</code> 对象，该对象再驱动 API 服务器终止选定的 Pod。</p>
<p>API 发起的驱逐将遵从你的
<a href="/zh/docs/tasks/run-application/configure-pdb/"><code>PodDisruptionBudgets</code></a>
和 <a href="/zh/docs/concepts/workloads/pods/pod-lifecycle#pod-termination"><code>terminationGracePeriodSeconds</code></a>
配置。</p>
<p>使用 API 创建 Eviction 对象，就像对 Pod 执行策略控制的
<a href="/zh/docs/reference/kubernetes-api/workload-resources/pod-v1/#delete-delete-a-pod"><code>DELETE</code> 操作</a></p>
<!--
## Calling the Eviction API

You can use a [Kubernetes language client](/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api)
to access the Kubernetes API and create an `Eviction` object. To do this, you
POST the attempted operation, similar to the following example:
-->
<h2 id="调用-eviction-api">调用 Eviction API</h2>
<p>你可以使用 <a href="/zh/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api">Kubernetes 语言客户端</a>
来访问 Kubernetes API 并创建 <code>Eviction</code> 对象。
要执行此操作，你应该用 POST 发出要尝试的请求，类似于下面的示例：</p>
<ul class="nav nav-tabs" id="eviction-example" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#eviction-example-0" role="tab" aria-controls="eviction-example-0" aria-selected="true">policy/v1</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#eviction-example-1" role="tab" aria-controls="eviction-example-1">policy/v1beta1</a></li></ul>
<div class="tab-content" id="eviction-example"><div id="eviction-example-0" class="tab-pane show active" role="tabpanel" aria-labelledby="eviction-example-0">

<p><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- `policy/v1` Eviction is available in v1.22+. Use `policy/v1beta1` with prior releases. -->
<p><code>policy/v1</code>  版本的 Eviction 在 v1.22 以及更高的版本中可用，之前的发行版本使用 <code>policy/v1beta1</code> 版本。
</div>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;policy/v1&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Eviction&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;metadata&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;quux&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;default&#34;</span>
  }
}
</code></pre></div></div>
  <div id="eviction-example-1" class="tab-pane" role="tabpanel" aria-labelledby="eviction-example-1">

<p><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!-- Deprecated in v1.22 in favor of `policy/v1` -->
<p>在 v1.22 版本废弃以支持 <code>policy/v1</code>
</div>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;policy/v1beta1&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Eviction&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;metadata&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;quux&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;default&#34;</span>
  }
}
</code></pre></div></div></div>

<!-- 
Alternatively, you can attempt an eviction operation by accessing the API using
`curl` or `wget`, similar to the following example:
-->
<p>或者，你可以通过使用 <code>curl</code> 或者 <code>wget</code> 来访问 API 以尝试驱逐操作，类似于以下示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -v -H <span style="color:#b44">&#39;Content-type: application/json&#39;</span> https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json
</code></pre></div><!-- 
## How API-initiated eviction works

When you request an eviction using the API, the API server performs admission
checks and responds in one of the following ways:
-->
<h2 id="api-发起驱逐的工作原理">API 发起驱逐的工作原理</h2>
<p>当你使用 API 来请求驱逐时，API 服务器将执行准入检查，并通过以下方式之一做出响应：</p>
<!-- 
* `200 OK`: the eviction is allowed, the `Eviction` subresource is created, and
  the Pod is deleted, similar to sending a `DELETE` request to the Pod URL.
* `429 Too Many Requests`: the eviction is not currently allowed because of the
  configured <a class='glossary-tooltip' title='Pod Disruption Budget 是这样一种对象：它保证在主动中断（ voluntary disruptions）时，多实例应用的 {{&lt; glossary_tooltip text=&quot;Pod&quot; term_id=&quot;pod&quot; &gt;}} 不会少于一定的数量。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-pod-disruption-budget' target='_blank' aria-label='PodDisruptionBudget'>PodDisruptionBudget</a>.
  You may be able to attempt the eviction again later. You might also see this
  response because of API rate limiting. 
* `500 Internal Server Error`: the eviction is not allowed because there is a
  misconfiguration, like if multiple PodDisruptionBudgets reference the same Pod.
-->
<ul>
<li><code>200 OK</code>：允许驱逐，子资源 <code>Eviction</code> 被创建，并且 Pod 被删除，
类似于发送一个 <code>DELETE</code> 请求到 Pod 地址。</li>
<li><code>429 Too Many Requests</code>：当前不允许驱逐，因为配置了 <a class='glossary-tooltip' title='Pod Disruption Budget 是这样一种对象：它保证在主动中断（ voluntary disruptions）时，多实例应用的 {{&lt; glossary_tooltip text=&quot;Pod&quot; term_id=&quot;pod&quot; &gt;}} 不会少于一定的数量。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-pod-disruption-budget' target='_blank' aria-label='PodDisruptionBudget'>PodDisruptionBudget</a>。
你可以稍后再尝试驱逐。你也可能因为 API 速率限制而看到这种响应。</li>
<li><code>500 Internal Server Error</code>：不允许驱逐，因为存在配置错误，
例如存在多个 PodDisruptionBudgets 引用同一个 Pod。</li>
</ul>
<!--
If the Pod you want to evict isn't part of a workload that has a
PodDisruptionBudget, the API server always returns `200 OK` and allows the
eviction. 

If the API server allows the eviction, the Pod is deleted as follows:
-->
<p>如果你想驱逐的 Pod 不属于有 PodDisruptionBudget 的工作负载，
API 服务器总是返回 <code>200 OK</code> 并且允许驱逐。</p>
<p>如果 API 服务器允许驱逐，Pod 按照如下方式删除：</p>
<!--
1. The `Pod` resource in the API server is updated with a deletion timestamp,
   after which the API server considers the `Pod` resource to be terminated. The
   `Pod` resource is also marked with the configured grace period.
1. The <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> on the node where the local Pod is running notices that the `Pod`
   resource is marked for termination and starts to gracefully shut down the
   local Pod.
1. While the kubelet is shutting the Pod down, the control plane removes the Pod
   from <a class='glossary-tooltip' title='端点负责记录与服务（Service）的选择器相匹配的 Pods 的 IP 地址。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-endpoint' target='_blank' aria-label='Endpoint'>Endpoint</a> and
   <a class='glossary-tooltip' title='一种将网络端点与 Kubernetes 资源组合在一起的方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/endpoint-slices/' target='_blank' aria-label='EndpointSlice'>EndpointSlice</a>
   objects. As a result, controllers no longer consider the Pod as a valid object.
1. After the grace period for the Pod expires, the kubelet forcefully terminates
   the local Pod.
1. The kubelet tells the API server to remove the `Pod` resource.
1. The API server deletes the `Pod` resource.
-->
<ol>
<li>API 服务器中的 <code>Pod</code> 资源会更新上删除时间戳，之后 API 服务器会认为此 <code>Pod</code> 资源将被终止。
此 <code>Pod</code> 资源还会标记上配置的宽限期。</li>
<li>本地运行状态的 Pod 所处的节点上的 <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>
注意到 <code>Pod</code> 资源被标记为终止，并开始优雅停止本地 Pod。</li>
<li>当 kubelet 停止 Pod 时，控制面从 <a class='glossary-tooltip' title='端点负责记录与服务（Service）的选择器相匹配的 Pods 的 IP 地址。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-endpoint' target='_blank' aria-label='Endpoint'>Endpoint</a>
和 <a class='glossary-tooltip' title='一种将网络端点与 Kubernetes 资源组合在一起的方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/endpoint-slices/' target='_blank' aria-label='EndpointSlice'>EndpointSlice</a>
对象中移除该 Pod。因此，控制器不再将此 Pod 视为有用对象。</li>
<li>Pod 的宽限期到期后，kubelet 强制终止本地 Pod。</li>
<li>kubelet 告诉 API 服务器删除 <code>Pod</code> 资源。</li>
<li>API 服务器删除 <code>Pod</code> 资源。</li>
</ol>
<!-- 
## Troubleshooting stuck evictions

In some cases, your applications may enter a broken state, where the Eviction
API will only return `429` or `500` responses until you intervene. This can 
happen if, for example, a ReplicaSet creates pods for your application but new 
pods do not enter a `Ready` state. You may also notice this behavior in cases
where the last evicted Pod had a long termination grace period.
-->
<h2 id="解决驱逐被卡住的问题">解决驱逐被卡住的问题</h2>
<p>在某些情况下，你的应用可能进入中断状态，
在你干预之前，驱逐 API 总是返回 <code>429</code> 或 <code>500</code>。
例如，如果 ReplicaSet 为你的应用程序创建了 Pod，
但新的 Pod 没有进入 <code>Ready</code> 状态，就会发生这种情况。
在最后一个被驱逐的 Pod 有很长的终止宽限期的情况下，你可能也会注意到这种行为。</p>
<!-- 
If you notice stuck evictions, try one of the following solutions: 

* Abort or pause the automated operation causing the issue. Investigate the stuck
  application before you restart the operation.
* Wait a while, then directly delete the Pod from your cluster control plane
  instead of using the Eviction API.
-->
<p>如果你注意到驱逐被卡住，请尝试以下解决方案之一：</p>
<ul>
<li>终止或暂停导致问题的自动化操作，重新启动操作之前，请检查被卡住的应用程序。</li>
<li>等待一段时间后，直接从集群控制平面删除 Pod，而不是使用 Eviction API。</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!-- 
* Learn how to protect your applications with a [Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).
* Learn about [Node-pressure Eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).
* Learn about [Pod Priority and Preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).
-->
<ul>
<li>了解如何使用 <a href="/zh/docs/tasks/run-application/configure-pdb/">Pod 干扰预算</a> 保护你的应用。</li>
<li>了解<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/">节点压力引发的驱逐</a>。</li>
<li>了解 <a href="/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/">Pod 优先级和抢占</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-961126cd43559012893979e568396a49">10.8 - 扩展资源的资源装箱</h1>
    
	<!--
---
reviewers:
- bsalamat
- k82cn
- ahg-g
title: Resource Bin Packing for Extended Resources
content_type: concept
weight: 80
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.16 [alpha]</code>
</div>


<!--
The kube-scheduler can be configured to enable bin packing of resources along with extended resources using `RequestedToCapacityRatioResourceAllocation` priority function. Priority functions can be used to fine-tune the kube-scheduler as per custom needs.
-->
<p>使用 <code>RequestedToCapacityRatioResourceAllocation</code> 优先级函数，可以将 kube-scheduler
配置为支持包含扩展资源在内的资源装箱操作。
优先级函数可用于根据自定义需求微调 kube-scheduler 。</p>
<!-- body -->
<!--
## Enabling Bin Packing using RequestedToCapacityRatioResourceAllocation

Kubernetes allows the users to specify the resources along with weights for
each resource to score nodes based on the request to capacity ratio. This
allows users to bin pack extended resources by using appropriate parameters
and improves the utilization of scarce resources in large clusters. The
behavior of the `RequestedToCapacityRatioResourceAllocation` priority function
can be controlled by a configuration option called `RequestedToCapacityRatioArgs`. 
This argument consists of two parameters `shape` and `resources`. The `shape` 
parameter allows the user to tune the function as least requested or most 
requested based on `utilization` and `score` values.  The `resources` parameter 
consists of `name` of the resource to be considered during scoring and `weight` 
specify the weight of each resource.

-->
<h2 id="使用-requestedtocapacityratioresourceallocation-启用装箱">使用 RequestedToCapacityRatioResourceAllocation 启用装箱</h2>
<p>Kubernetes 允许用户指定资源以及每类资源的权重，
以便根据请求数量与可用容量之比率为节点评分。
这就使得用户可以通过使用适当的参数来对扩展资源执行装箱操作，从而提高了大型集群中稀缺资源的利用率。
<code>RequestedToCapacityRatioResourceAllocation</code> 优先级函数的行为可以通过名为
<code>RequestedToCapacityRatioArgs</code> 的配置选项进行控制。
该标志由两个参数 <code>shape</code> 和 <code>resources</code> 组成。
<code>shape</code> 允许用户根据 <code>utilization</code> 和 <code>score</code> 值将函数调整为
最少请求（least requested）或最多请求（most requested）计算。
<code>resources</code> 包含由 <code>name</code> 和  <code>weight</code> 组成，<code>name</code> 指定评分时要考虑的资源，
<code>weight</code> 指定每种资源的权重。</p>
<!--
Below is an example configuration that sets
`requestedToCapacityRatioArguments` to bin packing behavior for extended
resources `intel.com/foo` and `intel.com/bar`.
-->
<p>以下是一个配置示例，该配置将 <code>requestedToCapacityRatioArguments</code> 设置为对扩展资源
<code>intel.com/foo</code> 和 <code>intel.com/bar</code> 的装箱行为</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1beta3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">profiles</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">pluginConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>RequestedToCapacityRatio<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> 
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">shape</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">score</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">score</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>intel.com/foo<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>intel.com/bar<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></code></pre></div><!--
Referencing the `KubeSchedulerConfiguration` file with the kube-scheduler 
flag `--config=/path/to/config/file` will pass the configuration to the 
scheduler.
-->
<p>使用 kube-scheduler 标志 <code>--config=/path/to/config/file</code>
引用 <code>KubeSchedulerConfiguration</code> 文件将配置传递给调度器。</p>
<!--
**This feature is disabled by default**
-->
<p><strong>默认情况下此功能处于被禁用状态</strong></p>
<!--
### Tuning RequestedToCapacityRatioResourceAllocation Priority Function

`shape` is used to specify the behavior of the `RequestedToCapacityRatioPriority` function.
-->
<h3 id="调整-requestedtocapacityratioresourceallocation-优先级函数">调整 RequestedToCapacityRatioResourceAllocation 优先级函数</h3>
<p><code>shape</code> 用于指定 <code>RequestedToCapacityRatioPriority</code> 函数的行为。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">shape</span>:<span style="color:#bbb">
</span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">   </span><span style="color:#008000;font-weight:bold">score</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">   </span><span style="color:#008000;font-weight:bold">score</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></code></pre></div><!--
The above arguments give the node a score of 0 if utilization is 0% and 10 for utilization 100%, thus enabling bin packing behavior. To enable least requested the score value must be reversed as follows.
-->
<p>上面的参数在 <code>utilization</code> 为 0% 时给节点评分为 0，在 <code>utilization</code> 为
100% 时给节点评分为 10，因此启用了装箱行为。
要启用最少请求（least requested）模式，必须按如下方式反转得分值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">shape</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">score</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">utilization</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">score</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></code></pre></div><!--
`resources` is an optional parameter which by defaults is set to:
-->
<p><code>resources</code> 是一个可选参数，默认情况下设置为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>memory<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></code></pre></div><!--
It can be used to add extended resources as follows:
-->
<p>它可以用来添加扩展资源，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>intel.com/foo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>memory<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></code></pre></div><!--
The weight parameter is optional and is set to 1 if not specified. Also, the weight cannot be set to a negative value.
-->
<p>weight 参数是可选的，如果未指定，则设置为 1。
同时，weight 不能设置为负值。</p>
<!--
### Node scoring for capacity allocation

This section is intended for those who want to understand the internal details
of this feature.
Below is an example of how the node score is calculated for a given set of values.
-->
<h3 id="节点容量分配的评分">节点容量分配的评分</h3>
<p>本节适用于希望了解此功能的内部细节的人员。
以下是如何针对给定的一组值来计算节点得分的示例。</p>
<pre><code>请求的资源

intel.com/foo : 2
memory: 256MB
cpu: 2

资源权重

intel.com/foo : 5
memory: 1
cpu: 3

FunctionShapePoint {{0, 0}, {100, 10}}

节点 Node 1 配置

可用：
  intel.com/foo : 4
  memory : 1 GB
  cpu: 8

已用：
  intel.com/foo: 1
  memory: 256MB
  cpu: 1

节点得分：

intel.com/foo  = resourceScoringFunction((2+1),4)
               = (100 - ((4-3)*100/4)
               = (100 - 25)
               = 75
               = rawScoringFunction(75)
               = 7

memory         = resourceScoringFunction((256+256),1024)
               = (100 -((1024-512)*100/1024))
               = 50
               = rawScoringFunction(50)
               = 5

cpu            = resourceScoringFunction((2+1),8)
               = (100 -((8-3)*100/8))
               = 37.5
               = rawScoringFunction(37.5)
               = 3

NodeScore   =  (7 * 5) + (5 * 1) + (3 * 3) / (5 + 1 + 3)
            =  5


节点 Node 2 配置

可用：
  intel.com/foo: 8
  memory: 1GB
  cpu: 8

已用：
  intel.com/foo: 2
  memory: 512MB
  cpu: 6

节点得分：

intel.com/foo  = resourceScoringFunction((2+2),8)
               = (100 - ((8-4)*100/8)
               = (100 - 50)
               = 50
               = rawScoringFunction(50)
               = 5

memory         = resourceScoringFunction((256+512),1024)
               = (100 -((1024-768)*100/1024))
               = 75
               = rawScoringFunction(75)
               = 7

cpu            = resourceScoringFunction((2+6),8)
               = (100 -((8-8)*100/8))
               = 100
               = rawScoringFunction(100)
               = 10

NodeScore   =  (5 * 5) + (7 * 1) + (10 * 3) / (5 + 1 + 3)
            =  7
</code></pre>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-602208c95fe7b1f1170310ce993f5814">10.9 - 调度框架</h1>
    
	<!--
---
reviewers:
- ahg-g
title: Scheduling Framework
content_type: concept
weight: 90
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.19 [stable]</code>
</div>


<!--
The scheduling framework is a pluggable architecture for the Kubernetes scheduler.
It adds a new set of "plugin" APIs to the existing scheduler. Plugins are compiled into the scheduler. The APIs allow most scheduling features to be implemented as plugins, while keeping the
scheduling "core" lightweight and maintainable. Refer to the [design proposal of the
scheduling framework][kep] for more technical information on the design of the
framework.
-->
<p>调度框架是面向 Kubernetes 调度器的一种插件架构，
它为现有的调度器添加了一组新的“插件” API。插件会被编译到调度器之中。
这些 API 允许大多数调度功能以插件的形式实现，同时使调度“核心”保持简单且可维护。
请参考<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md">调度框架的设计提案</a>
获取框架设计的更多技术信息。</p>
<!-- body -->
<!--
# Framework workflow
-->
<h1 id="框架工作流程">框架工作流程</h1>
<!--
The Scheduling Framework defines a few extension points. Scheduler plugins
register to be invoked at one or more extension points. Some of these plugins
can change the scheduling decisions and some are informational only.
-->
<p>调度框架定义了一些扩展点。调度器插件注册后在一个或多个扩展点处被调用。
这些插件中的一些可以改变调度决策，而另一些仅用于提供信息。</p>
<!--
Each attempt to schedule one Pod is split into two phases, the **scheduling
cycle** and the **binding cycle**.
-->
<p>每次调度一个 Pod 的尝试都分为两个阶段，即 <strong>调度周期</strong> 和 <strong>绑定周期</strong>。</p>
<!--
## Scheduling Cycle & Binding Cycle
-->
<h2 id="调度周期和绑定周期">调度周期和绑定周期</h2>
<!--
The scheduling cycle selects a node for the Pod, and the binding cycle applies
that decision to the cluster. Together, a scheduling cycle and binding cycle are
referred to as a "scheduling context".
-->
<p>调度周期为 Pod 选择一个节点，绑定周期将该决策应用于集群。
调度周期和绑定周期一起被称为“调度上下文”。</p>
<!--
Scheduling cycles are run serially, while binding cycles may run concurrently.
-->
<p>调度周期是串行运行的，而绑定周期可能是同时运行的。</p>
<!--
A scheduling or binding cycle can be aborted if the Pod is determined to
be unschedulable or if there is an internal error. The Pod will be returned to
the queue and retried.
-->
<p>如果确定 Pod 不可调度或者存在内部错误，则可以终止调度周期或绑定周期。
Pod 将返回队列并重试。</p>
<!--
## Extension points
-->
<h2 id="扩展点">扩展点</h2>
<!--
The following picture shows the scheduling context of a Pod and the extension
points that the scheduling framework exposes. In this picture "Filter" is
equivalent to "Predicate" and "Scoring" is equivalent to "Priority function".
-->
<p>下图显示了一个 Pod 的调度上下文以及调度框架公开的扩展点。
在此图片中，“过滤器”等同于“断言”，“评分”相当于“优先级函数”。</p>
<!--
One plugin may register at multiple extension points to perform more complex or
stateful tasks.
-->
<p>一个插件可以在多个扩展点处注册，以执行更复杂或有状态的任务。</p>
<!--

<figure class="diagram-large">
    <img src="/images/docs/scheduling-framework-extensions.png"/> <figcaption>
            <h4>scheduling framework extension points</h4>
        </figcaption>
</figure>

-->

<figure class="diagram-large">
    <img src="/images/docs/scheduling-framework-extensions.png"/> <figcaption>
            <h4>调度框架扩展点</h4>
        </figcaption>
</figure>

<!--
### QueueSort {#queue-sort}
-->
<h3 id="queue-sort">队列排序</h3>
<!--
These plugins are used to sort Pods in the scheduling queue. A queue sort plugin
essentially provides a `less(Pod1, Pod2)` function. Only one queue sort
plugin may be enabled at a time.
-->
<p>这些插件用于对调度队列中的 Pod 进行排序。
队列排序插件本质上提供 <code>less(Pod1, Pod2)</code> 函数。
一次只能启动一个队列插件。</p>
<!--
### PreFilter {#pre-filter}
-->
<h3 id="pre-filter">PreFilter</h3>
<!--
These plugins are used to pre-process info about the Pod, or to check certain
conditions that the cluster or the Pod must meet. If a PreFilter plugin returns
an error, the scheduling cycle is aborted.
-->
<p>这些插件用于预处理 Pod 的相关信息，或者检查集群或 Pod 必须满足的某些条件。
如果 PreFilter 插件返回错误，则调度周期将终止。</p>
<!--
### Filter
-->
<h3 id="filter">Filter</h3>
<!--
These plugins are used to filter out nodes that cannot run the Pod. For each
node, the scheduler will call filter plugins in their configured order. If any
filter plugin marks the node as infeasible, the remaining plugins will not be
called for that node. Nodes may be evaluated concurrently.
-->
<p>这些插件用于过滤出不能运行该 Pod 的节点。对于每个节点，
调度器将按照其配置顺序调用这些过滤插件。如果任何过滤插件将节点标记为不可行，
则不会为该节点调用剩下的过滤插件。节点可以被同时进行评估。</p>
<!--
### PostFilter {#post-filter}
-->
<h3 id="post-filter">PostFilter </h3>
<!--
These plugins are called after Filter phase, but only when no feasible nodes
were found for the pod. Plugins are called in their configured order. If
any postFilter plugin marks the node as `Schedulable`, the remaining plugins
will not be called. A typical PostFilter implementation is preemption, which
tries to make the pod schedulable by preempting other Pods.
-->
<p>这些插件在 Filter 阶段后调用，但仅在该 Pod 没有可行的节点时调用。
插件按其配置的顺序调用。如果任何 PostFilter 插件标记节点为“Schedulable”，
则其余的插件不会调用。典型的 PostFilter 实现是抢占，试图通过抢占其他 Pod
的资源使该 Pod 可以调度。</p>
<!--
### PreScore {#pre-score}
 -->
<h3 id="pre-score">PreScore</h3>
<!--
These plugins are used to perform "pre-scoring" work, which generates a sharable
state for Score plugins to use. If a PreScore plugin returns an error, the
scheduling cycle is aborted.
 -->
<p>这些插件用于执行 “前置评分（pre-scoring）” 工作，即生成一个可共享状态供 Score 插件使用。
如果 PreScore 插件返回错误，则调度周期将终止。</p>
<!--
### Score {#scoring}
 -->
<h3 id="scoring">Score </h3>
<!--
These plugins are used to rank nodes that have passed the filtering phase. The
scheduler will call each scoring plugin for each node. There will be a well
defined range of integers representing the minimum and maximum scores. After the
[NormalizeScore](#normalize-scoring) phase, the scheduler will combine node
scores from all plugins according to the configured plugin weights.
-->
<p>这些插件用于对通过过滤阶段的节点进行排序。调度器将为每个节点调用每个评分插件。
将有一个定义明确的整数范围，代表最小和最大分数。
在<a href="#normalize-scoring">标准化评分</a>阶段之后，调度器将根据配置的插件权重
合并所有插件的节点分数。</p>
<!--
### NormalizeScore {#normalize-scoring}
-->
<h3 id="normalize-scoring">NormalizeScore  </h3>
<!--
These plugins are used to modify scores before the scheduler computes a final
ranking of Nodes. A plugin that registers for this extension point will be
called with the [Score](#scoring) results from the same plugin. This is called
once per plugin per scheduling cycle.
-->
<p>这些插件用于在调度器计算 Node 排名之前修改分数。
在此扩展点注册的插件被调用时会使用同一插件的 <a href="#scoring">Score</a> 结果。
每个插件在每个调度周期调用一次。</p>
<!--
For example, suppose a plugin `BlinkingLightScorer` ranks Nodes based on how
many blinking lights they have.
-->
<p>例如，假设一个 <code>BlinkingLightScorer</code> 插件基于具有的闪烁指示灯数量来对节点进行排名。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#a2f;font-weight:bold">func</span> <span style="color:#00a000">ScoreNode</span>(_ <span style="color:#666">*</span>v1.pod, n <span style="color:#666">*</span>v1.Node) (<span style="color:#0b0;font-weight:bold">int</span>, <span style="color:#0b0;font-weight:bold">error</span>) {
   <span style="color:#a2f;font-weight:bold">return</span> <span style="color:#00a000">getBlinkingLightCount</span>(n)
}
</code></pre></div><!--
However, the maximum count of blinking lights may be small compared to
`NodeScoreMax`. To fix this, `BlinkingLightScorer` should also register for this
extension point.
-->
<p>然而，最大的闪烁灯个数值可能比 <code>NodeScoreMax</code> 小。要解决这个问题，
<code>BlinkingLightScorer</code> 插件还应该注册该扩展点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#a2f;font-weight:bold">func</span> <span style="color:#00a000">NormalizeScores</span>(scores <span style="color:#a2f;font-weight:bold">map</span>[<span style="color:#0b0;font-weight:bold">string</span>]<span style="color:#0b0;font-weight:bold">int</span>) {
   highest <span style="color:#666">:=</span> <span style="color:#666">0</span>
   <span style="color:#a2f;font-weight:bold">for</span> _, score <span style="color:#666">:=</span> <span style="color:#a2f;font-weight:bold">range</span> scores {
      highest = <span style="color:#00a000">max</span>(highest, score)
   }
   <span style="color:#a2f;font-weight:bold">for</span> node, score <span style="color:#666">:=</span> <span style="color:#a2f;font-weight:bold">range</span> scores {
      scores[node] = score<span style="color:#666">*</span>NodeScoreMax<span style="color:#666">/</span>highest
   }
}
</code></pre></div><!--
If any NormalizeScore plugin returns an error, the scheduling cycle is
aborted.
-->
<p>如果任何 NormalizeScore 插件返回错误，则调度阶段将终止。</p>
<!--
Plugins wishing to perform "pre-reserve" work should use the
NormalizeScore extension point.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 希望执行“预保留”工作的插件应该使用 NormalizeScore 扩展点。
</div>
<!--
### Reserve
-->
<h3 id="reserve">Reserve</h3>
<!--
This is an informational extension point. Plugins which maintain runtime state
(aka "stateful plugins") should use this extension point to be notified by the
scheduler when resources on a node are being reserved for a given Pod. This
happens before the scheduler actually binds the Pod to the Node, and it exists
to prevent race conditions while the scheduler waits for the bind to succeed.
-->
<p>Reserve 是一个信息性的扩展点。
管理运行时状态的插件（也成为“有状态插件”）应该使用此扩展点，以便
调度器在节点给指定 Pod 预留了资源时能够通知该插件。
这是在调度器真正将 Pod 绑定到节点之前发生的，并且它存在是为了防止
在调度器等待绑定成功时发生竞争情况。</p>
<!--
This is the last step in a scheduling cycle. Once a Pod is in the reserved
state, it will either trigger [Unreserve](#unreserve) plugins (on failure) or
[PostBind](#post-bind) plugins (on success) at the end of the binding cycle.
-->
<p>这个是调度周期的最后一步。
一旦 Pod 处于保留状态，它将在绑定周期结束时触发 <a href="#unreserve">Unreserve</a> 插件
（失败时）或 <a href="#post-bind">PostBind</a> 插件（成功时）。</p>
<!--
### Permit
-->
<h3 id="permit">Permit</h3>
<!--
_Permit_ plugins are invoked at the end of the scheduling cycle for each Pod, to
prevent or delay the binding to the candidate node. A permit plugin can do one of
the three things:
-->
<p><em>Permit</em> 插件在每个 Pod 调度周期的最后调用，用于防止或延迟 Pod 的绑定。
一个允许插件可以做以下三件事之一：</p>
<!--
1.  **approve** \
    Once all Permit plugins approve a Pod, it is sent for binding.
-->
<ol>
<li><strong>批准</strong><br>
一旦所有 Permit 插件批准 Pod 后，该 Pod 将被发送以进行绑定。</li>
</ol>
<!--
1.  **deny** \
    If any Permit plugin denies a Pod, it is returned to the scheduling queue.
    This will trigger [Unreserve](#unreserve) plugins.
-->
<ol>
<li><strong>拒绝</strong><br>
如果任何 Permit 插件拒绝 Pod，则该 Pod 将被返回到调度队列。
这将触发<a href="#unreserve">Unreserve</a> 插件。</li>
</ol>
<!--
1.  **wait** (with a timeout) \
    If a Permit plugin returns "wait", then the Pod is kept in an internal "waiting"
    Pods list, and the binding cycle of this Pod starts but directly blocks until it
    gets [approved](#frameworkhandle). If a timeout occurs, **wait** becomes **deny**
    and the Pod is returned to the scheduling queue, triggering [Unreserve](#unreserve)
    plugins.
-->
<ol>
<li><strong>等待</strong>（带有超时）<br>
如果一个 Permit 插件返回 “等待” 结果，则 Pod 将保持在一个内部的 “等待中”
的 Pod 列表，同时该 Pod 的绑定周期启动时即直接阻塞直到得到
<a href="#frameworkhandle">批准</a>。如果超时发生，<strong>等待</strong> 变成 <strong>拒绝</strong>，并且 Pod
将返回调度队列，从而触发 <a href="#unreserve">Unreserve</a> 插件。</li>
</ol>
<!--
While any plugin can access the list of "waiting" Pods and approve them
(see [`FrameworkHandle`](https://git.k8s.io/enhancements/keps/sig-scheduling/624-scheduling-framework#frameworkhandle)), we expect only the permit
plugins to approve binding of reserved Pods that are in "waiting" state. Once a Pod
is approved, it is sent to the [PreBind](#pre-bind) phase.
 -->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 尽管任何插件可以访问 “等待中” 状态的 Pod 列表并批准它们
(查看 <a href="https://git.k8s.io/enhancements/keps/sig-scheduling/624-scheduling-framework#frameworkhandle"><code>FrameworkHandle</code></a>)。
我们期望只有允许插件可以批准处于 “等待中” 状态的预留 Pod 的绑定。
一旦 Pod 被批准了，它将发送到 <a href="#pre-bind">PreBind</a> 阶段。
</div>
<!--
### Pre-bind {#pre-bind}
-->
<h3 id="pre-bind">PreBind </h3>
<!--
These plugins are used to perform any work required before a Pod is bound. For
example, a pre-bind plugin may provision a network volume and mount it on the
target node before allowing the Pod to run there.
-->
<p>这些插件用于执行 Pod 绑定前所需的所有工作。
例如，一个 PreBind 插件可能需要制备网络卷并且在允许 Pod 运行在该节点之前
将其挂载到目标节点上。</p>
<!--
If any PreBind plugin returns an error, the Pod is [rejected](#unreserve) and
returned to the scheduling queue.
-->
<p>如果任何 PreBind 插件返回错误，则 Pod 将被 <a href="#unreserve">拒绝</a> 并且
退回到调度队列中。</p>
<!--
### Bind
-->
<h3 id="bind">Bind</h3>
<!--
These plugins are used to bind a Pod to a Node. Bind plugins will not be called
until all PreBind plugins have completed. Each bind plugin is called in the
configured order. A bind plugin may choose whether or not to handle the given
Pod. If a bind plugin chooses to handle a Pod, **the remaining bind plugins are
skipped**.
-->
<p>Bind 插件用于将 Pod 绑定到节点上。直到所有的 PreBind 插件都完成，Bind 插件才会被调用。
各 Bind 插件按照配置顺序被调用。Bind 插件可以选择是否处理指定的 Pod。
如果某 Bind 插件选择处理某 Pod，<strong>剩余的 Bind 插件将被跳过</strong>。</p>
<!--
### PostBind {#post-bind}
-->
<h3 id="post-bind">PostBind </h3>
<!--
This is an informational extension point. Post-bind plugins are called after a
Pod is successfully bound. This is the end of a binding cycle, and can be used
to clean up associated resources.
-->
<p>这是个信息性的扩展点。
PostBind 插件在 Pod 成功绑定后被调用。这是绑定周期的结尾，可用于清理相关的资源。</p>
<!--
### Unreserve
-->
<h3 id="unreserve">Unreserve</h3>
<!--
This is an informational extension point. If a Pod was reserved and then
rejected in a later phase, then unreserve plugins will be notified. Unreserve
plugins should clean up state associated with the reserved Pod.
-->
<p>这是个信息性的扩展点。
如果 Pod 被保留，然后在后面的阶段中被拒绝，则 Unreserve 插件将被通知。
Unreserve 插件应该清楚保留 Pod 的相关状态。</p>
<!--
Plugins that use this extension point usually should also use
[Reserve](#reserve).
-->
<p>使用此扩展点的插件通常也使用 <a href="#reserve">Reserve</a>。</p>
<!--
## Plugin API
-->
<h2 id="插件-api">插件 API</h2>
<!--
There are two steps to the plugin API. First, plugins must register and get
configured, then they use the extension point interfaces. Extension point
interfaces have the following form.
-->
<p>插件 API 分为两个步骤。首先，插件必须完成注册并配置，然后才能使用扩展点接口。
扩展点接口具有以下形式。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#a2f;font-weight:bold">type</span> Plugin <span style="color:#a2f;font-weight:bold">interface</span> {
   <span style="color:#00a000">Name</span>() <span style="color:#0b0;font-weight:bold">string</span>
}

<span style="color:#a2f;font-weight:bold">type</span> QueueSortPlugin <span style="color:#a2f;font-weight:bold">interface</span> {
   Plugin
   <span style="color:#00a000">Less</span>(<span style="color:#666">*</span>v1.pod, <span style="color:#666">*</span>v1.pod) <span style="color:#0b0;font-weight:bold">bool</span>
}

<span style="color:#a2f;font-weight:bold">type</span> PreFilterPlugin <span style="color:#a2f;font-weight:bold">interface</span> {
   Plugin
   <span style="color:#00a000">PreFilter</span>(context.Context, <span style="color:#666">*</span>framework.CycleState, <span style="color:#666">*</span>v1.pod) <span style="color:#0b0;font-weight:bold">error</span>
}

<span style="color:#080;font-style:italic">// ...
</span></code></pre></div><!--
# Plugin Configuration
-->
<h1 id="插件配置">插件配置</h1>
<!--
You can enable or disable plugins in the scheduler configuration. If you are using
Kubernetes v1.18 or later, most scheduling
[plugins](/docs/reference/scheduling/config/#scheduling-plugins) are in use and
enabled by default.
 -->
<p>你可以在调度器配置中启用或禁用插件。
如果你在使用 Kubernetes v1.18 或更高版本，大部分调度
<a href="/zh/docs/reference/scheduling/config/#scheduling-plugins">插件</a>
都在使用中且默认启用。</p>
<!--
In addition to default plugins, you can also implement your own scheduling
plugins and get them configured along with default plugins. You can visit
[scheduler-plugins](https://github.com/kubernetes-sigs/scheduler-plugins) for more details.
 -->
<p>除了默认的插件，你还可以实现自己的调度插件并且将它们与默认插件一起配置。
你可以访问 <a href="https://github.com/kubernetes-sigs/scheduler-plugins">scheduler-plugins</a>
了解更多信息。</p>
<!--
If you are using Kubernetes v1.18 or later, you can configure a set of plugins as
a scheduler profile and then define multiple profiles to fit various kinds of workload.
Learn more at [multiple profiles](/docs/reference/scheduling/config/#multiple-profiles).
 -->
<p>如果你正在使用 Kubernetes v1.18 或更高版本，你可以将一组插件设置为
一个调度器配置文件，然后定义不同的配置文件来满足各类工作负载。
了解更多关于<a href="/zh/docs/reference/scheduling/config/#multiple-profiles">多配置文件</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d9574a30fcbc631b0d2a57850e161e89">10.10 - 调度器性能调优</h1>
    
	<!--
---
reviewers:
- bsalamat
title: Scheduler Performance Tuning
content_type: concept
weight: 100
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes 1.14 [beta]</code>
</div>


<!--
[kube-scheduler](/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler)
is the Kubernetes default scheduler. It is responsible for placement of Pods
on Nodes in a cluster.
-->
<p>作为 kubernetes 集群的默认调度器，
<a href="/zh/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler">kube-scheduler</a>
主要负责将 Pod 调度到集群的 Node 上。</p>
<!--
Nodes in a cluster that meet the scheduling requirements of a Pod are
called _feasible_ Nodes for the Pod. The scheduler finds feasible Nodes
for a Pod and then runs a set of functions to score the feasible Nodes,
picking a Node with the highest score among the feasible ones to run
the Pod. The scheduler then notifies the API server about this decision
in a process called _Binding_.
-->
<p>在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 <em>可调度</em> Node。
调度器先在集群中找到一个 Pod 的可调度 Node，然后根据一系列函数对这些可调度 Node 打分，
之后选出其中得分最高的 Node 来运行 Pod。
最后，调度器将这个调度决定告知 kube-apiserver，这个过程叫做 <em>绑定（Binding）</em>。</p>
<!--
This page explains performance tuning optimizations that are relevant for
large Kubernetes clusters.
-->
<p>这篇文章将会介绍一些在大规模 Kubernetes 集群下调度器性能优化的方式。</p>
<!-- body -->
<!--
In large clusters, you can tune the scheduler's behaviour balancing
scheduling outcomes between latency (new Pods are placed quickly) and
accuracy (the scheduler rarely makes poor placement decisions).

You configure this tuning setting via kube-scheduler setting
`percentageOfNodesToScore`. This KubeSchedulerConfiguration setting determines
a threshold for scheduling nodes in your cluster.
 -->
<p>在大规模集群中，你可以调节调度器的表现来平衡调度的延迟（新 Pod 快速就位）
和精度（调度器很少做出糟糕的放置决策）。</p>
<p>你可以通过设置 kube-scheduler 的 <code>percentageOfNodesToScore</code> 来配置这个调优设置。
这个 KubeSchedulerConfiguration 设置决定了调度集群中节点的阈值。</p>
<!--
### Setting the threshold
 -->
<h3 id="设置阈值">设置阈值</h3>
<!--
The `percentageOfNodesToScore` option accepts whole numeric values between 0
and 100. The value 0 is a special number which indicates that the kube-scheduler
should use its compiled-in default.
If you set `percentageOfNodesToScore` above 100, kube-scheduler acts as if you
had set a value of 100.
 -->
<p><code>percentageOfNodesToScore</code> 选项接受从 0 到 100 之间的整数值。
0 值比较特殊，表示 kube-scheduler 应该使用其编译后的默认值。
如果你设置 <code>percentageOfNodesToScore</code> 的值超过了 100，
kube-scheduler 的表现等价于设置值为 100。</p>
<!--
To change the value, edit the
[kube-scheduler configuration file](/docs/reference/config-api/kube-scheduler-config.v1beta2/)
and then restart the scheduler.
In many cases, the configuration file can be found at `/etc/kubernetes/config/kube-scheduler.yaml`
 -->
<p>要修改这个值，先编辑 <a href="/zh/docs/reference/config-api/kube-scheduler-config.v1beta3/">kube-scheduler 的配置文件</a>
然后重启调度器。
大多数情况下，这个配置文件是 <code>/etc/kubernetes/config/kube-scheduler.yaml</code>。</p>
<!--
After you have made this change, you can run
 -->
<p>修改完成后，你可以执行</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get pods -n kube-system | grep kube-scheduler
</code></pre></div><!--
to verify that the kube-scheduler component is healthy.
 -->
<p>来检查该 kube-scheduler 组件是否健康。</p>
<!--
## Node scoring threshold {#percentage-of-nodes-to-score}
 -->
<h2 id="percentage-of-nodes-to-score">节点打分阈值</h2>
<!--
To improve scheduling performance, the kube-scheduler can stop looking for
feasible nodes once it has found enough of them. In large clusters, this saves
time compared to a naive approach that would consider every node.
 -->
<p>要提升调度性能，kube-scheduler 可以在找到足够的可调度节点之后停止查找。
在大规模集群中，比起考虑每个节点的简单方法相比可以节省时间。</p>
<!--
You specify a threshold for how many nodes are enough, as a whole number percentage
of all the nodes in your cluster. The kube-scheduler converts this into an
integer number of nodes. During scheduling, if the kube-scheduler has identified
enough feasible nodes to exceed the configured percentage, the kube-scheduler
stops searching for more feasible nodes and moves on to the
[scoring phase](/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation).
 -->
<p>你可以使用整个集群节点总数的百分比作为阈值来指定需要多少节点就足够。
kube-scheduler 会将它转换为节点数的整数值。在调度期间，如果
kube-scheduler 已确认的可调度节点数足以超过了配置的百分比数量，
kube-scheduler 将停止继续查找可调度节点并继续进行
<a href="/zh/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation">打分阶段</a>。</p>
<!--
[How the scheduler iterates over Nodes](#how-the-scheduler-iterates-over-nodes)
describes the process in detail.
 -->
<p><a href="#how-the-scheduler-iterates-over-nodes">调度器如何遍历节点</a> 详细介绍了这个过程。</p>
<!--
### Default threshold
 -->
<h3 id="默认阈值">默认阈值</h3>
<!--
If you don't specify a threshold, Kubernetes calculates a figure using a
linear formula that yields 50% for a 100-node cluster and yields 10%
for a 5000-node cluster. The lower bound for the automatic value is 5%.
 -->
<p>如果你不指定阈值，Kubernetes 使用线性公式计算出一个比例，在 100-节点集群
下取 50%，在 5000-节点的集群下取 10%。这个自动设置的参数的最低值是 5%。</p>
<!--
This means that, the kube-scheduler always scores at least 5% of your cluster no
matter how large the cluster is, unless you have explicitly set
`percentageOfNodesToScore` to be smaller than 5.
 -->
<p>这意味着，调度器至少会对集群中 5% 的节点进行打分，除非用户将该参数设置的低于 5。</p>
<!--
If you want the scheduler to score all nodes in your cluster, set
`percentageOfNodesToScore` to 100.
 -->
<p>如果你想让调度器对集群内所有节点进行打分，则将 <code>percentageOfNodesToScore</code> 设置为 100。</p>
<!--
## Example
 -->
<h2 id="示例">示例</h2>
<!--
Below is an example configuration that sets `percentageOfNodesToScore` to 50%.
-->
<p>下面就是一个将 <code>percentageOfNodesToScore</code> 参数设置为 50% 的例子。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubescheduler.config.k8s.io/v1alpha1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeSchedulerConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">algorithmSource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">provider</span>:<span style="color:#bbb"> </span>DefaultProvider<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">percentageOfNodesToScore</span>:<span style="color:#bbb"> </span><span style="color:#666">50</span><span style="color:#bbb">
</span></code></pre></div><!--
### Tuning percentageOfNodesToScore
-->
<h3 id="调节-percentageofnodestoscore-参数">调节 percentageOfNodesToScore 参数</h3>
<!--
`percentageOfNodesToScore` must be a value between 1 and 100 with the default
value being calculated based on the cluster size. There is also a hardcoded
minimum value of 50 nodes.
-->
<p><code>percentageOfNodesToScore</code> 的值必须在 1 到 100 之间，而且其默认值是通过集群的规模计算得来的。
另外，还有一个 50 个 Node 的最小值是硬编码在程序中。</p>
<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>In clusters with less than 50 feasible nodes, the scheduler still
checks all the nodes because there are not enough feasible nodes to stop
the scheduler's search early.</p>
<p>In a small cluster, if you set a low value for <code>percentageOfNodesToScore</code>, your
change will have no or little effect, for a similar reason.</p>
<p>If your cluster has several hundred Nodes or fewer, leave this configuration option
at its default value. Making changes is unlikely to improve the
scheduler's performance significantly.
--&gt;</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>当集群中的可调度节点少于 50 个时，调度器仍然会去检查所有的 Node，
因为可调度节点太少，不足以停止调度器最初的过滤选择。</p>
<p>同理，在小规模集群中，如果你将 <code>percentageOfNodesToScore</code> 设置为
一个较低的值，则没有或者只有很小的效果。</p>
<p>如果集群只有几百个节点或者更少，请保持这个配置的默认值。
改变基本不会对调度器的性能有明显的提升。</p>
</div>
<!--
An important detail to consider when setting this value is that when a smaller
number of nodes in a cluster are checked for feasibility, some nodes are not
sent to be scored for a given Pod. As a result, a Node which could possibly
score a higher value for running the given Pod might not even be passed to the
scoring phase. This would result in a less than ideal placement of the Pod.

You should avoid setting `percentageOfNodesToScore` very low so that kube-scheduler
does not make frequent, poor Pod placement decisions. Avoid setting the
percentage to anything below 10%, unless the scheduler's throughput is critical
for your application and the score of nodes is not important. In other words, you
prefer to run the Pod on any Node as long as it is feasible.
-->
<p>值得注意的是，该参数设置后可能会导致只有集群中少数节点被选为可调度节点，
很多节点都没有进入到打分阶段。这样就会造成一种后果，
一个本来可以在打分阶段得分很高的节点甚至都不能进入打分阶段。</p>
<p>由于这个原因，这个参数不应该被设置成一个很低的值。
通常的做法是不会将这个参数的值设置的低于 10。
很低的参数值一般在调度器的吞吐量很高且对节点的打分不重要的情况下才使用。
换句话说，只有当你更倾向于在可调度节点中任意选择一个节点来运行这个 Pod 时，
才使用很低的参数设置。</p>
<!--
### How the scheduler iterates over Nodes
-->
<h3 id="how-the-scheduler-iterates-over-nodes">调度器做调度选择的时候如何覆盖所有的 Node</h3>
<!--
This section is intended for those who want to understand the internal details
of this feature.
-->
<p>如果你想要理解这一个特性的内部细节，那么请仔细阅读这一章节。</p>
<!--
In order to give all the Nodes in a cluster a fair chance of being considered
for running Pods, the scheduler iterates over the nodes in a round robin
fashion. You can imagine that Nodes are in an array. The scheduler starts from
the start of the array and checks feasibility of the nodes until it finds enough
Nodes as specified by `percentageOfNodesToScore`. For the next Pod, the
scheduler continues from the point in the Node array that it stopped at when
checking feasibility of Nodes for the previous Pod.
-->
<p>在将 Pod 调度到节点上时，为了让集群中所有节点都有公平的机会去运行这些 Pod，
调度器将会以轮询的方式覆盖全部的 Node。
你可以将 Node 列表想象成一个数组。调度器从数组的头部开始筛选可调度节点，
依次向后直到可调度节点的数量达到 <code>percentageOfNodesToScore</code> 参数的要求。
在对下一个 Pod 进行调度的时候，前一个 Pod 调度筛选停止的 Node 列表的位置，
将会来作为这次调度筛选 Node 开始的位置。</p>
<!--
If Nodes are in multiple zones, the scheduler iterates over Nodes in various
zones to ensure that Nodes from different zones are considered in the
feasibility checks. As an example, consider six nodes in two zones:
-->
<p>如果集群中的 Node 在多个区域，那么调度器将从不同的区域中轮询 Node，
来确保不同区域的 Node 接受可调度性检查。如下例，考虑两个区域中的六个节点：</p>
<pre><code>Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
</code></pre><!--
The Scheduler evaluates feasibility of the nodes in this order:
-->
<p>调度器将会按照如下的顺序去评估 Node 的可调度性：</p>
<pre><code>Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
</code></pre><!--
After going over all the Nodes, it goes back to Node 1.
-->
<p>在评估完所有 Node 后，将会返回到 Node 1，从头开始。</p>
<h2 id="what-s-next">What's next</h2>
<!-- * Check the [kube-scheduler configuration reference (v1beta3)](/docs/reference/config-api/kube-scheduler-config.v1beta3/) -->
<ul>
<li>参见 <a href="/zh/docs/reference/config-api/kube-scheduler-config.v1beta3/">kube-scheduler 配置参考 (v1beta3)</a></li>
</ul>

</div>
</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-285a3785fd3d20f437c28d87ca4dadca">11 - 集群管理</h1>
    <div class="lead">关于创建和管理 Kubernetes 集群的底层细节。</div>
	<!--
title: Cluster Administration
reviewers:
- davidopp
- lavalamp
weight: 100
content_type: concept
description: >
  Lower-level detail relevant to creating or administering a Kubernetes cluster.
no_list: true
-->
<!-- overview -->
<!--
The cluster administration overview is for anyone creating or administering a Kubernetes cluster.
It assumes some familiarity with core Kubernetes [concepts](/docs/concepts/).
-->
<p>集群管理概述面向任何创建和管理 Kubernetes 集群的读者人群。
我们假设你大概了解一些核心的 Kubernetes <a href="/zh/docs/concepts/">概念</a>。</p>
<!-- body -->
<!--
## Planning a cluster

See the guides in [Setup](/docs/setup/) for examples of how to plan, set up, and configure Kubernetes clusters. The solutions listed in this article are called *distros*.

Not all distros are actively maintained. Choose distros which have been tested with a recent version of Kubernetes.

Before choosing a guide, here are some considerations:
-->
<h2 id="planning-a-cluster">规划集群  </h2>
<p>查阅<a href="/zh/docs/setup/">安装</a>中的指导，获取如何规划、建立以及配置 Kubernetes
集群的示例。本文所列的文章称为<em>发行版</em> 。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 并非所有发行版都是被积极维护的。
请选择使用最近 Kubernetes 版本测试过的发行版。
</div>
<p>在选择一个指南前，有一些因素需要考虑：</p>
<!--
- Do you want to try out Kubernetes on your computer, or do you want to build a high-availability, multi-node cluster? Choose distros best suited for your needs.
- Will you be using **a hosted Kubernetes cluster**, such as [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine/), or **hosting your own cluster**?
- Will your cluster be **on-premises**, or **in the cloud (IaaS)**? Kubernetes does not directly support hybrid clusters. Instead, you can set up multiple clusters.
- **If you are configuring Kubernetes on-premises**, consider which [networking model](/docs/concepts/cluster-administration/networking/) fits best.
- Will you be running Kubernetes on **"bare metal" hardware** or on **virtual machines (VMs)**?
- Do you **want to run a cluster**, or do you expect to do **active development of Kubernetes project code**? If the
  latter, choose an actively-developed distro. Some distros only use binary releases, but
  offer a greater variety of choices.
- Familiarize yourself with the [components](/docs/concepts/overview/components/) needed to run a cluster.
-->
<ul>
<li>你是打算在你的计算机上尝试 Kubernetes，还是要构建一个高可用的多节点集群？
请选择最适合你需求的发行版。</li>
<li>你正在使用类似 <a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a>
这样的<strong>被托管的 Kubernetes 集群</strong>, 还是<strong>管理你自己的集群</strong>？</li>
<li>你的集群是在<strong>本地</strong>还是<strong>云（IaaS）</strong> 上？Kubernetes 不能直接支持混合集群。
作为代替，你可以建立多个集群。</li>
<li><strong>如果你在本地配置 Kubernetes</strong>，需要考虑哪种
<a href="/zh/docs/concepts/cluster-administration/networking/">网络模型</a>最适合。</li>
<li>你的 Kubernetes 在<strong>裸金属硬件</strong>上还是<strong>虚拟机（VMs）</strong> 上运行？</li>
<li>你是想<strong>运行一个集群</strong>，还是打算<strong>参与开发 Kubernetes 项目代码</strong>？
如果是后者，请选择一个处于开发状态的发行版。
某些发行版只提供二进制发布版，但提供更多的选择。</li>
<li>让你自己熟悉运行一个集群所需的<a href="/zh/docs/concepts/overview/components/">组件</a>。</li>
</ul>
<!--
## Managing a cluster

* Learn how to [manage nodes](/docs/concepts/nodes/node/).

* Learn how to set up and manage the [resource quota](/docs/concepts/policy/resource-quotas/) for shared clusters.
-->
<h2 id="managing-a-cluster">管理集群  </h2>
<ul>
<li>
<p>学习如何<a href="/zh/docs/concepts/architecture/nodes/">管理节点</a>。</p>
</li>
<li>
<p>学习如何设定和管理集群共享的<a href="/zh/docs/concepts/policy/resource-quotas/">资源配额</a> 。</p>
</li>
</ul>
<!--
## Securing a cluster

* [Generate Certificates](/docs/tasks/administer-cluster/certificates/) describes the steps to generate certificates using different tool chains.
* [Kubernetes Container Environment](/docs/concepts/containers/container-environment/) describes the environment for Kubelet managed containers on a Kubernetes node.
* [Controlling Access to the Kubernetes API](/docs/reference/access-authn-authz/controlling-access/) describes how to set up permissions for users and service accounts.
* [Authenticating](/docs/reference/access-authn-authz/authentication/) explains authentication in Kubernetes, including the various authentication options.
* [Authorization](/docs/reference/access-authn-authz/authorization/) is separate from authentication, and controls how HTTP calls are handled.
* [Using Admission Controllers](/docs/reference/access-authn-authz/admission-controllers/) explains plug-ins which intercepts requests to the Kubernetes API server after authentication and authorization.
* [Using Sysctls in a Kubernetes Cluster](/docs/concepts/cluster-administration/sysctl-cluster/) describes to an administrator how to use the `sysctl` command-line tool to set kernel parameters .
* [Auditing](/docs/tasks/debug/debug-cluster/audit/) describes how to interact with Kubernetes' audit logs.
-->
<h2 id="securing-a-cluster">保护集群 </h2>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/certificates/">生成证书</a>
节描述了使用不同的工具链生成证书的步骤。</li>
<li><a href="/zh/docs/concepts/containers/container-environment/">Kubernetes 容器环境</a>
描述了 Kubernetes 节点上由 Kubelet 管理的容器的环境。</li>
<li><a href="/zh/docs/concepts/security/controlling-access/">控制到 Kubernetes API 的访问</a>
描述了如何为用户和 service accounts 建立权限许可。</li>
<li><a href="/zh/docs/reference/access-authn-authz/authentication/">身份认证</a>
节阐述了 Kubernetes 中的身份认证功能，包括许多认证选项。</li>
<li><a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权</a>
与身份认证不同，用于控制如何处理 HTTP 请求。</li>
<li><a href="/zh/docs/reference/access-authn-authz/admission-controllers">使用准入控制器</a>
阐述了在认证和授权之后拦截到 Kubernetes API 服务的请求的插件。</li>
<li><a href="/zh/docs/tasks/administer-cluster/sysctl-cluster/">在 Kubernetes 集群中使用 Sysctls</a>
描述了管理员如何使用 <code>sysctl</code> 命令行工具来设置内核参数。</li>
<li><a href="/zh/docs/tasks/debug/debug-cluster/audit/">审计</a>
描述了如何与 Kubernetes 的审计日志交互。</li>
</ul>
<!--
### Securing the kubelet

* [Master-Node communication](/docs/concepts/architecture/master-node-communication/)
* [TLS bootstrapping](/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/)
* [Kubelet authentication/authorization](/docs/admin/kubelet-authentication-authorization/)
-->
<h3 id="securing-the-kubelet">保护 kubelet  </h3>
<ul>
<li><a href="/zh/docs/concepts/architecture/control-plane-node-communication/">主控节点通信</a></li>
<li><a href="/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">TLS 引导</a></li>
<li><a href="/zh/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/">Kubelet 认证/授权</a></li>
</ul>
<!--
## Optional Cluster Services

* [DNS Integration](/docs/concepts/services-networking/dns-pod-service/) describes how to resolve a DNS name directly to a Kubernetes service.
* [Logging and Monitoring Cluster Activity](/docs/concepts/cluster-administration/logging/) explains how logging in Kubernetes works and how to implement it.
-->
<h2 id="optional-cluster-services">可选集群服务  </h2>
<ul>
<li><a href="/zh/docs/concepts/services-networking/dns-pod-service/">DNS 集成</a>
描述了如何将一个 DNS 名解析到一个 Kubernetes service。</li>
<li><a href="/zh/docs/concepts/cluster-administration/logging/">记录和监控集群活动</a>
阐述了 Kubernetes 的日志如何工作以及怎样实现。</li>
</ul>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2bf9a93ab5ba014fb6ff70b22c29d432">11.1 - 证书</h1>
    
	<!--
title: Certificates
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
To learn how to generate certificates for your cluster, see [Certificates](/docs/tasks/administer-cluster/certificates/).
-->
<p>要了解如何为集群生成证书，参阅<a href="/zh/docs/tasks/administer-cluster/certificates/">证书</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3aeeecf7cdb2a21eb4b31db7a71c81e2">11.2 - 管理资源</h1>
    
	<!-- overview -->
<!--
You've deployed your application and exposed it via a service. Now what? Kubernetes provides a number of tools to help you manage your application deployment, including scaling and updating. Among the features that we will discuss in more depth are [configuration files](/docs/concepts/configuration/overview/) and [labels](/docs/concepts/overview/working-with-objects/labels/).
 -->
<p>你已经部署了应用并通过服务暴露它。然后呢？
Kubernetes 提供了一些工具来帮助管理你的应用部署，包括扩缩容和更新。
我们将更深入讨论的特性包括
<a href="/zh/docs/concepts/configuration/overview/">配置文件</a>和
<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签</a>。</p>
<!-- body -->
<!--
## Organizing resource configurations

Many applications require multiple resources to be created, such as a Deployment and a Service. Management of multiple resources can be simplified by grouping them together in the same file (separated by  in YAML). For example:
 -->
<h2 id="组织资源配置">组织资源配置</h2>
<p>许多应用需要创建多个资源，例如 Deployment 和 Service。
可以通过将多个资源组合在同一个文件中（在 YAML 中以 <code>---</code> 分隔）
来简化对它们的管理。例如：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/nginx-app.yaml" download="application/nginx-app.yaml"><code>application/nginx-app.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-nginx-app-yaml')" title="Copy application/nginx-app.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-nginx-app-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx-svc<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Multiple resources can be created the same way as a single resource:
 -->
<p>可以用创建单个资源相同的方式来创建多个资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml
</code></pre></div><pre><code>service/my-nginx-svc created
deployment.apps/my-nginx created
</code></pre><!--
The resources will be created in the order they appear in the file. Therefore, it's best to specify the service first, since that will ensure the scheduler can spread the pods associated with the service as they are created by the controller(s), such as Deployment.
 -->
<p>资源将按照它们在文件中的顺序创建。
因此，最好先指定服务，这样在控制器（例如 Deployment）创建 Pod 时能够
确保调度器可以将与服务关联的多个 Pod 分散到不同节点。</p>
<!--
`kubectl apply` also accepts multiple `-f` arguments:
 -->
<p><code>kubectl create</code> 也接受多个 <code>-f</code> 参数:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</code></pre></div><!--
And a directory can be specified rather than or in addition to individual files:
 -->
<p>还可以指定目录路径，而不用添加多个单独的文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/nginx/
</code></pre></div><!--
`kubectl` will read any files with suffixes `.yaml`, `.yml`, or `.json`.

It is a recommended practice to put resources related to the same microservice or application tier into the same file, and to group all of the files associated with your application in the same directory. If the tiers of your application bind to each other using DNS, then you can deploy all of the components of your stack together.

A URL can also be specified as a configuration source, which is handy for deploying directly from configuration files checked into Github:
 -->
<p><code>kubectl</code> 将读取任何后缀为 <code>.yaml</code>、<code>.yml</code> 或者 <code>.json</code> 的文件。</p>
<p>建议的做法是，将同一个微服务或同一应用层相关的资源放到同一个文件中，
将同一个应用相关的所有文件按组存放到同一个目录中。
如果应用的各层使用 DNS 相互绑定，那么你可以将堆栈的所有组件一起部署。</p>
<p>还可以使用 URL 作为配置源，便于直接使用已经提交到 Github 上的配置文件进行部署：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/nginx/nginx-deployment.yaml
</code></pre></div><pre><code>deployment.apps/my-nginx created
</code></pre><!--
## Bulk operations in kubectl

Resource creation isn't the only operation that `kubectl` can perform in bulk. It can also extract resource names from configuration files in order to perform other operations, in particular to delete the same resources you created:
 -->
<h2 id="kubectl-中的批量操作">kubectl 中的批量操作</h2>
<p>资源创建并不是 <code>kubectl</code> 可以批量执行的唯一操作。
<code>kubectl</code> 还可以从配置文件中提取资源名，以便执行其他操作，
特别是删除你之前创建的资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete -f https://k8s.io/examples/application/nginx-app.yaml
</code></pre></div><pre><code>deployment.apps &quot;my-nginx&quot; deleted
service &quot;my-nginx-svc&quot; deleted
</code></pre><!--
In the case of two resources, it's also easy to specify both on the command line using the resource/name syntax:
 -->
<p>在仅有两种资源的情况下，可以使用&quot;资源类型/资源名&quot;的语法在命令行中
同时指定这两个资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployments/my-nginx services/my-nginx-svc
</code></pre></div><!--
For larger numbers of resources, you'll find it easier to specify the selector (label query) specified using `-l` or `--selector`, to filter resources by their labels:
-->
<p>对于资源数目较大的情况，你会发现使用 <code>-l</code> 或 <code>--selector</code>
指定筛选器（标签查询）能很容易根据标签筛选资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment,services -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>deployment.apps &quot;my-nginx&quot; deleted
service &quot;my-nginx-svc&quot; deleted
</code></pre><!--
Because `kubectl` outputs resource names in the same syntax it accepts, you can chain operations using `$()` or `xargs`:
-->
<p>由于 <code>kubectl</code> 用来输出资源名称的语法与其所接受的资源名称语法相同，
你可以使用 <code>$()</code> 或 <code>xargs</code> 进行链式操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get <span style="color:#a2f;font-weight:bold">$(</span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service<span style="color:#a2f;font-weight:bold">)</span>
kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service | xargs -i kubectl get <span style="color:#666">{}</span>
</code></pre></div><pre><code>NAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)      AGE
my-nginx-svc   LoadBalancer   10.0.0.208   &lt;pending&gt;     80/TCP       0s
</code></pre><!--
With the above commands, we first create resources under `examples/application/nginx/` and print the resources created with `-o name` output format
(print each resource as resource/name). Then we `grep` only the "service", and then print it with `kubectl get`.
 -->
<p>上面的命令中，我们首先使用 <code>examples/application/nginx/</code> 下的配置文件创建资源，
并使用 <code>-o name</code> 的输出格式（以&quot;资源/名称&quot;的形式打印每个资源）打印所创建的资源。
然后，我们通过 <code>grep</code> 来过滤 &quot;service&quot;，最后再打印 <code>kubectl get</code> 的内容。</p>
<!--
If you happen to organize your resources across several subdirectories within a particular directory, you can recursively perform the operations on the subdirectories also, by specifying `--recursive` or `-R` alongside the `--filename,-f` flag.
 -->
<p>如果你碰巧在某个路径下的多个子路径中组织资源，那么也可以递归地在所有子路径上
执行操作，方法是在 <code>--filename,-f</code> 后面指定 <code>--recursive</code> 或者 <code>-R</code>。</p>
<!--
For instance, assume there is a directory `project/k8s/development` that holds all of the manifests needed for the development environment, organized by resource type:
 -->
<p>例如，假设有一个目录路径为 <code>project/k8s/development</code>，它保存开发环境所需的
所有清单，并按资源类型组织：</p>
<pre><code>project/k8s/development
├── configmap
│   └── my-configmap.yaml
├── deployment
│   └── my-deployment.yaml
└── pvc
    └── my-pvc.yaml
</code></pre><!--
By default, performing a bulk operation on `project/k8s/development` will stop at the first level of the directory, not processing any subdirectories. If we had tried to create the resources in this directory using the following command, we would have encountered an error:
 -->
<p>默认情况下，对 <code>project/k8s/development</code> 执行的批量操作将停止在目录的第一级，
而不是处理所有子目录。
如果我们试图使用以下命令在此目录中创建资源，则会遇到一个错误：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f project/k8s/development
</code></pre></div><pre><code>error: you must provide one or more resources by argument or filename (.json|.yaml|.yml|stdin)
</code></pre><!--
Instead, specify the `--recursive` or `-R` flag with the `--filename,-f` flag as such:
 -->
<p>正确的做法是，在 <code>--filename,-f</code> 后面标明 <code>--recursive</code> 或者 <code>-R</code> 之后：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f project/k8s/development --recursive
</code></pre></div><pre><code>configmap/my-config created
deployment.apps/my-deployment created
persistentvolumeclaim/my-pvc created
</code></pre><!--
The `--recursive` flag works with any operation that accepts the `--filename,-f` flag such as: `kubectl {create,get,delete,describe,rollout} etc.`

The `--recursive` flag also works when multiple `-f` arguments are provided:
 -->
<p><code>--recursive</code> 可以用于接受 <code>--filename,-f</code> 参数的任何操作，例如：
<code>kubectl {create,get,delete,describe,rollout}</code> 等。</p>
<p>有多个 <code>-f</code> 参数出现的时候，<code>--recursive</code> 参数也能正常工作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive
</code></pre></div><pre><code>namespace/development created
namespace/staging created
configmap/my-config created
deployment.apps/my-deployment created
persistentvolumeclaim/my-pvc created
</code></pre><!--
If you're interested in learning more about `kubectl`, go ahead and read [Command line tool (kubectl)](/docs/reference/kubectl/).
-->
<p>如果你有兴趣进一步学习关于 <code>kubectl</code> 的内容，请阅读
<a href="/zh/docs/reference/kubectl/">命令行工具（kubectl）</a>。</p>
<!--
## Using labels effectively

The examples we've used so far apply at most a single label to any resource. There are many scenarios where multiple labels should be used to distinguish sets from one another.
-->
<h2 id="有效地使用标签">有效地使用标签</h2>
<p>到目前为止我们使用的示例中的资源最多使用了一个标签。
在许多情况下，应使用多个标签来区分集合。</p>
<!--
For instance, different applications would use different values for the `app` label, but a multi-tier application, such as the [guestbook example](https://github.com/kubernetes/examples/tree/master/guestbook/), would additionally need to distinguish each tier. The frontend could carry the following labels:
-->
<p>例如，不同的应用可能会为 <code>app</code> 标签设置不同的值。
但是，类似 <a href="https://github.com/kubernetes/examples/tree/master/guestbook/">guestbook 示例</a>
这样的多层应用，还需要区分每一层。前端可以带以下标签：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></code></pre></div><!--
while the Redis master and slave would have different `tier` labels, and perhaps even an additional `role` label:
 -->
<p>Redis 的主节点和从节点会有不同的 <code>tier</code> 标签，甚至还有一个额外的 <code>role</code> 标签：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>master<span style="color:#bbb">
</span></code></pre></div><!-- and -->
<p>以及</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>slave<span style="color:#bbb">
</span></code></pre></div><!--
The labels allow us to slice and dice our resources along any dimension specified by a label:
 -->
<p>标签允许我们按照标签指定的任何维度对我们的资源进行切片和切块：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml
kubectl get pods -Lapp -Ltier -Lrole
</code></pre></div><pre><code>NAME                           READY     STATUS    RESTARTS   AGE       APP         TIER       ROLE
guestbook-fe-4nlpb             1/1       Running   0          1m        guestbook   frontend   &lt;none&gt;
guestbook-fe-ght6d             1/1       Running   0          1m        guestbook   frontend   &lt;none&gt;
guestbook-fe-jpy62             1/1       Running   0          1m        guestbook   frontend   &lt;none&gt;
guestbook-redis-master-5pg3b   1/1       Running   0          1m        guestbook   backend    master
guestbook-redis-slave-2q2yf    1/1       Running   0          1m        guestbook   backend    slave
guestbook-redis-slave-qgazl    1/1       Running   0          1m        guestbook   backend    slave
my-nginx-divi2                 1/1       Running   0          29m       nginx       &lt;none&gt;     &lt;none&gt;
my-nginx-o0ef1                 1/1       Running   0          29m       nginx       &lt;none&gt;     &lt;none&gt;
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -lapp<span style="color:#666">=</span>guestbook,role<span style="color:#666">=</span>slave
</code></pre></div><pre><code>NAME                          READY     STATUS    RESTARTS   AGE
guestbook-redis-slave-2q2yf   1/1       Running   0          3m
guestbook-redis-slave-qgazl   1/1       Running   0          3m
</code></pre><!--
## Canary deployments

Another scenario where multiple labels are needed is to distinguish deployments of different releases or configurations of the same component. It is common practice to deploy a *canary* of a new application release (specified via image tag in the pod template) side by side with the previous release so that the new release can receive live production traffic before fully rolling it out.
 -->
<h2 id="canary-deployments">金丝雀部署（Canary Deployments）  </h2>
<p>另一个需要多标签的场景是用来区分同一组件的不同版本或者不同配置的多个部署。
常见的做法是部署一个使用<em>金丝雀发布</em>来部署新应用版本
（在 Pod 模板中通过镜像标签指定），保持新旧版本应用同时运行。
这样，新版本在完全发布之前也可以接收实时的生产流量。</p>
<!--
For instance, you can use a `track` label to differentiate different releases.

The primary, stable release would have a `track` label with value as `stable`:
 -->
<p>例如，你可以使用 <code>track</code> 标签来区分不同的版本。</p>
<p>主要稳定的发行版将有一个 <code>track</code> 标签，其值为 <code>stable</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">     </span>...<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">track</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span><span style="color:#bbb">     </span>...<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gb-frontend:v3<span style="color:#bbb">
</span></code></pre></div><!--
and then you can create a new release of the guestbook frontend that carries the `track` label with different value (i.e. `canary`), so that two sets of pods would not overlap:
 -->
<p>然后，你可以创建 guestbook 前端的新版本，让这些版本的 <code>track</code> 标签带有不同的值
（即 <code>canary</code>），以便两组 Pod 不会重叠：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend-canary<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">     </span>...<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">track</span>:<span style="color:#bbb"> </span>canary<span style="color:#bbb">
</span><span style="color:#bbb">     </span>...<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gb-frontend:v4<span style="color:#bbb">
</span></code></pre></div><!--
The frontend service would span both sets of replicas by selecting the common subset of their labels (i.e. omitting the `track` label), so that the traffic will be redirected to both applications:
 -->
<p>前端服务通过选择标签的公共子集（即忽略 <code>track</code> 标签）来覆盖两组副本，
以便流量可以转发到两个应用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>guestbook<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></code></pre></div><!--
You can tweak the number of replicas of the stable and canary releases to determine the ratio of each release that will receive live production traffic (in this case, 3:1).
Once you're confident, you can update the stable track to the new application release and remove the canary one.
 -->
<p>你可以调整 <code>stable</code> 和 <code>canary</code> 版本的副本数量，以确定每个版本将接收
实时生产流量的比例（在本例中为 3:1）。
一旦有信心，你就可以将新版本应用的 <code>track</code> 标签的值从
<code>canary</code> 替换为 <code>stable</code>，并且将老版本应用删除。</p>
<!--
For a more concrete example, check the [tutorial of deploying Ghost](https://github.com/kelseyhightower/talks/tree/master/kubecon-eu-2016/demo#deploy-a-canary).
 -->
<p>想要了解更具体的示例，请查看
<a href="https://github.com/kelseyhightower/talks/tree/master/kubecon-eu-2016/demo#deploy-a-canary">Ghost 部署教程</a>。</p>
<!--
## Updating labels

Sometimes existing pods and other resources need to be relabeled before creating new resources. This can be done with `kubectl label`.
For example, if you want to label all your nginx pods as frontend tier, run:
 -->
<h2 id="updating-labels">更新标签 </h2>
<p>有时，现有的 pod 和其它资源需要在创建新资源之前重新标记。
这可以用 <code>kubectl label</code> 完成。
例如，如果想要将所有 nginx pod 标记为前端层，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl label pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx <span style="color:#b8860b">tier</span><span style="color:#666">=</span>fe
</code></pre></div><pre><code>pod/my-nginx-2035384211-j5fhi labeled
pod/my-nginx-2035384211-u2c7e labeled
pod/my-nginx-2035384211-u3t6x labeled
</code></pre><!--
This first filters all pods with the label "app=nginx", and then labels them with the "tier=fe".
To see the pods you labeled, run:
 -->
<p>首先用标签 &quot;app=nginx&quot; 过滤所有的 Pod，然后用 &quot;tier=fe&quot; 标记它们。
想要查看你刚才标记的 Pod，请运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx -L tier
</code></pre></div><pre><code>NAME                        READY     STATUS    RESTARTS   AGE       TIER
my-nginx-2035384211-j5fhi   1/1       Running   0          23m       fe
my-nginx-2035384211-u2c7e   1/1       Running   0          23m       fe
my-nginx-2035384211-u3t6x   1/1       Running   0          23m       fe
</code></pre><!--
This outputs all "app=nginx" pods, with an additional label column of pods' tier (specified with `-L` or `--label-columns`).

For more information, please see [labels](/docs/concepts/overview/working-with-objects/labels/) and [kubectl label](/docs/reference/generated/kubectl/kubectl-commands/#label).
 -->
<p>这将输出所有 &quot;app=nginx&quot; 的 Pod，并有一个额外的描述 Pod 的 tier 的标签列
（用参数 <code>-L</code> 或者 <code>--label-columns</code> 标明）。</p>
<p>想要了解更多信息，请参考
<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签</a> 和
<a href="/docs/reference/generated/kubectl/kubectl-commands/#label"><code>kubectl label</code></a>
命令文档。</p>
<!--
## Updating annotations

Sometimes you would want to attach annotations to resources. Annotations are arbitrary non-identifying metadata for retrieval by API clients such as tools, libraries, etc. This can be done with `kubectl annotate`. For example:
 -->
<h2 id="updating-annotations">更新注解  </h2>
<p>有时，你可能希望将注解附加到资源中。注解是 API 客户端（如工具、库等）
用于检索的任意非标识元数据。这可以通过 <code>kubectl annotate</code> 来完成。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl annotate pods my-nginx-v4-9gw19 <span style="color:#b8860b">description</span><span style="color:#666">=</span><span style="color:#b44">&#39;my frontend running nginx&#39;</span>
kubectl get pods my-nginx-v4-9gw19 -o yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">apiVersion: v1
kind: pod
metadata:
  annotations:
    description: my frontend running nginx
...
</code></pre></div><!--
For more information, please see [annotations](/docs/concepts/overview/working-with-objects/annotations/) and [kubectl annotate](/docs/reference/generated/kubectl/kubectl-commands/#annotate) document.
 -->
<p>想要了解更多信息，请参考
<a href="/zh/docs/concepts/overview/working-with-objects/annotations/">注解</a>和
<a href="/docs/reference/generated/kubectl/kubectl-commands/#annotate"><code>kubectl annotate</code></a>
命令文档。</p>
<!--
## Scaling your application

When load on your application grows or shrinks, use `kubectl` to scale you application. For instance, to decrease the number of nginx replicas from 3 to 1, do:
 -->
<h2 id="扩缩你的应用">扩缩你的应用</h2>
<p>当应用上的负载增长或收缩时，使用 <code>kubectl</code> 能够实现应用规模的扩缩。
例如，要将 nginx 副本的数量从 3 减少到 1，请执行以下操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale deployment/my-nginx --replicas<span style="color:#666">=</span><span style="color:#666">1</span>
</code></pre></div><pre><code>deployment.extensions/my-nginx scaled
</code></pre><!--
Now you only have one pod managed by the deployment.
 -->
<p>现在，你的 Deployment 管理的 Pod 只有一个了。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><pre><code>NAME                        READY     STATUS    RESTARTS   AGE
my-nginx-2035384211-j5fhi   1/1       Running   0          30m
</code></pre><!--
To have the system automatically choose the number of nginx replicas as needed, ranging from 1 to 3, do:
 -->
<p>想要让系统自动选择需要 nginx 副本的数量，范围从 1 到 3，请执行以下操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl autoscale deployment/my-nginx --min<span style="color:#666">=</span><span style="color:#666">1</span> --max<span style="color:#666">=</span><span style="color:#666">3</span>
</code></pre></div><pre><code>horizontalpodautoscaler.autoscaling/my-nginx autoscaled
</code></pre><!--
Now your nginx replicas will be scaled up and down as needed, automatically.

For more information, please see [kubectl scale](/docs/reference/generated/kubectl/kubectl-commands/#scale), [kubectl autoscale](/docs/reference/generated/kubectl/kubectl-commands/#autoscale) and [horizontal pod autoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/) document.
 -->
<p>现在，你的 nginx 副本将根据需要自动地增加或者减少。</p>
<p>想要了解更多信息，请参考
<a href="/docs/reference/generated/kubectl/kubectl-commands/#scale">kubectl scale</a>命令文档、
<a href="/docs/reference/generated/kubectl/kubectl-commands/#autoscale">kubectl autoscale</a> 命令文档和
<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale/">水平 Pod 自动伸缩</a> 文档。</p>
<!--
## In-place updates of resources

Sometimes it's necessary to make narrow, non-disruptive updates to resources you've created.
 -->
<h2 id="in-place-updates-of-resources">就地更新资源 </h2>
<p>有时，有必要对你所创建的资源进行小范围、无干扰地更新。</p>
<h3 id="kubectl-apply">kubectl apply</h3>
<!--
It is suggested to maintain a set of configuration files in source control (see [configuration as code](http://martinfowler.com/bliki/InfrastructureAsCode.html)),
so that they can be maintained and versioned along with the code for the resources they configure.
Then, you can use [`kubectl apply`](/docs/reference/generated/kubectl/kubectl-commands/#apply) to push your configuration changes to the cluster.
 -->
<p>建议在源代码管理中维护一组配置文件
（参见<a href="https://martinfowler.com/bliki/InfrastructureAsCode.html">配置即代码</a>），
这样，它们就可以和应用代码一样进行维护和版本管理。
然后，你可以用 <a href="/docs/reference/generated/kubectl/kubectl-commands/#apply"><code>kubectl apply</code></a>
将配置变更应用到集群中。</p>
<!--
This command will compare the version of the configuration that you're pushing with the previous version and apply the changes you've made, without overwriting any automated changes to properties you haven't specified.
 -->
<p>这个命令将会把推送的版本与以前的版本进行比较，并应用你所做的更改，
但是不会自动覆盖任何你没有指定更改的属性。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
deployment.apps/my-nginx configured
</code></pre></div><!--
Note that `kubectl apply` attaches an annotation to the resource in order to determine the changes to the configuration since the previous invocation. When it's invoked, `kubectl apply` does a three-way diff between the previous configuration, the provided input and the current configuration of the resource, in order to determine how to modify the resource.
 -->
<p>注意，<code>kubectl apply</code> 将为资源增加一个额外的注解，以确定自上次调用以来对配置的更改。
执行时，<code>kubectl apply</code> 会在以前的配置、提供的输入和资源的当前配置之间
找出三方差异，以确定如何修改资源。</p>
<!--
Currently, resources are created without this annotation, so the first invocation of `kubectl apply` will fall back to a two-way diff between the provided input and the current configuration of the resource. During this first invocation, it cannot detect the deletion of properties set when the resource was created. For this reason, it will not remove them.
 -->
<p>目前，新创建的资源是没有这个注解的，所以，第一次调用 <code>kubectl apply</code> 时
将使用提供的输入和资源的当前配置双方之间差异进行比较。
在第一次调用期间，它无法检测资源创建时属性集的删除情况。
因此，kubectl 不会删除它们。</p>
<!--
All subsequent calls to `kubectl apply`, and other commands that modify the configuration, such as `kubectl replace` and `kubectl edit`, will update the annotation, allowing subsequent calls to `kubectl apply` to detect and perform deletions using a three-way diff.
 -->
<p>所有后续的 <code>kubectl apply</code> 操作以及其他修改配置的命令，如 <code>kubectl replace</code>
和 <code>kubectl edit</code>，都将更新注解，并允许随后调用的 <code>kubectl apply</code>
使用三方差异进行检查和执行删除。</p>
<!--
To use apply, always create resource initially with either `kubectl apply` or `kubectl create --save-config`.
 -->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 想要使用 apply，请始终使用 <code>kubectl apply</code> 或 <code>kubectl create --save-config</code> 创建资源。
</div>
<h3 id="kubectl-edit">kubectl edit</h3>
<!--
Alternatively, you may also update resources with `kubectl edit`:
 -->
<p>或者，你也可以使用 <code>kubectl edit</code> 更新资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit deployment/my-nginx
</code></pre></div><!--
This is equivalent to first `get` the resource, edit it in text editor, and then `apply` the resource with the updated version:
 -->
<p>这相当于首先 <code>get</code> 资源，在文本编辑器中编辑它，然后用更新的版本 <code>apply</code> 资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment my-nginx -o yaml &gt; /tmp/nginx.yaml
vi /tmp/nginx.yaml
<span style="color:#080;font-style:italic"># do some edit, and then save the file</span>

kubectl apply -f /tmp/nginx.yaml
deployment.apps/my-nginx configured

rm /tmp/nginx.yaml
</code></pre></div><!--
This allows you to do more significant changes more easily. Note that you can specify the editor with your `EDITOR` or `KUBE_EDITOR` environment variables.

For more information, please see [kubectl edit](/docs/reference/generated/kubectl/kubectl-commands/#edit) document.
 -->
<p>这使你可以更加容易地进行更重大的更改。
请注意，可以使用 <code>EDITOR</code> 或 <code>KUBE_EDITOR</code> 环境变量来指定编辑器。</p>
<p>想要了解更多信息，请参考
<a href="/docs/reference/generated/kubectl/kubectl-commands/#edit">kubectl edit</a> 文档。</p>
<h3 id="kubectl-patch">kubectl patch</h3>
<!--
You can use `kubectl patch` to update API objects in place. This command supports JSON patch,
JSON merge patch, and strategic merge patch. See
[Update API Objects in Place Using kubectl patch](/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/)
and
[kubectl patch](/docs/reference/generated/kubectl/kubectl-commands/#patch).
 -->
<p>你可以使用 <code>kubectl patch</code> 来更新 API 对象。此命令支持 JSON patch、
JSON merge patch、以及 strategic merge patch。 请参考
<a href="/zh/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">使用 kubectl patch 更新 API 对象</a>
和
<a href="/docs/reference/generated/kubectl/kubectl-commands/#patch">kubectl patch</a>.</p>
<!--
## Disruptive updates

In some cases, you may need to update resource fields that cannot be updated once initialized, or you may want to make a recursive change immediately, such as to fix broken pods created by a Deployment. To change such fields, use `replace --force`, which deletes and re-creates the resource. In this case, you can modify your original configuration file:
 -->
<h2 id="disruptive-updates">破坏性的更新 </h2>
<p>在某些情况下，你可能需要更新某些初始化后无法更新的资源字段，或者你可能只想立即进行递归更改，
例如修复 Deployment 创建的不正常的 Pod。若要更改这些字段，请使用 <code>replace --force</code>，
它将删除并重新创建资源。在这种情况下，你可以修改原始配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force
</code></pre></div><pre><code>deployment.apps/my-nginx deleted
deployment.apps/my-nginx replaced
</code></pre><!--
## Updating your application without a service outage
 -->
<h2 id="在不中断服务的情况下更新应用">在不中断服务的情况下更新应用</h2>
<!--
At some point, you'll eventually need to update your deployed application, typically by specifying a new image or image tag, as in the canary deployment scenario above. `kubectl` supports several update operations, each of which is applicable to different scenarios.
 -->
<p>在某些时候，你最终需要更新已部署的应用，通常都是通过指定新的镜像或镜像标签，
如上面的金丝雀发布的场景中所示。<code>kubectl</code> 支持几种更新操作，
每种更新操作都适用于不同的场景。</p>
<!--
We'll guide you through how to create and update applications with Deployments.
 -->
<p>我们将指导你通过 Deployment 如何创建和更新应用。</p>
<!--
Let's say you were running version 1.14.2 of nginx:
 -->
<p>假设你正运行的是 1.14.2 版本的 nginx：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment my-nginx --image<span style="color:#666">=</span>nginx:1.14.2
</code></pre></div><pre><code>deployment.apps/my-nginx created
</code></pre><!--
To update to version 1.16.1, change `.spec.template.spec.containers[0].image` from `nginx:1.14.2` to `nginx:1.16.1`, with the previous kubectl commands.
 -->
<p>要更新到 1.16.1 版本，只需使用我们前面学到的 kubectl 命令将
<code>.spec.template.spec.containers[0].image</code> 从 <code>nginx:1.14.2</code> 修改为 <code>nginx:1.16.1</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit deployment/my-nginx
</code></pre></div><!--
That's it! The Deployment will declaratively update the deployed nginx application progressively behind the scene. It ensures that only a certain number of old replicas may be down while they are being updated, and only a certain number of new replicas may be created above the desired number of pods. To learn more details about it, visit [Deployment page](/docs/concepts/workloads/controllers/deployment/).
 -->
<p>没错，就是这样！Deployment 将在后台逐步更新已经部署的 nginx 应用。
它确保在更新过程中，只有一定数量的旧副本被开闭，并且只有一定基于所需 Pod 数量的新副本被创建。
想要了解更多细节，请参考 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a>。</p>
<h2 id="what-s-next">What's next</h2>
<!--
- [Learn about how to use `kubectl` for application introspection and debugging.](/docs/tasks/debug-application-cluster/debug-application-introspection/)
- [Configuration Best Practices and Tips](/docs/concepts/configuration/overview/)
 -->
<ul>
<li>学习<a href="/zh/docs/tasks/debug-application-cluster/debug-application-introspection/">如何使用 <code>kubectl</code> 观察和调试应用</a></li>
<li>阅读<a href="/zh/docs/concepts/configuration/overview/">配置最佳实践和技巧</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d649067a69d8d5c7e71564b42b96909e">11.3 - 集群网络系统</h1>
    
	<!-- overview -->
<!--
Networking is a central part of Kubernetes, but it can be challenging to
understand exactly how it is expected to work.  There are 4 distinct networking
problems to address:

1. Highly-coupled container-to-container communications: this is solved by
   <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> and `localhost` communications.
2. Pod-to-Pod communications: this is the primary focus of this document.
3. Pod-to-Service communications: this is covered by [services](/docs/concepts/services-networking/service/).
4. External-to-Service communications: this is covered by [services](/docs/concepts/services-networking/service/).
-->
<p>集群网络系统是 Kubernetes 的核心部分，但是想要准确了解它的工作原理可是个不小的挑战。
下面列出的是网络系统的的四个主要问题：</p>
<ol>
<li>高度耦合的容器间通信：这个已经被 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>
和 <code>localhost</code> 通信解决了。</li>
<li>Pod 间通信：这个是本文档的重点要讲述的。</li>
<li>Pod 和服务间通信：这个已经在<a href="/zh/docs/concepts/services-networking/service/">服务</a>里讲述过了。</li>
<li>外部和服务间通信：这也已经在<a href="/zh/docs/concepts/services-networking/service/">服务</a>讲述过了。</li>
</ol>
<!-- body -->
<!--
Kubernetes is all about sharing machines between applications.  Typically,
sharing machines requires ensuring that two applications do not try to use the
same ports.  Coordinating ports across multiple developers is very difficult to
do at scale and exposes users to cluster-level issues outside of their control.

Dynamic port allocation brings a lot of complications to the system - every
application has to take ports as flags, the API servers have to know how to
insert dynamic port numbers into configuration blocks, services have to know
how to find each other, etc.  Rather than deal with this, Kubernetes takes a
different approach.

To learn about the Kubernetes networking model, see [here](/docs/concepts/services-networking/).
-->
<p>Kubernetes 的宗旨就是在应用之间共享机器。
通常来说，共享机器需要两个应用之间不能使用相同的端口，但是在多个应用开发者之间
去大规模地协调端口是件很困难的事情，尤其是还要让用户暴露在他们控制范围之外的集群级别的问题上。</p>
<p>动态分配端口也会给系统带来很多复杂度 - 每个应用都需要设置一个端口的参数，
而 API 服务器还需要知道如何将动态端口数值插入到配置模块中，服务也需要知道如何找到对方等等。
与其去解决这些问题，Kubernetes 选择了其他不同的方法。</p>
<p>要了解 Kubernetes 网络模型，请参阅<a href="/zh/docs/concepts/services-networking/">此处</a>。</p>
<!--
## How to implement the Kubernetes networking model

There are a number of ways that this network model can be implemented.  This
document is not an exhaustive study of the various methods, but hopefully serves
as an introduction to various technologies and serves as a jumping-off point.

The following networking options are sorted alphabetically - the order does not
imply any preferential status.
-->
<h2 id="如何实现-kubernetes-的网络模型">如何实现 Kubernetes 的网络模型</h2>
<p>有很多种方式可以实现这种网络模型，本文档并不是对各种实现技术的详细研究，
但是希望可以作为对各种技术的详细介绍，并且成为你研究的起点。</p>
<p>接下来的网络技术是按照首字母排序，顺序本身并无其他意义。</p>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<!--
### ACI

[Cisco Application Centric Infrastructure](https://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/index.html) offers an integrated overlay and underlay SDN solution that supports containers, virtual machines, and bare metal servers. [ACI](https://www.github.com/noironetworks/aci-containers) provides container networking integration for ACI. An overview of the integration is provided [here](https://www.cisco.com/c/dam/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/solution-overview-c22-739493.pdf).
-->
<h3 id="aci">ACI</h3>
<p><a href="https://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/index.html">Cisco Application Centric Infrastructure</a>
提供了一个集成覆盖网络和底层 SDN 的解决方案来支持容器、虚拟机和其他裸机服务器。
<a href="https://www.github.com/noironetworks/aci-containers">ACI</a> 为 ACI 提供了容器网络集成。
点击<a href="https://www.cisco.com/c/dam/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/solution-overview-c22-739493.pdf">这里</a>查看概述。</p>
<!--
### Antrea

Project [Antrea](https://github.com/vmware-tanzu/antrea) is an opensource Kubernetes networking solution intended to be Kubernetes native. It leverages Open vSwitch as the networking data plane. Open vSwitch is a high-performance programmable virtual switch that supports both Linux and Windows. Open vSwitch enables Antrea to implement Kubernetes Network Policies in a high-performance and efficient manner.
Thanks to the "programmable" characteristic of Open vSwitch, Antrea is able to implement an extensive set of networking and security features and services on top of Open vSwitch.
-->
<h3 id="antrea">Antrea</h3>
<p><a href="https://github.com/vmware-tanzu/antrea">Antrea</a> 项目是一个开源的联网解决方案，旨在成为
Kubernetes 原生的网络解决方案。它利用 Open vSwitch 作为网络数据平面。
Open vSwitch 是一个高性能可编程的虚拟交换机，支持 Linux 和 Windows 平台。
Open vSwitch 使 Antrea 能够以高性能和高效的方式实现 Kubernetes 的网络策略。
借助 Open vSwitch 可编程的特性，Antrea 能够在 Open vSwitch 之上实现广泛的联网、安全功能和服务。</p>
<!--
### AWS VPC CNI for Kubernetes

The [AWS VPC CNI](https://github.com/aws/amazon-vpc-cni-k8s) offers integrated AWS Virtual Private Cloud (VPC) networking for Kubernetes clusters. This CNI plugin offers high throughput and availability, low latency, and minimal network jitter. Additionally, users can apply existing AWS VPC networking and security best practices for building Kubernetes clusters. This includes the ability to use VPC flow logs, VPC routing policies, and security groups for network traffic isolation.

Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. The CNI allocates AWS Elastic Networking Interfaces (ENIs) to each Kubernetes node and using the secondary IP range from each ENI for pods on the node. The CNI includes controls for pre-allocation of ENIs and IP addresses for fast pod startup times and enables large clusters of up to 2,000 nodes.

Additionally, the CNI can be run alongside [Calico for network policy enforcement](https://docs.aws.amazon.com/eks/latest/userguide/calico.html). The AWS VPC CNI project is open source with [documentation on GitHub](https://github.com/aws/amazon-vpc-cni-k8s).
-->
<h3 id="kubernetes-的-aws-vpc-cni">Kubernetes 的 AWS VPC CNI</h3>
<p><a href="https://github.com/aws/amazon-vpc-cni-k8s">AWS VPC CNI</a> 为 Kubernetes 集群提供了集成的
AWS 虚拟私有云（VPC）网络。该 CNI 插件提供了高吞吐量和可用性，低延迟以及最小的网络抖动。
此外，用户可以使用现有的 AWS VPC 网络和安全最佳实践来构建 Kubernetes 集群。
这包括使用 VPC 流日志、VPC 路由策略和安全组进行网络流量隔离的功能。</p>
<p>使用该 CNI 插件，可使 Kubernetes Pod 拥有与在 VPC 网络上相同的 IP 地址。
CNI 将 AWS 弹性网络接口（ENI）分配给每个 Kubernetes 节点，并将每个 ENI 的辅助 IP 范围用于该节点上的 Pod 。
CNI 包含用于 ENI 和 IP 地址的预分配的控件，以便加快 Pod 的启动时间，并且能够支持多达 2000 个节点的大型集群。</p>
<p>此外，CNI 可以与
<a href="https://docs.aws.amazon.com/eks/latest/userguide/calico.html">用于执行网络策略的 Calico</a> 一起运行。
AWS VPC CNI 项目是开源的，请查看 <a href="https://github.com/aws/amazon-vpc-cni-k8s">GitHub 上的文档</a>。</p>
<!--
### Azure CNI for Kubernetes 
[Azure CNI](https://docs.microsoft.com/en-us/azure/virtual-network/container-networking-overview) is an [open source](https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md) plugin that integrates Kubernetes Pods with an Azure Virtual Network (also known as VNet) providing network performance at par with VMs. Pods can connect to peered VNet and to on-premises over Express Route or site-to-site VPN and are also directly reachable from these networks. Pods can access Azure services, such as storage and SQL, that are protected by Service Endpoints or Private Link. You can use VNet security policies and routing to filter Pod traffic. The plugin assigns VNet IPs to Pods by utilizing a pool of secondary IPs pre-configured on the Network Interface of a Kubernetes node.

Azure CNI is available natively in the [Azure Kubernetes Service (AKS)](https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni).
-->
<h3 id="kubernetes-的-azure-cni">Kubernetes 的 Azure CNI</h3>
<p><a href="https://docs.microsoft.com/en-us/azure/virtual-network/container-networking-overview">Azure CNI</a>
是一个<a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md">开源插件</a>，
将 Kubernetes Pods 和 Azure 虚拟网络（也称为 VNet）集成在一起，可提供与 VM 相当的网络性能。
Pod 可以通过 Express Route 或者 站点到站点的 VPN 来连接到对等的 VNet ，
也可以从这些网络来直接访问 Pod。Pod 可以访问受服务端点或者受保护链接的 Azure 服务，比如存储和 SQL。
你可以使用 VNet 安全策略和路由来筛选 Pod 流量。
该插件通过利用在 Kubernetes 节点的网络接口上预分配的辅助 IP 池将 VNet 分配给 Pod 。</p>
<p>Azure CNI 可以在
<a href="https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni">Azure Kubernetes Service (AKS)</a> 中获得。</p>
<!--
### Calico

[Calico](https://projectcalico.docs.tigera.io/about/about-calico/) is an open source networking and network security solution for containers, virtual machines, and native host-based workloads. Calico supports multiple data planes including: a pure Linux eBPF dataplane, a standard Linux networking dataplane, and a Windows HNS dataplane. Calico provides a full networking stack but can also be used in conjunction with [cloud provider CNIs](https://docs.projectcalico.org/networking/determine-best-networking#calico-compatible-cni-plugins-and-cloud-provider-integrations) to provide network policy enforcement.
-->
<h3 id="calico">Calico</h3>
<p><a href="https://projectcalico.docs.tigera.io/about/about-calico/">Calico</a> 是一个开源的联网及网络安全方案，
用于基于容器、虚拟机和本地主机的工作负载。
Calico 支持多个数据面，包括：纯 Linux eBPF 的数据面、标准的 Linux 联网数据面
以及 Windows HNS 数据面。Calico 在提供完整的联网堆栈的同时，还可与
<a href="https://docs.projectcalico.org/networking/determine-best-networking#calico-compatible-cni-plugins-and-cloud-provider-integrations">云驱动 CNIs</a> 联合使用，以保证网络策略实施。</p>
<!--
### Cilium

[Cilium](https://github.com/cilium/cilium) is open source software for
providing and transparently securing network connectivity between application
containers. Cilium is L7/HTTP aware and can enforce network policies on L3-L7
using an identity based security model that is decoupled from network
addressing, and it can be used in combination with other CNI plugins.
-->
<h3 id="cilium">Cilium</h3>
<p><a href="https://github.com/cilium/cilium">Cilium</a> 是一个开源软件，用于提供并透明保护应用容器间的网络连接。
Cilium 支持 L7/HTTP，可以在 L3-L7 上通过使用与网络分离的基于身份的安全模型寻址来实施网络策略，
并且可以与其他 CNI 插件结合使用。</p>
<!--
### CNI-Genie from Huawei

[CNI-Genie](https://github.com/cni-genie/CNI-Genie) is a CNI plugin that enables Kubernetes to [simultaneously have access to different implementations](https://github.com/cni-genie/CNI-Genie/blob/master/docs/multiple-cni-plugins/README.md#what-cni-genie-feature-1-multiple-cni-plugins-enables) of the [Kubernetes network model](/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model) in runtime. This includes any implementation that runs as a [CNI plugin](https://github.com/containernetworking/cni#3rd-party-plugins), such as [Flannel](https://github.com/coreos/flannel#flannel), [Calico](https://projectcalico.docs.tigera.io/about/about-calico/), [Weave-net](https://www.weave.works/oss/net/).

CNI-Genie also supports [assigning multiple IP addresses to a pod](https://github.com/cni-genie/CNI-Genie/blob/master/docs/multiple-ips/README.md#feature-2-extension-cni-genie-multiple-ip-addresses-per-pod), each from a different CNI plugin.
-->
<h3 id="华为的-cni-genie">华为的 CNI-Genie</h3>
<p><a href="https://github.com/cni-genie/CNI-Genie">CNI-Genie</a> 是一个 CNI 插件，
可以让 Kubernetes 在运行时使用不同的<a href="#the-kubernetes-network-model">网络模型</a>的
<a href="https://github.com/cni-genie/CNI-Genie/blob/master/docs/multiple-cni-plugins/README.md#what-cni-genie-feature-1-multiple-cni-plugins-enables">实现同时被访问</a>。
这包括以
<a href="https://github.com/containernetworking/cni#3rd-party-plugins">CNI 插件</a>运行的任何实现，比如
<a href="https://github.com/coreos/flannel#flannel">Flannel</a>、
<a href="https://projectcalico.docs.tigera.io/about/about-calico/">Calico</a>、
<a href="https://www.weave.works/oss/net/">Weave-net</a>。</p>
<p>CNI-Genie 还支持<a href="https://github.com/cni-genie/CNI-Genie/blob/master/docs/multiple-ips/README.md#feature-2-extension-cni-genie-multiple-ip-addresses-per-pod">将多个 IP 地址分配给 Pod</a>，
每个都来自不同的 CNI 插件。</p>
<!--
### cni-ipvlan-vpc-k8s
[cni-ipvlan-vpc-k8s](https://github.com/lyft/cni-ipvlan-vpc-k8s) contains a set
of CNI and IPAM plugins to provide a simple, host-local, low latency, high
throughput, and compliant networking stack for Kubernetes within Amazon Virtual
Private Cloud (VPC) environments by making use of Amazon Elastic Network
Interfaces (ENI) and binding AWS-managed IPs into Pods using the Linux kernel's
IPvlan driver in L2 mode.

The plugins are designed to be straightforward to configure and deploy within a
VPC. Kubelets boot and then self-configure and scale their IP usage as needed
without requiring the often recommended complexities of administering overlay
networks, BGP, disabling source/destination checks, or adjusting VPC route
tables to provide per-instance subnets to each host (which is limited to 50-100
entries per VPC). In short, cni-ipvlan-vpc-k8s significantly reduces the
network complexity required to deploy Kubernetes at scale within AWS.
-->
<h3 id="cni-ipvlan-vpc-k8s">cni-ipvlan-vpc-k8s</h3>
<p><a href="https://github.com/lyft/cni-ipvlan-vpc-k8s">cni-ipvlan-vpc-k8s</a>
包含了一组 CNI 和 IPAM 插件来提供一个简单的、本地主机、低延迟、高吞吐量
以及通过使用 Amazon 弹性网络接口（ENI）并使用 Linux 内核的 IPv2 驱动程序
以 L2 模式将 AWS 管理的 IP 绑定到 Pod 中，
在 Amazon Virtual Private Cloud（VPC）环境中为 Kubernetes 兼容的网络堆栈。</p>
<p>这些插件旨在直接在 VPC 中进行配置和部署，Kubelets 先启动，
然后根据需要进行自我配置和扩展它们的 IP 使用率，而无需经常建议复杂的管理
覆盖网络、BGP、禁用源/目标检查或调整 VPC 路由表以向每个主机提供每个实例子网的
复杂性（每个 VPC 限制为50-100个条目）。
简而言之，cni-ipvlan-vpc-k8s 大大降低了在 AWS 中大规模部署 Kubernetes 所需的网络复杂性。</p>
<!--
### Coil

[Coil](https://github.com/cybozu-go/coil) is a CNI plugin designed for ease of integration, providing flexible egress networking.
Coil operates with a low overhead compared to bare metal, and allows you to define arbitrary egress NAT gateways for external networks.

-->
<h3 id="coil">Coil</h3>
<p><a href="https://github.com/cybozu-go/coil">Coil</a> 是一个为易于集成、提供灵活的出站流量网络而设计的 CNI 插件。
与裸机相比，Coil 的额外操作开销低，并允许针对外部网络的出站流量任意定义 NAT 网关。</p>
<!--
### Contiv-VPP

[Contiv-VPP](https://contivpp.io/) is a user-space, performance-oriented network plugin for
Kubernetes, using the [fd.io](https://fd.io/) data plane.
-->
<h3 id="contiv-vpp">Contiv-VPP</h3>
<p><a href="https://contivpp.io/">Contiv-VPP</a> 是用于 Kubernetes 的用户空间、面向性能的网络插件，使用 <a href="https://fd.io/">fd.io</a> 数据平面。</p>
<!--
### Contrail/Tungsten Fabric

[Contrail](https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/), based on [Tungsten Fabric](https://tungsten.io), is a truly open, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with various orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide different isolation modes for virtual machines, containers/pods and bare metal workloads.
-->
<h3 id="contrail-tungsten-fabric">Contrail/Tungsten Fabric</h3>
<p><a href="https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/">Contrail</a>
是基于 <a href="https://tungsten.io">Tungsten Fabric</a> 的，真正开放的多云网络虚拟化和策略管理平台。
Contrail 和 Tungsten Fabric 与各种编排系统集成在一起，例如 Kubernetes、OpenShift、OpenStack 和 Mesos，
并为虚拟机、容器或 Pods 以及裸机工作负载提供了不同的隔离模式。</p>
<!--
### DANM

[DANM](https://github.com/nokia/danm) is a networking solution for telco workloads running in a Kubernetes cluster. It's built up from the following components:

   * A CNI plugin capable of provisioning IPVLAN interfaces with advanced features
   * An in-built IPAM module with the capability of managing multiple, cluster-wide, discontinuous L3 networks and provide a dynamic, static, or no IP allocation scheme on-demand
   * A CNI metaplugin capable of attaching multiple network interfaces to a container, either through its own CNI, or through delegating the job to any of the popular CNI solution like SRI-OV, or Flannel in parallel
   * A Kubernetes controller capable of centrally managing both VxLAN and VLAN interfaces of all Kubernetes hosts
   * Another Kubernetes controller extending Kubernetes' Service-based service discovery concept to work over all network interfaces of a Pod

With this toolset DANM is able to provide multiple separated network interfaces, the possibility to use different networking back ends and advanced IPAM features for the pods.
-->
<h3 id="danm">DANM</h3>
<p><a href="https://github.com/nokia/danm">DANM</a> 是一个针对在 Kubernetes 集群中运行的电信工作负载的网络解决方案。
它由以下几个组件构成：</p>
<ul>
<li>能够配置具有高级功能的 IPVLAN 接口的 CNI 插件</li>
<li>一个内置的 IPAM 模块，能够管理多个、群集内的、不连续的 L3 网络，并按请求提供动态、静态或无 IP 分配方案</li>
<li>CNI 元插件能够通过自己的 CNI 或通过将任务授权给其他任何流行的 CNI 解决方案（例如 SRI-OV 或 Flannel）来实现将多个网络接口连接到容器</li>
<li>Kubernetes 控制器能够集中管理所有 Kubernetes 主机的 VxLAN 和 VLAN 接口</li>
<li>另一个 Kubernetes 控制器扩展了 Kubernetes 的基于服务的服务发现概念，以在 Pod 的所有网络接口上工作</li>
</ul>
<p>通过这个工具集，DANM 可以提供多个分离的网络接口，可以为 Pod 使用不同的网络后端和高级 IPAM 功能。</p>
<!--
### Flannel

[Flannel](https://github.com/flannel-io/flannel#flannel) is a very simple overlay
network that satisfies the Kubernetes requirements. Many
people have reported success with Flannel and Kubernetes.
-->
<h3 id="flannel">Flannel</h3>
<p><a href="https://github.com/flannel-io/flannel#flannel">Flannel</a> 是一个非常简单的能够满足
Kubernetes 所需要的覆盖网络。已经有许多人报告了使用 Flannel 和 Kubernetes 的成功案例。</p>
<!--
### Hybridnet

[Hybridnet](https://github.com/alibaba/hybridnet) is an open source CNI plugin designed for hybrid clouds which provides both overlay and underlay networking for containers in one or more clusters. Overlay and underlay containers can run on the same node and have cluster-wide bidirectional network connectivity.
-->
<h3 id="hybridnet">Hybridnet</h3>
<p><a href="https://github.com/alibaba/hybridnet">Hybridnet</a> 是一个为混合云设计的开源 CNI 插件，
它为一个或多个集群中的容器提供覆盖和底层网络。 Overlay 和 underlay 容器可以在同一个节点上运行，
并具有集群范围的双向网络连接。</p>
<!--
### Jaguar

[Jaguar](https://gitlab.com/sdnlab/jaguar) is an open source solution for Kubernetes's network based on OpenDaylight. Jaguar provides overlay network using vxlan and Jaguar CNIPlugin provides one IP address per pod.

### k-vswitch

[k-vswitch](https://github.com/k-vswitch/k-vswitch) is a simple Kubernetes networking plugin based on [Open vSwitch](https://www.openvswitch.org/). It leverages existing functionality in Open vSwitch to provide a robust networking plugin that is easy-to-operate, performant and secure.
-->
<h3 id="jaguar">Jaguar</h3>
<p><a href="https://gitlab.com/sdnlab/jaguar">Jaguar</a> 是一个基于 OpenDaylight 的 Kubernetes 网络开源解决方案。
Jaguar 使用 vxlan 提供覆盖网络，而 Jaguar CNIPlugin 为每个 Pod 提供一个 IP 地址。</p>
<h3 id="k-vswitch">k-vswitch</h3>
<p><a href="https://github.com/k-vswitch/k-vswitch">k-vswitch</a> 是一个基于
<a href="https://www.openvswitch.org/">Open vSwitch</a> 的简易 Kubernetes 网络插件。
它利用 Open vSwitch 中现有的功能来提供强大的网络插件，该插件易于操作，高效且安全。</p>
<!--
### Knitter

[Knitter](https://github.com/ZTE/Knitter/) is a network solution which supports multiple networking in Kubernetes. It provides the ability of tenant management and network management. Knitter includes a set of end-to-end NFV container networking solutions besides multiple network planes, such as keeping IP address for applications, IP address migration, etc.

### Kube-OVN

[Kube-OVN](https://github.com/alauda/kube-ovn) is an OVN-based kubernetes network fabric for enterprises. With the help of OVN/OVS, it provides some advanced overlay network features like subnet, QoS, static IP allocation, traffic mirroring, gateway, openflow-based network policy and service proxy.
-->
<h3 id="knitter">Knitter</h3>
<p><a href="https://github.com/ZTE/Knitter/">Knitter</a> 是一个支持 Kubernetes 中实现多个网络系统的解决方案。
它提供了租户管理和网络管理的功能。除了多个网络平面外，Knitter 还包括一组端到端的 NFV 容器网络解决方案，
例如为应用程序保留 IP 地址、IP 地址迁移等。</p>
<h3 id="kube-ovn">Kube-OVN</h3>
<p><a href="https://github.com/alauda/kube-ovn">Kube-OVN</a> 是一个基于 OVN 的用于企业的 Kubernetes 网络架构。
借助于 OVN/OVS ，它提供了一些高级覆盖网络功能，例如子网、QoS、静态 IP 分配、流量镜像、网关、
基于 openflow 的网络策略和服务代理。</p>
<!--
### Kube-router

[Kube-router](https://github.com/cloudnativelabs/kube-router) is a purpose-built networking solution for Kubernetes that aims to provide high performance and operational simplicity. Kube-router provides a Linux [LVS/IPVS](https://www.linuxvirtualserver.org/software/ipvs.html)-based service proxy, a Linux kernel forwarding-based pod-to-pod networking solution with no overlays, and iptables/ipset-based network policy enforcer.
-->
<h3 id="kube-router">Kube-router</h3>
<p><a href="https://github.com/cloudnativelabs/kube-router">Kube-router</a> 是 Kubernetes 的专用网络解决方案，
旨在提供高性能和易操作性。
Kube-router 提供了一个基于 Linux <a href="https://www.linuxvirtualserver.org/software/ipvs.html">LVS/IPVS</a>
的服务代理、一个基于 Linux 内核转发的无覆盖 Pod-to-Pod 网络解决方案和基于 iptables/ipset 的网络策略执行器。</p>
<!--
### L2 networks and linux bridging

If you have a "dumb" L2 network, such as a simple switch in a "bare-metal"
environment, you should be able to do something similar to the above GCE setup.
Note that these instructions have only been tried very casually - it seems to
work, but has not been thoroughly tested.  If you use this technique and
perfect the process, please let us know.

Follow the "With Linux Bridge devices" section of [this very nice
tutorial](https://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/) from
Lars Kellogg-Stedman.
-->
<h3 id="l2-networks-and-linux-bridging">L2 networks and linux bridging</h3>
<p>如果你具有一个“哑”的L2网络，例如“裸机”环境中的简单交换机，则应该能够执行与上述 GCE 设置类似的操作。
请注意，这些说明仅是非常简单的尝试过-似乎可行，但尚未经过全面测试。
如果您使用此技术并完善了流程，请告诉我们。</p>
<p>根据 Lars Kellogg-Stedman 的这份非常不错的“Linux 网桥设备”
<a href="https://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/">使用说明</a>来进行操作。</p>
<!--
### Multus (a Multi Network plugin)

Multus is a Multi CNI plugin to support the Multi Networking feature in Kubernetes using CRD based network objects in Kubernetes.

Multus supports all [reference plugins](https://github.com/containernetworking/plugins) (eg. [Flannel](https://github.com/containernetworking/cni.dev/blob/main/content/plugins/v0.9/meta/flannel.md), [DHCP](https://github.com/containernetworking/plugins/tree/master/plugins/ipam/dhcp), [Macvlan](https://github.com/containernetworking/plugins/tree/master/plugins/main/macvlan)) that implement the CNI specification and 3rd party plugins (eg. [Calico](https://github.com/projectcalico/cni-plugin), [Weave](https://github.com/weaveworks/weave), [Cilium](https://github.com/cilium/cilium), [Contiv](https://github.com/contiv/netplugin)). In addition to it, Multus supports [SRIOV](https://github.com/hustcat/sriov-cni), [DPDK](https://github.com/Intel-Corp/sriov-cni), [OVS-DPDK & VPP](https://github.com/intel/vhost-user-net-plugin) workloads in Kubernetes with both cloud native and NFV based applications in Kubernetes.
-->
<h3 id="multus-a-multi-network-plugin">Multus (a Multi Network plugin)</h3>
<p><a href="https://github.com/Intel-Corp/multus-cni">Multus</a> 是一个多 CNI 插件，
使用 Kubernetes 中基于 CRD 的网络对象来支持实现 Kubernetes 多网络系统。</p>
<p>Multus 支持所有<a href="https://github.com/containernetworking/plugins">参考插件</a>（比如：
<a href="https://github.com/containernetworking/cni.dev/blob/main/content/plugins/v0.9/meta/flannel.md">Flannel</a>、
<a href="https://github.com/containernetworking/plugins/tree/master/plugins/ipam/dhcp">DHCP</a>、
<a href="https://github.com/containernetworking/plugins/tree/master/plugins/main/macvlan">Macvlan</a> ）
来实现 CNI 规范和第三方插件（比如：
<a href="https://github.com/projectcalico/cni-plugin">Calico</a>、
<a href="https://github.com/weaveworks/weave">Weave</a>、
<a href="https://github.com/cilium/cilium">Cilium</a>、
<a href="https://github.com/contiv/netplugin">Contiv</a>）。
除此之外， Multus 还支持
<a href="https://github.com/hustcat/sriov-cni">SRIOV</a>、
<a href="https://github.com/Intel-Corp/sriov-cni">DPDK</a>、
<a href="https://github.com/intel/vhost-user-net-plugin">OVS-DPDK &amp; VPP</a> 的工作负载，
以及 Kubernetes 中基于云的本机应用程序和基于 NFV 的应用程序。</p>
<!--
### NSX-T

[VMware NSX-T](https://docs.vmware.com/en/VMware-NSX-T/index.html) is a network virtualization and security platform. NSX-T can provide network virtualization for a multi-cloud and multi-hypervisor environment and is focused on emerging application frameworks and architectures that have heterogeneous endpoints and technology stacks. In addition to vSphere hypervisors, these environments include other hypervisors such as KVM, containers, and bare metal.

[NSX-T Container Plug-in (NCP)](https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf) provides integration between NSX-T and container orchestrators such as Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and OpenShift.
-->
<h3 id="nsx-t">NSX-T</h3>
<p><a href="https://docs.vmware.com/en/VMware-NSX-T/index.html">VMware NSX-T</a> 是一个网络虚拟化和安全平台。
NSX-T 可以为多云及多系统管理程序环境提供网络虚拟化，并专注于具有异构端点和技术堆栈的新兴应用程序框架和体系结构。
除了 vSphere 管理程序之外，这些环境还包括其他虚拟机管理程序，例如 KVM、容器和裸机。</p>
<p><a href="https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf">NSX-T Container Plug-in (NCP)</a>
提供了 NSX-T 与容器协调器（例如 Kubernetes）之间的结合，
以及 NSX-T 与基于容器的 CaaS/PaaS 平台（例如 Pivotal Container Service（PKS）和 OpenShift）之间的集成。</p>
<!--
### OVN (Open Virtual Networking)

OVN is an opensource network virtualization solution developed by the
Open vSwitch community.  It lets one create logical switches, logical routers,
stateful ACLs, load-balancers etc to build different virtual networking
topologies.  The project has a specific Kubernetes plugin and documentation
at [ovn-kubernetes](https://github.com/openvswitch/ovn-kubernetes).
-->
<h3 id="ovn-开放式虚拟网络">OVN (开放式虚拟网络)</h3>
<p>OVN 是一个由 Open vSwitch 社区开发的开源的网络虚拟化解决方案。
它允许创建逻辑交换器、逻辑路由、状态 ACL、负载均衡等等来建立不同的虚拟网络拓扑。
该项目有一个特定的Kubernetes插件和文档 <a href="https://github.com/openvswitch/ovn-kubernetes">ovn-kubernetes</a>。</p>
<!--
### Weave Net from Weaveworks

[Weave Net](https://www.weave.works/oss/net/) is a
resilient and simple to use network for Kubernetes and its hosted applications.
Weave Net runs as a [CNI plug-in](https://www.weave.works/docs/net/latest/cni-plugin/)
or stand-alone.  In either version, it doesn't require any configuration or extra code
to run, and in both cases, the network provides one IP address per pod - as is standard for Kubernetes.
-->
<h3 id="weaveworks-的-weave-net">Weaveworks 的 Weave Net</h3>
<p><a href="https://www.weave.works/oss/net/">Weave Net</a> 是 Kubernetes 及其
托管应用程序的弹性且易于使用的网络系统。
Weave Net 可以作为 <a href="https://www.weave.works/docs/net/latest/cni-plugin/">CNI 插件</a> 运行或者独立运行。
在这两种运行方式里，都不需要任何配置或额外的代码即可运行，并且在两种情况下，
网络都为每个 Pod 提供一个 IP 地址 -- 这是 Kubernetes 的标准配置。</p>
<h2 id="what-s-next">What's next</h2>
<!--
The early design of the networking model and its rationale, and some future
plans are described in more detail in the [networking design
document](https://git.k8s.io/community/contributors/design-proposals/network/networking.md).
-->
<p>网络模型的早期设计、运行原理以及未来的一些计划，都在
<a href="https://git.k8s.io/community/contributors/design-proposals/network/networking.md">联网设计文档</a>
里有更详细的描述。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-cbfd3654996eae9fcdef009f70fa83f0">11.4 - Kubernetes 系统组件指标</h1>
    
	<!--
title: Metrics For Kubernetes System Components
reviewers:
- brancz
- logicalhan
- RainbowMango
content_type: concept
weight: 60
-->
<!-- overview -->
<!--
System component metrics can give a better look into what is happening inside them. Metrics are particularly useful for building dashboards and alerts.

Kubernetes components emit metrics in [Prometheus format](https://prometheus.io/docs/instrumenting/exposition_formats/).
This format is structured plain text, designed so that people and machines can both read it.
-->
<p>通过系统组件指标可以更好地了解系统组个内部发生的情况。系统组件指标对于构建仪表板和告警特别有用。</p>
<p>Kubernetes 组件以 <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">Prometheus 格式</a>
生成度量值。
这种格式是结构化的纯文本，旨在使人和机器都可以阅读。</p>
<!-- body -->
<!--
## Metrics in Kubernetes

In most cases metrics are available on `/metrics` endpoint of the HTTP server. For components that doesn't expose endpoint by default it can be enabled using `--bind-address` flag.

Examples of those components:
-->
<h2 id="kubernetes-中组件的指标">Kubernetes 中组件的指标</h2>
<p>在大多数情况下，可以通过 HTTP 访问组件的 <code>/metrics</code> 端点来获取组件的度量值。
对于那些默认情况下不暴露端点的组件，可以使用 <code>--bind-address</code> 标志启用。</p>
<p>这些组件的示例：</p>
<ul>
<li><a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a></li>
<li><a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a></li>
<li><a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='kube-apiserver'>kube-apiserver</a></li>
<li><a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='kube-scheduler'>kube-scheduler</a></li>
<li><a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a></li>
</ul>
<!--
In a production environment you may want to configure [Prometheus Server](https://prometheus.io/) or some other metrics scraper
to periodically gather these metrics and make them available in some kind of time series database.

Note that <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> also exposes metrics in `/metrics/cadvisor`, `/metrics/resource` and `/metrics/probes` endpoints. Those metrics do not have same lifecycle.

If your cluster uses <a class='glossary-tooltip' title='管理授权决策，允许管理员通过 Kubernetes API 动态配置访问策略。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/access-authn-authz/rbac/' target='_blank' aria-label='RBAC'>RBAC</a>, reading metrics requires authorization via a user, group or ServiceAccount with a ClusterRole that allows accessing `/metrics`.
For example:
-->
<p>在生产环境中，你可能需要配置 <a href="https://prometheus.io/">Prometheus 服务器</a> 或
某些其他指标搜集器以定期收集这些指标，并使它们在某种时间序列数据库中可用。</p>
<p>请注意，<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> 还会在 <code>/metrics/cadvisor</code>，
<code>/metrics/resource</code> 和 <code>/metrics/probes</code> 端点中公开度量值。这些度量值的生命周期各不相同。</p>
<p>如果你的集群使用了 <a class='glossary-tooltip' title='管理授权决策，允许管理员通过 Kubernetes API 动态配置访问策略。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/access-authn-authz/rbac/' target='_blank' aria-label='RBAC'>RBAC</a>，
则读取指标需要通过基于用户、组或 ServiceAccount 的鉴权，要求具有允许访问
<code>/metrics</code> 的 ClusterRole。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>prometheus<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">nonResourceURLs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;/metrics&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- get<span style="color:#bbb">
</span></code></pre></div><!--
## Metric lifecycle

Alpha metric →  Stable metric →  Deprecated metric →  Hidden metric → Deleted metric

Alpha metrics have no stability guarantees. These metrics can be modified or deleted at any time.

Stable metrics are guaranteed to not change. This means:
* A stable metric without a deprecated signature will not be deleted or renamed
* A stable metric's type will not be modified

Deprecated metrics are slated for deletion, but are still available for use.
These metrics include an annotation about the version in which they became deprecated.
-->
<h2 id="指标生命周期">指标生命周期</h2>
<p>Alpha 指标 →  稳定的指标 →  弃用的指标 →  隐藏的指标 → 删除的指标</p>
<p>Alpha 指标没有稳定性保证。这些指标可以随时被修改或者删除。</p>
<p>稳定的指标可以保证不会改变。这意味着：</p>
<ul>
<li>稳定的、不包含已弃用（deprecated）签名的指标不会被删除（或重命名）</li>
<li>稳定的指标的类型不会被更改</li>
</ul>
<p>已弃用的指标最终将被删除，不过仍然可用。
这类指标包含注解，标明其被废弃的版本。</p>
<!--
For example:

* Before deprecation
-->
<p>例如：</p>
<ul>
<li>
<p>被弃用之前：</p>
<pre><code># HELP some_counter this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li>
</ul>
<!--
* After deprecation
-->
<ul>
<li>
<p>被弃用之后：</p>
<pre><code># HELP some_counter (Deprecated since 1.15.0) this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li>
</ul>
<!--
Hidden metrics are no longer published for scraping, but are still available for use. To use a hidden metric, please refer to the [Show hidden metrics](#show-hidden-metrics) section. 

Deleted metrics are no longer published and cannot be used.
-->
<p>隐藏的指标不会再被发布以供抓取，但仍然可用。
要使用隐藏指标，请参阅<a href="#show-hidden-metrics">显式隐藏指标</a>节。</p>
<p>删除的指标不再被发布，亦无法使用。</p>
<!--
## Show hidden metrics

As described above, admins can enable hidden metrics through a command-line flag on a specific binary. This intends to be used as an escape hatch for admins if they missed the migration of the metrics deprecated in the last release.

The flag `show-hidden-metrics-for-version` takes a version for which you want to show metrics deprecated in that release. The version is expressed as x.y, where x is the major version, y is the minor version. The patch version is not needed even though a metrics can be deprecated in a patch release, the reason for that is the metrics deprecation policy runs against the minor release.

The flag can only take the previous minor version as it's value. All metrics hidden in previous will be emitted if admins set the previous version to `show-hidden-metrics-for-version`. The too old version is not allowed because this violates the metrics deprecated policy.

Take metric `A` as an example, here assumed that `A` is deprecated in 1.n. According to metrics deprecated policy, we can reach the following conclusion:
-->
<h2 id="show-hidden-metrics">显示隐藏指标  </h2>
<p>如上所述，管理员可以通过设置可执行文件的命令行参数来启用隐藏指标，
如果管理员错过了上一版本中已经弃用的指标的迁移，则可以把这个用作管理员的逃生门。</p>
<p><code>show-hidden-metrics-for-version</code> 标志接受版本号作为取值，版本号给出
你希望显示该发行版本中已弃用的指标。
版本表示为 x.y，其中 x 是主要版本，y 是次要版本。补丁程序版本不是必须的，
即使指标可能会在补丁程序发行版中弃用，原因是指标弃用策略规定仅针对次要版本。</p>
<p>该参数只能使用前一个次要版本。如果管理员将先前版本设置为 <code>show-hidden-metrics-for-version</code>，
则先前版本中隐藏的度量值会再度生成。不允许使用过旧的版本，因为那样会违反指标弃用策略。</p>
<p>以指标 <code>A</code> 为例，此处假设 <code>A</code> 在 1.n 中已弃用。根据指标弃用策略，我们可以得出以下结论：</p>
<!--
* In release `1.n`, the metric is deprecated, and it can be emitted by default.
* In release `1.n+1`, the metric is hidden by default and it can be emitted by command line `show-hidden-metrics-for-version=1.n`.
* In release `1.n+2`, the metric should be removed from the codebase. No escape hatch anymore.

If you're upgrading from release `1.12` to `1.13`, but still depend on a metric `A` deprecated in `1.12`, you should set hidden metrics via command line: `--show-hidden-metrics=1.12` and remember to remove this metric dependency before upgrading to `1.14`
-->
<ul>
<li>在版本 <code>1.n</code> 中，这个指标已经弃用，且默认情况下可以生成。</li>
<li>在版本 <code>1.n+1</code> 中，这个指标默认隐藏，可以通过命令行参数 <code>show-hidden-metrics-for-version=1.n</code> 来再度生成。</li>
<li>在版本 <code>1.n+2</code> 中，这个指标就将被从代码中移除，不会再有任何逃生窗口。</li>
</ul>
<p>如果你要从版本 <code>1.12</code> 升级到 <code>1.13</code>，但仍依赖于 <code>1.12</code> 中弃用的指标 <code>A</code>，则应通过命令行设置隐藏指标：
<code>--show-hidden-metrics=1.12</code>，并记住在升级到 <code>1.14</code> 版本之前删除此指标依赖项。</p>
<!--
## Disable accelerator metrics

The kubelet collects accelerator metrics through cAdvisor. To collect these metrics, for accelerators like NVIDIA GPUs, kubelet held an open handle on the driver. This meant that in order to perform infrastructure changes (for example, updating the driver), a cluster administrator needed to stop the kubelet agent.

The responsibility for collecting accelerator metrics now belongs to the vendor rather than the kubelet. Vendors must provide a container that collects metrics and exposes them to the metrics service (for example, Prometheus).

The [`DisableAcceleratorUsageMetrics` feature gate](/docs/reference/command-line-tools-reference/feature-gates/) disables metrics collected by the kubelet, with a [timeline for enabling this feature by default](https://github.com/kubernetes/enhancements/tree/411e51027db842355bd489691af897afc1a41a5e/keps/sig-node/1867-disable-accelerator-usage-metrics#graduation-criteria).
-->
<h2 id="禁用加速器指标">禁用加速器指标</h2>
<p>kubelet 通过 cAdvisor 收集加速器指标。为了收集这些指标，对于 NVIDIA GPU 之类的加速器，
kubelet 在驱动程序上保持打开状态。这意味着为了执行基础结构更改（例如更新驱动程序），
集群管理员需要停止 kubelet 代理。</p>
<p>现在，收集加速器指标的责任属于供应商，而不是 kubelet。供应商必须提供一个收集指标的容器，
并将其公开给指标服务（例如 Prometheus）。</p>
<p><a href="/zh/docs/reference/command-line-tools-reference/feature-gates/"><code>DisableAcceleratorUsageMetrics</code> 特性门控</a>
禁止由 kubelet 收集的指标。
关于<a href="https://github.com/kubernetes/enhancements/tree/411e51027db842355bd489691af897afc1a41a5e/keps/sig-node/1867-disable-accelerator-usage-metrics#graduation-criteria">何时会在默认情况下启用此功能也有一定规划</a>。</p>
<!--
## Component metrics

### kube-controller-manager metrics

Controller manager metrics provide important insight into the performance and health of the controller manager.
These metrics include common Go language runtime metrics such as go_routine count and controller specific metrics such as
etcd request latencies or Cloudprovider (AWS, GCE, OpenStack) API latencies that can be used
to gauge the health of a cluster.

Starting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage operations for GCE, AWS, Vsphere and OpenStack.
These metrics can be used to monitor health of persistent volume operations.

For example, for GCE these metrics are called:
-->
<h2 id="组件指标">组件指标</h2>
<h3 id="kube-controller-manager-指标">kube-controller-manager 指标</h3>
<p>控制器管理器指标可提供有关控制器管理器性能和运行状况的重要洞察。
这些指标包括通用的 Go 语言运行时指标（例如 go_routine 数量）和控制器特定的度量指标，
例如可用于评估集群运行状况的 etcd 请求延迟或云提供商（AWS、GCE、OpenStack）的 API 延迟等。</p>
<p>从 Kubernetes 1.7 版本开始，详细的云提供商指标可用于 GCE、AWS、Vsphere 和 OpenStack 的存储操作。
这些指标可用于监控持久卷操作的运行状况。</p>
<p>比如，对于 GCE，这些指标称为：</p>
<pre><code>cloudprovider_gce_api_request_duration_seconds { request = &quot;instance_list&quot;}
cloudprovider_gce_api_request_duration_seconds { request = &quot;disk_insert&quot;}
cloudprovider_gce_api_request_duration_seconds { request = &quot;disk_delete&quot;}
cloudprovider_gce_api_request_duration_seconds { request = &quot;attach_disk&quot;}
cloudprovider_gce_api_request_duration_seconds { request = &quot;detach_disk&quot;}
cloudprovider_gce_api_request_duration_seconds { request = &quot;list_disk&quot;}
</code></pre><!--
### kube-scheduler metrics
-->
<h3 id="kube-scheduler-metrics">kube-scheduler 指标  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code>
</div>


<!--
The scheduler exposes optional metrics that reports the requested resources and the desired limits of all running pods. These metrics can be used to build capacity planning dashboards, assess current or historical scheduling limits, quickly identify workloads that cannot schedule due to lack of resources, and compare actual usage to the pod's request.
-->
<p>调度器会暴露一些可选的指标，报告所有运行中 Pods 所请求的资源和期望的约束值。
这些指标可用来构造容量规划监控面板、访问调度约束的当前或历史数据、
快速发现因为缺少资源而无法被调度的负载，或者将 Pod 的实际资源用量
与其请求值进行比较。</p>
<!--
The kube-scheduler identifies the resource [requests and limits](/docs/concepts/configuration/manage-resources-containers/) configured for each Pod; when either a request or limit is non-zero, the kube-scheduler reports a metrics timeseries. The time series is labelled by:
- namespace
- pod name
- the node where the pod is scheduled or an empty string if not yet scheduled
- priority
- the assigned scheduler for that pod
- the name of the resource (for example, `cpu`)
- the unit of the resource if known (for example, `cores`)
-->
<p>kube-scheduler 组件能够辩识各个 Pod 所配置的资源
<a href="/zh/docs/concepts/configuration/manage-resources-containers/">请求和约束</a>。
在 Pod 的资源请求值或者约束值非零时，kube-scheduler 会以度量值时间序列的形式
生成报告。该时间序列值包含以下标签：</p>
<ul>
<li>名字空间</li>
<li>Pod 名称</li>
<li>Pod 调度所处节点，或者当 Pod 未被调度时用空字符串表示</li>
<li>优先级</li>
<li>为 Pod 所指派的调度器</li>
<li>资源的名称（例如，<code>cpu</code>）</li>
<li>资源的单位，如果知道的话（例如，<code>cores</code>）</li>
</ul>
<!--
Once a pod reaches completion (has a `restartPolicy` of `Never` or `OnFailure` and is in the `Succeeded` or `Failed` pod phase, or has been deleted and all containers have a terminated state) the series is no longer reported since the scheduler is now free to schedule other pods to run. The two metrics are called `kube_pod_resource_request` and `kube_pod_resource_limit`.

The metrics are exposed at the HTTP endpoint `/metrics/resources` and require the same authorization as the `/metrics`
endpoint on the scheduler. You must use the `-show-hidden-metrics-for-version=1.20` flag to expose these alpha stability metrics.
-->
<p>一旦 Pod 进入完成状态（其 <code>restartPolicy</code> 为 <code>Never</code> 或 <code>OnFailure</code>，且
其处于 <code>Succeeded</code> 或 <code>Failed</code> Pod 阶段，或者已经被删除且所有容器都具有
终止状态），该时间序列停止报告，因为调度器现在可以调度其它 Pod 来执行。
这两个指标称作 <code>kube_pod_resource_request</code> 和 <code>kube_pod_resource_limit</code>。</p>
<p>指标暴露在 HTTP 端点 <code>/metrics/resources</code>，与调度器上的 <code>/metrics</code> 端点
一样要求相同的访问授权。你必须使用
<code>--show-hidden-metrics-for-version=1.20</code> 标志才能暴露那些稳定性为 Alpha
的指标。</p>
<!--
## Disabling metrics

You can explicitly turn off metrics via command line flag `--disabled-metrics`. This may be desired if, for example, a metric is causing a performance problem. The input is a list of disabled metrics (i.e. `--disabled-metrics=metric1,metric2`).
-->
<h2 id="disabling-metrics">禁用指标</h2>
<p>你可以通过命令行标志 <code>--disabled-metrics</code> 来关闭某指标。
在例如某指标会带来性能问题的情况下，这一操作可能是有用的。
标志的参数值是一组被禁止的指标（例如：<code>--disabled-metrics=metric1,metric2</code>）。</p>
<!--
## Metric cardinality enforcement

Metrics with unbounded dimensions could cause memory issues in the components they instrument. To limit resource use, you can use the `--allow-label-value` command line option to dynamically configure an allow-list of label values for a metric.
-->
<h2 id="metric-cardinality-enforcement">指标顺序性保证   </h2>
<p>在 Alpha 阶段，标志只能接受一组映射值作为可以使用的指标标签。
每个映射值的格式为<code>&lt;指标名称&gt;,&lt;标签名称&gt;=&lt;可用标签列表&gt;</code>，其中
<code>&lt;可用标签列表&gt;</code> 是一个用逗号分隔的、可接受的标签名的列表。</p>
<!--
The overall format looks like:
`--allow-label-value <metric_name>,<label_name>='<allow_value1>, <allow_value2>...', <metric_name2>,<label_name>='<allow_value1>, <allow_value2>...', ...`.
-->
<p>最终的格式看起来会是这样：
<code>--allow-label-value &lt;指标名称&gt;,&lt;标签名称&gt;='&lt;可用值1&gt;,&lt;可用值2&gt;...', &lt;指标名称2&gt;,&lt;标签名称&gt;='&lt;可用值1&gt;, &lt;可用值2&gt;...', ...</code>.</p>
<!--
Here is an example:
-->
<p>下面是一个例子：</p>
<p><code>--allow-label-value number_count_metric,odd_number='1,3,5', number_count_metric,even_number='2,4,6', date_gauge_metric,weekend='Saturday,Sunday'</code></p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about the [Prometheus text format](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format) for metrics
* Read about the [Kubernetes deprecation policy](/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior)
-->
<ul>
<li>阅读有关指标的 <a href="https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format">Prometheus 文本格式</a></li>
<li>阅读有关 <a href="/zh/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior">Kubernetes 弃用策略</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c4b1e87a84441f8a90699a345ce48d68">11.5 - 日志架构</h1>
    
	<!--
reviewers:
- piosz
- x13n
title: Logging Architecture
content_type: concept
weight: 60
-->
<!-- overview -->
<!--
Application logs can help you understand what is happening inside your application. The logs are particularly useful for debugging problems and monitoring cluster activity. Most modern applications have some kind of logging mechanism. Likewise, container engines are designed to support logging. The easiest and most adopted logging method for containerized applications is writing to standard output and standard error streams.
-->
<p>应用日志可以让你了解应用内部的运行状况。日志对调试问题和监控集群活动非常有用。
大部分现代化应用都有某种日志记录机制。同样地，容器引擎也被设计成支持日志记录。
针对容器化应用，最简单且最广泛采用的日志记录方式就是写入标准输出和标准错误流。</p>
<!--
However, the native functionality provided by a container engine or runtime is usually not enough for a complete logging solution.
For example, you may want to access your application's logs if a container crashes; a pod gets evicted; or a node dies,
In a cluster, logs should have a separate storage and lifecycle independent of nodes, pods, or containers. This concept is called _cluster-level-logging_.
-->
<p>但是，由容器引擎或运行时提供的原生功能通常不足以构成完整的日志记录方案。
例如，如果发生容器崩溃、Pod 被逐出或节点宕机等情况，你可能想访问应用日志。
在集群中，日志应该具有独立的存储和生命周期，与节点、Pod 或容器的生命周期相独立。
这个概念叫 <em>集群级的日志</em> 。</p>
<!-- body -->
<!--
Cluster-level logging architectures require a separate backend to store, analyze, and query logs. Kubernetes
does not provide a native storage solution for log data. Instead, there are many logging solutions that
integrate with Kubernetes. The following sections describe how to handle and store logs on nodes.
-->
<p>集群级日志架构需要一个独立的后端用来存储、分析和查询日志。
Kubernetes 并不为日志数据提供原生的存储解决方案。
相反，有很多现成的日志方案可以集成到 Kubernetes 中。
下面各节描述如何在节点上处理和存储日志。</p>
<!--
## Basic logging in Kubernetes

This example uses a `Pod` specification with a container
to write text to the standard output stream once per second.
-->
<h2 id="kubernetes-中的基本日志记录">Kubernetes 中的基本日志记录</h2>
<p>这里的示例使用包含一个容器的 Pod 规约，每秒钟向标准输出写入数据。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/debug/counter-pod.yaml" download="debug/counter-pod.yaml"><code>debug/counter-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('debug-counter-pod-yaml')" title="Copy debug/counter-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="debug-counter-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>counter<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>count<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[/bin/sh, -c,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#39;i=0; while true; do echo &#34;$i: $(date)&#34;; i=$((i+1)); sleep 1; done&#39;</span>]<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
To run this pod, use the following command:
-->
<p>用下面的命令运行 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml
</code></pre></div><!--
The output is:
-->
<p>输出结果为：</p>
<pre><code>pod/counter created
</code></pre><!--
To fetch the logs, use the `kubectl logs` command, as follows:
-->
<p>像下面这样，使用 <code>kubectl logs</code> 命令获取日志:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs counter
</code></pre></div><!--
The output is:
-->
<p>输出结果为：</p>
<pre><code>0: Mon Jan  1 00:00:00 UTC 2001
1: Mon Jan  1 00:00:01 UTC 2001
2: Mon Jan  1 00:00:02 UTC 2001
...
</code></pre><!--
You can use `kubectl logs --previous` to retrieve logs from a previous instantiation of a container.
If your pod has multiple containers, specify which container's logs you want to access by
appending a container name to the command, with a `-c` flag, like so:
-->
<p>你可以使用命令 <code>kubectl logs --previous</code> 检索之前容器实例的日志。
如果 Pod 中有多个容器，你应该为该命令附加容器名以访问对应容器的日志。
详见 <a href="/docs/reference/generated/kubectl/kubectl-commands#logs"><code>kubectl logs</code> 文档</a>。
如果 Pod 有多个容器，你应该为该命令附加容器名以访问对应容器的日志，
使用 <code>-c</code> 标志来指定要访问的容器的日志，如下所示：</p>
<pre><code class="language-console" data-lang="console">kubectl logs counter -c count
</code></pre><!--
See the [`kubectl logs` documentation](/docs/reference/generated/kubectl/kubectl-commands#logs) for more details.
-->
<p>详见 <a href="/docs/reference/generated/kubectl/kubectl-commands#logs"><code>kubectl logs</code> 文档</a>。</p>
<!--
## Logging at the node level

![Node level logging](/images/docs/user-guide/logging/logging-node-level.png)
-->
<h2 id="节点级日志记录">节点级日志记录</h2>
<p><img src="/images/docs/user-guide/logging/logging-node-level.png" alt="节点级别的日志记录"></p>
<!--
A container engine handles and redirects any output generated to a containerized application's `stdout` and `stderr` streams.
For example, the Docker container engine redirects those two streams to [a logging driver](https://docs.docker.com/engine/admin/logging/overview), which is configured in Kubernetes to write to a file in JSON format.
-->
<p>容器化应用写入 <code>stdout</code> 和 <code>stderr</code> 的任何数据，都会被容器引擎捕获并被重定向到某个位置。
例如，Docker 容器引擎将这两个输出流重定向到某个
<a href="https://docs.docker.com/engine/admin/logging/overview">日志驱动（Logging Driver）</a> ，
该日志驱动在 Kubernetes 中配置为以 JSON 格式写入文件。</p>
<!--
The Docker json logging driver treats each line as a separate message. When using the Docker logging driver, there is no direct support for multi-line messages. You need to handle multi-line messages at the logging agent level or higher.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> Docker JSON 日志驱动将日志的每一行当作一条独立的消息。
该日志驱动不直接支持多行消息。你需要在日志代理级别或更高级别处理多行消息。
</div>
<!--
By default, if a container restarts, the kubelet keeps one terminated container with its logs. If a pod is evicted from the node, all corresponding containers are also evicted, along with their logs.
-->
<p>默认情况下，如果容器重启，kubelet 会保留被终止的容器日志。
如果 Pod 在工作节点被驱逐，该 Pod 中所有的容器也会被驱逐，包括容器日志。</p>
<!--
An important consideration in node-level logging is implementing log rotation,
so that logs don't consume all available storage on the node. Kubernetes
is not responsible for rotating logs, but rather a deployment tool
should set up a solution to address that.
For example, in Kubernetes clusters, deployed by the `kube-up.sh` script,
there is a [`logrotate`](https://linux.die.net/man/8/logrotate)
tool configured to run each hour. You can also set up a container runtime to
rotate application's logs automatically.
-->
<p>节点级日志记录中，需要重点考虑实现日志的轮转，以此来保证日志不会消耗节点上全部可用空间。
Kubernetes 并不负责轮转日志，而是通过部署工具建立一个解决问题的方案。
例如，在用 <code>kube-up.sh</code> 部署的 Kubernetes 集群中，存在一个
<a href="https://linux.die.net/man/8/logrotate"><code>logrotate</code></a>，每小时运行一次。
你也可以设置容器运行时来自动地轮转应用日志。</p>
<!--
As an example, you can find detailed information about how `kube-up.sh` sets
up logging for COS image on GCP in the corresponding
[`configure-helper` script](https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh).
-->
<p>例如，你可以找到关于 <code>kube-up.sh</code> 为 GCP 环境的 COS 镜像设置日志的详细信息，
脚本为
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh"><code>configure-helper</code> 脚本</a>。</p>
<!--
When using a **CRI container runtime**, the kubelet is responsible for rotating the logs and managing the logging directory structure. The kubelet
sends this information to the CRI container runtime and the runtime writes the container logs to the given location. 
The two kubelet parameters [`containerLogMaxSize` and `containerLogMaxFiles`](/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration)
in [kubelet config file](/docs/tasks/administer-cluster/kubelet-config-file/)
can be used to configure the maximum size for each log file and the maximum number of files allowed for each container respectively.
-->
<p>当使用某 <em>CRI 容器运行时</em> 时，kubelet 要负责对日志进行轮换，并
管理日志目录的结构。kubelet 将此信息发送给 CRI 容器运行时，后者
将容器日志写入到指定的位置。在 <a href="/docs/tasks/administer-cluster/kubelet-config-file/">kubelet 配置文件</a>
中的两个 kubelet 参数
<a href="/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration"><code>containerLogMaxSize</code> 和 <code>containerLogMaxFiles</code></a>
可以用来配置每个日志文件的最大长度和每个容器可以生成的日志文件个数上限。</p>
<!--
When you run [`kubectl logs`](/docs/reference/generated/kubectl/kubectl-commands#logs) as in
the basic logging example, the kubelet on the node handles the request and
reads directly from the log file. The kubelet returns the content of the log file.
-->
<p>当运行 <a href="/docs/reference/generated/kubectl/kubectl-commands#logs"><code>kubectl logs</code></a> 时，
节点上的 kubelet 处理该请求并直接读取日志文件，同时在响应中返回日志文件内容。</p>
<!--
If an external system has performed the rotation or a CRI container runtime is used,
only the contents of the latest log file will be available through
`kubectl logs`. For example, if there's a 10MB file, `logrotate` performs
the rotation and there are two files: one file that is 10MB in size and a second file that is empty.
`kubectl logs` returns the latest log file which in this example is an empty response.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 如果有外部系统执行日志轮转或者使用了 CRI 容器运行时，那么 <code>kubectl logs</code>
仅可查询到最新的日志内容。
比如，对于一个 10MB 大小的文件，通过 <code>logrotate</code> 执行轮转后生成两个文件，
一个 10MB 大小，一个为空，<code>kubectl logs</code> 返回最新的日志文件，而该日志文件
在这个例子中为空。
</div>
<!--
### System component logs

There are two types of system components: those that run in a container and those
that do not run in a container. For example:
-->
<h3 id="系统组件日志">系统组件日志</h3>
<p>系统组件有两种类型：在容器中运行的和不在容器中运行的。例如：</p>
<!--
* The Kubernetes scheduler and kube-proxy run in a container.
* The kubelet and container runtime do not run in containers.
-->
<ul>
<li>在容器中运行的 kube-scheduler 和 kube-proxy。</li>
<li>不在容器中运行的 kubelet 和容器运行时。</li>
</ul>
<!--
On machines with systemd, the kubelet and container runtime write to journald. If
systemd is not present, the kubelet and container runtime write to `.log` files
in the `/var/log` directory. System components inside containers always write
to the `/var/log` directory, bypassing the default logging mechanism.
They use the [`klog`](https://github.com/kubernetes/klog)
logging library. You can find the conventions for logging severity for those
components in the [development docs on logging](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md).
-->
<p>在使用 systemd 机制的服务器上，kubelet 和容器容器运行时将日志写入到 journald 中。
如果没有 systemd，它们将日志写入到 <code>/var/log</code> 目录下的 <code>.log</code> 文件中。
容器中的系统组件通常将日志写到 <code>/var/log</code> 目录，绕过了默认的日志机制。
他们使用 <a href="https://github.com/kubernetes/klog">klog</a> 日志库。
你可以在<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md">日志开发文档</a>
找到这些组件的日志告警级别约定。</p>
<!--
Similar to the container logs, system component logs in the `/var/log`
directory should be rotated. In Kubernetes clusters brought up by
the `kube-up.sh` script, those logs are configured to be rotated by
the `logrotate` tool daily or once the size exceeds 100MB.
-->
<p>和容器日志类似，<code>/var/log</code> 目录中的系统组件日志也应该被轮转。
通过脚本 <code>kube-up.sh</code> 启动的 Kubernetes 集群中，日志被工具 <code>logrotate</code>
执行每日轮转，或者日志大小超过 100MB 时触发轮转。</p>
<!--
## Cluster-level logging architectures

While Kubernetes does not provide a native solution for cluster-level logging, there are several common approaches you can consider. Here are some options:

* Use a node-level logging agent that runs on every node.
* Include a dedicated sidecar container for logging in an application pod.
* Push logs directly to a backend from within an application.
-->
<h2 id="集群级日志架构">集群级日志架构</h2>
<p>虽然Kubernetes没有为集群级日志记录提供原生的解决方案，但你可以考虑几种常见的方法。
以下是一些选项：</p>
<ul>
<li>使用在每个节点上运行的节点级日志记录代理。</li>
<li>在应用程序的 Pod 中，包含专门记录日志的边车（Sidecar）容器。</li>
<li>将日志直接从应用程序中推送到日志记录后端。</li>
</ul>
<!--
### Using a node logging agent

![Using a node level logging agent](/images/docs/user-guide/logging/logging-with-node-agent.png)
-->
<h3 id="使用节点级日志代理">使用节点级日志代理</h3>
<p><img src="/images/docs/user-guide/logging/logging-with-node-agent.png" alt="使用节点日志记录代理"></p>
<!--
You can implement cluster-level logging by including a _node-level logging agent_ on each node. The logging agent is a dedicated tool that exposes logs or pushes logs to a backend. Commonly, the logging agent is a container that has access to a directory with log files from all of the application containers on that node.
-->
<p>你可以通过在每个节点上使用 <em>节点级的日志记录代理</em> 来实现群集级日志记录。
日志记录代理是一种用于暴露日志或将日志推送到后端的专用工具。
通常，日志记录代理程序是一个容器，它可以访问包含该节点上所有应用程序容器的日志文件的目录。</p>
<!--
Because the logging agent must run on every node, it's common to run the agent
as a `DaemonSet`.
Node-level logging creates only one agent per node, and doesn't require any changes to the applications running on the node. 
-->
<p>由于日志记录代理必须在每个节点上运行，通常可以用 <code>DaemonSet</code> 的形式运行该代理。
节点级日志在每个节点上仅创建一个代理，不需要对节点上的应用做修改。</p>
<!--
Containers write to stdout and stderr, but with no agreed format. A node-level agent collects these logs and forwards them for aggregation.
-->
<p>容器向标准输出和标准错误输出写出数据，但在格式上并不统一。
节点级代理
收集这些日志并将其进行转发以完成汇总。</p>
<!--
### Using a sidecar container with the logging agent {#sidecar-container-with-logging-agent}

You can use a sidecar container in one of the following ways:
-->
<h3 id="sidecar-container-with-logging-agent">使用 sidecar 容器运行日志代理  </h3>
<p>你可以通过以下方式之一使用边车（Sidecar）容器：</p>
<!--
* The sidecar container streams application logs to its own `stdout`.
* The sidecar container runs a logging agent, which is configured to pick up logs from an application container.
-->
<ul>
<li>边车容器将应用程序日志传送到自己的标准输出。</li>
<li>边车容器运行一个日志代理，配置该日志代理以便从应用容器收集日志。</li>
</ul>
<!--
#### Streaming sidecar container

![Sidecar container with a streaming container](/images/docs/user-guide/logging/logging-with-streaming-sidecar.png)

By having your sidecar containers stream to their own `stdout` and `stderr`
streams, you can take advantage of the kubelet and the logging agent that
already run on each node. The sidecar containers read logs from a file, a socket,
or the journald. Each sidecar container prints log to its own `stdout` or `stderr` stream.
-->
<h4 id="传输数据流的-sidecar-容器">传输数据流的 sidecar 容器</h4>
<p><img src="/images/docs/user-guide/logging/logging-with-streaming-sidecar.png" alt="带数据流容器的边车容器"></p>
<p>利用边车容器向自己的 <code>stdout</code> 和 <code>stderr</code> 传输流的方式，
你就可以利用每个节点上的 kubelet 和日志代理来处理日志。
边车容器从文件、套接字或 journald 读取日志。
每个边车容器向自己的 <code>stdout</code> 和 <code>stderr</code> 流中输出日志。</p>
<!--
This approach allows you to separate several log streams from different
parts of your application, some of which can lack support
for writing to `stdout` or `stderr`. The logic behind redirecting logs
is minimal, so it's hardly a significant overhead. Additionally, because
`stdout` and `stderr` are handled by the kubelet, you can use built-in tools
like `kubectl logs`.
-->
<p>这种方法允许你将日志流从应用程序的不同部分分离开，其中一些可能缺乏对写入
<code>stdout</code> 或 <code>stderr</code> 的支持。重定向日志背后的逻辑是最小的，因此它的开销几乎可以忽略不计。
另外，因为 <code>stdout</code>、<code>stderr</code> 由 kubelet 处理，你可以使用内置的工具 <code>kubectl logs</code>。</p>
<!--
For example, a pod runs a single container, and the container
writes to two different log files, using two different formats. Here's a
configuration file for the Pod:
-->
<p>例如，某 Pod 中运行一个容器，该容器向两个文件写不同格式的日志。
下面是这个 pod 的配置文件:</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/logging/two-files-counter-pod.yaml" download="admin/logging/two-files-counter-pod.yaml"><code>admin/logging/two-files-counter-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-logging-two-files-counter-pod-yaml')" title="Copy admin/logging/two-files-counter-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-logging-two-files-counter-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>counter<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>count<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- /bin/sh<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- &gt;<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">      i=0;
</span><span style="color:#b44;font-style:italic">      while true;
</span><span style="color:#b44;font-style:italic">      do
</span><span style="color:#b44;font-style:italic">        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span><span style="color:#b44;font-style:italic">        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span><span style="color:#b44;font-style:italic">        i=$((i+1));
</span><span style="color:#b44;font-style:italic">        sleep 1;
</span><span style="color:#b44;font-style:italic">      done</span><span style="color:#bbb">      
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
It is not recommended to write log entries with different formats to the same log
stream, even if you managed to redirect both components to the `stdout` stream of
the container. Instead, you can create two sidecar containers. Each sidecar
container could tail a particular log file from a shared volume and then redirect
the logs to its own `stdout` stream.
-->
<p>不建议在同一个日志流中写入不同格式的日志条目，即使你成功地将其重定向到容器的
<code>stdout</code> 流。相反，你可以创建两个边车容器。每个边车容器可以从共享卷
跟踪特定的日志文件，并将文件内容重定向到各自的 <code>stdout</code> 流。</p>
<!--
Here's a configuration file for a pod that has two sidecar containers:
-->
<p>下面是运行两个边车容器的 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml" download="admin/logging/two-files-counter-pod-streaming-sidecar.yaml"><code>admin/logging/two-files-counter-pod-streaming-sidecar.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-logging-two-files-counter-pod-streaming-sidecar-yaml')" title="Copy admin/logging/two-files-counter-pod-streaming-sidecar.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-logging-two-files-counter-pod-streaming-sidecar-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>counter<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>count<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- /bin/sh<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- &gt;<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">      i=0;
</span><span style="color:#b44;font-style:italic">      while true;
</span><span style="color:#b44;font-style:italic">      do
</span><span style="color:#b44;font-style:italic">        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span><span style="color:#b44;font-style:italic">        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span><span style="color:#b44;font-style:italic">        i=$((i+1));
</span><span style="color:#b44;font-style:italic">        sleep 1;
</span><span style="color:#b44;font-style:italic">      done</span><span style="color:#bbb">      
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>count-log-1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[/bin/sh, -c, &#39;tail -n+1 -F /var/log/1.log&#39;]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>count-log-2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[/bin/sh, -c, &#39;tail -n+1 -F /var/log/2.log&#39;]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Now when you run this pod, you can access each log stream separately by
running the following commands:
-->
<p>现在当你运行这个 Pod 时，你可以运行如下命令分别访问每个日志流：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs counter count-log-1
</code></pre></div><!--
The output is:
-->
<p>输出为：</p>
<pre><code class="language-console" data-lang="console">0: Mon Jan  1 00:00:00 UTC 2001
1: Mon Jan  1 00:00:01 UTC 2001
2: Mon Jan  1 00:00:02 UTC 2001
...
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs counter count-log-2
</code></pre></div><!--
The output is:
-->
<p>输出为：</p>
<pre><code class="language-console" data-lang="console">Mon Jan  1 00:00:00 UTC 2001 INFO 0
Mon Jan  1 00:00:01 UTC 2001 INFO 1
Mon Jan  1 00:00:02 UTC 2001 INFO 2
...
</code></pre><!--
The node-level agent installed in your cluster picks up those log streams
automatically without any further configuration. If you like, you can configure
the agent to parse log lines depending on the source container.
-->
<p>集群中安装的节点级代理会自动获取这些日志流，而无需进一步配置。
如果你愿意，你也可以配置代理程序来解析源容器的日志行。</p>
<!--
Note, that despite low CPU and memory usage (order of couple of millicores
for cpu and order of several megabytes for memory), writing logs to a file and
then streaming them to `stdout` can double disk usage. If you have
an application that writes to a single file, it's generally better to set
`/dev/stdout` as destination rather than implementing the streaming sidecar
container approach.
-->
<p>注意，尽管 CPU 和内存使用率都很低（以多个 CPU 毫核指标排序或者按内存的兆字节排序），
向文件写日志然后输出到 <code>stdout</code> 流仍然会成倍地增加磁盘使用率。
如果你的应用向单一文件写日志，通常最好设置 <code>/dev/stdout</code> 作为目标路径，
而不是使用流式的边车容器方式。</p>
<!--
Sidecar containers can also be used to rotate log files that cannot be
rotated by the application itself. An example of this approach is a small container running logrotate periodically.
However, it's recommended to use `stdout` and `stderr` directly and leave rotation
and retention policies to the kubelet.
-->
<p>应用本身如果不具备轮转日志文件的功能，可以通过边车容器实现。
该方式的一个例子是运行一个小的、定期轮转日志的容器。
然而，还是推荐直接使用 <code>stdout</code> 和 <code>stderr</code>，将日志的轮转和保留策略
交给 kubelet。</p>
<!--
#### Sidecar container with a logging agent

![Sidecar container with a logging agent](/images/docs/user-guide/logging/logging-with-sidecar-agent.png)
-->
<h3 id="具有日志代理功能的边车容器">具有日志代理功能的边车容器</h3>
<p><img src="/images/docs/user-guide/logging/logging-with-sidecar-agent.png" alt="含日志代理的边车容器"></p>
<!--
If the node-level logging agent is not flexible enough for your situation, you
can create a sidecar container with a separate logging agent that you have
configured specifically to run with your application.
-->
<p>如果节点级日志记录代理程序对于你的场景来说不够灵活，你可以创建一个
带有单独日志记录代理的边车容器，将代理程序专门配置为与你的应用程序一起运行。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
Using a logging agent in a sidecar container can lead
to significant resource consumption. Moreover, you won't be able to access
those logs using `kubectl logs` command, because they are not controlled
by the kubelet.
-->
<p>在边车容器中使用日志代理会带来严重的资源损耗。
此外，你不能使用 <code>kubectl logs</code> 命令访问日志，因为日志并没有被 kubelet 管理。
</div>
<!--
Here are two configuration files that you can use to implement a sidecar container with a logging agent. The first file contains
a [ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/) to configure fluentd.
-->
<p>下面是两个配置文件，可以用来实现一个带日志代理的边车容器。
第一个文件包含用来配置 fluentd 的
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/logging/fluentd-sidecar-config.yaml" download="admin/logging/fluentd-sidecar-config.yaml"><code>admin/logging/fluentd-sidecar-config.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-logging-fluentd-sidecar-config-yaml')" title="Copy admin/logging/fluentd-sidecar-config.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-logging-fluentd-sidecar-config-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">fluentd.conf</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    &lt;source&gt;
</span><span style="color:#b44;font-style:italic">      type tail
</span><span style="color:#b44;font-style:italic">      format none
</span><span style="color:#b44;font-style:italic">      path /var/log/1.log
</span><span style="color:#b44;font-style:italic">      pos_file /var/log/1.log.pos
</span><span style="color:#b44;font-style:italic">      tag count.format1
</span><span style="color:#b44;font-style:italic">    &lt;/source&gt;
</span><span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    &lt;source&gt;
</span><span style="color:#b44;font-style:italic">      type tail
</span><span style="color:#b44;font-style:italic">      format none
</span><span style="color:#b44;font-style:italic">      path /var/log/2.log
</span><span style="color:#b44;font-style:italic">      pos_file /var/log/2.log.pos
</span><span style="color:#b44;font-style:italic">      tag count.format2
</span><span style="color:#b44;font-style:italic">    &lt;/source&gt;
</span><span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    &lt;match **&gt;
</span><span style="color:#b44;font-style:italic">      type google_cloud
</span><span style="color:#b44;font-style:italic">    &lt;/match&gt;</span><span style="color:#bbb">    
</span></code></pre></div>
    </div>
</div>


<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
For information about configuring fluentd, see the [fluentd documentation](https://docs.fluentd.org/).
-->
<p>要进一步了解如何配置 fluentd，请参考 <a href="https://docs.fluentd.org/">fluentd 官方文档</a>。
</div>
<!--
The second file describes a pod that has a sidecar container running fluentd.
The pod mounts a volume where fluentd can pick up its configuration data.
-->
<p>第二个文件描述了运行 fluentd 边车容器的 Pod 。
flutend 通过 Pod 的挂载卷获取它的配置数据。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/logging/two-files-counter-pod-agent-sidecar.yaml" download="admin/logging/two-files-counter-pod-agent-sidecar.yaml"><code>admin/logging/two-files-counter-pod-agent-sidecar.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-logging-two-files-counter-pod-agent-sidecar-yaml')" title="Copy admin/logging/two-files-counter-pod-agent-sidecar.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-logging-two-files-counter-pod-agent-sidecar-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>counter<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>count<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:1.28<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- /bin/sh<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- &gt;<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">      i=0;
</span><span style="color:#b44;font-style:italic">      while true;
</span><span style="color:#b44;font-style:italic">      do
</span><span style="color:#b44;font-style:italic">        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span><span style="color:#b44;font-style:italic">        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span><span style="color:#b44;font-style:italic">        i=$((i+1));
</span><span style="color:#b44;font-style:italic">        sleep 1;
</span><span style="color:#b44;font-style:italic">      done</span><span style="color:#bbb">      
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>count-agent<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/fluentd-gcp:1.30<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>FLUENTD_ARGS<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>-c /etc/fluentd-config/fluentd.conf<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-volume<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/fluentd-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-config<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the sample configurations, you can replace fluentd with any logging agent, reading from any source inside an application container.
-->
<p>在示例配置中，你可以将 fluentd 替换为任何日志代理，从应用容器内
的任何来源读取数据。</p>
<!--
### Exposing logs directly from the application

![Exposing logs directly from the application](/images/docs/user-guide/logging/logging-from-application.png)
-->
<h3 id="从应用中直接暴露日志目录">从应用中直接暴露日志目录</h3>
<p><img src="/images/docs/user-guide/logging/logging-from-application.png" alt="直接从应用程序暴露日志"></p>
<!--
Cluster-logging that exposes or pushes logs directly from every application is outside the scope of Kubernetes.
-->
<p>从各个应用中直接暴露和推送日志数据的集群日志机制
已超出 Kubernetes 的范围。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5cc31ecfba86467f8884856412cfb6b2">11.6 - 系统日志</h1>
    
	<!-- 
reviewers:
- dims
- 44past4
title: System Logs
content_type: concept
weight: 60
-->
<!-- overview -->
<!--
System component logs record events happening in cluster, which can be very useful for debugging.
You can configure log verbosity to see more or less detail.
Logs can be as coarse-grained as showing errors within a component, or as fine-grained as showing step-by-step traces of events (like HTTP access logs, pod state changes, controller actions, or scheduler decisions).
-->
<p>系统组件的日志记录集群中发生的事件，这对于调试非常有用。
你可以配置日志的精细度，以展示更多或更少的细节。
日志可以是粗粒度的，如只显示组件内的错误，
也可以是细粒度的，如显示事件的每一个跟踪步骤（比如 HTTP 访问日志、pod 状态更新、控制器动作或调度器决策）。</p>
<!-- body -->
<h2 id="klog">Klog</h2>
<!--
klog is the Kubernetes logging library. [klog](https://github.com/kubernetes/klog)
generates log messages for the Kubernetes system components.

For more information about klog configuration, see the [Command line tool reference](/docs/reference/command-line-tools-reference/).
-->
<p>klog 是 Kubernetes 的日志库。
<a href="https://github.com/kubernetes/klog">klog</a>
为 Kubernetes 系统组件生成日志消息。</p>
<p>有关 klog 配置的更多信息，请参见<a href="/zh/docs/reference/command-line-tools-reference/">命令行工具参考</a>。</p>
<!--
Kubernetes is in the process of simplifying logging in its components. The
following klog command line flags [are
deprecated](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components)
starting with Kubernetes 1.23 and will be removed in a future release:
-->
<p>Kubernetes 正在进行简化其组件日志的努力。下面的 klog 命令行参数从 Kubernetes 1.23
开始<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components">已被废弃</a>，
会在未来版本中移除：</p>
<ul>
<li><code>--add-dir-header</code></li>
<li><code>--alsologtostderr</code></li>
<li><code>--log-backtrace-at</code></li>
<li><code>--log-dir</code></li>
<li><code>--log-file</code></li>
<li><code>--log-file-max-size</code></li>
<li><code>--logtostderr</code></li>
<li><code>--one-output</code></li>
<li><code>--skip-headers</code></li>
<li><code>--skip-log-headers</code></li>
<li><code>--stderrthreshold</code></li>
</ul>
<!--
Output will always be written to stderr, regardless of the output
format. Output redirection is expected to be handled by the component which
invokes a Kubernetes component. This can be a POSIX shell or a tool like
systemd.
-->
<p>输出总会被写到标准错误输出（stderr）之上，无论输出格式如何。
对输出的重定向将由调用 Kubernetes 组件的软件来处理。
这一软件可以是 POSIX Shell 或者类似 systemd 这样的工具。</p>
<!--
In some cases, for example a distroless container or a Windows system service,
those options are not available. Then the
[`kube-log-runner`](https://github.com/kubernetes/kubernetes/blob/d2a8a81639fcff8d1221b900f66d28361a170654/staging/src/k8s.io/component-base/logs/kube-log-runner/README.md)
binary can be used as wrapper around a Kubernetes component to redirect
output. A prebuilt binary is included in several Kubernetes base images under
its traditional name as `/go-runner` and as `kube-log-runner` in server and
node release archives.
-->
<p>在某些场合下，例如对于无发行主体的（distroless）容器或者 Windows 系统服务，
这些替代方案都是不存在的。那么你可以使用
<a href="https://github.com/kubernetes/kubernetes/blob/d2a8a81639fcff8d1221b900f66d28361a170654/staging/src/k8s.io/component-base/logs/kube-log-runner/README.md"><code>kube-log-runner</code></a>
可执行文件来作为 Kubernetes 的封装层，完成对输出的重定向。
在很多 Kubernetes 基础镜像中，都包含一个预先构建的可执行程序。
这个程序原来称作 <code>/go-runner</code>，而在服务器和节点的发行版本库中，称作 <code>kube-log-runner</code>。</p>
<!--
This table shows how `kube-log-runner` invocations correspond to shell redirection:
-->
<p>下表展示的是 <code>kube-log-runner</code> 调用与 Shell 重定向之间的对应关系：</p>
<!--
| Usage                                    | POSIX shell (such as bash) | `kube-log-runner <options> <cmd>`                           |
| -----------------------------------------|----------------------------|-------------------------------------------------------------|
| Merge stderr and stdout, write to stdout | `2>&1`                     | `kube-log-runner` (default behavior)                        |
| Redirect both into log file              | `1>>/tmp/log 2>&1`         | `kube-log-runner -log-file=/tmp/log`                        |
| Copy into log file and to stdout         | `2>&1 \| tee -a /tmp/log`  | `kube-log-runner -log-file=/tmp/log -also-stdout`           |
| Redirect only stdout into log file       | `>/tmp/log`                | `kube-log-runner -log-file=/tmp/log -redirect-stderr=false` |
-->
<table>
<thead>
<tr>
<th>用法</th>
<th>POSIX Shell（例如 Bash）</th>
<th><code>kube-log-runner &lt;options&gt; &lt;cmd&gt;</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>合并 stderr 与 stdout，写出到 stdout</td>
<td><code>2&gt;&amp;1</code></td>
<td><code>kube-log-runner</code>（默认行为 ）</td>
</tr>
<tr>
<td>将 stderr 与 stdout 重定向到日志文件</td>
<td><code>1&gt;&gt;/tmp/log 2&gt;&amp;1</code></td>
<td><code>kube-log-runner -log-file=/tmp/log</code></td>
</tr>
<tr>
<td>输出到 stdout 并复制到日志文件中</td>
<td><code>2&gt;&amp;1 | tee -a /tmp/log</code></td>
<td><code>kube-log-runner -log-file=/tmp/log -also-stdout</code></td>
</tr>
<tr>
<td>仅将 stdout 重定向到日志</td>
<td><code>&gt;/tmp/log</code></td>
<td><code>kube-log-runner -log-file=/tmp/log -redirect-stderr=false</code></td>
</tr>
</tbody>
</table>
<!--
### Klog output

An example of the traditional klog native format:
-->
<h3 id="klog-输出">klog 输出</h3>
<p>传统的 klog 原生格式示例：</p>
<pre><code>I1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756]
</code></pre><!--
The message string may contain line breaks:
-->
<p>消息字符串可能包含换行符：</p>
<pre><code>I1025 00:15:15.525108       1 example.go:79] This is a message
which has a line break.
</code></pre><!-- 
### Structured Logging 
-->
<h3 id="结构化日志">结构化日志</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
Migration to structured log messages is an ongoing process. Not all log messages are structured in this version. When parsing log files, you must also handle unstructured log messages.

Log formatting and value serialization are subject to change.
-->
<p>迁移到结构化日志消息是一个正在进行的过程。在此版本中，并非所有日志消息都是结构化的。
解析日志文件时，你也必须要处理非结构化日志消息。</p>
<p>日志格式和值的序列化可能会发生变化。</p>

</div>


<!--
Structured logging introduces a uniform structure in log messages allowing for programmatic extraction of information. You can store and process structured logs with less effort and cost.
The code which generates a log message determines whether it uses the traditional unstructured klog output
or structured logging.
-->
<p>结构化日志记录旨在日志消息中引入统一结构，以便以编程方式提取信息。
你可以方便地用更小的开销来处理结构化日志。
生成日志消息的代码决定其使用传统的非结构化的 klog 还是结构化的日志。</p>
<!--
The default formatting of structured log messages is as text, with a format that
is backward compatible with traditional klog:
-->
<p>默认的结构化日志消息是以文本形式呈现的，其格式与传统的 klog 保持向后兼容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#b44">&lt;klog header&gt; &#34;&lt;message&gt;&#34; &lt;key1&gt;</span><span style="color:#666">=</span><span style="color:#b44">&#34;&lt;value1&gt;&#34; &lt;key2&gt;=&#34;&lt;value2&gt;&#34; ...</span>
</code></pre></div><!--
Example:
-->
<p>示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ini" data-lang="ini"><span style="color:#b44">I1025 00:15:15.525108       1 controller_utils.go:116] &#34;Pod status updated&#34; pod</span><span style="color:#666">=</span><span style="color:#b44">&#34;kube-system/kubedns&#34; status=&#34;ready&#34;</span>
</code></pre></div><!--
Strings are quoted. Other values are formatted with
[`%+v`](https://pkg.go.dev/fmt#hdr-Printing), which may cause log messages to
continue on the next line [depending on the data](https://github.com/kubernetes/kubernetes/issues/106428).
-->
<p>字符串在输出时会被添加引号。其他数值类型都使用 <a href="https://pkg.go.dev/fmt#hdr-Printing"><code>%+v</code></a>
来格式化，因此可能导致日志消息会延续到下一行，
<a href="https://github.com/kubernetes/kubernetes/issues/106428">具体取决于数据本身</a>。</p>
<pre><code>I1025 00:15:15.525108       1 example.go:116] &quot;Example&quot; data=&quot;This is text with a line break\nand \&quot;quotation marks\&quot;.&quot; someInt=1 someFloat=0.1 someStruct={StringField: First line,
second line.}
</code></pre><!--
### JSON log format
-->
<h3 id="json-日志格式">JSON 日志格式</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.19 [alpha]</code>
</div>


<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
JSON output does not support many standard klog flags. For list of unsupported klog flags, see the [Command line tool reference](/docs/reference/command-line-tools-reference/).

Not all logs are guaranteed to be written in JSON format (for example, during process start). If you intend to parse logs, make sure you can handle log lines that are not JSON as well.

Field names and JSON serialization are subject to change.
-->
<p>JSON 输出并不支持太多标准 klog 参数。对于不受支持的 klog 参数的列表，
请参见<a href="/zh/docs/reference/command-line-tools-reference/">命令行工具参考</a>。</p>
<p>并不是所有日志都保证写成 JSON 格式（例如，在进程启动期间）。
如果你打算解析日志，请确保可以处理非 JSON 格式的日志行。</p>
<p>字段名和 JSON 序列化可能会发生变化。</p>

</div>


<!--
The `--logging-format=json` flag changes the format of logs from klog native format to JSON format.
Example of JSON log format (pretty printed):
-->
<p><code>--logging-format=json</code> 参数将日志格式从 klog 原生格式改为 JSON 格式。
JSON 日志格式示例（美化输出）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#008000;font-weight:bold">&#34;ts&#34;</span>: <span style="color:#666">1580306777.04728</span>,
   <span style="color:#008000;font-weight:bold">&#34;v&#34;</span>: <span style="color:#666">4</span>,
   <span style="color:#008000;font-weight:bold">&#34;msg&#34;</span>: <span style="color:#b44">&#34;Pod status updated&#34;</span>,
   <span style="color:#008000;font-weight:bold">&#34;pod&#34;</span>:{
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;nginx-1&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;default&#34;</span>
   },
   <span style="color:#008000;font-weight:bold">&#34;status&#34;</span>: <span style="color:#b44">&#34;ready&#34;</span>
}
</code></pre></div><!--
Keys with special meaning:

* `ts` - timestamp as Unix time (required, float)
* `v` - verbosity (only for info and not for error messages, int)
* `err` - error string (optional, string)
* `msg` - message (required, string)

List of components currently supporting JSON format:
-->
<p>具有特殊意义的 key：</p>
<ul>
<li><code>ts</code> - Unix 时间风格的时间戳（必选项，浮点值）</li>
<li><code>v</code> - 精细度（仅用于 info 级别，不能用于错误信息，整数）</li>
<li><code>err</code> - 错误字符串（可选项，字符串）</li>
<li><code>msg</code> - 消息（必选项，字符串）</li>
</ul>
<p>当前支持JSON格式的组件列表：</p>
<ul>
<li><a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a></li>
<li><a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='kube-apiserver'>kube-apiserver</a></li>
<li><a class='glossary-tooltip' title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-scheduler/' target='_blank' aria-label='kube-scheduler'>kube-scheduler</a></li>
<li><a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a></li>
</ul>
<!--
### Log sanitization
-->
<h3 id="log-sanitization">日志清洗   </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>


<div class="alert alert-danger warning callout" role="alert">
  <strong>Warning:</strong> <!--
Log sanitization might incur significant computation overhead and therefore should not be enabled in production.
-->
<p>日志清洗（Log Sanitization）可能会导致大量的计算开销，因此不应在生产环境中启用。
</div>


<!--
The `--experimental-logging-sanitization` flag enables the klog sanitization filter.
If enabled all log arguments are inspected for fields tagged as sensitive data (e.g. passwords, keys, tokens) and logging of these fields will be prevented.
-->
<p><code>--experimental-logging-sanitization</code> 参数可用来启用 klog 清洗过滤器。
如果启用后，将检查所有日志参数中是否有标记为敏感数据的字段（比如：密码，密钥，令牌），
并且将阻止这些字段的记录。</p>
<!--
List of components currently supporting log sanitization:
-->
<p>当前支持日志清洗的组件列表：</p>
<ul>
<li>kube-controller-manager</li>
<li>kube-apiserver</li>
<li>kube-scheduler</li>
<li>kubelet</li>
</ul>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
The Log sanitization filter does not prevent user workload logs from leaking sensitive data.
-->
<p>日志清洗过滤器不会阻止用户工作负载日志泄漏敏感数据。
</div>
<!--
### Log verbosity level

The `-v` flag controls log verbosity. Increasing the value increases the number of logged events. Decreasing the value decreases the number of logged events.
Increasing verbosity settings logs increasingly less severe events. A verbosity setting of 0 logs only critical events.
-->
<h3 id="日志精细度级别">日志精细度级别</h3>
<p>参数 <code>-v</code> 控制日志的精细度。增大该值会增大日志事件的数量。
减小该值可以减小日志事件的数量。增大精细度会记录更多的不太严重的事件。
精细度设置为 0 时只记录关键（critical）事件。</p>
<!--
### Log location

There are two types of system components: those that run in a container and those
that do not run in a container. For example:

* The Kubernetes scheduler and kube-proxy run in a container.
* The kubelet and <a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>
  do not run in containers.
-->
<h3 id="日志位置">日志位置</h3>
<p>有两种类型的系统组件：运行在容器中的组件和不运行在容器中的组件。例如：</p>
<ul>
<li>Kubernetes 调度器和 kube-proxy 在容器中运行。</li>
<li>kubelet 和<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>不在容器中运行。</li>
</ul>
<!--
On machines with systemd, the kubelet and container runtime write to journald.
Otherwise, they write to `.log` files in the `/var/log` directory.
System components inside containers always write to `.log` files in the `/var/log` directory,
bypassing the default logging mechanism.
Similar to the container logs, you should rotate system component logs in the `/var/log` directory.
In Kubernetes clusters created by the `kube-up.sh` script, log rotation is configured by the `logrotate` tool.
The `logrotate` tool rotates logs daily, or once the log size is greater than 100MB.
-->
<p>在使用 systemd 的系统中，kubelet 和容器运行时写入 journald。
在别的系统中，日志写入 <code>/var/log</code> 目录下的 <code>.log</code> 文件中。
容器中的系统组件总是绕过默认的日志记录机制，写入 <code>/var/log</code> 目录下的 <code>.log</code> 文件。
与容器日志类似，你应该轮转 <code>/var/log</code> 目录下系统组件日志。
在 <code>kube-up.sh</code> 脚本创建的 Kubernetes 集群中，日志轮转由 <code>logrotate</code> 工具配置。
<code>logrotate</code> 工具，每天或者当日志大于 100MB 时，轮转日志。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Read about the [Kubernetes Logging Architecture](/docs/concepts/cluster-administration/logging/)
* Read about [Structured Logging](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging)
* Read about [deprecation of klog flags](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components)
* Read about the [Conventions for logging severity](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md)
-->
<ul>
<li>阅读 <a href="/zh/docs/concepts/cluster-administration/logging/">Kubernetes 日志架构</a></li>
<li>阅读<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging">结构化日志提案（英文）</a></li>
<li>阅读 <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components">klog 参数的废弃（英文）</a></li>
<li>阅读<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md">日志严重级别约定（英文）</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3da54ad355f6fe6574d67bd9a9a42bcb">11.7 - 追踪 Kubernetes 系统组件</h1>
    
	<!-- 
---
title: Traces For Kubernetes System Components
reviewers:
- logicalhan
- lilic
content_type: concept
weight: 60
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>


<!-- 
System component traces record the latency of and relationships between operations in the cluster.
-->
<p>系统组件追踪功能记录各个集群操作的时延信息和这些操作之间的关系。</p>
<!-- 
Kubernetes components emit traces using the
[OpenTelemetry Protocol](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/otlp.md#opentelemetry-protocol-specification)
with the gRPC exporter and can be collected and routed to tracing backends using an
[OpenTelemetry Collector](https://github.com/open-telemetry/opentelemetry-collector#-opentelemetry-collector).
-->
<p>Kubernetes 组件基于 gRPC 导出器的
<a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/otlp.md#opentelemetry-protocol-specification">OpenTelemetry 协议</a>
发送追踪信息，并用
<a href="https://github.com/open-telemetry/opentelemetry-collector#-opentelemetry-collector">OpenTelemetry Collector</a>
收集追踪信息，再将其转交给追踪系统的后台。</p>
<!-- body -->
<!-- 
## Trace Collection
-->
<h2 id="trace-collection">追踪信息的收集</h2>
<!-- 
For a complete guide to collecting traces and using the collector, see
[Getting Started with the OpenTelemetry Collector](https://opentelemetry.io/docs/collector/getting-started/).
However, there are a few things to note that are specific to Kubernetes components.
-->
<p>关于收集追踪信息、以及使用收集器的完整指南，可参见
<a href="https://opentelemetry.io/docs/collector/getting-started/">Getting Started with the OpenTelemetry Collector</a>。
不过，还有一些特定于 Kubernetes 组件的事项值得注意。</p>
<!-- 
By default, Kubernetes components export traces using the grpc exporter for OTLP on the
[IANA OpenTelemetry port](https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=opentelemetry), 4317.
As an example, if the collector is running as a sidecar to a Kubernetes component,
the following receiver configuration will collect spans and log them to standard output:
-->
<p>默认情况下，Kubernetes 组件使用 gRPC 的 OTLP 导出器来导出追踪信息，将信息写到
<a href="https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=opentelemetry">IANA OpenTelemetry 端口</a>。
举例来说，如果收集器以 Kubernetes 组件的边车模式运行，以下接收器配置会收集 spans 信息，并将它们写入到标准输出。</p>
<!-- 
# Replace this exporter with the exporter for your backend
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">receivers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">otlp</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocols</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">grpc</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">exporters</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 用适合你后端环境的导出器替换此处的导出器</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">logging</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">logLevel</span>:<span style="color:#bbb"> </span>debug<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">pipelines</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">traces</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">receivers</span>:<span style="color:#bbb"> </span>[otlp]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">exporters</span>:<span style="color:#bbb"> </span>[logging]<span style="color:#bbb">
</span></code></pre></div><!-- 
## Component traces

### kube-apiserver traces
-->
<h2 id="component-traces">组件追踪</h2>
<h3 id="kube-apiserver-traces">kube-apiserver 追踪</h3>
<!-- 
The kube-apiserver generates spans for incoming HTTP requests, and for outgoing requests
to webhooks, etcd, and re-entrant requests. It propagates the
[W3C Trace Context](https://www.w3.org/TR/trace-context/) with outgoing requests
but does not make use of the trace context attached to incoming requests,
as the kube-apiserver is often a public endpoint.
-->
<p>kube-apiserver 为传入的 HTTP 请求、传出到 webhook 和 etcd 的请求以及重入的请求生成 spans。
由于 kube-apiserver 通常是一个公开的端点，所以它通过出站的请求传播
<a href="https://www.w3.org/TR/trace-context/">W3C 追踪上下文</a>，
但不使用入站请求的追踪上下文。</p>
<!-- 
#### Enabling tracing in the kube-apiserver
-->
<h4 id="enabling-tracing-in-the-kube-apiserver">在 kube-apiserver 中启用追踪</h4>
<!-- 
To enable tracing, enable the `APIServerTracing`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
on the kube-apiserver. Also, provide the kube-apiserver with a tracing configration file
with `--tracing-config-file=<path-to-config>`. This is an example config that records
spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:
-->
<p>要启用追踪特性，需要启用 kube-apiserver 上的  <code>APIServerTracing</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
然后，使用 <code>--tracing-config-file=&lt;&lt;配置文件路径&gt;</code> 为 kube-apiserver 提供追踪配置文件。
下面是一个示例配置，它为万分之一的请求记录 spans，并使用了默认的 OpenTelemetry 端口。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1alpha1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>TracingConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># default value</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic">#endpoint: localhost:4317</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">samplingRatePerMillion</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></code></pre></div><!-- 
For more information about the `TracingConfiguration` struct, see
[API server config API (v1alpha1)](/docs/reference/config-api/apiserver-config.v1alpha1/#apiserver-k8s-io-v1alpha1-TracingConfiguration).
-->
<p>有关 TracingConfiguration 结构体的更多信息，请参阅
<a href="/zh/docs/reference/config-api/apiserver-config.v1alpha1/#apiserver-k8s-io-v1alpha1-TracingConfiguration">API 服务器配置 API (v1alpha1)</a>。</p>
<!-- 
## Stability
-->
<h2 id="stability">稳定性</h2>
<!-- 
Tracing instrumentation is still under active development, and may change
in a variety of ways. This includes span names, attached attributes,
instrumented endpoints, etc. Until this feature graduates to stable,
there are no guarantees of backwards compatibility for tracing instrumentation.
-->
<p>追踪工具仍在积极开发中，未来它会以多种方式发生变化。
这些变化包括：span 名称、附加属性、检测端点等等。
此类特性在达到稳定版本之前，不能保证追踪工具的向后兼容性。</p>
<h2 id="what-s-next">What's next</h2>
<!-- 
* Read about [Getting Started with the OpenTelemetry Collector](https://opentelemetry.io/docs/collector/getting-started/)
-->
<ul>
<li>阅读<a href="https://opentelemetry.io/docs/collector/getting-started/">Getting Started with the OpenTelemetry Collector</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-08e94e6a480e0d6b2de72d84a1b97617">11.8 - Kubernetes 中的代理</h1>
    
	<!--
title: Proxies in Kubernetes
content_type: concept
weight: 90
-->
<!-- overview -->
<!--
This page explains proxies used with Kubernetes.
-->
<p>本文讲述了 Kubernetes 中所使用的代理。</p>
<!-- body -->
<!--
## Proxies

There are several different proxies you may encounter when using Kubernetes:
-->
<h2 id="proxies">代理 </h2>
<p>用户在使用 Kubernetes 的过程中可能遇到几种不同的代理（proxy）：</p>
<!--
1.  The [kubectl proxy](/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api):

    - runs on a user's desktop or in a pod
    - proxies from a localhost address to the Kubernetes apiserver
    - client to proxy uses HTTP
    - proxy to apiserver uses HTTPS
    - locates apiserver
    - adds authentication headers
-->
<ol>
<li>
<p><a href="/zh/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api">kubectl proxy</a>：</p>
<ul>
<li>运行在用户的桌面或 pod 中</li>
<li>从本机地址到 Kubernetes apiserver 的代理</li>
<li>客户端到代理使用 HTTP 协议</li>
<li>代理到 apiserver 使用 HTTPS 协议</li>
<li>指向 apiserver</li>
<li>添加认证头信息</li>
</ul>
</li>
</ol>
<!--
1.  The [apiserver proxy](/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services):

    - is a bastion built into the apiserver
    - connects a user outside of the cluster to cluster IPs which otherwise might not be reachable
    - runs in the apiserver processes
    - client to proxy uses HTTPS (or http if apiserver so configured)
    - proxy to target may use HTTP or HTTPS as chosen by proxy using available information
    - can be used to reach a Node, Pod, or Service
    - does load balancing when used to reach a Service
-->
<ol start="2">
<li>
<p><a href="/zh/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services">apiserver proxy</a>：</p>
<ul>
<li>是一个建立在 apiserver 内部的“堡垒”</li>
<li>将集群外部的用户与群集 IP 相连接，这些IP是无法通过其他方式访问的</li>
<li>运行在 apiserver 进程内</li>
<li>客户端到代理使用 HTTPS 协议 (如果配置 apiserver 使用 HTTP 协议，则使用 HTTP 协议)</li>
<li>通过可用信息进行选择，代理到目的地可能使用 HTTP 或 HTTPS 协议</li>
<li>可以用来访问 Node、 Pod 或 Service</li>
<li>当用来访问 Service 时，会进行负载均衡</li>
</ul>
</li>
</ol>
<!--
1.  The [kube proxy](/docs/concepts/services-networking/service/#ips-and-vips):

    - runs on each node
    - proxies UDP, TCP and SCTP
    - does not understand HTTP
    - provides load balancing
    - is only used to reach services
-->
<ol start="3">
<li>
<p><a href="/zh/docs/concepts/services-networking/service/#ips-and-vips">kube proxy</a>：</p>
<ul>
<li>在每个节点上运行</li>
<li>代理 UDP、TCP 和 SCTP</li>
<li>不支持 HTTP</li>
<li>提供负载均衡能力</li>
<li>只用来访问 Service</li>
</ul>
</li>
</ol>
<!--
1.  A Proxy/Load-balancer in front of apiserver(s):

    - existence and implementation varies from cluster to cluster (e.g. nginx)
    - sits between all clients and one or more apiservers
    - acts as load balancer if there are several apiservers.
-->
<ol start="4">
<li>
<p>apiserver 之前的代理/负载均衡器：</p>
<ul>
<li>在不同集群中的存在形式和实现不同 (如 nginx)</li>
<li>位于所有客户端和一个或多个 API 服务器之间</li>
<li>存在多个 API 服务器时，扮演负载均衡器的角色</li>
</ul>
</li>
</ol>
<!--
1.  Cloud Load Balancers on external services:

    - are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)
    - are created automatically when the Kubernetes service has type `LoadBalancer`
    - usually supports UDP/TCP only
    - SCTP support is up to the load balancer implementation of the cloud provider
    - implementation varies by cloud provider.
-->
<ol start="5">
<li>
<p>外部服务的云负载均衡器：</p>
<ul>
<li>由一些云供应商提供 (如 AWS ELB、Google Cloud Load Balancer)</li>
<li>Kubernetes 服务类型为 <code>LoadBalancer</code> 时自动创建</li>
<li>通常仅支持 UDP/TCP 协议</li>
<li>SCTP 支持取决于云供应商的负载均衡器实现</li>
<li>不同云供应商的云负载均衡器实现不同</li>
</ul>
</li>
</ol>
<!--
Kubernetes users will typically not need to worry about anything other than the first two types.  The cluster admin
will typically ensure that the latter types are setup correctly.
-->
<p>Kubernetes 用户通常只需要关心前两种类型的代理，集群管理员通常需要确保后面几种类型的代理设置正确。</p>
<!--
## Requesting redirects

Proxies have replaced redirect capabilities.  Redirects have been deprecated.
-->
<h2 id="请求重定向">请求重定向</h2>
<p>代理已经取代重定向功能，重定向功能已被弃用。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-31c9327d2332c585341b64ddafa19cdd">11.9 - API 优先级和公平性</h1>
    
	<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code>
</div>


<!--
Controlling the behavior of the Kubernetes API server in an overload situation
is a key task for cluster administrators. The <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='kube-apiserver'>kube-apiserver</a> has some controls available
(i.e. the `--max-requests-inflight` and `--max-mutating-requests-inflight`
command-line flags) to limit the amount of outstanding work that will be
accepted, preventing a flood of inbound requests from overloading and
potentially crashing the API server, but these flags are not enough to ensure
that the most important requests get through in a period of high traffic.
-->
<p>对于集群管理员来说，控制 Kubernetes API 服务器在过载情况下的行为是一项关键任务。
<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='kube-apiserver'>kube-apiserver</a>
有一些控件（例如：命令行标志 <code>--max-requests-inflight</code> 和 <code>--max-mutating-requests-inflight</code> ）,
可以限制将要接受的未处理的请求，从而防止过量请求入站，潜在导致 API 服务器崩溃。
但是这些标志不足以保证在高流量期间，最重要的请求仍能被服务器接受。</p>
<!--
The API Priority and Fairness feature (APF) is an alternative that improves upon
aforementioned max-inflight limitations. APF classifies
and isolates requests in a more fine-grained way. It also introduces
a limited amount of queuing, so that no requests are rejected in cases
of very brief bursts.  Requests are dispatched from queues using a
fair queuing technique so that, for example, a poorly-behaved
<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> need not
starve others (even at the same priority level).
-->
<p>API 优先级和公平性（APF）是一种替代方案，可提升上述最大并发限制。
APF 以更细粒度的方式对请求进行分类和隔离。
它还引入了空间有限的排队机制，因此在非常短暂的突发情况下，API 服务器不会拒绝任何请求。
通过使用公平排队技术从队列中分发请求，这样，
一个行为不佳的 <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>
就不会饿死其他控制器（即使优先级相同）。</p>
<!--
This feature is designed to work well with standard controllers, which
use informers and react to failures of API requests with exponential
back-off, and other clients that also work this way.
-->
<p>本功能特性在设计上期望其能与标准控制器一起工作得很好；
这类控制器使用通知组件（Informers）获得信息并对 API 请求的失效作出反应，
在处理失效时能够执行指数型回退。其他客户端也以类似方式工作。</p>
<!--
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> Requests classified as &quot;long-running&quot; — primarily watches — are not
subject to the API Priority and Fairness filter. This is also true for
the <code>--max-requests-inflight</code> flag without the API Priority and
Fairness feature enabled.
</div>

-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 属于“长时间运行”类型的请求（主要是 watch）不受 API 优先级和公平性过滤器的约束。
如果未启用 APF 特性，即便设置 <code>--max-requests-inflight</code> 标志，该类请求也不受约束。
</div>

<!-- body -->
<!--
## Enabling/Disabling API Priority and Fairness
-->
<h2 id="enabling-api-priority-and-fairness">启用/禁用 API 优先级和公平性   </h2>
<!--
The API Priority and Fairness feature is controlled by a feature gate
and is enabled by default.  See
[Feature Gates](/docs/reference/command-line-tools-reference/feature-gates/)
for a general explanation of feature gates and how to enable and
disable them.  The name of the feature gate for APF is
"APIPriorityAndFairness".  This feature also involves an <a class='glossary-tooltip' title='Kubernetes API 中的一组相关路径' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning' target='_blank' aria-label='API Group'>API Group</a> with: (a) a
`v1alpha1` version, disabled by default, and (b) a `v1beta1` and
`v1beta21 versions,  enabled by default.  You can disable the feature
gate and API group beta version by adding the following
command-line flags to your `kube-apiserver` invocation:
-->
<p>API 优先级与公平性（APF）特性由特性门控控制，默认情况下启用。
有关特性门控的一般性描述以及如何启用和禁用特性门控，
请参见<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
APF 的特性门控称为 <code>APIPriorityAndFairness</code>。
此特性也与某个 <a class='glossary-tooltip' title='Kubernetes API 中的一组相关路径' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning' target='_blank' aria-label='API 组'>API 组</a>
相关：
(a) <code>v1alpha1</code> 版本，默认被禁用；
(b) <code>v1beta1</code> 和 <code>v1beta2</code> 版本，默认被启用。
你可以在启动 <code>kube-apiserver</code> 时，添加以下命令行标志来禁用此功能门控及 API Beta 组：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kube-apiserver <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--feature-gates<span style="color:#666">=</span><span style="color:#b8860b">APIPriorityAndFairness</span><span style="color:#666">=</span><span style="color:#a2f">false</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--runtime-config<span style="color:#666">=</span>flowcontrol.apiserver.k8s.io/v1beta1<span style="color:#666">=</span>false,flowcontrol.apiserver.k8s.io/v1beta2<span style="color:#666">=</span><span style="color:#a2f">false</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#080;font-style:italic"># ...其他配置不变</span>
</code></pre></div><!--
Alternatively, you can enable the v1alpha1 version of the API group
with `--runtime-config=flowcontrol.apiserver.k8s.io/v1alpha1=true`.
-->
<p>或者，你也可以通过
<code>--runtime-config=flowcontrol.apiserver.k8s.io/v1alpha1=true</code>
启用 API 组的 v1alpha1 版本。</p>
<!--
The command-line flag `--enable-priority-and-fairness=false` will disable the
API Priority and Fairness feature, even if other flags have enabled it.
-->
<p>命令行标志 <code>--enable-priority-fairness=false</code> 将彻底禁用 APF 特性，即使其他标志启用它也是无效。</p>
<!--
## Concepts
There are several distinct features involved in the API Priority and Fairness
feature. Incoming requests are classified by attributes of the request using
_FlowSchemas_, and assigned to priority levels. Priority levels add a degree of
isolation by maintaining separate concurrency limits, so that requests assigned
to different priority levels cannot starve each other. Within a priority level,
a fair-queuing algorithm prevents requests from different _flows_ from starving
each other, and allows for requests to be queued to prevent bursty traffic from
causing failed requests when the average load is acceptably low.
-->
<h2 id="concepts">概念   </h2>
<p>APF 特性包含几个不同的功能。
传入的请求通过 <em>FlowSchema</em> 按照其属性分类，并分配优先级。
每个优先级维护自定义的并发限制，加强了隔离度，这样不同优先级的请求，就不会相互饿死。
在同一个优先级内，公平排队算法可以防止来自不同 <em>flow</em> 的请求相互饿死。
该算法将请求排队，通过排队机制，防止在平均负载较低时，通信量突增而导致请求失败。</p>
<!--
### Priority Levels
Without APF enabled, overall concurrency in the API server is limited by the
`kube-apiserver` flags `--max-requests-inflight` and
`--max-mutating-requests-inflight`. With APF enabled, the concurrency limits
defined by these flags are summed and then the sum is divided up among a
configurable set of _priority levels_. Each incoming request is assigned to a
single priority level, and each priority level will only dispatch as many
concurrent requests as its configuration allows.
-->
<h3 id="Priority-Levels">优先级   </h3>
<p>如果未启用 APF，API 服务器中的整体并发量将受到 <code>kube-apiserver</code> 的参数
<code>--max-requests-inflight</code> 和 <code>--max-mutating-requests-inflight</code> 的限制。
启用 APF 后，将对这些参数定义的并发限制进行求和，然后将总和分配到一组可配置的 <em>优先级</em> 中。
每个传入的请求都会分配一个优先级；每个优先级都有各自的配置，设定允许分发的并发请求数。</p>
<!--
The default configuration, for example, includes separate priority levels for
leader-election requests, requests from built-in controllers, and requests from
Pods. This means that an ill-behaved Pod that floods the API server with
requests cannot prevent leader election or actions by the built-in controllers
from succeeding.
-->
<p>例如，默认配置包括针对领导者选举请求、内置控制器请求和 Pod 请求都单独设置优先级。
这表示即使异常的 Pod 向 API 服务器发送大量请求，也无法阻止领导者选举或内置控制器的操作执行成功。</p>
<!--
### Queuing
Even within a priority level there may be a large number of distinct sources of
traffic. In an overload situation, it is valuable to prevent one stream of
requests from starving others (in particular, in the relatively common case of a
single buggy client flooding the kube-apiserver with requests, that buggy client
would ideally not have much measurable impact on other clients at all). This is
handled by use of a fair-queuing algorithm to process requests that are assigned
the same priority level. Each request is assigned to a _flow_, identified by the
name of the matching FlowSchema plus a _flow distinguisher_ — which
is either the requesting user, the target resource's namespace, or nothing — and the
system attempts to give approximately equal weight to requests in different
flows of the same priority level.
To enable distinct handling of distinct instances, controllers that have
many instances should authenticate with distinct usernames
-->
<h3 id="Queuing">排队   </h3>
<p>即使在同一优先级内，也可能存在大量不同的流量源。
在过载情况下，防止一个请求流饿死其他流是非常有价值的
（尤其是在一个较为常见的场景中，一个有故障的客户端会疯狂地向 kube-apiserver 发送请求，
理想情况下，这个有故障的客户端不应对其他客户端产生太大的影响）。
公平排队算法在处理具有相同优先级的请求时，实现了上述场景。
每个请求都被分配到某个 <em>流</em> 中，该 <em>流</em> 由对应的 FlowSchema 的名字加上一个
<em>流区分项（Flow Distinguisher）</em> 来标识。
这里的流区分项可以是发出请求的用户、目标资源的名称空间或什么都不是。
系统尝试为不同流中具有相同优先级的请求赋予近似相等的权重。
要启用对不同实例的不同处理方式，多实例的控制器要分别用不同的用户名来执行身份认证。</p>
<!--
After classifying a request into a flow, the API Priority and Fairness
feature then may assign the request to a queue.  This assignment uses
a technique known as <a class='glossary-tooltip' title='一种将请求指派给队列的技术，其隔离性好过对队列个数哈希取模的方式。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-shuffle-sharding' target='_blank' aria-label='shuffle sharding'>shuffle sharding</a>, which makes relatively efficient use of
queues to insulate low-intensity flows from high-intensity flows.
-->
<p>将请求划分到流中之后，APF 功能将请求分配到队列中。
分配时使用一种称为 <a class='glossary-tooltip' title='一种将请求指派给队列的技术，其隔离性好过对队列个数哈希取模的方式。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-shuffle-sharding' target='_blank' aria-label='混洗分片（Shuffle-Sharding）'>混洗分片（Shuffle-Sharding）</a>
的技术。
该技术可以相对有效地利用队列隔离低强度流与高强度流。</p>
<!--
The details of the queuing algorithm are tunable for each priority level, and
allow administrators to trade off memory use, fairness (the property that
independent flows will all make progress when total traffic exceeds capacity),
tolerance for bursty traffic, and the added latency induced by queuing.
-->
<p>排队算法的细节可针对每个优先等级进行调整，并允许管理员在内存占用、
公平性（当总流量超标时，各个独立的流将都会取得进展）、
突发流量的容忍度以及排队引发的额外延迟之间进行权衡。</p>
<!--
### Exempt requests

Some requests are considered sufficiently important that they are not subject to
any of the limitations imposed by this feature. These exemptions prevent an
improperly-configured flow control configuration from totally disabling an API
server.
-->
<h3 id="Exempt-requests">豁免请求   </h3>
<p>某些特别重要的请求不受制于此特性施加的任何限制。这些豁免可防止不当的流控配置完全禁用 API 服务器。</p>
<!--
## Resources

The flow control API involves two kinds of resources.
[PriorityLevelConfigurations](/docs/reference/generated/kubernetes-api/v1.23/#prioritylevelconfiguration-v1beta1-flowcontrol-apiserver-k8s-io)
define the available isolation classes, the share of the available concurrency
budget that each can handle, and allow for fine-tuning queuing behavior.
[FlowSchemas](/docs/reference/generated/kubernetes-api/v1.23/#flowschema-v1beta1-flowcontrol-apiserver-k8s-io)
are used to classify individual inbound requests, matching each to a
single PriorityLevelConfiguration. There is also a `v1alpha1` version
of the same API group, and it has the same Kinds with the same syntax and
semantics.
-->
<h2 id="Resources">资源   </h2>
<p>流控 API 涉及两种资源。
<a href="/docs/reference/generated/kubernetes-api/v1.23/#prioritylevelconfiguration-v1alpha1-flowcontrol-apiserver-k8s-io">PriorityLevelConfigurations</a>
定义隔离类型和可处理的并发预算量，还可以微调排队行为。
<a href="/docs/reference/generated/kubernetes-api/v1.23/#flowschema-v1alpha1-flowcontrol-apiserver-k8s-io">FlowSchemas</a>
用于对每个入站请求进行分类，并与一个 PriorityLevelConfigurations 相匹配。
此外同一 API 组还有一个 <code>v1alpha1</code> 版本，其中包含语法和语义都相同的资源类别。</p>
<!--
### PriorityLevelConfiguration
A PriorityLevelConfiguration represents a single isolation class. Each
PriorityLevelConfiguration has an independent limit on the number of outstanding
requests, and limitations on the number of queued requests.
-->
<h3 id="PriorityLevelConfiguration">PriorityLevelConfiguration   </h3>
<p>一个 PriorityLevelConfiguration 表示单个隔离类型。每个 PriorityLevelConfigurations
对未完成的请求数有各自的限制，对排队中的请求数也有限制。</p>
<!--
Concurrency limits for PriorityLevelConfigurations are not specified in absolute
number of requests, but rather in "concurrency shares." The total concurrency
limit for the API Server is distributed among the existing
PriorityLevelConfigurations in proportion with these shares. This allows a
cluster administrator to scale up or down the total amount of traffic to a
server by restarting `kube-apiserver` with a different value for
`--max-requests-inflight` (or `--max-mutating-requests-inflight`), and all
PriorityLevelConfigurations will see their maximum allowed concurrency go up (or
down) by the same fraction.
-->
<p>PriorityLevelConfigurations 的并发限制不是指定请求绝对数量，而是在“并发份额”中指定。
API 服务器的总并发量限制通过这些份额按例分配到现有 PriorityLevelConfigurations 中。
集群管理员可以更改 <code>--max-requests-inflight</code> （或 <code>--max-mutating-requests-inflight</code> ）的值，
再重新启动 <code>kube-apiserver</code> 来增加或减小服务器的总流量，
然后所有的 PriorityLevelConfigurations 将看到其最大并发增加（或减少）了相同的比例。</p>
<!--
With the Priority and Fairness feature enabled, the total concurrency limit for
the server is set to the sum of `--max-requests-inflight` and
`--max-mutating-requests-inflight`. There is no longer any distinction made
between mutating and non-mutating requests; if you want to treat them
separately for a given resource, make separate FlowSchemas that match the
mutating and non-mutating verbs respectively.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 启用 APF 功能后，服务器的总并发量限制将设置为
<code>--max-requests-inflight</code> 和 <code>--max-mutating-requests-inflight</code> 之和。
可变请求和不可变请求之间不再有任何区别；
如果对于某种资源，你需要区别对待不同请求，请创建不同的 FlowSchema 分别匹配可变请求和不可变请求。
</div>

<!--
When the volume of inbound requests assigned to a single
PriorityLevelConfiguration is more than its permitted concurrency level, the
`type` field of its specification determines what will happen to extra requests.
A type of `Reject` means that excess traffic will immediately be rejected with
an HTTP 429 (Too Many Requests) error. A type of `Queue` means that requests
above the threshold will be queued, with the shuffle sharding and fair queuing techniques used
to balance progress between request flows.
-->
<p>当入站请求的数量大于分配的 PriorityLevelConfigurations 中允许的并发级别时， <code>type</code> 字段将确定对额外请求的处理方式。
<code>Reject</code> 类型，表示多余的流量将立即被 HTTP 429（请求过多）错误所拒绝。
<code>Queue</code> 类型，表示对超过阈值的请求进行排队，将使用阈值分片和公平排队技术来平衡请求流之间的进度。</p>
<!--
The queuing configuration allows tuning the fair queuing algorithm for a
priority level. Details of the algorithm can be read in the
[enhancement proposal](#whats-next), but in short:
-->
<p>公平排队算法支持通过排队配置对优先级微调。 可以在<a href="#whats-next">增强建议</a>中阅读算法的详细信息，但总之：</p>
<!--
* Increasing `queues` reduces the rate of collisions between different flows, at
  the cost of increased memory usage. A value of 1 here effectively disables the
  fair-queuing logic, but still allows requests to be queued.
-->
<ul>
<li><code>queues</code> 递增能减少不同流之间的冲突概率，但代价是增加了内存使用量。
值为 1 时，会禁用公平排队逻辑，但仍允许请求排队。</li>
</ul>
<!--
* Increasing `queueLengthLimit` allows larger bursts of traffic to be
  sustained without dropping any requests, at the cost of increased
  latency and memory usage.
-->
<ul>
<li><code>queueLengthLimit</code> 递增可以在不丢弃任何请求的情况下支撑更大的突发流量，
但代价是增加了等待时间和内存使用量。</li>
</ul>
<!--
* Changing `handSize` allows you to adjust the probability of collisions between
  different flows and the overall concurrency available to a single flow in an
  overload situation.

  <div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> A larger <code>handSize</code> makes it less likely for two individual flows to collide
(and therefore for one to be able to starve the other), but more likely that
a small number of flows can dominate the apiserver. A larger <code>handSize</code> also
potentially increases the amount of latency that a single high-traffic flow
can cause. The maximum number of queued requests possible from a
single flow is <code>handSize * queueLengthLimit</code>.
</div>
-->
<ul>
<li>
<p>修改 <code>handSize</code> 允许你调整过载情况下不同流之间的冲突概率以及单个流可用的整体并发性。</p>
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 较大的 <code>handSize</code> 使两个单独的流程发生碰撞的可能性较小（因此，一个流可以饿死另一个流），
但是更有可能的是少数流可以控制 apiserver。
较大的 <code>handSize</code> 还可能增加单个高并发流的延迟量。
单个流中可能排队的请求的最大数量为 <code>handSize * queueLengthLimit</code> 。
</div>
</li>
</ul>
<!--
Following is a table showing an interesting collection of shuffle
sharding configurations, showing for each the probability that a
given mouse (low-intensity flow) is squished by the elephants (high-intensity flows) for
an illustrative collection of numbers of elephants. See
https://play.golang.org/p/Gi0PLgVHiUg , which computes this table.
-->
<p>下表显示了有趣的随机分片配置集合，
每行显示给定的老鼠（低强度流）被不同数量的大象挤压（高强度流）的概率。
表来源请参阅： <a href="https://play.golang.org/p/Gi0PLgVHiUg">https://play.golang.org/p/Gi0PLgVHiUg</a></p>





<!-- HandSize | Queues | 1 elephant | 4 elephants | 16 elephants -->
<table><caption style="display: none;">Example Shuffle Sharding Configurations</caption>
<thead>
<tr>
<th>随机分片</th>
<th>队列数</th>
<th>1 个大象</th>
<th>4 个大象</th>
<th>16 个大象</th>
</tr>
</thead>
<tbody>
<tr>
<td>12</td>
<td>32</td>
<td>4.428838398950118e-09</td>
<td>0.11431348830099144</td>
<td>0.9935089607656024</td>
</tr>
<tr>
<td>10</td>
<td>32</td>
<td>1.550093439632541e-08</td>
<td>0.0626479840223545</td>
<td>0.9753101519027554</td>
</tr>
<tr>
<td>10</td>
<td>64</td>
<td>6.601827268370426e-12</td>
<td>0.00045571320990370776</td>
<td>0.49999929150089345</td>
</tr>
<tr>
<td>9</td>
<td>64</td>
<td>3.6310049976037345e-11</td>
<td>0.00045501212304112273</td>
<td>0.4282314876454858</td>
</tr>
<tr>
<td>8</td>
<td>64</td>
<td>2.25929199850899e-10</td>
<td>0.0004886697053040446</td>
<td>0.35935114681123076</td>
</tr>
<tr>
<td>8</td>
<td>128</td>
<td>6.994461389026097e-13</td>
<td>3.4055790161620863e-06</td>
<td>0.02746173137155063</td>
</tr>
<tr>
<td>7</td>
<td>128</td>
<td>1.0579122850901972e-11</td>
<td>6.960839379258192e-06</td>
<td>0.02406157386340147</td>
</tr>
<tr>
<td>7</td>
<td>256</td>
<td>7.597695465552631e-14</td>
<td>6.728547142019406e-08</td>
<td>0.0006709661542533682</td>
</tr>
<tr>
<td>6</td>
<td>256</td>
<td>2.7134626662687968e-12</td>
<td>2.9516464018476436e-07</td>
<td>0.0008895654642000348</td>
</tr>
<tr>
<td>6</td>
<td>512</td>
<td>4.116062922897309e-14</td>
<td>4.982983350480894e-09</td>
<td>2.26025764343413e-05</td>
</tr>
<tr>
<td>6</td>
<td>1024</td>
<td>6.337324016514285e-16</td>
<td>8.09060164312957e-11</td>
<td>4.517408062903668e-07</td>
</tr>
</tbody>
</table>

<!-- ### FlowSchema -->
<h3 id="FlowSchema">FlowSchema   </h3>
<!--
A FlowSchema matches some inbound requests and assigns them to a
priority level. Every inbound request is tested against every
FlowSchema in turn, starting with those with numerically lowest ---
which we take to be the logically highest --- `matchingPrecedence` and
working onward.  The first match wins.
-->
<p>FlowSchema 匹配一些入站请求，并将它们分配给优先级。
每个入站请求都会对所有 FlowSchema 测试是否匹配，
首先从 <code>matchingPrecedence</code> 数值最低的匹配开始（我们认为这是逻辑上匹配度最高），
然后依次进行，直到首个匹配出现。</p>
<!--
Only the first matching FlowSchema for a given request matters. If multiple
FlowSchemas match a single inbound request, it will be assigned based on the one
with the highest `matchingPrecedence`. If multiple FlowSchemas with equal
`matchingPrecedence` match the same request, the one with lexicographically
smaller `name` will win, but it's better not to rely on this, and instead to
ensure that no two FlowSchemas have the same `matchingPrecedence`.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 对一个请求来说，只有首个匹配的 FlowSchema  才有意义。
如果一个入站请求与多个 FlowSchema 匹配，则将基于 <code>matchingPrecedence</code> 值最高的请求进行筛选。
如果一个请求匹配多个 FlowSchema 且 <code>matchingPrecedence</code> 的值相同，则按 <code>name</code> 的字典序选择最小，
但是最好不要依赖它，而是确保不存在两个 FlowSchema 具有相同的 <code>matchingPrecedence</code> 值。
</div>

<!--
A FlowSchema matches a given request if at least one of its `rules`
matches. A rule matches if at least one of its `subjects` *and* at least
one of its `resourceRules` or `nonResourceRules` (depending on whether the
incoming request is for a resource or non-resource URL) matches the request.
-->
<p>当给定的请求与某个 FlowSchema 的 <code>rules</code> 的其中一条匹配，那么就认为该请求与该 FlowSchema 匹配。
判断规则与该请求是否匹配，<strong>不仅</strong>要求该条规则的 <code>subjects</code> 字段至少存在一个与该请求相匹配，
<strong>而且</strong>要求该条规则的 <code>resourceRules</code> 或 <code>nonResourceRules</code>
（取决于传入请求是针对资源URL还是非资源URL）字段至少存在一个与该请求相匹配。</p>
<!--
For the `name` field in subjects, and the `verbs`, `apiGroups`, `resources`,
`namespaces`, and `nonResourceURLs` fields of resource and non-resource rules,
the wildcard `*` may be specified to match all values for the given field,
effectively removing it from consideration.
-->
<p>对于 <code>subjects</code> 中的 <code>name</code> 字段和资源和非资源规则的
<code>verbs</code>，<code>apiGroups</code>，<code>resources</code>，<code>namespaces</code> 和 <code>nonResourceURLs</code> 字段，
可以指定通配符 <code>*</code> 来匹配任意值，从而有效地忽略该字段。</p>
<!--
A FlowSchema's `distinguisherMethod.type` determines how requests matching that
schema will be separated into flows. It may be
either `ByUser`, in which case one requesting user will not be able to starve
other users of capacity, or `ByNamespace`, in which case requests for resources
in one namespace will not be able to starve requests for resources in other
namespaces of capacity, or it may be blank (or `distinguisherMethod` may be
omitted entirely), in which case all requests matched by this FlowSchema will be
considered part of a single flow. The correct choice for a given FlowSchema
depends on the resource and your particular environment.
-->
<p>FlowSchema 的 <code>distinguisherMethod.type</code> 字段决定了如何把与该模式匹配的请求分散到各个流中。
可能是 <code>ByUser</code> ，在这种情况下，一个请求用户将无法饿死其他容量的用户；
或者是 <code>ByNamespace</code> ，在这种情况下，一个名称空间中的资源请求将无法饿死其它名称空间的资源请求；
或者它可以为空（或者可以完全省略 <code>distinguisherMethod</code>），
在这种情况下，与此 FlowSchema 匹配的请求将被视为单个流的一部分。
资源和你的特定环境决定了如何选择正确一个 FlowSchema。</p>
<!--
## Defaults

Each kube-apiserver maintains two sorts of APF configuration objects:
mandatory and suggested.
-->
<h2 id="defaults">默认值   </h2>
<p>每个 kube-apiserver 会维护两种类型的 APF 配置对象：强制的（Mandatory）和建议的（Suggested）。</p>
<!--
### Mandatory Configuration Objects

The four mandatory configuration objects reflect fixed built-in
guardrail behavior.  This is behavior that the servers have before
those objects exist, and when those objects exist their specs reflect
this behavior.  The four mandatory objects are as follows.
-->
<h3 id="强制的配置对象">强制的配置对象</h3>
<p>有四种强制的配置对象对应内置的守护行为。这里的行为是服务器在还未创建对象之前就具备的行为，
而当这些对象存在时，其规约反映了这类行为。四种强制的对象如下：</p>
<!--
* The mandatory `exempt` priority level is used for requests that are
  not subject to flow control at all: they will always be dispatched
  immediately. The mandatory `exempt` FlowSchema classifies all
  requests from the `system:masters` group into this priority
  level. You may define other FlowSchemas that direct other requests
  to this priority level, if appropriate.
-->
<ul>
<li>强制的 <code>exempt</code> 优先级用于完全不受流控限制的请求：它们总是立刻被分发。
强制的 <code>exempt</code> FlowSchema 把 <code>system:masters</code> 组的所有请求都归入该优先级。
如果合适，你可以定义新的 FlowSchema，将其他请求定向到该优先级。</li>
</ul>
<!--
* The mandatory `catch-all` priority level is used in combination with
  the mandatory `catch-all` FlowSchema to make sure that every request
  gets some kind of classification. Typically you should not rely on
  this catch-all configuration, and should create your own catch-all
  FlowSchema and PriorityLevelConfiguration (or use the suggested
  `global-default` priority level that is installed by default) as
  appropriate. Because it is not expected to be used normally, the
  mandatory `catch-all` priority level has a very small concurrency
  share and does not queue requests.
-->
<ul>
<li>强制的 <code>catch-all</code> 优先级与强制的 <code>catch-all</code> FlowSchema 结合使用，
以确保每个请求都分类。一般而言，你不应该依赖于 <code>catch-all</code> 的配置，
而应适当地创建自己的 <code>catch-all</code> FlowSchema 和 PriorityLevelConfiguration
（或使用默认安装的 <code>global-default</code> 配置）。
因为这一优先级不是正常场景下要使用的，<code>catch-all</code> 优先级的并发度份额很小，
并且不会对请求进行排队。</li>
</ul>
<!--
### Suggested Configuration Objects

The suggested FlowSchemas and PriorityLevelConfigurations constitute a
reasonable default configuration.  You can modify these and/or create
additional configuration objects if you want.  If your cluster is
likely to experience heavy load then you should consider what
configuration will work best.

The suggested configuration groups requests into six priority levels:
-->
<h3 id="建议的配置对象">建议的配置对象</h3>
<p>建议的 FlowSchema 和 PriorityLevelConfiguration 包含合理的默认配置。
你可以修改这些对象或者根据需要创建新的配置对象。如果你的集群可能承受较重负载，
那么你就要考虑哪种配置最合适。</p>
<p>建议的配置把请求分为六个优先级：</p>
<!--
* The `node-high` priority level is for health updates from nodes.
-->
<ul>
<li><code>node-high</code> 优先级用于来自节点的健康状态更新。</li>
</ul>
<!--
* The `system` priority level is for non-health requests from the
  `system:nodes` group, i.e. Kubelets, which must be able to contact
  the API server in order for workloads to be able to schedule on
  them.
-->
<ul>
<li><code>system</code> 优先级用于 <code>system:nodes</code> 组（即 kubelet）的与健康状态更新无关的请求；
kubelets 必须能连上 API 服务器，以便工作负载能够调度到其上。</li>
</ul>
<!--
* The `leader-election` priority level is for leader election requests from
  built-in controllers (in particular, requests for `endpoints`, `configmaps`,
  or `leases` coming from the `system:kube-controller-manager` or
  `system:kube-scheduler` users and service accounts in the `kube-system`
  namespace). These are important to isolate from other traffic because failures
  in leader election cause their controllers to fail and restart, which in turn
  causes more expensive traffic as the new controllers sync their informers.
-->
<ul>
<li><code>leader-election</code> 优先级用于内置控制器的领导选举的请求
（特别是来自 <code>kube-system</code> 名称空间中 <code>system:kube-controller-manager</code> 和
<code>system:kube-scheduler</code> 用户和服务账号，针对 <code>endpoints</code>、<code>configmaps</code> 或 <code>leases</code> 的请求）。
将这些请求与其他流量相隔离非常重要，因为领导者选举失败会导致控制器发生故障并重新启动，
这反过来会导致新启动的控制器在同步信息时，流量开销更大。</li>
</ul>
<!--
* The `workload-high` priority level is for other requests from built-in
  controllers.
* The `workload-low` priority level is for requests from any other service
  account, which will typically include all requests from controllers running in
  Pods.
* The `global-default` priority level handles all other traffic, e.g.
  interactive `kubectl` commands run by nonprivileged users.
-->
<ul>
<li><code>workload-high</code> 优先级用于内置控制器的其他请求。</li>
<li><code>workload-low</code> 优先级用于来自所有其他服务帐户的请求，通常包括来自 Pod
中运行的控制器的所有请求。</li>
<li><code>global-default</code> 优先级可处理所有其他流量，例如：非特权用户运行的交互式
<code>kubectl</code> 命令。</li>
</ul>
<!--
The suggested FlowSchemas serve to steer requests into the above
priority levels, and are not enumerated here.
-->
<p>建议的 FlowSchema 用来将请求导向上述的优先级内，这里不再一一列举。</p>
<!--
### Maintenance of the Mandatory and Suggested Configuration Objects

Each `kube-apiserver` independently maintains the mandatory and
suggested configuration objects, using initial and periodic behavior.
Thus, in a situation with a mixture of servers of different versions
there may be thrashing as long as different servers have different
opinions of the proper content of these objects.
-->
<h3 id="强制的与建议的配置对象的维护">强制的与建议的配置对象的维护</h3>
<p>每个 <code>kube-apiserver</code> 都独立地维护其强制的与建议的配置对象，
这一维护操作既是服务器的初始行为，也是其周期性操作的一部分。
因此，当存在不同版本的服务器时，如果各个服务器对于配置对象中的合适内容有不同意见，
就可能出现抖动。</p>
<!--
Each `kube-apiserver` makes an inital maintenance pass over the
mandatory and suggested configuration objects, and after that does
periodic maintenance (once per minute) of those objects.

For the mandatory configuration objects, maintenance consists of
ensuring that the object exists and, if it does, has the proper spec.
The server refuses to allow a creation or update with a spec that is
inconsistent with the server's guardrail behavior.
-->
<p>每个 <code>kube-apiserver</code> 都会对强制的与建议的配置对象执行初始的维护操作，
之后（每分钟）对这些对象执行周期性的维护。</p>
<p>对于强制的配置对象，维护操作包括确保对象存在并且包含合适的规约（如果存在的话）。
服务器会拒绝创建或更新与其守护行为不一致的规约。</p>
<!--
Maintenance of suggested configuration objects is designed to allow
their specs to be overridden.  Deletion, on the other hand, is not
respected: maintenance will restore the object.  If you do not want a
suggested configuration object then you need to keep it around but set
its spec to have minimal consequences.  Maintenance of suggested
objects is also designed to support automatic migration when a new
version of the `kube-apiserver` is rolled out, albeit potentially with
thrashing while there is a mixed population of servers.
-->
<p>对建议的配置对象的维护操作被设计为允许其规约被重载。删除操作是不允许的，
维护操作期间会重建这类配置对象。如果你不需要某个建议的配置对象，
你需要将它放在一边，并让其规约所产生的影响最小化。
对建议的配置对象而言，其维护方面的设计也支持在上线新的 <code>kube-apiserver</code>
时完成自动的迁移动作，即便可能因为当前的服务器集合存在不同的版本而可能造成抖动仍是如此。</p>
<!--
Maintenance of a suggested configuration object consists of creating
it --- with the server's suggested spec --- if the object does not
exist.  OTOH, if the object already exists, maintenance behavior
depends on whether the `kube-apiservers` or the users control the
object.  In the former case, the server ensures that the object's spec
is what the server suggests; in the latter case, the spec is left
alone.
-->
<p>对建议的配置对象的维护操作包括基于服务器建议的规约创建对象
（如果对象不存在的话）。反之，如果对象已经存在，维护操作的行为取决于是否
<code>kube-apiserver</code> 或者用户在控制对象。如果 <code>kube-apiserver</code> 在控制对象，
则服务器确保对象的规约与服务器所给的建议匹配，如果用户在控制对象，
对象的规约保持不变。</p>
<!--
The question of who controls the object is answered by first looking
for an annotation with key `apf.kubernetes.io/autoupdate-spec`.  If
there is such an annotation and its value is `true` then the
kube-apiservers control the object.  If there is such an annotation
and its value is `false` then the users control the object.  If
neither of those condtions holds then the `metadata.generation` of the
object is consulted.  If that is 1 then the kube-apiservers control
the object.  Otherwise the users control the object.  These rules were
introduced in release 1.22 and their consideration of
`metadata.generation` is for the sake of migration from the simpler
earlier behavior.  Users who wish to control a suggested configuration
object should set its `apf.kubernetes.io/autoupdate-spec` annotation
to `false`.
-->
<p>关于谁在控制对象这个问题，首先要看对象上的 <code>apf.kubernetes.io/autoupdate-spec</code>
注解。如果对象上存在这个注解，并且其取值为<code>true</code>，则 kube-apiserver
在控制该对象。如果存在这个注解，并且其取值为<code>false</code>，则用户在控制对象。
如果这两个条件都不满足，则需要进一步查看对象的 <code>metadata.generation</code>。
如果该值为 1，则 kube-apiserver 控制对象，否则用户控制对象。
这些规则是在 1.22 发行版中引入的，而对 <code>metadata.generation</code>
的考量是为了便于从之前较简单的行为迁移过来。希望控制建议的配置对象的用户应该将对象的
<code>apf.kubernetes.io/autoupdate-spec</code> 注解设置为 <code>false</code>。</p>
<!--
Maintenance of a mandatory or suggested configuration object also
includes ensuring that it has an `apf.kubernetes.io/autoupdate-spec`
annotation that accurately reflects whether the kube-apiservers
control the object.

Maintenance also includes deleting objects that are neither mandatory
nor suggested but are annotated
`apf.kubernetes.io/autoupdate-spec=true`.
-->
<p>对强制的或建议的配置对象的维护操作也包括确保对象上存在 <code>apf.kubernetes.io/autoupdate-spec</code>
这一注解，并且其取值准确地反映了是否 kube-apiserver 在控制着对象。</p>
<p>维护操作还包括删除那些既非强制又非建议的配置，同时注解配置为
<code>apf.kubernetes.io/autoupdate-spec=true</code> 的对象。</p>
<!--
## Health check concurrency exemption
-->
<h2 id="Health-check-concurrency-exemption">健康检查并发豁免   </h2>
<!--
The suggested configuration gives no special treatment to the health
check requests on kube-apiservers from their local kubelets --- which
tend to use the secured port but supply no credentials.  With the
suggested config, these requests get assigned to the `global-default`
FlowSchema and the corresponding `global-default` priority level,
where other traffic can crowd them out.
-->
<p>推荐配置没有为本地 kubelet 对 kube-apiserver 执行健康检查的请求进行任何特殊处理
——它们倾向于使用安全端口，但不提供凭据。
在推荐配置中，这些请求将分配 <code>global-default</code> FlowSchema 和 <code>global-default</code> 优先级，
这样其他流量可以排除健康检查。</p>
<!--
If you add the following additional FlowSchema, this exempts those
requests from rate limiting.
-->
<p>如果添加以下 FlowSchema，健康检查请求不受速率限制。</p>
<!--
Making this change also allows any hostile party to then send
health-check requests that match this FlowSchema, at any volume they
like.  If you have a web traffic filter or similar external security
mechanism to protect your cluster's API server from general internet
traffic, you can configure rules to block any health check requests
that originate from outside your cluster.
-->
<div class="alert alert-warning caution callout" role="alert">
  <strong>Caution:</strong> 进行此更改后，任何敌对方都可以发送与此 FlowSchema 匹配的任意数量的健康检查请求。
如果你有 Web 流量过滤器或类似的外部安全机制保护集群的 API 服务器免受常规网络流量的侵扰，
则可以配置规则，阻止所有来自集群外部的健康检查请求。
</div>



 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/priority-and-fairness/health-for-strangers.yaml" download="priority-and-fairness/health-for-strangers.yaml"><code>priority-and-fairness/health-for-strangers.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('priority-and-fairness-health-for-strangers-yaml')" title="Copy priority-and-fairness/health-for-strangers.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="priority-and-fairness-health-for-strangers-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>flowcontrol.apiserver.k8s.io/v1beta2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>FlowSchema<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>health-for-strangers<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">matchingPrecedence</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">priorityLevelConfiguration</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>exempt<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">nonResourceRules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">nonResourceURLs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;/healthz&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;/livez&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;/readyz&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;*&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">subjects</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Group<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:unauthenticated<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
## Diagnostics

Every HTTP response from an API server with the priority and fairness feature
enabled has two extra headers: `X-Kubernetes-PF-FlowSchema-UID` and
`X-Kubernetes-PF-PriorityLevel-UID`, noting the flow schema that matched the request
and the priority level to which it was assigned, respectively. The API objects'
names are not included in these headers in case the requesting user does not
have permission to view them, so when debugging you can use a command like
-->
<h2 id="diagnostics">问题诊断   </h2>
<p>启用了 APF 的 API 服务器，它每个 HTTP 响应都有两个额外的 HTTP 头：
<code>X-Kubernetes-PF-FlowSchema-UID</code> 和 <code>X-Kubernetes-PF-PriorityLevel-UID</code>，
注意与请求匹配的 FlowSchema 和已分配的优先级。
如果请求用户没有查看这些对象的权限，则这些 HTTP 头中将不包含 API 对象的名称，
因此在调试时，你可以使用类似如下的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get flowschemas -o custom-columns<span style="color:#666">=</span><span style="color:#b44">&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
kubectl get prioritylevelconfigurations -o custom-columns<span style="color:#666">=</span><span style="color:#b44">&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</code></pre></div><!--
to get a mapping of UIDs to names for both FlowSchemas and
PriorityLevelConfigurations.
-->
<p>来获取 UID 到 FlowSchema 的名称和 UID 到 PriorityLevelConfigurations 的名称的映射。</p>
<!--
## Observability

### Metrics
-->
<h2 id="Observability">可观察性   </h2>
<h3 id="Metrics">指标   </h3>
<!--
In versions of Kubernetes before v1.20, the labels `flow_schema` and
`priority_level` were inconsistently named `flowSchema` and `priorityLevel`,
respectively. If you're running Kubernetes versions v1.19 and earlier, you
should refer to the documentation for your version.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 在 Kubernetes v1.20 之前的版本中，标签 <code>flow_schema</code> 和 <code>priority_level</code>
的名称有时被写作 <code>flowSchema</code> 和 <code>priorityLevel</code>，即存在不一致的情况。
如果你在运行 Kubernetes v1.19 或者更早版本，你需要参考你所使用的集群
版本对应的文档。
</div>
<!--
When you enable the API Priority and Fairness feature, the kube-apiserver
exports additional metrics. Monitoring these can help you determine whether your
configuration is inappropriately throttling important traffic, or find
poorly-behaved workloads that may be harming system health.
-->
<p>当你开启了 APF 后，kube-apiserver 会暴露额外指标。
监视这些指标有助于判断你的配置是否不当地限制了重要流量，
或者发现可能会损害系统健康的，行为不良的工作负载。</p>
<!--
* `apiserver_flowcontrol_rejected_requests_total` is a counter vector
  (cumulative since server start) of requests that were rejected,
  broken down by the labels `flow_schema` (indicating the one that
  matched the request), `priority_evel` (indicating the one to which
  the request was assigned), and `reason`.  The `reason` label will be
  have one of the following values:
-->
<ul>
<li><code>apiserver_flowcontrol_rejected_requests_total</code> 是一个计数器向量，
记录被拒绝的请求数量（自服务器启动以来累计值），
由标签 <code>flow_chema</code>（表示与请求匹配的 FlowSchema），<code>priority_evel</code>
（表示分配给请该求的优先级）和 <code>reason</code> 来区分。
<code>reason</code> 标签将具有以下值之一：
<!--
* `queue-full`, indicating that too many requests were already
  queued,
* `concurrency-limit`, indicating that the
  PriorityLevelConfiguration is configured to reject rather than
  queue excess requests, or
* `time-out`, indicating that the request was still in the queue
  when its queuing time limit expired.
-->
<ul>
<li><code>queue-full</code> ，表明已经有太多请求排队，</li>
<li><code>concurrency-limit</code>，表示将 PriorityLevelConfiguration 配置为
<code>Reject</code> 而不是 <code>Queue</code> ，或者</li>
<li><code>time-out</code>, 表示在其排队时间超期的请求仍在队列中。</li>
</ul>
</li>
</ul>
<!--
* `apiserver_flowcontrol_dispatched_requests_total` is a counter
  vector (cumulative since server start) of requests that began
  executing, broken down by the labels `flow_schema` (indicating the
  one that matched the request) and `priority_level` (indicating the
  one to which the request was assigned).
-->
<ul>
<li><code>apiserver_flowcontrol_dispatched_requests_total</code> 是一个计数器向量，
记录开始执行的请求数量（自服务器启动以来的累积值），
由标签 <code>flow_schema</code>（表示与请求匹配的 FlowSchema）和
<code>priority_level</code>（表示分配给该请求的优先级）来区分。</li>
</ul>
<!--
* `apiserver_current_inqueue_requests` is a gauge vector of recent
  high water marks of the number of queued requests, grouped by a
  label named `request_kind` whose value is `mutating` or `readOnly`.
  These high water marks describe the largest number seen in the one
  second window most recently completed.  These complement the older
  `apiserver_current_inflight_requests` gauge vector that holds the
  last window's high water mark of number of requests actively being
  served.
-->
<ul>
<li><code>apiserver_current_inqueue_requests</code> 是一个表向量，
记录最近排队请求数量的高水位线，
由标签 <code>request_kind</code> 分组，标签的值为 <code>mutating</code> 或 <code>readOnly</code>。
这些高水位线表示在最近一秒钟内看到的最大数字。
它们补充说明了老的表向量 <code>apiserver_current_inflight_requests</code>
（该量保存了最后一个窗口中，正在处理的请求数量的高水位线）。</li>
</ul>
<!--
* `apiserver_flowcontrol_read_vs_write_request_count_samples` is a
  histogram vector of observations of the then-current number of
  requests, broken down by the labels `phase` (which takes on the
  values `waiting` and `executing`) and `request_kind` (which takes on
  the values `mutating` and `readOnly`).  The observations are made
  periodically at a high rate.
-->
<ul>
<li><code>apiserver_flowcontrol_read_vs_write_request_count_samples</code> 是一个直方图向量，
记录当前请求数量的观察值，
由标签 <code>phase</code>（取值为 <code>waiting</code> 和 <code>executing</code>）和 <code>request_kind</code>
（取值 <code>mutating</code> 和 <code>readOnly</code>）拆分。定期以高速率观察该值。</li>
</ul>
<!--
* `apiserver_flowcontrol_read_vs_write_request_count_watermarks` is a
  histogram vector of high or low water marks of the number of
  requests broken down by the labels `phase` (which takes on the
  values `waiting` and `executing`) and `request_kind` (which takes on
  the values `mutating` and `readOnly`); the label `mark` takes on
  values `high` and `low`.  The water marks are accumulated over
  windows bounded by the times when an observation was added to
  `apiserver_flowcontrol_read_vs_write_request_count_samples`.  These
  water marks show the range of values that occurred between samples.
-->
<ul>
<li><code>apiserver_flowcontrol_read_vs_write_request_count_watermarks</code> 是一个直方图向量，
记录请求数量的高/低水位线，
由标签 <code>phase</code>（取值为 <code>waiting</code> 和 <code>executing</code>）和 <code>request_kind</code>
（取值为 <code>mutating</code> 和 <code>readOnly</code>）拆分；标签 <code>mark</code> 取值为 <code>high</code> 和 <code>low</code> 。
<code>apiserver_flowcontrol_read_vs_write_request_count_samples</code> 向量观察到有值新增，
则该向量累积。这些水位线显示了样本值的范围。</li>
</ul>
<!--
* `apiserver_flowcontrol_current_inqueue_requests` is a gauge vector
  holding the instantaneous number of queued (not executing) requests,
  broken down by the labels `priorityLevel` and `flowSchema`.
-->
<ul>
<li><code>apiserver_flowcontrol_current_inqueue_requests</code> 是一个表向量，
记录包含排队中的（未执行）请求的瞬时数量，
由标签 <code>priorityLevel</code> 和 <code>flowSchema</code> 拆分。</li>
</ul>
<!--
* `apiserver_flowcontrol_current_executing_requests` is a gauge vector
  holding the instantaneous number of executing (not waiting in a
  queue) requests, broken down by the labels `priority_level` and
  `flow_schema`.
-->
<ul>
<li><code>apiserver_flowcontrol_current_executing_requests</code> 是一个表向量，
记录包含执行中（不在队列中等待）请求的瞬时数量，
由标签 <code>priority_level</code> 和 <code>flow_schema</code> 进一步区分。</li>
</ul>
<!-- 
* `apiserver_flowcontrol_request_concurrency_in_use` is a gauge vector
  holding the instantaneous number of occupied seats, broken down by
  the labels `priority_level` and `flow_schema`.
-->
<ul>
<li><code>apiserver_flowcontrol_request_concurrency_in_use</code> 是一个规范向量，
包含占用座位的瞬时数量，由标签 <code>priority_level</code> 和 <code>flow_schema</code> 进一步区分。</li>
</ul>
<!--
* `apiserver_flowcontrol_priority_level_request_count_samples` is a
  histogram vector of observations of the then-current number of
  requests broken down by the labels `phase` (which takes on the
  values `waiting` and `executing`) and `priority_level`.  Each
  histogram gets observations taken periodically, up through the last
  activity of the relevant sort.  The observations are made at a high
  rate.
-->
<ul>
<li><code>apiserver_flowcontrol_priority_level_request_count_samples</code> 是一个直方图向量，
记录当前请求的观测值，由标签 <code>phase</code>（取值为<code>waiting</code> 和 <code>executing</code>）和
<code>priority_level</code> 进一步区分。
每个直方图都会定期进行观察，直到相关类别的最后活动为止。观察频率高。</li>
</ul>
<!--
* `apiserver_flowcontrol_priority_level_request_count_watermarks` is a
  histogram vector of high or low water marks of the number of
  requests broken down by the labels `phase` (which takes on the
  values `waiting` and `executing`) and `priority_level`; the label
  `mark` takes on values `high` and `low`.  The water marks are
  accumulated over windows bounded by the times when an observation
  was added to
  `apiserver_flowcontrol_priority_level_request_count_samples`.  These
  water marks show the range of values that occurred between samples.
-->
<ul>
<li><code>apiserver_flowcontrol_priority_level_request_count_watermarks</code> 是一个直方图向量，
记录请求数的高/低水位线，由标签 <code>phase</code>（取值为 <code>waiting</code> 和 <code>executing</code>）和
<code>priority_level</code> 拆分；
标签 <code>mark</code> 取值为 <code>high</code> 和 <code>low</code> 。
<code>apiserver_flowcontrol_priority_level_request_count_samples</code> 向量观察到有值新增，
则该向量累积。这些水位线显示了样本值的范围。</li>
</ul>
<!--
* `apiserver_flowcontrol_request_queue_length_after_enqueue` is a
  histogram vector of queue lengths for the queues, broken down by
  the labels `priority_level` and `flow_schema`, as sampled by the
  enqueued requests.  Each request that gets queued contributes one
  sample to its histogram, reporting the length of the queue immediately
  after the request was added.  Note that this produces different
  statistics than an unbiased survey would.
-->
<ul>
<li><code>apiserver_flowcontrol_request_queue_length_after_enqueue</code> 是一个直方图向量，
记录请求队列的长度，由标签 <code>priority_level</code> 和 <code>flow_schema</code> 进一步区分。
每个排队中的请求都会为其直方图贡献一个样本，并在添加请求后立即上报队列的长度。
请注意，这样产生的统计数据与无偏调查不同。
<!--
An outlier value in a histogram here means it is likely that a single flow
(i.e., requests by one user or for one namespace, depending on
configuration) is flooding the API server, and being throttled. By contrast,
if one priority level's histogram shows that all queues for that priority
level are longer than those for other priority levels, it may be appropriate
to increase that PriorityLevelConfiguration's concurrency shares.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 直方图中的离群值在这里表示单个流（即，一个用户或一个名称空间的请求，
具体取决于配置）正在疯狂地向 API 服务器发请求，并受到限制。
相反，如果一个优先级的直方图显示该优先级的所有队列都比其他优先级的队列长，
则增加 PriorityLevelConfigurations 的并发份额是比较合适的。
</div>
</li>
</ul>
<!--
* `apiserver_flowcontrol_request_concurrency_limit` is a gauge vector
  hoding the computed concurrency limit (based on the API server's
  total concurrency limit and PriorityLevelConfigurations' concurrency
  shares), broken down by the label `priority_level`.
-->
<ul>
<li><code>apiserver_flowcontrol_request_concurrency_limit</code> 是一个表向量，
记录并发限制的计算值（基于 API 服务器的总并发限制和 PriorityLevelConfigurations
的并发份额），并按标签 <code>priority_level</code> 进一步区分。</li>
</ul>
<!--
* `apiserver_flowcontrol_request_wait_duration_seconds` is a histogram
  vector of how long requests spent queued, broken down by the labels
  `flowSchema` (indicating which one matched the request),
  `priorityLevel` (indicating the one to which the request was
  assigned), and `execute` (indicating whether the request started
  executing).
-->
<ul>
<li><code>apiserver_flowcontrol_request_wait_duration_seconds</code> 是一个直方图向量，
记录请求排队的时间，
由标签 <code>flow_schema</code>（表示与请求匹配的 FlowSchema ），
<code>priority_level</code>（表示分配该请求的优先级）
和 <code>execute</code>（表示请求是否开始执行）进一步区分。
<!--
Since each FlowSchema always assigns requests to a single
PriorityLevelConfiguration, you can add the histograms for all the
FlowSchemas for one priority level to get the effective histogram for
requests assigned to that priority level.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 由于每个 FlowSchema 总会给请求分配 PriorityLevelConfigurations，
因此你可以为一个优先级添加所有 FlowSchema 的直方图，以获取分配给
该优先级的请求的有效直方图。
</div>
</li>
</ul>
<!--
* `apiserver_flowcontrol_request_execution_seconds` is a histogram
  vector of how long requests took to actually execute, broken down by
  the labels `flowSchema` (indicating which one matched the request)
  and `priorityLevel` (indicating the one to which the request was
  assigned).
-->
<ul>
<li><code>apiserver_flowcontrol_request_execution_seconds</code> 是一个直方图向量，
记录请求实际执行需要花费的时间，
由标签 <code>flow_schema</code>（表示与请求匹配的 FlowSchema ）和
<code>priority_level</code>（表示分配给该请求的优先级）进一步区分。</li>
</ul>
<!--
### Debug endpoints

When you enable the API Priority and Fairness feature, the `kube-apiserver`
serves the following additional paths at its HTTP[S] ports.
-->
<h3 id="Debug-endpoints">调试端点   </h3>
<p>启用 APF 特性后， kube-apiserver 会在其 HTTP/HTTPS 端口提供以下路径：</p>
<!--
- `/debug/api_priority_and_fairness/dump_priority_levels` - a listing of
  all the priority levels and the current state of each.  You can fetch like this:
-->
<ul>
<li>
<p><code>/debug/api_priority_and_fairness/dump_priority_levels</code> ——
所有优先级及其当前状态的列表。你可以这样获取：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get --raw /debug/api_priority_and_fairness/dump_priority_levels
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code class="language-none" data-lang="none">PriorityLevelName, ActiveQueues, IsIdle, IsQuiescing, WaitingRequests, ExecutingRequests,
workload-low,      0,            true,   false,       0,               0,
global-default,    0,            true,   false,       0,               0,
exempt,            &lt;none&gt;,       &lt;none&gt;, &lt;none&gt;,      &lt;none&gt;,          &lt;none&gt;,
catch-all,         0,            true,   false,       0,               0,
system,            0,            true,   false,       0,               0,
leader-election,   0,            true,   false,       0,               0,
workload-high,     0,            true,   false,       0,               0,
</code></pre></li>
</ul>
<!--
- `/debug/api_priority_and_fairness/dump_queues` - a listing of all the
  queues and their current state.  You can fetch like this:
-->
<ul>
<li>
<p><code>/debug/api_priority_and_fairness/dump_queues</code> —— 所有队列及其当前状态的列表。
你可以这样获取：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get --raw /debug/api_priority_and_fairness/dump_queues
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code class="language-none" data-lang="none">PriorityLevelName, Index,  PendingRequests, ExecutingRequests, VirtualStart,
workload-high,     0,      0,               0,                 0.0000,
workload-high,     1,      0,               0,                 0.0000,
workload-high,     2,      0,               0,                 0.0000,
...
leader-election,   14,     0,               0,                 0.0000,
leader-election,   15,     0,               0,                 0.0000,
</code></pre></li>
</ul>
<!--
- `/debug/api_priority_and_fairness/dump_requests` - a listing of all the requests
  that are currently waiting in a queue.  You can fetch like this:
-->
<ul>
<li>
<p><code>/debug/api_priority_and_fairness/dump_requests</code> ——当前正在队列中等待的所有请求的列表。
你可以这样获取：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get --raw /debug/api_priority_and_fairness/dump_requests
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code class="language-none" data-lang="none">PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,
exempt,            &lt;none&gt;,         &lt;none&gt;,     &lt;none&gt;,              &lt;none&gt;,                &lt;none&gt;,
system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:26:57.179170694Z,
</code></pre><!--
In addition to the queued requests, the output includes one phantom line
for each priority level that is exempt from limitation.
-->
<p>针对每个优先级别，输出中还包含一条虚拟记录，对应豁免限制。</p>
<!-- You can get a more detailed listing with a command like this: -->
<p>你可以使用以下命令获得更详细的清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get --raw <span style="color:#b44">&#39;/debug/api_priority_and_fairness/dump_requests?includeRequestDetails=1&#39;</span>
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre><code class="language-none" data-lang="none">PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,                     UserName,              Verb,   APIPath,                                                     Namespace, Name,   APIVersion, Resource, SubResource,
system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:31:03.583823404Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,
system,            system-nodes,   12,         1,                   system:node:127.0.0.1, 2020-07-23T15:31:03.594555947Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,
</code></pre></li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
For background information on design details for API priority and fairness, see
the [enhancement proposal](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness).
You can make suggestions and feature requests via [SIG API Machinery](https://github.com/kubernetes/community/tree/master/sig-api-machinery) 
or the feature's [slack channel](https://kubernetes.slack.com/messages/api-priority-and-fairness).
-->
<p>有关 API 优先级和公平性的设计细节的背景信息，
请参阅<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness">增强提案</a>。
你可以通过 <a href="https://github.com/kubernetes/community/tree/master/sig-api-machinery/">SIG API Machinery</a>
或特性的 <a href="https://kubernetes.slack.com/messages/api-priority-and-fairness/">Slack 频道</a>
提出建议和特性请求。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-85d633ae590aa20ec024f1b7af1d74fc">11.10 - 安装扩展（Addons）</h1>
    
	<!-- overview -->
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<!--
Add-ons extend the functionality of Kubernetes.

This page lists some of the available add-ons and links to their respective installation instructions.

Add-ons in each section are sorted alphabetically - the ordering does not imply any preferential status.
-->
<p>Add-ons 扩展了 Kubernetes 的功能。</p>
<p>本文列举了一些可用的 add-ons 以及到它们各自安装说明的链接。</p>
<p>每个 Add-ons 按字母顺序排序 - 顺序不代表任何优先地位。</p>
<!-- body -->
<!--
## Networking and Network Policy

* [ACI](https://www.github.com/noironetworks/aci-containers) provides integrated container networking and network security with Cisco ACI.
* [Antrea](https://antrea.io/) operates at Layer 3/4 to provide networking and security services for Kubernetes, leveraging Open vSwitch as the networking data plane.
* [Calico](https://docs.projectcalico.org/latest/getting-started/kubernetes/) is a secure L3 networking and network policy provider.
* [Canal](https://github.com/tigera/canal/tree/master/k8s-install) unites Flannel and Calico, providing networking and network policy.
* [Cilium](https://github.com/cilium/cilium) is a L3 network and network policy plugin that can enforce HTTP/API/L7 policies transparently. Both routing and overlay/encapsulation mode are supported.
* [CNI-Genie](https://github.com/Huawei-PaaS/CNI-Genie) enables Kubernetes to seamlessly connect to a choice of CNI plugins, such as Calico, Canal, Flannel, Romana, or Weave.
* [Contiv](https://contivpp.io/) provides configurable networking (native L3 using BGP, overlay using vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich policy framework. Contiv project is fully [open sourced](https://github.com/contiv). The [installer](https://github.com/contiv/install) provides both kubeadm and non-kubeadm based installation options.
* [Contrail](http://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/), based on [Tungsten Fabric](https://tungsten.io), is an open source, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods and bare metal workloads.
* [Flannel](https://github.com/flannel-io/flannel#deploying-flannel-manually) is an overlay network provider that can be used with Kubernetes.
* [Knitter](https://github.com/ZTE/Knitter/) is a network solution supporting multiple networking in Kubernetes.
* Multus is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK and VPP based workloads in Kubernetes.
* [OVN-Kubernetes](https://github.com/ovn-org/ovn-kubernetes/) is a networking provider for Kubernetes based on [OVN (Open Virtual Network)](https://github.com/ovn-org/ovn/), a virtual networking implementation that came out of the Open vSwitch (OVS) project. OVN-Kubernetes provides an overlay based networking implementation for Kubernetes, including an OVS based implementation of load balancing and network policy.
* [OVN4NFV-K8S-Plugin](https://github.com/opnfv/ovn4nfv-k8s-plugin) is OVN based CNI controller plugin to provide cloud native based Service function chaining(SFC), Multiple OVN overlay networking, dynamic subnet creation, dynamic creation of virtual networks, VLAN Provider network, Direct provider network and pluggable with other Multi-network plugins, ideal for edge based cloud native workloads in Multi-cluster networking
* [NSX-T](https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf) Container Plug-in (NCP) provides integration between VMware NSX-T and container orchestrators such as Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and OpenShift.
* [Nuage](https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst) is an SDN platform that provides policy-based networking between Kubernetes Pods and non-Kubernetes environments with visibility and security monitoring.
* **Romana** is a Layer 3 networking solution for pod networks that also supports the [NetworkPolicy API](/docs/concepts/services-networking/network-policies/). Kubeadm add-on installation details available [here](https://github.com/romana/romana/tree/master/containerize).
* [Weave Net](https://www.weave.works/docs/net/latest/kubernetes/kube-addon/) provides networking and network policy, will carry on working on both sides of a network partition, and does not require an external database.
-->
<h2 id="网络和网络策略">网络和网络策略</h2>
<ul>
<li><a href="https://www.github.com/noironetworks/aci-containers">ACI</a> 通过 Cisco ACI 提供集成的容器网络和安全网络。</li>
<li><a href="https://antrea.io/">Antrea</a> 在第 3/4 层执行操作，为 Kubernetes
提供网络连接和安全服务。Antrea 利用 Open vSwitch 作为网络的数据面。</li>
<li><a href="https://docs.projectcalico.org/v3.11/getting-started/kubernetes/installation/calico">Calico</a>
是一个安全的 L3 网络和网络策略驱动。</li>
<li><a href="https://github.com/tigera/canal/tree/master/k8s-install">Canal</a> 结合 Flannel 和 Calico，提供网络和网络策略。</li>
<li><a href="https://github.com/cilium/cilium">Cilium</a> 是一个 L3 网络和网络策略插件，能够透明的实施 HTTP/API/L7 策略。
同时支持路由（routing）和覆盖/封装（overlay/encapsulation）模式。</li>
<li><a href="https://github.com/Huawei-PaaS/CNI-Genie">CNI-Genie</a> 使 Kubernetes 无缝连接到一种 CNI 插件，
例如：Flannel、Calico、Canal、Romana 或者 Weave。</li>
<li><a href="https://contivpp.io/">Contiv</a> 为各种用例和丰富的策略框架提供可配置的网络
（使用 BGP 的本机 L3、使用 vxlan 的覆盖、标准 L2 和 Cisco-SDN/ACI）。
Contiv 项目完全<a href="https://github.com/contiv">开源</a>。
<a href="https://github.com/contiv/install">安装程序</a> 提供了基于 kubeadm 和非 kubeadm 的安装选项。</li>
<li>基于 <a href="https://tungsten.io">Tungsten Fabric</a> 的
<a href="https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/">Contrail</a>
是一个开源的多云网络虚拟化和策略管理平台，Contrail 和 Tungsten Fabric 与业务流程系统
（例如 Kubernetes、OpenShift、OpenStack和Mesos）集成在一起，
为虚拟机、容器或 Pod 以及裸机工作负载提供了隔离模式。</li>
<li><a href="https://github.com/flannel-io/flannel#deploying-flannel-manually">Flannel</a>
是一个可以用于 Kubernetes 的 overlay 网络提供者。</li>
<li><a href="https://github.com/ZTE/Knitter/">Knitter</a> 是为 kubernetes 提供复合网络解决方案的网络组件。</li>
<li>Multus 是一个多插件，可在 Kubernetes 中提供多种网络支持，
以支持所有 CNI 插件（例如 Calico，Cilium，Contiv，Flannel），
而且包含了在 Kubernetes 中基于 SRIOV、DPDK、OVS-DPDK 和 VPP 的工作负载。</li>
<li><a href="https://github.com/ovn-org/ovn-kubernetes/">OVN-Kubernetes</a> 是一个 Kubernetes 网络驱动，
基于 <a href="https://github.com/ovn-org/ovn/">OVN（Open Virtual Network）</a>实现，是从 Open vSwitch (OVS)
项目衍生出来的虚拟网络实现。
OVN-Kubernetes 为 Kubernetes 提供基于覆盖网络的网络实现，包括一个基于 OVS 实现的负载均衡器
和网络策略。</li>
<li><a href="https://github.com/opnfv/ovn4nfv-k8s-plugin">OVN4NFV-K8S-Plugin</a> 是一个基于 OVN 的 CNI
控制器插件，提供基于云原生的服务功能链条（Service Function Chaining，SFC）、多种 OVN 覆盖
网络、动态子网创建、动态虚拟网络创建、VLAN 驱动网络、直接驱动网络，并且可以
驳接其他的多网络插件，适用于基于边缘的、多集群联网的云原生工作负载。</li>
<li><a href="https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf">NSX-T</a> 容器插件（NCP）
提供了 VMware NSX-T 与容器协调器（例如 Kubernetes）之间的集成，以及 NSX-T 与基于容器的
CaaS / PaaS 平台（例如关键容器服务（PKS）和 OpenShift）之间的集成。</li>
<li><a href="https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst">Nuage</a>
是一个 SDN 平台，可在 Kubernetes Pods 和非 Kubernetes 环境之间提供基于策略的联网，并具有可视化和安全监控。</li>
<li>Romana 是一个 pod 网络的第三层解决方案，并支持
<a href="/zh/docs/concepts/services-networking/network-policies/">NetworkPolicy API</a>。
Kubeadm add-on 安装细节可以在<a href="https://github.com/romana/romana/tree/master/containerize">这里</a>找到。</li>
<li><a href="https://www.weave.works/docs/net/latest/kubernetes/kube-addon/">Weave Net</a>
提供在网络分组两端参与工作的网络和网络策略，并且不需要额外的数据库。</li>
</ul>
<!--
## Service Discovery

* [CoreDNS](https://coredns.io) is a flexible, extensible DNS server which can be [installed](https://github.com/coredns/deployment/tree/master/kubernetes) as the in-cluster DNS for pods.
-->
<h2 id="服务发现">服务发现</h2>
<ul>
<li><a href="https://coredns.io">CoreDNS</a> 是一种灵活的，可扩展的 DNS 服务器，可以
<a href="https://github.com/coredns/deployment/tree/master/kubernetes">安装</a>为集群内的 Pod 提供 DNS 服务。</li>
</ul>
<!--
## Visualization &amp; Control

* [Dashboard](https://github.com/kubernetes/dashboard#kubernetes-dashboard) is a dashboard web interface for Kubernetes.
* [Weave Scope](https://www.weave.works/documentation/scope-latest-installing/#k8s) is a tool for graphically visualizing your containers, pods, services etc. Use it in conjunction with a [Weave Cloud account](https://cloud.weave.works/) or host the UI yourself.
-->
<h2 id="可视化管理">可视化管理</h2>
<ul>
<li><a href="https://github.com/kubernetes/dashboard#kubernetes-dashboard">Dashboard</a> 是一个 Kubernetes 的 Web 控制台界面。</li>
<li><a href="https://www.weave.works/documentation/scope-latest-installing/#k8s">Weave Scope</a> 是一个图形化工具，
用于查看你的容器、Pod、服务等。请和一个 <a href="https://cloud.weave.works/">Weave Cloud 账号</a> 一起使用，
或者自己运行 UI。</li>
</ul>
<!--
## Infrastructure

* [KubeVirt](https://kubevirt.io/user-guide/#/installation/installation) is an add-on to run virtual machines on Kubernetes. Usually run on bare-metal clusters.
* The
  [node problem detector](https://github.com/kubernetes/node-problem-detector)
  runs on Linux nodes and reports system issues as either
  [Events](/docs/reference/kubernetes-api/cluster-resources/event-v1/) or
  [Node conditions](/docs/concepts/architecture/nodes/#condition).
-->
<h2 id="基础设施">基础设施</h2>
<ul>
<li><a href="https://kubevirt.io/user-guide/#/installation/installation">KubeVirt</a> 是可以让 Kubernetes
运行虚拟机的 add-ons。通常运行在裸机集群上。</li>
<li><a href="https://github.com/kubernetes/node-problem-detector">节点问题检测器</a> 在 Linux 节点上运行，
并将系统问题报告为<a href="/docs/reference/kubernetes-api/cluster-resources/event-v1/">事件</a>
或<a href="/zh/docs/concepts/architecture/nodes/#condition">节点状况</a>。</li>
</ul>
<!--
## Legacy Add-ons

There are several other add-ons documented in the deprecated [cluster/addons](https://git.k8s.io/kubernetes/cluster/addons) directory.

Well-maintained ones should be linked to here. PRs welcome!
-->
<h2 id="遗留-add-ons">遗留 Add-ons</h2>
<p>还有一些其它 add-ons 归档在已废弃的 <a href="https://git.k8s.io/kubernetes/cluster/addons">cluster/addons</a> 路径中。</p>
<p>维护完善的 add-ons 应该被链接到这里。欢迎提出 PRs！</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7e0d97616b15e2c383c6a0a96ec442cb">12 - 扩展 Kubernetes</h1>
    <div class="lead">改变你的 Kubernetes 集群的行为的若干方法。</div>
	<!--
title: Extending Kubernetes
weight: 110
description: Different ways to change the behavior of your Kubernetes cluster.
reviewers:
- erictune
- lavalamp
- cheftako
- chenopis
feature:
  title: Designed for extensibility
  description: >
    Add features to your Kubernetes cluster without changing upstream source code.
content_type: concept
no_list: true
-->
<!-- overview -->
<!--
Kubernetes is highly configurable and extensible. As a result,
there is rarely a need to fork or submit patches to the Kubernetes
project code.

This guide describes the options for customizing a Kubernetes
cluster. It is aimed at <a class='glossary-tooltip' title='配置、控制、监控集群的人。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cluster-operator' target='_blank' aria-label='cluster operators'>cluster operators</a> who want to
understand how to adapt their Kubernetes cluster to the needs of
their work environment. Developers who are prospective <a class='glossary-tooltip' title='定制 Kubernetes 平台以满足自己的项目需求的人。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-platform-developer' target='_blank' aria-label='Platform Developers'>Platform Developers</a> or Kubernetes Project <a class='glossary-tooltip' title='通过贡献代码、文档或者投入时间等方式来帮助 Kubernetes 项目或社区的人。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-contributor' target='_blank' aria-label='Contributors'>Contributors</a> will also find it
useful as an introduction to what extension points and patterns
exist, and their trade-offs and limitations.
-->
<p>Kubernetes 是高度可配置且可扩展的。因此，大多数情况下，你不需要
派生自己的 Kubernetes 副本或者向项目代码提交补丁。</p>
<p>本指南描述定制 Kubernetes 的可选方式。主要针对的读者是希望了解如何针对自身工作环境
需要来调整 Kubernetes 的<a class='glossary-tooltip' title='配置、控制、监控集群的人。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cluster-operator' target='_blank' aria-label='集群管理者'>集群管理者</a>。
对于那些充当<a class='glossary-tooltip' title='定制 Kubernetes 平台以满足自己的项目需求的人。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-platform-developer' target='_blank' aria-label='平台开发人员'>平台开发人员</a>
的开发人员或 Kubernetes 项目的<a class='glossary-tooltip' title='通过贡献代码、文档或者投入时间等方式来帮助 Kubernetes 项目或社区的人。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-contributor' target='_blank' aria-label='贡献者'>贡献者</a>
而言，他们也会在本指南中找到有用的介绍信息，了解系统中存在哪些扩展点和扩展模式，
以及它们所附带的各种权衡和约束等等。</p>
<!-- body -->
<!--
## Overview

Customization approaches can be broadly divided into *configuration*, which only involves changing flags, local configuration files, or API resources; and *extensions*, which involve running additional programs or services. This document is primarily about extensions.
-->
<h2 id="overview">概述 </h2>
<p>定制化的方法主要可分为 <em>配置（Configuration）</em> 和 <em>扩展（Extensions）</em> 两种。
前者主要涉及改变参数标志、本地配置文件或者 API 资源；
后者则需要额外运行一些程序或服务。
本文主要关注扩展。</p>
<!--
## Configuration

*Configuration files* and *flags* are documented in the Reference section of the online documentation, under each binary:

* [kubelet](/docs/reference/command-line-tools-reference/kubelet/)
* [kube-proxy](/docs/reference/command-line-tools-reference/kube-proxy/)
* [kube-apiserver](/docs/reference/command-line-tools-reference/kube-apiserver/)
* [kube-controller-manager](/docs/reference/command-line-tools-reference/kube-controller-manager/)
* [kube-scheduler](/docs/reference/command-line-tools-reference/kube-scheduler/).
-->
<h2 id="configuration">配置  </h2>
<p>配置文件和参数标志的说明位于在线文档的参考章节，按可执行文件组织：</p>
<ul>
<li><a href="/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet</a></li>
<li><a href="/zh/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a></li>
<li><a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a></li>
<li><a href="/zh/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a></li>
<li><a href="/zh/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler</a>.</li>
</ul>
<!--
Flags and configuration files may not always be changeable in a hosted Kubernetes service or a distribution with managed installation. When they are changeable, they are usually only changeable by the cluster administrator. Also, they are subject to change in future Kubernetes versions, and setting them may require restarting processes. For those reasons, they should be used only when there are no other options.
-->
<p>在托管的 Kubernetes 服务中或者受控安装的发行版本中，参数标志和配置文件不总是可以
修改的。即使它们是可修改的，通常其修改权限也仅限于集群管理员。
此外，这些内容在将来的 Kubernetes 版本中很可能发生变化，设置新参数或配置文件可能
需要重启进程。
有鉴于此，通常应该在没有其他替代方案时才应考虑更改参数标志和配置文件。</p>
<!--
*Built-in Policy APIs*, such as [ResourceQuota](/docs/concepts/policy/resource-quotas/), [PodSecurityPolicies](/docs/concepts/security/pod-security-policy/), [NetworkPolicy](/docs/concepts/services-networking/network-policies/) and Role-based Access Control ([RBAC](/docs/reference/access-authn-authz/rbac/)), are built-in Kubernetes APIs. APIs are typically used with hosted Kubernetes services and with managed Kubernetes installations. They are declarative and use the same conventions as other Kubernetes resources like pods, so new cluster configuration can be repeatable and be managed the same way as applications. And, where they are stable, they enjoy a [defined support policy](/docs/reference/using-api/deprecation-policy/) like other Kubernetes APIs. For these reasons, they are preferred over *configuration files* and *flags* where suitable.
-->
<p><em>内置的策略 API</em>，例如<a href="/zh/docs/concepts/policy/resource-quotas/">ResourceQuota</a>、
<a href="/zh/docs/concepts/security/pod-security-policy/">PodSecurityPolicies</a>、
<a href="/zh/docs/concepts/services-networking/network-policies/">NetworkPolicy</a>
和基于角色的访问控制（<a href="/zh/docs/reference/access-authn-authz/rbac/">RBAC</a>）
等等都是内置的 Kubernetes API。
API 通常用于托管的 Kubernetes 服务和受控的 Kubernetes 安装环境中。
这些 API 是声明式的，与 Pod 这类其他 Kubernetes 资源遵从相同的约定，
所以新的集群配置是可复用的，并且可以当作应用程序来管理。
此外，对于稳定版本的 API 而言，它们与其他 Kubernetes API 一样，
采纳的是一种<a href="/zh/docs/reference/using-api/deprecation-policy/">预定义的支持策略</a>。
出于以上原因，在条件允许的情况下，基于 API 的方案应该优先于配置文件和参数标志。</p>
<!--
## Extensions

Extensions are software components that extend and deeply integrate with Kubernetes.
They adapt it to support new types and new kinds of hardware.

Many cluster administrators use a hosted or distribution instance of Kubernetes. 
These clusters come with extensions pre-installed. As a result, most Kubernetes 
users will not need to install extensions and even fewer users will need to author new ones.
-->
<h2 id="extensions">扩展   </h2>
<p>扩展（Extensions）是一些扩充 Kubernetes 能力并与之深度集成的软件组件。
它们调整 Kubernetes 的工作方式使之支持新的类型和新的硬件种类。</p>
<p>大多数集群管理员会使用一种托管的 Kubernetes 服务或者其某种发行版本。
这类集群通常都预先安装了扩展。因此，大多数 Kubernetes 用户不需要安装扩展，
至于需要自己编写新的扩展的情况就更少了。</p>
<!--
## Extension Patterns

Kubernetes is designed to be automated by writing client programs. Any
program that reads and/or writes to the Kubernetes API can provide useful
automation. *Automation* can run on the cluster or off it. By following
the guidance in this doc you can write highly available and robust automation.
Automation generally works with any Kubernetes cluster, including hosted
clusters and managed installations.
-->
<h2 id="extension-patterns">扩展模式  </h2>
<p>Kubernetes 从设计上即支持通过编写客户端程序来将其操作自动化。
任何能够对 Kubernetes API 发出读写指令的程序都可以提供有用的自动化能力。
<em>自动化组件</em>可以运行在集群上，也可以运行在集群之外。
通过遵从本文中的指南，你可以编写高度可用的、运行稳定的自动化组件。
自动化组件通常可以用于所有 Kubernetes 集群，包括托管的集群和受控的安装环境。</p>
<!--
There is a specific pattern for writing client programs that work well with
Kubernetes called the *Controller* pattern. Controllers typically read an
object's `.spec`, possibly do things, and then update the object's `.status`.

A controller is a client of Kubernetes. When Kubernetes is the client and
calls out to a remote service, it is called a *Webhook*. The remote service
is called a *Webhook Backend*. Like Controllers, Webhooks do add a point of
failure.
-->
<p>编写客户端程序有一种特殊的 Controller（控制器）模式，能够与 Kubernetes
很好地协同工作。控制器通常会读取某个对象的 <code>.spec</code>，或许还会执行一些操作，
之后更新对象的 <code>.status</code>。</p>
<p>Controller 是 Kubernetes 的客户端。当 Kubernetes 充当客户端，
调用某远程服务时，对应的远程组件称作 <em>Webhook</em>，远程服务称作 Webhook 后端。
与控制器模式相似，Webhook 也会在整个架构中引入新的失效点（Point of Failure）。</p>
<!--
In the webhook model, Kubernetes makes a network request to a remote service.
In the *Binary Plugin* model, Kubernetes executes a binary (program).
Binary plugins are used by the kubelet (e.g.
[Flex Volume Plugins](/docs/concepts/storage/volumes/#flexvolume)
and [Network Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/))
and by kubectl.

Below is a diagram showing how the extension points interact with the
Kubernetes control plane.
-->
<p>在 Webhook 模式中，Kubernetes 向远程服务发起网络请求。
在 <strong>可执行文件插件（Binary Plugin）</strong> 模式中，Kubernetes
执行某个可执行文件（程序）。可执行文件插件在 kubelet （例如，
<a href="/zh/docs/concepts/storage/volumes/#flexvolume">FlexVolume 插件</a>)
和<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件</a>）
和 kubectl 中使用。</p>
<p>下面的示意图中展示了这些扩展点如何与 Kubernetes 控制面交互。</p>
<!-- image source drawing https://docs.google.com/drawings/d/1muJ7Oxuj_7Gtv7HV9-2zJbOnkQJnjxq-v1ym_kZfB-4/edit?ts=5a01e054 -->
<!--
![Extension Points and the Control Plane](/docs/concepts/extend-kubernetes/control-plane.png)
-->
<p><img src="/docs/concepts/extend-kubernetes/control-plane.png" alt="扩展点与控制面"></p>
<!--
## Extension Points

This diagram shows the extension points in a Kubernetes system.
-->
<h2 id="extension-points">扩展点  </h2>
<p>此示意图显示的是 Kubernetes 系统中的扩展点。</p>
<!-- image source diagrams: https://docs.google.com/drawings/d/1k2YdJgNTtNfW7_A8moIIkij-DmVgEhNrn3y2OODwqQQ/view -->
<!--
![Extension Points](/docs/concepts/extend-kubernetes/extension-points.png)
-->
<p><img src="/docs/concepts/extend-kubernetes/extension-points.png" alt="扩展点"></p>
<!--
1.   Users often interact with the Kubernetes API using `kubectl`. [Kubectl plugins](/docs/tasks/extend-kubectl/kubectl-plugins/) extend the kubectl binary. They only affect the individual user's local environment, and so cannot enforce site-wide policies.
2.   The apiserver handles all requests. Several types of extension points in the apiserver allow authenticating requests, or blocking them based on their content, editing content, and handling deletion. These are described in the [API Access Extensions](#api-access-extensions) section.
3.   The apiserver serves various kinds of *resources*. *Built-in resource kinds*, like `pods`, are defined by the Kubernetes project and can't be changed. You can also add resources that you define, or that other projects have defined, called *Custom Resources*, as explained in the [Custom Resources](#user-defined-types) section. Custom Resources are often used with API Access Extensions.
4.   The Kubernetes scheduler decides which nodes to place pods on. There are several ways to extend scheduling. These are described in the [Scheduler Extensions](#scheduler-extensions) section.
5.   Much of the behavior of Kubernetes is implemented by programs called Controllers which are clients of the API-Server. Controllers are often used in conjunction with Custom Resources.
6.   The kubelet runs on servers, and helps pods appear like virtual servers with their own IPs on the cluster network. [Network Plugins](#network-plugins) allow for different implementations of pod networking.
7.  The kubelet also mounts and unmounts volumes for containers. New types of storage can be supported via [Storage Plugins](#storage-plugins).

If you are unsure where to start, this flowchart can help. Note that some solutions may involve several types of extensions.
-->
<ol>
<li>
<p>用户通常使用 <code>kubectl</code> 与 Kubernetes API 交互。
<a href="/zh/docs/tasks/extend-kubectl/kubectl-plugins/">kubectl 插件</a>能够扩展 kubectl 程序的行为。
这些插件只会影响到每个用户的本地环境，因此无法用来强制实施整个站点范围的策略。</p>
</li>
<li>
<p>API 服务器处理所有请求。API 服务器中的几种扩展点能够使用户对请求执行身份认证、
基于其内容阻止请求、编辑请求内容、处理删除操作等等。
这些扩展点在 <a href="#api-access-extensions">API 访问扩展</a>节详述。</p>
</li>
<li>
<p>API 服务器向外提供不同类型的资源（resources）。
内置的资源类型，如 <code>pods</code>，是由 Kubernetes 项目所定义的，无法改变。
你也可以添加自己定义的或者其他项目所定义的称作自定义资源（Custom Resources）
的资源，正如<a href="#user-defined-types">自定义资源</a>节所描述的那样。
自定义资源通常与 API 访问扩展点结合使用。</p>
</li>
<li>
<p>Kubernetes 调度器负责决定 Pod 要放置到哪些节点上执行。
有几种方式来扩展调度行为。这些方法将在
<a href="#scheduler-extensions">调度器扩展</a>节中展开。</p>
</li>
<li>
<p>Kubernetes 中的很多行为都是通过称为控制器（Controllers）的程序来实现的，
这些程序也都是 API 服务器的客户端。控制器常常与自定义资源结合使用。</p>
</li>
<li>
<p>组件 kubelet 运行在各个节点上，帮助 Pod 展现为虚拟的服务器并在集群网络中拥有自己的 IP。
<a href="#network-plugins">网络插件</a>使得 Kubernetes 能够采用不同实现技术来连接
Pod 网络。</p>
</li>
<li>
<p>组件 kubelet 也会为容器增加或解除存储卷的挂载。
通过<a href="#storage-plugins">存储插件</a>，可以支持新的存储类型。</p>
</li>
</ol>
<p>如果你无法确定从何处入手，下面的流程图可能对你有些帮助。
注意，某些方案可能需要同时采用几种类型的扩展。</p>
<!-- image source drawing: https://docs.google.com/drawings/d/1sdviU6lDz4BpnzJNHfNpQrqI9F19QZ07KnhnxVrp2yg/edit -->
<!--
![Flowchart for Extension](/docs/concepts/extend-kubernetes/flowchart.png)
-->
<p><img src="/docs/concepts/extend-kubernetes/flowchart.png" alt="扩展流程图"></p>
<!--
## API Extensions

### User-Defined Types

Consider adding a Custom Resource to Kubernetes if you want to define new controllers, application configuration objects or other declarative APIs, and to manage them using Kubernetes tools, such as `kubectl`.

Do not use a Custom Resource as data storage for application, user, or monitoring data.

For more about Custom Resources, see the [Custom Resources concept guide](/docs/concepts/extend-kubernetes/api-extension/custom-resources/).
-->
<h2 id="api-extensions">API 扩展 </h2>
<h3 id="user-defined-types">用户定义的类型  </h3>
<p>如果你想要定义新的控制器、应用配置对象或者其他声明式 API，并且使用 Kubernetes
工具（如 <code>kubectl</code>）来管理它们，可以考虑向 Kubernetes 添加自定义资源。</p>
<p>不要使用自定义资源来充当应用、用户或者监控数据的数据存储。</p>
<p>关于自定义资源的更多信息，可参见<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源概念指南</a>。</p>
<!--
### Combining New APIs with Automation

The combination of a custom resource API and a control loop is called the [Operator pattern](/docs/concepts/extend-kubernetes/operator/). The Operator pattern is used to manage specific, usually stateful, applications. These custom APIs and control loops can also be used to control other resources, such as storage or policies.
-->
<h3 id="combinding-new-apis-with-automation">结合使用新 API 与自动化组件</h3>
<p>自定义资源 API 与控制回路的组合称作
<a href="/zh/docs/concepts/extend-kubernetes/operator/">Operator 模式</a>。
Operator 模式用来管理特定的、通常是有状态的应用。
这些自定义 API 和控制回路也可用来控制其他资源，如存储或策略。</p>
<!--
### Changing Built-in Resources

When you extend the Kubernetes API by adding custom resources, the added resources always fall into a new API Groups. You cannot replace or change existing API groups.
Adding an API does not directly let you affect the behavior of existing APIs (e.g. Pods), but API Access Extensions do.
-->
<h3 id="changing-built-in-resources">更改内置资源  </h3>
<p>当你通过添加自定义资源来扩展 Kubernetes 时，所添加的资源通常会被放在一个新的
API 组中。你不可以替换或更改现有的 API 组。
添加新的 API 不会直接让你影响现有 API （如 Pods）的行为，不过 API
访问扩展能够实现这点。</p>
<!--
### API Access Extensions

When a request reaches the Kubernetes API Server, it is first Authenticated, then Authorized, then subject to various types of Admission Control. See [Controlling Access to the Kubernetes API](/docs/concepts/security/controlling-access/) for more on this flow.

Each of these steps offers extension points.

Kubernetes has several built-in authentication methods that it supports. It can also sit behind an authenticating proxy, and it can send a token from an Authorization header to a remote service for verification (a webhook). All of these methods are covered in the [Authentication documentation](/docs/reference/access-authn-authz/authentication/).
-->
<h3 id="api-access-extensions">API 访问扩展   </h3>
<p>当请求到达 Kubernetes API 服务器时，首先要经过身份认证，之后是鉴权操作，
再之后要经过若干类型的准入控制器的检查。
参见<a href="/zh/docs/concepts/security/controlling-access/">控制 Kubernetes API 访问</a>
以了解此流程的细节。</p>
<p>这些步骤中都存在扩展点。</p>
<p>Kubernetes 提供若干内置的身份认证方法。它也可以运行在某种身份认证代理的后面，
并且可以将来自鉴权头部的令牌发送到某个远程服务（Webhook）来执行验证操作。
所有这些方法都在<a href="/zh/docs/reference/access-authn-authz/authentication/">身份认证文档</a>
中有详细论述。</p>
<!--
### Authentication

[Authentication](/docs/reference/access-authn-authz/authentication/) maps headers or certificates in all requests to a username for the client making the request.

Kubernetes provides several built-in authentication methods, and an [Authentication webhook](/docs/reference/access-authn-authz/authentication/#webhook-token-authentication) method if those don't meet your needs.
-->
<h3 id="authentication">身份认证   </h3>
<p><a href="/zh/docs/reference/access-authn-authz/authentication/">身份认证</a>负责将所有请求中
的头部或证书映射到发出该请求的客户端的用户名。</p>
<p>Kubernetes 提供若干种内置的认证方法，以及
<a href="/zh/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">认证 Webhook</a>
方法以备内置方法无法满足你的要求。</p>
<!--
### Authorization

[Authorization](/docs/reference/access-authn-authz/authorization/) determines whether specific users can read, write, and do other operations on API resources. It works at the level of whole resources - it doesn't discriminate based on arbitrary object fields. If the built-in authorization options don't meet your needs, and [Authorization webhook](/docs/reference/access-authn-authz/webhook/) allows calling out to user-provided code to make an authorization decision.
-->
<h3 id="authorization">鉴权   </h3>
<p><a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权</a>
操作负责确定特定的用户是否可以读、写 API 资源或对其执行其他操作。
此操作仅在整个资源集合的层面进行。
换言之，它不会基于对象的特定字段作出不同的判决。
如果内置的鉴权选项无法满足你的需要，你可以使用
<a href="/zh/docs/reference/access-authn-authz/webhook/">鉴权 Webhook</a>来调用用户提供
的代码，执行定制的鉴权操作。</p>
<!--
### Dynamic Admission Control

After a request is authorized, if it is a write operation, it also goes through [Admission Control](/docs/reference/access-authn-authz/admission-controllers/) steps. In addition to the built-in steps, there are several extensions:

*   The [Image Policy webhook](/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook) restricts what images can be run in containers.
*   To make arbitrary admission control decisions, a general [Admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks) can be used. Admission Webhooks can reject creations or updates.
-->
<h3 id="dynamic-admission-control">动态准入控制 </h3>
<p>请求的鉴权操作结束之后，如果请求的是写操作，还会经过
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/">准入控制</a>处理步骤。
除了内置的处理步骤，还存在一些扩展点：</p>
<ul>
<li><a href="/zh/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook">镜像策略 Webhook</a>
能够限制容器中可以运行哪些镜像。</li>
<li>为了执行任意的准入控制，可以使用一种通用的
<a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">准入 Webhook</a>
机制。这类 Webhook 可以拒绝对象创建或更新请求。</li>
</ul>
<!--
## Infrastructure Extensions

### Storage Plugins

[Flex Volumes](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/flexvolume-deployment.md
) allow users to mount volume types without built-in support by having the
Kubelet call a Binary Plugin to mount the volume.
-->
<h2 id="infrastructure-extensions">基础设施扩展   </h2>
<h3 id="storage-plugins">存储插件 </h3>
<p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/flexvolume-deployment.md">FlexVolumes</a>
卷可以让用户挂载无需内建支持的卷类型，
kubelet 会调用可执行文件插件来挂载对应的存储卷。</p>
<!--
FlexVolume is deprecated since Kubernetes v1.23. The Out-of-tree CSI driver is the recommended way to write volume drivers in Kubernetes. See [Kubernetes Volume Plugin FAQ for Storage Vendors](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors) for more information.
-->
<p>从 Kubernetes v1.23 开始，FlexVolume 被弃用。
在 Kubernetes 中编写卷驱动的推荐方式是使用树外（Out-of-tree）CSI 驱动。
详细信息可参阅 <a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors">Kubernetes Volume Plugin FAQ for Storage Vendors</a>。</p>
<!--
### Device Plugins

Device plugins allow a node to discover new Node resources (in addition to the
builtin ones like cpu and memory) via a [Device
Plugin](/docs/concepts/cluster-administration/device-plugins/).

### Network Plugins

Different networking fabrics can be supported via node-level [Network Plugins](/docs/admin/network-plugins/).
-->
<h3 id="device-plugins">设备插件   </h3>
<p>使用<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">设备插件</a>，
节点能够发现新的节点资源（除了内置的类似 CPU 和内存这类资源）。</p>
<h3 id="network-plugins">网络插件  </h3>
<p>通过节点层面的<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件</a>，
可以支持不同的网络设施。</p>
<!--
### Scheduler Extensions

The scheduler is a special type of controller that watches pods, and assigns
pods to nodes. The default scheduler can be replaced entirely, while
continuing to use other Kubernetes components, or
[multiple schedulers](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)
can run at the same time.

This is a significant undertaking, and almost all Kubernetes users find they
do not need to modify the scheduler.

The scheduler also supports a
[webhook](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md)
that permits a webhook backend (scheduler extension) to filter and prioritize
the nodes chosen for a pod.
-->
<h3 id="scheduler-extensions">调度器扩展  </h3>
<p>调度器是一种特殊的控制器，负责监视 Pod 变化并将 Pod 分派给节点。
默认的调度器可以被整体替换掉，同时继续使用其他 Kubernetes 组件。
或者也可以在同一时刻使用
<a href="/zh/docs/tasks/extend-kubernetes/configure-multiple-schedulers/">多个调度器</a>。</p>
<p>这是一项非同小可的任务，几乎绝大多数 Kubernetes
用户都会发现其实他们不需要修改调度器。</p>
<p>调度器也支持一种
<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md">Webhook</a>，
允许使用某种 Webhook 后端（调度器扩展）来为 Pod 可选的节点执行过滤和优先排序操作。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn more about [Custom Resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
* Learn about [Dynamic admission control](/docs/reference/access-authn-authz/extensible-admission-controllers/)
* Learn more about Infrastructure extensions
  * [Network Plugins](/docs/concepts/cluster-administration/network-plugins/)
  * [Device Plugins](/docs/concepts/cluster-administration/device-plugins/)
* Learn about [kubectl plugins](/docs/tasks/extend-kubectl/kubectl-plugins/)
* Learn about the [Operator pattern](/docs/concepts/extend-kubernetes/operator/)
-->
<ul>
<li>进一步了解<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</a></li>
<li>了解<a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/">动态准入控制</a></li>
<li>进一步了解基础设施扩展
<ul>
<li><a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件</a></li>
<li><a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">设备插件</a></li>
</ul>
</li>
<li>了解 <a href="/zh/docs/tasks/extend-kubectl/kubectl-plugins/">kubectl 插件</a></li>
<li>了解 <a href="/zh/docs/concepts/extend-kubernetes/operator/">Operator 模式</a></li>
</ul>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0af41d3bd7c785621b58b7564793396a">12.1 - 扩展 Kubernetes API</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-342388440304e19ce30c0f8ada1c77ce">12.1.1 - 定制资源</h1>
    
	<!--
title: Custom Resources
reviewers:
- enisoc
- deads2k
content_type: concept
weight: 10
-->
<!-- overview -->
<!--
*Custom resources* are extensions of the Kubernetes API. This page discusses when to add a custom
resource to your Kubernetes cluster and when to use a standalone service. It describes the two
methods for adding custom resources and how to choose between them.
-->
<p><em>定制资源（Custom Resource）</em> 是对 Kubernetes API 的扩展。
本页讨论何时向 Kubernetes 集群添加定制资源，何时使用独立的服务。
本页描述添加定制资源的两种方法以及怎样在二者之间做出抉择。</p>
<!-- body -->
<!--
## Custom resources

A *resource* is an endpoint in the
[Kubernetes API](/docs/concepts/overview/kubernetes-api/) that stores a collection of
[API objects](/docs/concepts/overview/working-with-objects/kubernetes-objects/) of
a certain kind; for example, the built-in *pods* resource contains a
collection of Pod objects.
-->
<h2 id="定制资源">定制资源</h2>
<p><em>资源（Resource）</em> 是
<a href="/zh/docs/concepts/overview/kubernetes-api/">Kubernetes API</a> 中的一个端点，
其中存储的是某个类别的
<a href="/zh/docs/concepts/overview/working-with-objects/kubernetes-objects/">API 对象</a>
的一个集合。
例如内置的 <em>pods</em> 资源包含一组 Pod 对象。</p>
<!--
A *custom resource* is an extension of the Kubernetes API that is not necessarily available in a default
Kubernetes installation. It represents a customization of a particular Kubernetes installation. However,
many core Kubernetes functions are now built using custom resources, making Kubernetes more modular.

Custom resources can appear and disappear in a running cluster through dynamic registration,
and cluster admins can update custom resources independently of the cluster itself.
Once a custom resource is installed, users can create and access its objects using
[kubectl](/docs/reference/kubectl/), just as they do for built-in resources like
*Pods*.
-->
<p><em>定制资源（Custom Resource）</em> 是对 Kubernetes API 的扩展，不一定在默认的
Kubernetes 安装中就可用。定制资源所代表的是对特定 Kubernetes 安装的一种定制。
不过，很多 Kubernetes 核心功能现在都用定制资源来实现，这使得 Kubernetes
更加模块化。</p>
<p>定制资源可以通过动态注册的方式在运行中的集群内或出现或消失，集群管理员可以独立于集群
更新定制资源。一旦某定制资源被安装，用户可以使用
<a href="/docs/reference/kubectl/">kubectl</a>
来创建和访问其中的对象，就像他们为 <em>pods</em> 这种内置资源所做的一样。</p>
<!--
## Custom controllers

On their own, custom resources let you store and retrieve structured data.
When you combine a custom resource with a *custom controller*, custom resources
provide a true _declarative API_.
-->
<h2 id="custom-controllers">定制控制器  </h2>
<p>就定制资源本身而言，它只能用来存取结构化的数据。
当你将定制资源与 <em>定制控制器（Custom Controller）</em> 相结合时，定制资源就能够
提供真正的 <em>声明式 API（Declarative API）</em>。</p>
<!--
A [declarative API](/docs/concepts/overview/kubernetes-api/)
allows you to _declare_ or specify the desired state of your resource and tries to
keep the current state of Kubernetes objects in sync with the desired state.
The controller interprets the structured data as a record of the user's
desired state, and continually maintains this state.
-->
<p>使用<a href="/zh/docs/concepts/overview/kubernetes-api/">声明式 API</a>，
你可以 <em>声明</em> 或者设定你的资源的期望状态，并尝试让 Kubernetes 对象的当前状态
同步到其期望状态。控制器负责将结构化的数据解释为用户所期望状态的记录，并
持续地维护该状态。</p>
<!--
You can deploy and update a custom controller on a running cluster, independently
of the cluster's lifecycle. Custom controllers can work with any kind of resource,
but they are especially effective when combined with custom resources. The
[Operator pattern](/docs/concepts/extend-kubernetes/operator/) combines custom
resources and custom controllers. You can use custom controllers to encode domain knowledge
for specific applications into an extension of the Kubernetes API.
-->
<p>你可以在一个运行中的集群上部署和更新定制控制器，这类操作与集群的生命周期无关。
定制控制器可以用于任何类别的资源，不过它们与定制资源结合起来时最为有效。
<a href="/zh/docs/concepts/extend-kubernetes/operator/">Operator 模式</a>就是将定制资源
与定制控制器相结合的。你可以使用定制控制器来将特定于某应用的领域知识组织
起来，以编码的形式构造对 Kubernetes API 的扩展。</p>
<!--
## Should I add a custom resource to my Kubernetes Cluster?

When creating a new API, consider whether to
[aggregate your API with the Kubernetes cluster APIs](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)
or let your API stand alone.
-->
<h2 id="我是否应该向我的-kubernetes-集群添加定制资源">我是否应该向我的 Kubernetes 集群添加定制资源？</h2>
<p>在创建新的 API 时，请考虑是
<a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">将你的 API 与 Kubernetes 集群 API 聚合起来</a>
还是让你的 API 独立运行。</p>
<!--
| Consider API aggregation if: | Prefer a stand-alone API if: |
| ---------------------------- | ---------------------------- |
| Your API is [Declarative](#declarative-apis). | Your API does not fit the [Declarative](#declarative-apis) model. |
| You want your new types to be readable and writable using `kubectl`.| `kubectl` support is not required |
| You want to view your new types in a Kubernetes UI, such as dashboard, alongside built-in types. | Kubernetes UI support is not required. |
| You are developing a new API. | You already have a program that serves your API and works well. |
| You are willing to accept the format restriction that Kubernetes puts on REST resource paths, such as API Groups and Namespaces. (See the [API Overview](/docs/concepts/overview/kubernetes-api/).) | You need to have specific REST paths to be compatible with an already defined REST API. |
| Your resources are naturally scoped to a cluster or namespaces of a cluster. | Cluster or namespace scoped resources are a poor fit; you need control over the specifics of resource paths. |
| You want to reuse [Kubernetes API support features](#common-features).  | You don't need those features. |
-->
<table>
<thead>
<tr>
<th>考虑 API 聚合的情况</th>
<th>优选独立 API 的情况</th>
</tr>
</thead>
<tbody>
<tr>
<td>你的 API 是<a href="#declarative-apis">声明式的</a>。</td>
<td>你的 API 不符合<a href="#declarative-apis">声明式</a>模型。</td>
</tr>
<tr>
<td>你希望可以是使用 <code>kubectl</code> 来读写你的新资源类别。</td>
<td>不要求 <code>kubectl</code> 支持。</td>
</tr>
<tr>
<td>你希望在 Kubernetes UI （如仪表板）中和其他内置类别一起查看你的新资源类别。</td>
<td>不需要 Kubernetes UI 支持。</td>
</tr>
<tr>
<td>你在开发新的 API。</td>
<td>你已经有一个提供 API 服务的程序并且工作良好。</td>
</tr>
<tr>
<td>你有意愿取接受 Kubernetes 对 REST 资源路径所作的格式限制，例如 API 组和名字空间。（参阅 <a href="/zh/docs/concepts/overview/kubernetes-api/">API 概述</a>）</td>
<td>你需要使用一些特殊的 REST 路径以便与已经定义的 REST API 保持兼容。</td>
</tr>
<tr>
<td>你的资源可以自然地界定为集群作用域或集群中某个名字空间作用域。</td>
<td>集群作用域或名字空间作用域这种二分法很不合适；你需要对资源路径的细节进行控制。</td>
</tr>
<tr>
<td>你希望复用 <a href="#common-features">Kubernetes API 支持特性</a>。</td>
<td>你不需要这类特性。</td>
</tr>
</tbody>
</table>
<!--
### Declarative APIs

In a Declarative API, typically:

 - Your API consists of a relatively small number of relatively small objects (resources).
 - The objects define configuration of applications or infrastructure.
 - The objects are updated relatively infrequently.
 - Humans often need to read and write the objects.
 - The main operations on the objects are CRUD-y (creating, reading, updating and deleting).
 - Transactions across objects are not required: the API represents a desired state, not an exact state.
-->
<h3 id="declarative-apis">声明式 APIs</h3>
<p>典型地，在声明式 API 中：</p>
<ul>
<li>你的 API 包含相对而言为数不多的、尺寸较小的对象（资源）。</li>
<li>对象定义了应用或者基础设施的配置信息。</li>
<li>对象更新操作频率较低。</li>
<li>通常需要人来读取或写入对象。</li>
<li>对象的主要操作是 CRUD 风格的（创建、读取、更新和删除）。</li>
<li>不需要跨对象的事务支持：API 对象代表的是期望状态而非确切实际状态。</li>
</ul>
<!--
Imperative APIs are not declarative.
Signs that your API might not be declarative include:

 - The client says "do this", and then gets a synchronous response back when it is done.
 - The client says "do this", and then gets an operation ID back, and has to check a separate Operation object to determine completion of the request.
 - You talk about Remote Procedure Calls (RPCs).
 - Directly storing large amounts of data; for example, > a few kB per object, or > 1000s of objects.
 - High bandwidth access (10s of requests per second sustained) needed.
 - Store end-user data (such as images, PII, etc.) or other large-scale data processed by applications.
 - The natural operations on the objects are not CRUD-y.
 - The API is not easily modeled as objects.
 - You chose to represent pending operations with an operation ID or an operation object.
-->
<p>命令式 API（Imperative API）与声明式有所不同。
以下迹象表明你的 API 可能不是声明式的：</p>
<ul>
<li>客户端发出“做这个操作”的指令，之后在该操作结束时获得同步响应。</li>
<li>客户端发出“做这个操作”的指令，并获得一个操作 ID，之后需要检查一个 Operation（操作）
对象来判断请求是否成功完成。</li>
<li>你会将你的 API 类比为远程过程调用（Remote Procedure Call，RPCs）。</li>
<li>直接存储大量数据；例如每个对象几 kB，或者存储上千个对象。</li>
<li>需要较高的访问带宽（长期保持每秒数十个请求）。</li>
<li>存储有应用来处理的最终用户数据（如图片、个人标识信息（PII）等）或者其他大规模数据。</li>
<li>在对象上执行的常规操作并非 CRUD 风格。</li>
<li>API 不太容易用对象来建模。</li>
<li>你决定使用操作 ID 或者操作对象来表现悬决的操作。</li>
</ul>
<!--
## Should I use a configMap or a custom resource?

Use a ConfigMap if any of the following apply:

* There is an existing, well-documented config file format, such as a `mysql.cnf` or `pom.xml`.
* You want to put the entire config file into one key of a configMap.
* The main use of the config file is for a program running in a Pod on your cluster to consume the file to configure itself.
* Consumers of the file prefer to consume via file in a Pod or environment variable in a pod, rather than the Kubernetes API.
* You want to perform rolling updates via Deployment, etc., when the file is updated.
-->
<h2 id="我应该使用一个-configmap-还是一个定制资源">我应该使用一个 ConfigMap 还是一个定制资源？</h2>
<p>如果满足以下条件之一，应该使用 ConfigMap：</p>
<ul>
<li>存在一个已有的、文档完备的配置文件格式约定，例如 <code>mysql.cnf</code> 或 <code>pom.xml</code>。</li>
<li>你希望将整个配置文件放到某 configMap 中的一个主键下面。</li>
<li>配置文件的主要用途是针对运行在集群中 Pod 内的程序，供后者依据文件数据配置自身行为。</li>
<li>文件的使用者期望以 Pod 内文件或者 Pod 内环境变量的形式来使用文件数据，
而不是通过 Kubernetes API。</li>
<li>你希望当文件被更新时通过类似 Deployment 之类的资源完成滚动更新操作。</li>
</ul>
<!--
Use a [secret](/docs/concepts/configuration/secret/) for sensitive data, which is similar to a configMap but more secure.
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> 请使用 <a href="/zh/docs/concepts/configuration/secret/">Secret</a> 来保存敏感数据。
Secret 类似于 configMap，但更为安全。
</div>
<!--
Use a custom resource (CRD or Aggregated API) if most of the following apply:

* You want to use Kubernetes client libraries and CLIs to create and update the new resource.
* You want top-level support from `kubectl`; for example, `kubectl get my-object object-name`.
* You want to build new automation that watches for updates on the new object, and then CRUD other objects, or vice versa.
* You want to write automation that handles updates to the object.
* You want to use Kubernetes API conventions like `.spec`, `.status`, and `.metadata`.
* You want the object to be an abstraction over a collection of controlled resources, or a summarization of other resources.
-->
<p>如果以下条件中大多数都被满足，你应该使用定制资源（CRD 或者 聚合 API）：</p>
<ul>
<li>你希望使用 Kubernetes 客户端库和 CLI 来创建和更改新的资源。</li>
<li>你希望 <code>kubectl</code> 能够直接支持你的资源；例如，<code>kubectl get my-object object-name</code>。</li>
<li>你希望构造新的自动化机制，监测新对象上的更新事件，并对其他对象执行 CRUD
操作，或者监测后者更新前者。</li>
<li>你希望编写自动化组件来处理对对象的更新。</li>
<li>你希望使用 Kubernetes API 对诸如 <code>.spec</code>、<code>.status</code> 和 <code>.metadata</code> 等字段的约定。</li>
<li>你希望对象是对一组受控资源的抽象，或者对其他资源的归纳提炼。</li>
</ul>
<!--
## Adding custom resources

Kubernetes provides two ways to add custom resources to your cluster:

- CRDs are simple and can be created without any programming.
- [API Aggregation](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/) requires programming, but allows more control over API behaviors like how data is stored and conversion between API versions.
-->
<h2 id="adding-custom-resources">添加定制资源  </h2>
<p>Kubernetes 提供了两种方式供你向集群中添加定制资源：</p>
<ul>
<li>CRD 相对简单，创建 CRD 可以不必编程。</li>
<li><a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">API 聚合</a>
需要编程，但支持对 API 行为进行更多的控制，例如数据如何存储以及在不同 API 版本间如何转换等。</li>
</ul>
<!--
Kubernetes provides these two options to meet the needs of different users, so that neither ease of use nor flexibility is compromised.

Aggregated APIs are subordinate API servers that sit behind the primary API server, which acts as a proxy. This arrangement is called [API Aggregation](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/) (AA). To users, the Kubernetes API is extended.

CRDs allow users to create new types of resources without adding another API server. You do not need to understand API Aggregation to use CRDs.

Regardless of how they are installed, the new resources are referred to as Custom Resources to distinguish them from built-in Kubernetes resources (like pods).
-->
<p>Kubernetes 提供这两种选项以满足不同用户的需求，这样就既不会牺牲易用性也不会牺牲灵活性。</p>
<p>聚合 API 指的是一些下位的 API 服务器，运行在主 API 服务器后面；主 API
服务器以代理的方式工作。这种组织形式称作
<a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">API 聚合（API Aggregation，AA）</a> 。
对用户而言，看起来仅仅是 Kubernetes API 被扩展了。</p>
<p>CRD 允许用户创建新的资源类别同时又不必添加新的 API 服务器。
使用 CRD 时，你并不需要理解 API 聚合。</p>
<p>无论以哪种方式安装定制资源，新的资源都会被当做定制资源，以便与内置的
Kubernetes 资源（如 Pods）相区分。</p>
<!--
## CustomResourceDefinitions

The [CustomResourceDefinition](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)
API resource allows you to define custom resources.
Defining a CRD object creates a new custom resource with a name and schema that you specify.
The Kubernetes API serves and handles the storage of your custom resource.
The name of a CRD object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<h2 id="customresourcedefinitions">CustomResourceDefinitions</h2>
<p><a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">CustomResourceDefinition</a>
API 资源允许你定义定制资源。
定义 CRD 对象的操作会使用你所设定的名字和模式定义（Schema）创建一个新的定制资源，
Kubernetes API 负责为你的定制资源提供存储和访问服务。
CRD 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。</p>
<!--
This frees you from writing your own API server to handle the custom resource,
but the generic nature of the implementation means you have less flexibility than with
[API server aggregation](#api-server-aggregation).

Refer to the [custom controller example](https://github.com/kubernetes/sample-controller)
for an example of how to register a new custom resource, work with instances of your new resource type,
and use a controller to handle events.
-->
<p>CRD 使得你不必编写自己的 API 服务器来处理定制资源，不过其背后实现的通用性也意味着
你所获得的灵活性要比 <a href="#api-server-aggregation">API 服务器聚合</a>少很多。</p>
<p>关于如何注册新的定制资源、使用新资源类别的实例以及如何使用控制器来处理事件，
相关的例子可参见<a href="https://github.com/kubernetes/sample-controller">定制控制器示例</a>。</p>
<!--
## API server aggregation

Usually, each resource in the Kubernetes API requires code that handles REST requests and manages persistent storage of objects. The main Kubernetes API server handles built-in resources like *pods* and *services*, and can also generically handle custom resources through [CRDs](#customresourcedefinitions).

The [aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/) allows you to provide specialized
implementations for your custom resources by writing and deploying your own API server.
The main API server delegates requests to your API server for the custom resources that you handle,
making them available to all of its clients.

-->
<h2 id="api-server-aggregation">API 服务器聚合 </h2>
<p>通常，Kubernetes API 中的每个资源都需要处理 REST 请求和管理对象持久性存储的代码。
Kubernetes API 主服务器能够处理诸如 <em>pods</em> 和 <em>services</em> 这些内置资源，也可以
按通用的方式通过 <a href="#customresourcedefinitions">CRD</a> 来处理定制资源。</p>
<p><a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">聚合层（Aggregation Layer）</a>
使得你可以通过编写和部署你自己的 API 服务器来为定制资源提供特殊的实现。
主 API 服务器将针对你要处理的定制资源的请求全部委托给你自己的 API 服务器来处理，同时将这些资源
提供给其所有客户端。</p>
<!--
## Choosing a method for adding custom resources

CRDs are easier to use. Aggregated APIs are more flexible. Choose the method that best meets your needs.

Typically, CRDs are a good fit if:

* You have a handful of fields
* You are using the resource within your company, or as part of a small open-source project (as opposed to a commercial product)
-->
<h2 id="选择添加定制资源的方法">选择添加定制资源的方法</h2>
<p>CRD 更为易用；聚合 API 则更为灵活。请选择最符合你的需要的方法。</p>
<p>通常，如何存在以下情况，CRD 可能更合适：</p>
<ul>
<li>定制资源的字段不多；</li>
<li>你在组织内部使用该资源或者在一个小规模的开源项目中使用该资源，而不是
在商业产品中使用。</li>
</ul>
<!--
### Comparing ease of use

CRDs are easier to create than Aggregated APIs.
-->
<h3 id="compare-ease-of-use">比较易用性 </h3>
<p>CRD 比聚合 API 更容易创建</p>
<!--
| CRDs                        | Aggregated API |
| --------------------------- | -------------- |
| Do not require programming. Users can choose any language for a CRD controller. | Requires programming in Go and building binary and image. |
| No additional service to run; CRDs are handled by API server. | An additional service to create and that could fail. |
| No ongoing support once the CRD is created. Any bug fixes are picked up as part of normal Kubernetes Master upgrades. | May need to periodically pickup bug fixes from upstream and rebuild and update the Aggregated API server. |
| No need to handle multiple versions of your API; for example, when you control the client for this resource, you can upgrade it in sync with the API. | You need to handle multiple versions of your API; for example, when developing an extension to share with the world. |
-->
<table>
<thead>
<tr>
<th>CRDs</th>
<th>聚合 API</th>
</tr>
</thead>
<tbody>
<tr>
<td>无需编程。用户可选择任何语言来实现 CRD 控制器。</td>
<td>需要使用 Go 来编程，并构建可执行文件和镜像。</td>
</tr>
<tr>
<td>无需额外运行服务；CRD 由 API 服务器处理。</td>
<td>需要额外创建服务，且该服务可能失效。</td>
</tr>
<tr>
<td>一旦 CRD 被创建，不需要持续提供支持。Kubernetes 主控节点升级过程中自动会带入缺陷修复。</td>
<td>可能需要周期性地从上游提取缺陷修复并更新聚合 API 服务器。</td>
</tr>
<tr>
<td>无需处理 API 的多个版本；例如，当你控制资源的客户端时，你可以更新它使之与 API 同步。</td>
<td>你需要处理 API 的多个版本；例如，在开发打算与很多人共享的扩展时。</td>
</tr>
</tbody>
</table>
<!--
### Advanced features and flexibility

Aggregated APIs offer more advanced API features and customization of other features; for example, the storage layer.
-->
<h3 id="advanced-features-and-flexibility">高级特性与灵活性 </h3>
<p>聚合 API 可提供更多的高级 API 特性，也可对其他特性实行定制；例如，对存储层进行定制。</p>
<!--
| Feature  | Description  | CRDs | Aggregated API   |
| -------- | ------------ | ---- | ---------------- |
| Validation | Help users prevent errors and allow you to evolve your API independently of your clients. These features are most useful when there are many clients who can't all update at the same time. | Yes.  Most validation can be specified in the CRD using [OpenAPI v3.0 validation](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation).  Any other validations supported by addition of a [Validating Webhook](/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook-alpha-in-1-8-beta-in-1-9). | Yes, arbitrary validation checks |
| Defaulting | See above | Yes, either via [OpenAPI v3.0 validation](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting) `default` keyword (GA in 1.17), or via a [Mutating Webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook) (though this will not be run when reading from etcd for old objects). | Yes |
| Multi-versioning | Allows serving the same object through two API versions. Can help ease API changes like renaming fields. Less important if you control your client versions. | [Yes](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning) | Yes |
| Custom Storage | If you need storage with a different performance mode (for example, a time-series database instead of key-value store) or isolation for security (for example, encryption of sensitive information, etc.) | No | Yes |
| Custom Business Logic | Perform arbitrary checks or actions when creating, reading, updating or deleting an object | Yes, using [Webhooks](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks). | Yes |
| Scale Subresource | Allows systems like HorizontalPodAutoscaler and PodDisruptionBudget interact with your new resource | [Yes](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource)  | Yes |
| Status Subresource | Allows fine-grained access control where user writes the spec section and the controller writes the status section. Allows incrementing object Generation on custom resource data mutation (requires separate spec and status sections in the resource) | [Yes](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#status-subresource) | Yes |
| Other Subresources | Add operations other than CRUD, such as "logs" or "exec". | No | Yes |
| strategic-merge-patch | The new endpoints support PATCH with `Content-Type: application/strategic-merge-patch+json`. Useful for updating objects that may be modified both locally, and by the server. For more information, see ["Update API Objects in Place Using kubectl patch"](/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/) | No | Yes |
| Protocol Buffers | The new resource supports clients that want to use Protocol Buffers | No | Yes |
| OpenAPI Schema | Is there an OpenAPI (swagger) schema for the types that can be dynamically fetched from the server? Is the user protected from misspelling field names by ensuring only allowed fields are set? Are types enforced (in other words, don't put an `int` in a `string` field?) | Yes, based on the [OpenAPI v3.0 validation](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation) schema (GA in 1.16). | Yes |
-->
<table>
<thead>
<tr>
<th>特性</th>
<th>描述</th>
<th>CRDs</th>
<th>聚合 API</th>
</tr>
</thead>
<tbody>
<tr>
<td>合法性检查</td>
<td>帮助用户避免错误，允许你独立于客户端版本演化 API。这些特性对于由很多无法同时更新的客户端的场合。</td>
<td>可以。大多数验证可以使用 <a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation">OpenAPI v3.0 合法性检查</a> 来设定。其他合法性检查操作可以通过添加<a href="/zh/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook-alpha-in-1-8-beta-in-1-9">合法性检查 Webhook</a>来实现。</td>
<td>可以，可执行任何合法性检查。</td>
</tr>
<tr>
<td>默认值设置</td>
<td>同上</td>
<td>可以。可通过 <a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting">OpenAPI v3.0 合法性检查</a>的 <code>default</code> 关键词（自 1.17 正式发布）或<a href="/zh/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">更改性（Mutating）Webhook</a>来实现（不过从 etcd 中读取老的对象时不会执行这些 Webhook）。</td>
<td>可以。</td>
</tr>
<tr>
<td>多版本支持</td>
<td>允许通过两个 API 版本同时提供同一对象。可帮助简化类似字段更名这类 API 操作。如果你能控制客户端版本，这一特性将不再重要。</td>
<td><a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning">可以</a>。</td>
<td>可以。</td>
</tr>
<tr>
<td>定制存储</td>
<td>支持使用具有不同性能模式的存储（例如，要使用时间序列数据库而不是键值存储），或者因安全性原因对存储进行隔离（例如对敏感信息执行加密）。</td>
<td>不可以。</td>
<td>可以。</td>
</tr>
<tr>
<td>定制业务逻辑</td>
<td>在创建、读取、更新或删除对象时，执行任意的检查或操作。</td>
<td>可以。要使用 <a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">Webhook</a>。</td>
<td>可以。</td>
</tr>
<tr>
<td>支持 scale 子资源</td>
<td>允许 HorizontalPodAutoscaler 和 PodDisruptionBudget 这类子系统与你的新资源交互。</td>
<td><a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource">可以</a>。</td>
<td>可以。</td>
</tr>
<tr>
<td>支持 status 子资源</td>
<td>允许在用户写入 spec 部分而控制器写入 status 部分时执行细粒度的访问控制。允许在对定制资源的数据进行更改时增加对象的代际（Generation）；这需要资源对 spec 和 status 部分有明确划分。</td>
<td><a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#status-subresource">可以</a>。</td>
<td>可以。</td>
</tr>
<tr>
<td>其他子资源</td>
<td>添加 CRUD 之外的操作，例如 &quot;logs&quot; 或 &quot;exec&quot;。</td>
<td>不可以。</td>
<td>可以。</td>
</tr>
<tr>
<td>strategic-merge-patch</td>
<td>新的端点要支持标记了 <code>Content-Type: application/strategic-merge-patch+json</code> 的 PATCH 操作。对于更新既可在本地更改也可在服务器端更改的对象而言是有用的。要了解更多信息，可参见<a href="/zh/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">使用 <code>kubectl patch</code> 来更新 API 对象</a>。</td>
<td>不可以。</td>
<td>可以。</td>
</tr>
<tr>
<td>支持协议缓冲区</td>
<td>新的资源要支持想要使用协议缓冲区（Protocol Buffer）的客户端。</td>
<td>不可以。</td>
<td>可以。</td>
</tr>
<tr>
<td>OpenAPI Schema</td>
<td>是否存在新资源类别的 OpenAPI（Swagger）Schema 可供动态从服务器上读取？是否存在机制确保只能设置被允许的字段以避免用户犯字段拼写错误？是否实施了字段类型检查（换言之，不允许在 <code>string</code> 字段设置 <code>int</code> 值）？</td>
<td>可以，依据 <a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation">OpenAPI v3.0 合法性检查</a> 模式（1.16 中进入正式发布状态）。</td>
<td>可以。</td>
</tr>
</tbody>
</table>
<!--
### Common Features

When you create a custom resource, either via a CRD or an AA, you get many features for your API, compared to implementing it outside the Kubernetes platform:
-->
<h3 id="common-features">公共特性 </h3>
<p>与在 Kubernetes 平台之外实现定制资源相比，
无论是通过 CRD 还是通过聚合 API 来创建定制资源，你都会获得很多 API 特性：</p>
<!--
| Feature  | What it does |
| -------- | ------------ |
| CRUD | The new endpoints support CRUD basic operations via HTTP and `kubectl` |
| Watch | The new endpoints support Kubernetes Watch operations via HTTP |
| Discovery | Clients like `kubectl` and dashboard automatically offer list, display, and field edit operations on your resources |
| json-patch | The new endpoints support PATCH with `Content-Type: application/json-patch+json` |
| merge-patch | The new endpoints support PATCH with `Content-Type: application/merge-patch+json` |
| HTTPS | The new endpoints uses HTTPS |
| Built-in Authentication | Access to the extension uses the core API server (aggregation layer) for authentication |
| Built-in Authorization | Access to the extension can reuse the authorization used by the core API server; for example, RBAC. |
| Finalizers | Block deletion of extension resources until external cleanup happens. |
| Admission Webhooks | Set default values and validate extension resources during any create/update/delete operation. |
| UI/CLI Display | Kubectl, dashboard can display extension resources. |
| Unset versus Empty | Clients can distinguish unset fields from zero-valued fields. |
| Client Libraries Generation | Kubernetes provides generic client libraries, as well as tools to generate type-specific client libraries. |
| Labels and annotations | Common metadata across objects that tools know how to edit for core and custom resources. |
-->
<table>
<thead>
<tr>
<th>功能特性</th>
<th>具体含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>CRUD</td>
<td>新的端点支持通过 HTTP 和 <code>kubectl</code> 发起的 CRUD 基本操作</td>
</tr>
<tr>
<td>监测（Watch）</td>
<td>新的端点支持通过 HTTP 发起的 Kubernetes Watch 操作</td>
</tr>
<tr>
<td>发现（Discovery）</td>
<td>类似 <code>kubectl</code> 和仪表盘（Dashboard）这类客户端能够自动提供列举、显示、在字段级编辑你的资源的操作</td>
</tr>
<tr>
<td>json-patch</td>
<td>新的端点支持带 <code>Content-Type: application/json-patch+json</code> 的 PATCH 操作</td>
</tr>
<tr>
<td>merge-patch</td>
<td>新的端点支持带 <code>Content-Type: application/merge-patch+json</code> 的 PATCH 操作</td>
</tr>
<tr>
<td>HTTPS</td>
<td>新的端点使用 HTTPS</td>
</tr>
<tr>
<td>内置身份认证</td>
<td>对扩展的访问会使用核心 API 服务器（聚合层）来执行身份认证操作</td>
</tr>
<tr>
<td>内置鉴权授权</td>
<td>对扩展的访问可以复用核心 API 服务器所使用的鉴权授权机制；例如，RBAC</td>
</tr>
<tr>
<td>Finalizers</td>
<td>在外部清除工作结束之前阻止扩展资源被删除</td>
</tr>
<tr>
<td>准入 Webhooks</td>
<td>在创建、更新和删除操作中对扩展资源设置默认值和执行合法性检查</td>
</tr>
<tr>
<td>UI/CLI 展示</td>
<td><code>kubectl</code> 和仪表盘（Dashboard）可以显示扩展资源</td>
</tr>
<tr>
<td>区分未设置值和空值</td>
<td>客户端能够区分哪些字段是未设置的，哪些字段的值是被显式设置为零值的。</td>
</tr>
<tr>
<td>生成客户端库</td>
<td>Kubernetes 提供通用的客户端库，以及用来生成特定类别客户端库的工具</td>
</tr>
<tr>
<td>标签和注解</td>
<td>提供涵盖所有对象的公共元数据结构，且工具知晓如何编辑核心资源和定制资源的这些元数据</td>
</tr>
</tbody>
</table>
<!--
## Preparing to install a custom resource

There are several points to be aware of before adding a custom resource to your cluster.
-->
<h2 id="准备安装定制资源">准备安装定制资源</h2>
<p>在向你的集群添加定制资源之前，有些事情需要搞清楚。</p>
<!--
### Third party code and new points of failure

While creating a CRD does not automatically add any new points of failure (for example, by causing third party code to run on your API server), packages (for example, Charts) or other installation bundles often include CRDs as well as a Deployment of third-party code that implements the business logic for a new custom resource.

Installing an Aggregated API server always involves running a new Deployment.
-->
<h3 id="第三方代码和新的失效点的问题">第三方代码和新的失效点的问题</h3>
<p>尽管添加新的 CRD 不会自动带来新的失效点（Point of
Failure），例如导致第三方代码被在 API 服务器上运行，
类似 Helm Charts 这种软件包或者其他安装包通常在提供 CRD 的同时还包含带有第三方
代码的 Deployment，负责实现新的定制资源的业务逻辑。</p>
<p>安装聚合 API 服务器时，也总会牵涉到运行一个新的 Deployment。</p>
<!--
### Storage

Custom resources consume storage space in the same way that ConfigMaps do. Creating too many custom resources may overload your API server's storage space.

Aggregated API servers may use the same storage as the main API server, in which case the same warning applies.
-->
<h3 id="存储">存储</h3>
<p>定制资源和 ConfigMap 一样也会消耗存储空间。创建过多的定制资源可能会导致
API 服务器上的存储空间超载。</p>
<p>聚合 API 服务器可以使用主 API 服务器的同一存储。如果是这样，你也要注意
此警告。</p>
<!--
### Authentication, authorization, and auditing

CRDs always use the same authentication, authorization, and audit logging as the built-in resources of your API server.

If you use RBAC for authorization, most RBAC roles will not grant access to the new resources (except the cluster-admin role or any role created with wildcard rules). You'll need to explicitly grant access to the new resources. CRDs and Aggregated APIs often come bundled with new role definitions for the types they add.

Aggregated API servers may or may not use the same authentication, authorization, and auditing as the primary API server.
-->
<h3 id="身份认证-鉴权授权以及审计">身份认证、鉴权授权以及审计</h3>
<p>CRD 通常与 API 服务器上的内置资源一样使用相同的身份认证、鉴权授权
和审计日志机制。</p>
<p>如果你使用 RBAC 来执行鉴权授权，大多数 RBAC 角色都会授权对新资源的访问
（除了 cluster-admin 角色以及使用通配符规则创建的其他角色）。
你要显式地为新资源的访问授权。CRD 和聚合 API 通常在交付时会包含
针对所添加的类别的新的角色定义。</p>
<p>聚合 API 服务器可能会使用主 API 服务器相同的身份认证、鉴权授权和审计
机制，也可能不会。</p>
<!--
## Accessing a custom resource

Kubernetes [client libraries](/docs/reference/using-api/client-libraries/) can be used to access custom resources. Not all client libraries support custom resources. The _Go_ and _Python_ client libraries do.

When you add a custom resource, you can access it using:

- `kubectl`
- The kubernetes dynamic client.
- A REST client that you write.
- A client generated using [Kubernetes client generation tools](https://github.com/kubernetes/code-generator) (generating one is an advanced undertaking, but some projects may provide a client along with the CRD or AA).
-->
<h2 id="访问定制资源">访问定制资源</h2>
<p>Kubernetes <a href="/zh/docs/reference/using-api/client-libraries/">客户端库</a>可用来访问定制资源。
并非所有客户端库都支持定制资源。<em>Go</em> 和 <em>Python</em> 客户端库是支持的。</p>
<p>当你添加了新的定制资源后，可以用如下方式之一访问它们：</p>
<ul>
<li><code>kubectl</code></li>
<li>Kubernetes 动态客户端</li>
<li>你所编写的 REST 客户端</li>
<li>使用 <a href="https://github.com/kubernetes/code-generator">Kubernetes 客户端生成工具</a>
所生成的客户端。生成客户端的工作有些难度，不过某些项目可能会随着 CRD 或
聚合 API 一起提供一个客户端</li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn how to [Extend the Kubernetes API with the aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/).
* Learn how to [Extend the Kubernetes API with CustomResourceDefinition](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/).
-->
<ul>
<li>了解如何<a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">使用聚合层扩展 Kubernetes API</a></li>
<li>了解如何<a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">使用 CustomResourceDefinition 来扩展 Kubernetes API</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1ea4977c0ebf97569bf54a477faa7fa5">12.1.2 - 通过聚合层扩展 Kubernetes API</h1>
    
	<!--
title: Extending the Kubernetes API with the aggregation layer
reviewers:
- lavalamp
- cheftako
- chenopis
content_type: concept
weight: 20
-->
<!-- overview -->
<!--
The aggregation layer allows Kubernetes to be extended with additional APIs, beyond what is offered by the core Kubernetes APIs.
-->
<p>使用聚合层（Aggregation Layer），用户可以通过额外的 API 扩展 Kubernetes，
而不局限于 Kubernetes 核心 API 提供的功能。</p>
<!--
The additional APIs can either be ready-made solutions such as a [metrics server](https://github.com/kubernetes-sigs/metrics-server), or APIs that you develop yourself.

The aggregation layer is different from [Custom Resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/), which are a way to make the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='kube-apiserver'>kube-apiserver</a> recognise new kinds of object.
-->
<p>这里的附加 API 可以是现成的解决方案比如
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics server</a>,
或者你自己开发的 API。</p>
<p>聚合层不同于
<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">定制资源（Custom Resources）</a>。
后者的目的是让 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='kube-apiserver'>kube-apiserver</a>
能够认识新的对象类别（Kind）。</p>
<!-- body -->
<!--
## Aggregation layer

The aggregation layer runs in-process with the kube-apiserver. Until an extension resource is registered, the aggregation layer will do nothing. To register an API, users must add an APIService object, which "claims" the URL path in the Kubernetes API. At that point, the aggregation layer will proxy anything sent to that API path (e.g. /apis/myextension.mycompany.io/v1/…) to the registered APIService.
-->
<h2 id="aggregation-layer">聚合层 </h2>
<p>聚合层在 kube-apiserver 进程内运行。在扩展资源注册之前，聚合层不做任何事情。
要注册 API，用户必须添加一个 APIService 对象，用它来“申领” Kubernetes API 中的 URL 路径。
自此以后，聚合层将会把发给该 API 路径的所有内容（例如 <code>/apis/myextension.mycompany.io/v1/…</code>）
转发到已注册的 APIService。</p>
<!--
The most common way to implement the APIService is to run an *extension API server* in Pod(s) that run in your cluster. If you're using the extension API server to manage resources in your cluster, the extension API server (also written as "extension-apiserver") is typically paired with one or more <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controllers'>controllers</a>. The apiserver-builder library provides a skeleton for both extension API servers and the associated controller(s).
-->
<p>APIService 的最常见实现方式是在集群中某 Pod 内运行 <em>扩展 API 服务器</em>。
如果你在使用扩展 API 服务器来管理集群中的资源，该扩展 API 服务器（也被写成“extension-apiserver”）
一般需要和一个或多个<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>一起使用。
apiserver-builder 库同时提供构造扩展 API 服务器和控制器框架代码。</p>
<!--
### Response latency

Extension API servers should have low latency networking to and from the kube-apiserver.
Discovery requests are required to round-trip from the kube-apiserver in five seconds or less.

If your extension API server cannot achieve that latency requirement, consider making changes that let you meet it.
-->
<h3 id="response-latency">反应延迟 </h3>
<p>扩展 API 服务器与 kube-apiserver 之间需要存在低延迟的网络连接。
发现请求需要在五秒钟或更短的时间内完成到 kube-apiserver 的往返。</p>
<p>如果你的扩展 API 服务器无法满足这一延迟要求，应考虑如何更改配置以满足需要。</p>
<h2 id="what-s-next">What's next</h2>
<!--
* To get the aggregator working in your environment, [configure the aggregation layer](/docs/tasks/access-kubernetes-api/configure-aggregation-layer/).
* Then, [setup an extension api-server](/docs/tasks/access-kubernetes-api/setup-extension-api-server/) to work with the aggregation layer.
* Read about [APIService](/docs/reference/kubernetes-api/cluster-resources/api-service-v1/) in the API reference

Alternatively: learn how to [extend the Kubernetes API using Custom Resource Definitions](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/).
-->
<ul>
<li>阅读<a href="/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/">配置聚合层</a> 文档，
了解如何在自己的环境中启用聚合器。</li>
<li>接下来，了解<a href="/zh/docs/tasks/extend-kubernetes/setup-extension-api-server/">安装扩展 API 服务器</a>，
开始使用聚合层。</li>
<li>从 API 参考资料中研究关于 <a href="/docs/reference/kubernetes-api/cluster-resources/api-service-v1/">APIService</a> 的内容。</li>
</ul>
<p>或者，学习如何<a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">使用自定义资源定义扩展 Kubernetes API</a>。</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3131452556176159fb269593c1a52012">12.2 - Operator 模式</h1>
    
	<!--
title: Operator pattern
content_type: concept
weight: 30
-->
<!-- overview -->
<!--
Operators are software extensions to Kubernetes that make use of [custom
resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
to manage applications and their components. Operators follow
Kubernetes principles, notably the [control loop](/docs/concepts/architecture/controller/).
-->
<p>Operator 是 Kubernetes 的扩展软件，它利用
<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">定制资源</a>
管理应用及其组件。
Operator 遵循 Kubernetes 的理念，特别是在<a href="/zh/docs/concepts/architecture/controller/">控制器</a>
方面。</p>
<!-- body -->
<!--
## Motivation

The Operator pattern aims to capture the key aim of a human operator who
is managing a service or set of services. Human operators who look after
specific applications and services have deep knowledge of how the system
ought to behave, how to deploy it, and how to react if there are problems.

People who run workloads on Kubernetes often like to use automation to take
care of repeatable tasks. The Operator pattern captures how you can write
code to automate a task beyond what Kubernetes itself provides.
-->
<h2 id="初衷">初衷</h2>
<p>Operator 模式旨在捕获（正在管理一个或一组服务的）运维人员的关键目标。
负责特定应用和 service 的运维人员，在系统应该如何运行、如何部署以及出现问题时如何处理等方面有深入的了解。</p>
<p>在 Kubernetes 上运行工作负载的人们都喜欢通过自动化来处理重复的任务。
Operator 模式会封装你编写的（Kubernetes 本身提供功能以外的）任务自动化代码。</p>
<!--
## Operators in Kubernetes

Kubernetes is designed for automation. Out of the box, you get lots of
built-in automation from the core of Kubernetes. You can use Kubernetes
to automate deploying and running workloads, *and* you can automate how
Kubernetes does that.

Kubernetes' <a class='glossary-tooltip' title='一种用于管理自定义资源的专用控制器' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/operator/' target='_blank' aria-label='operator pattern'>operator pattern</a> concept lets you extend the cluster's behaviour without modifying the code of Kubernetes itself by linking <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controllers'>controllers</a> to one or more custom resources.
Operators are clients of the Kubernetes API that act as controllers for
a [Custom Resource](/docs/concepts/extend-kubernetes/api-extension/custom-resources/).
-->
<h2 id="kubernetes-上的-operator">Kubernetes 上的 Operator</h2>
<p>Kubernetes 为自动化而生。无需任何修改，你即可以从 Kubernetes 核心中获得许多内置的自动化功能。
你可以使用 Kubernetes 自动化部署和运行工作负载， <em>甚至</em> 可以自动化 Kubernetes 自身。</p>
<p>Kubernetes 的 <a class='glossary-tooltip' title='一种用于管理自定义资源的专用控制器' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/operator/' target='_blank' aria-label='Operator 模式'>Operator 模式</a>概念允许你在不修改
Kubernetes 自身代码的情况下，通过为一个或多个自定义资源关联<a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>
来扩展集群的能力。
Operator 是 Kubernetes API 的客户端，充当
<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</a>
的控制器。</p>
<!--
## An example Operator {#example}

Some of the things that you can use an operator to automate include:

* deploying an application on demand
* taking and restoring backups of that application's state
* handling upgrades of the application code alongside related changes such
  as database schemas or extra configuration settings
* publishing a Service to applications that don't support Kubernetes APIs to
  discover them
* simulating failure in all or part of your cluster to test its resilience
* choosing a leader for a distributed application without an internal
  member election process
-->
<h2 id="example">Operator 示例</h2>
<p>使用 Operator 可以自动化的事情包括：</p>
<ul>
<li>按需部署应用</li>
<li>获取/还原应用状态的备份</li>
<li>处理应用代码的升级以及相关改动。例如，数据库 schema 或额外的配置设置</li>
<li>发布一个 service，要求不支持 Kubernetes API 的应用也能发现它</li>
<li>模拟整个或部分集群中的故障以测试其稳定性</li>
<li>在没有内部成员选举程序的情况下，为分布式应用选择首领角色</li>
</ul>
<!--
What might an Operator look like in more detail? Here's an example:

1. A custom resource named SampleDB, that you can configure into the cluster.
2. A Deployment that makes sure a Pod is running that contains the
   controller part of the operator.
3. A container image of the operator code.
4. Controller code that queries the control plane to find out what SampleDB
   resources are configured.
5. The core of the Operator is code to tell the API server how to make
   reality match the configured resources.
   * If you add a new SampleDB, the operator sets up PersistentVolumeClaims
     to provide durable database storage, a StatefulSet to run SampleDB and
     a Job to handle initial configuration.
   * If you delete it, the Operator takes a snapshot, then makes sure that
     the StatefulSet and Volumes are also removed.
6. The operator also manages regular database backups. For each SampleDB
   resource, the operator determines when to create a Pod that can connect
   to the database and take backups. These Pods would rely on a ConfigMap
   and / or a Secret that has database connection details and credentials.
7. Because the Operator aims to provide robust automation for the resource
   it manages, there would be additional supporting code. For this example,
   code checks to see if the database is running an old version and, if so,
   creates Job objects that upgrade it for you.
-->
<p>想要更详细的了解 Operator？下面是一个示例：</p>
<ol>
<li>有一个名为 SampleDB 的自定义资源，你可以将其配置到集群中。</li>
<li>一个包含 Operator 控制器部分的 Deployment，用来确保 Pod 处于运行状态。</li>
<li>Operator 代码的容器镜像。</li>
<li>控制器代码，负责查询控制平面以找出已配置的 SampleDB 资源。</li>
<li>Operator 的核心是告诉 API 服务器，如何使现实与代码里配置的资源匹配。
<ul>
<li>如果添加新的 SampleDB，Operator 将设置 PersistentVolumeClaims 以提供
持久化的数据库存储，设置 StatefulSet 以运行 SampleDB，并设置 Job
来处理初始配置。</li>
<li>如果你删除它，Operator 将建立快照，然后确保 StatefulSet 和 Volume 已被删除。</li>
</ul>
</li>
<li>Operator 也可以管理常规数据库的备份。对于每个 SampleDB 资源，Operator
会确定何时创建（可以连接到数据库并进行备份的）Pod。这些 Pod 将依赖于
ConfigMap 和/或具有数据库连接详细信息和凭据的 Secret。</li>
<li>由于 Operator 旨在为其管理的资源提供强大的自动化功能，因此它还需要一些
额外的支持性代码。在这个示例中，代码将检查数据库是否正运行在旧版本上，
如果是，则创建 Job 对象为你升级数据库。</li>
</ol>
<!--
## Deploying Operators

The most common way to deploy an Operator is to add the
Custom Resource Definition and its associated Controller to your cluster.
The Controller will normally run outside of the
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>,
much as you would run any containerized application.
For example, you can run the controller in your cluster as a Deployment.
-->
<h2 id="部署-operator">部署 Operator</h2>
<p>部署 Operator 最常见的方法是将自定义资源及其关联的控制器添加到你的集群中。
跟运行容器化应用一样，控制器通常会运行在
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a> 之外。
例如，你可以在集群中将控制器作为 Deployment 运行。</p>
<!--
## Using an Operator {#using-operators}

Once you have an Operator deployed, you'd use it by adding, modifying or
deleting the kind of resource that the Operator uses. Following the above
example, you would set up a Deployment for the Operator itself, and then:

```shell
kubectl get SampleDB                   # find configured databases

kubectl edit SampleDB/example-database # manually change some settings
```
-->
<h2 id="using-operators">使用 Operator</h2>
<p>部署 Operator 后，你可以对 Operator 所使用的资源执行添加、修改或删除操作。
按照上面的示例，你将为 Operator 本身建立一个 Deployment，然后：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get SampleDB                   <span style="color:#080;font-style:italic"># 查找所配置的数据库</span>

kubectl edit SampleDB/example-database <span style="color:#080;font-style:italic"># 手动修改某些配置</span>
</code></pre></div><!--
&hellip;and that's it! The Operator will take care of applying the changes as well as keeping the existing service in good shape.
-->
<p>可以了！Operator 会负责应用所作的更改并保持现有服务处于良好的状态。</p>
<!--
## Writing your own Operator {#writing-operator}
-->
<h2 id="writing-operator">编写你自己的 Operator</h2>
<!--
If there isn't an Operator in the ecosystem that implements the behavior you
want, you can code your own.

You also implement an Operator (that is, a Controller) using any language / runtime
that can act as a [client for the Kubernetes API](/docs/reference/using-api/client-libraries/).
-->
<p>如果生态系统中没可以实现你目标的 Operator，你可以自己编写代码。</p>
<p>你还可以使用任何支持 <a href="/zh/docs/reference/using-api/client-libraries/">Kubernetes API 客户端</a>
的语言或运行时来实现 Operator（即控制器）。</p>
<!--
Following are a few libraries and tools you can use to write your own cloud native
Operator.

<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>


* [Charmed Operator Framework](https://juju.is/)
* [Kopf](https://github.com/nolar/kopf) (Kubernetes Operator Pythonic Framework)
* [kubebuilder](https://book.kubebuilder.io/)
* [KubeOps](https://buehler.github.io/dotnet-operator-sdk/) (dotnet operator SDK)
* [KUDO](https://kudo.dev/) (Kubernetes Universal Declarative Operator)
* [Metacontroller](https://metacontroller.github.io/metacontroller/intro.html) along with WebHooks that 
you implement yourself
* [Operator Framework](https://operatorframework.io)
* [shell-operator](https://github.com/flant/shell-operator)
-->
<p>以下是一些库和工具，你可用于编写自己的云原生 Operator。</p>
<div class="alert alert-secondary callout third-party-content" role="alert"><strong>Note:</strong>
  This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href="/docs/contribute/style/content-guide/#third-party-content">content guide</a> before submitting a change. <a href="#third-party-content-disclaimer">More information.</a></div>
<ul>
<li><a href="https://juju.is/">Charmed Operator Framework</a></li>
<li><a href="https://github.com/nolar/kopf">Kopf</a> (Kubernetes Operator Pythonic Framework)</li>
<li><a href="https://book.kubebuilder.io/">kubebuilder</a></li>
<li><a href="https://buehler.github.io/dotnet-operator-sdk/">KubeOps</a> (dotnet operator SDK)</li>
<li><a href="https://kudo.dev/">KUDO</a> (Kubernetes 通用声明式 Operator)</li>
<li><a href="https://metacontroller.github.io/metacontroller/intro.html">Metacontroller</a>，可与 Webhooks 结合使用，以实现自己的功能。</li>
<li><a href="https://operatorframework.io">Operator Framework</a></li>
<li><a href="https://github.com/flant/shell-operator">shell-operator</a></li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
* Read the <a class='glossary-tooltip' title='云原生计算基金会' data-toggle='tooltip' data-placement='top' href='https://cncf.io/' target='_blank' aria-label='CNCF'>CNCF</a> [Operator White Paper](https://github.com/cncf/tag-app-delivery/blob/eece8f7307f2970f46f100f51932db106db46968/operator-wg/whitepaper/Operator-WhitePaper_v1-0.md).
* Learn more about [Custom Resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
* Find ready-made operators on [OperatorHub.io](https://operatorhub.io/) to suit your use case
* [Publish](https://operatorhub.io/) your operator for other people to use
* Read [CoreOS' original article](https://web.archive.org/web/20170129131616/https://coreos.com/blog/introducing-operators.html) that introduced the Operator pattern (this is an archived version of the original article).
* Read an [article](https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-building-kubernetes-operators-and-stateful-apps) from Google Cloud about best practices for building Operators
-->
<ul>
<li>阅读 <a class='glossary-tooltip' title='云原生计算基金会' data-toggle='tooltip' data-placement='top' href='https://cncf.io/' target='_blank' aria-label='CNCF'>CNCF</a> <a href="https://github.com/cncf/tag-app-delivery/blob/eece8f7307f2970f46f100f51932db106db46968/operator-wg/whitepaper/Operator-WhitePaper_v1-0.md">Operator 白皮书</a>。</li>
<li>详细了解 <a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">定制资源</a></li>
<li>在 <a href="https://operatorhub.io/">OperatorHub.io</a> 上找到现成的、适合你的 Operator</li>
<li><a href="https://operatorhub.io/">发布</a>你的 Operator，让别人也可以使用</li>
<li>阅读 <a href="https://web.archive.org/web/20170129131616/https://coreos.com/blog/introducing-operators.html">CoreOS 原始文章</a>，它介绍了 Operator 模式（这是一个存档版本的原始文章）。</li>
<li>阅读这篇来自谷歌云的关于构建 Operator 最佳实践的
<a href="https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-building-kubernetes-operators-and-stateful-apps">文章</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c8937cdc9df96f3328becf04f8211292">12.3 - 计算、存储和网络扩展</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-1ac2260db9ecccbf0303a899bc27ce6d">12.3.1 - 网络插件</h1>
    
	<!--
title: Network Plugins
content_type: concept
weight: 10
-->
<!-- overview -->
<!--
Network plugins in Kubernetes come in a few flavors:

* CNI plugins: adhere to the [Container Network Interface](https://github.com/containernetworking/cni) (CNI) specification, designed for interoperability.
  * Kubernetes follows the [v0.4.0](https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md) release of the CNI specification.
* Kubenet plugin: implements basic `cbr0` using the `bridge` and `host-local` CNI plugins
-->
<p>Kubernetes中的网络插件有几种类型：</p>
<ul>
<li>CNI 插件：遵守<a href="https://github.com/containernetworking/cni">容器网络接口（Container Network Interface，CNI）</a>
规范，其设计上偏重互操作性。
<ul>
<li>Kubernetes 遵从 CNI 规范的
<a href="https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md">v0.4.0</a>
版本。</li>
</ul>
</li>
<li>Kubenet 插件：使用 <code>bridge</code> 和 <code>host-local</code> CNI 插件实现了基本的 <code>cbr0</code>。</li>
</ul>
<!-- body -->
<!--
## Installation

The kubelet has a single default network plugin, and a default network common to the entire cluster. It probes for plugins when it starts up, remembers what it finds, and executes the selected plugin at appropriate times in the pod lifecycle (this is only true for Docker, as CRI manages its own CNI plugins). There are two Kubelet command line parameters to keep in mind when using plugins:

* `cni-bin-dir`: Kubelet probes this directory for plugins on startup
* `network-plugin`: The network plugin to use from `cni-bin-dir`.  It must match the name reported by a plugin probed from the plugin directory.  For CNI plugins, this is "cni".
-->
<h2 id="安装">安装</h2>
<p>kubelet 有一个单独的默认网络插件，以及一个对整个集群通用的默认网络。
它在启动时探测插件，记住找到的内容，并在 Pod 生命周期的适当时间执行
所选插件（这仅适用于 Docker，因为 CRI 管理自己的 CNI 插件）。
在使用插件时，需要记住两个 kubelet 命令行参数：</p>
<ul>
<li><code>cni-bin-dir</code>： kubelet 在启动时探测这个目录中的插件</li>
<li><code>network-plugin</code>： 要使用的网络插件来自 <code>cni-bin-dir</code>。
它必须与从插件目录探测到的插件报告的名称匹配。
对于 CNI 插件，其值为 &quot;cni&quot;。</li>
</ul>
<!--
## Network Plugin Requirements

Besides providing the [`NetworkPlugin` interface](https://github.com/kubernetes/kubernetes/tree/v1.23.0/pkg/kubelet/dockershim/network/plugins.go) to configure and clean up pod networking, the plugin may also need specific support for kube-proxy.  The iptables proxy obviously depends on iptables, and the plugin may need to ensure that container traffic is made available to iptables.  For example, if the plugin connects containers to a Linux bridge, the plugin must set the `net/bridge/bridge-nf-call-iptables` sysctl to `1` to ensure that the iptables proxy functions correctly.  If the plugin does not use a Linux bridge (but instead something like Open vSwitch or some other mechanism) it should ensure container traffic is appropriately routed for the proxy.

By default if no kubelet network plugin is specified, the `noop` plugin is used, which sets `net/bridge/bridge-nf-call-iptables=1` to ensure simple configurations (like Docker with a bridge) work correctly with the iptables proxy.
-->
<h2 id="网络插件要求">网络插件要求</h2>
<p>除了提供
<a href="https://github.com/kubernetes/kubernetes/tree/v1.23.0/pkg/kubelet/dockershim/network/plugins.go"><code>NetworkPlugin</code> 接口</a>
来配置和清理 Pod 网络之外，该插件还可能需要对 kube-proxy 的特定支持。
iptables 代理显然依赖于 iptables，插件可能需要确保 iptables 能够监控容器的网络通信。
例如，如果插件将容器连接到 Linux 网桥，插件必须将 <code>net/bridge/bridge-nf-call-iptables</code>
系统参数设置为<code>1</code>，以确保 iptables 代理正常工作。
如果插件不使用 Linux 网桥（而是类似于 Open vSwitch 或者其它一些机制），
它应该确保为代理对容器通信执行正确的路由。</p>
<p>默认情况下，如果未指定 kubelet 网络插件，则使用 <code>noop</code> 插件，
该插件设置 <code>net/bridge/bridge-nf-call-iptables=1</code>，以确保简单的配置
（如带网桥的 Docker ）与 iptables 代理正常工作。</p>
<!--
### CNI

The CNI plugin is selected by passing Kubelet the `--network-plugin=cni` command-line option.  Kubelet reads a file from `--cni-conf-dir` (default `/etc/cni/net.d`) and uses the CNI configuration from that file to set up each pod's network.  The CNI configuration file must match the [CNI specification](https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration), and any required CNI plugins referenced by the configuration must be present in `--cni-bin-dir` (default `/opt/cni/bin`).

If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order.

In addition to the CNI plugin specified by the configuration file, Kubernetes requires the standard CNI [`lo`](https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go) plugin, at minimum version 0.2.0
-->
<h3 id="cni">CNI</h3>
<p>通过给 Kubelet 传递 <code>--network-plugin=cni</code> 命令行选项可以选择 CNI 插件。
Kubelet 从 <code>--cni-conf-dir</code> （默认是 <code>/etc/cni/net.d</code>） 读取文件并使用
该文件中的 CNI 配置来设置各个 Pod 的网络。
CNI 配置文件必须与
<a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration">CNI 规约</a>
匹配，并且配置所引用的所有所需的 CNI 插件都应存在于
<code>--cni-bin-dir</code>（默认是 <code>/opt/cni/bin</code>）下。</p>
<p>如果这个目录中有多个 CNI 配置文件，kubelet 将会使用按文件名的字典顺序排列
的第一个作为配置文件。</p>
<p>除了配置文件指定的 CNI 插件外，Kubernetes 还需要标准的 CNI
<a href="https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go"><code>lo</code></a>
插件，最低版本是0.2.0。</p>
<!--
#### Support hostPort

The CNI networking plugin supports `hostPort`. You can use the official [portmap](https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap)
plugin offered by the CNI plugin team or use your own plugin with portMapping functionality.

If you want to enable `hostPort` support, you must specify `portMappings capability` in your `cni-conf-dir`.
For example:
-->
<h4 id="支持-hostport">支持 hostPort</h4>
<p>CNI 网络插件支持 <code>hostPort</code>。 你可以使用官方
<a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap">portmap</a>
插件，它由 CNI 插件团队提供，或者使用你自己的带有 portMapping 功能的插件。</p>
<p>如果你想要启动 <code>hostPort</code> 支持，则必须在 <code>cni-conf-dir</code> 指定 <code>portMappings capability</code>。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;k8s-pod-network&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;cniVersion&#34;</span>: <span style="color:#b44">&#34;0.3.0&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;plugins&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;calico&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;log_level&#34;</span>: <span style="color:#b44">&#34;info&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;datastore_type&#34;</span>: <span style="color:#b44">&#34;kubernetes&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;nodename&#34;</span>: <span style="color:#b44">&#34;127.0.0.1&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;ipam&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;host-local&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;subnet&#34;</span>: <span style="color:#b44">&#34;usePodCidr&#34;</span>
      },
      <span style="color:#008000;font-weight:bold">&#34;policy&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;k8s&#34;</span>
      },
      <span style="color:#008000;font-weight:bold">&#34;kubernetes&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;kubeconfig&#34;</span>: <span style="color:#b44">&#34;/etc/cni/net.d/calico-kubeconfig&#34;</span>
      }
    },
    {
      <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;portmap&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;capabilities&#34;</span>: {<span style="color:#008000;font-weight:bold">&#34;portMappings&#34;</span>: <span style="color:#a2f;font-weight:bold">true</span>}
    }
  ]
}
</code></pre></div><!--
#### Support traffic shaping

**Experimental Feature**

The CNI networking plugin also supports pod ingress and egress traffic shaping. You can use the official [bandwidth](https://github.com/containernetworking/plugins/tree/master/plugins/meta/bandwidth)
plugin offered by the CNI plugin team or use your own plugin with bandwidth control functionality.

If you want to enable traffic shaping support, you must add the `bandwidth` plugin to your CNI configuration file
(default `/etc/cni/net.d`) and ensure that the binary is included in your CNI bin dir (default `/opt/cni/bin`).
-->
<h4 id="支持流量整形">支持流量整形</h4>
<p><strong>实验功能</strong></p>
<p>CNI 网络插件还支持 pod 入口和出口流量整形。
你可以使用 CNI 插件团队提供的
<a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/bandwidth">bandwidth</a>
插件，也可以使用你自己的具有带宽控制功能的插件。</p>
<p>如果你想要启用流量整形支持，你必须将 <code>bandwidth</code> 插件添加到 CNI 配置文件
（默认是 <code>/etc/cni/net.d</code>）并保证该可执行文件包含在你的 CNI 的 bin
文件夹内 (默认为 <code>/opt/cni/bin</code>)。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;k8s-pod-network&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;cniVersion&#34;</span>: <span style="color:#b44">&#34;0.3.0&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;plugins&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;calico&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;log_level&#34;</span>: <span style="color:#b44">&#34;info&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;datastore_type&#34;</span>: <span style="color:#b44">&#34;kubernetes&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;nodename&#34;</span>: <span style="color:#b44">&#34;127.0.0.1&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;ipam&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;host-local&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;subnet&#34;</span>: <span style="color:#b44">&#34;usePodCidr&#34;</span>
      },
      <span style="color:#008000;font-weight:bold">&#34;policy&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;k8s&#34;</span>
      },
      <span style="color:#008000;font-weight:bold">&#34;kubernetes&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;kubeconfig&#34;</span>: <span style="color:#b44">&#34;/etc/cni/net.d/calico-kubeconfig&#34;</span>
      }
    },
    {
      <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;bandwidth&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;capabilities&#34;</span>: {<span style="color:#008000;font-weight:bold">&#34;bandwidth&#34;</span>: <span style="color:#a2f;font-weight:bold">true</span>}
    }
  ]
}
</code></pre></div><!--
Now you can add the `kubernetes.io/ingress-bandwidth` and `kubernetes.io/egress-bandwidth` annotations to your pod.
For example:
-->
<p>现在，你可以将 <code>kubernetes.io/ingress-bandwidth</code> 和 <code>kubernetes.io/egress-bandwidth</code>
注解添加到 pod 中。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/ingress-bandwidth</span>:<span style="color:#bbb"> </span>1M<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/egress-bandwidth</span>:<span style="color:#bbb"> </span>1M<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
### kubenet

Kubenet is a very basic, simple network plugin, on Linux only.  It does not, of itself, implement more advanced features like cross-node networking or network policy.  It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments.

Kubenet creates a Linux bridge named `cbr0` and creates a veth pair for each pod with the host end of each pair connected to `cbr0`.  The pod end of the pair is assigned an IP address allocated from a range assigned to the node either through configuration or by the controller-manager.  `cbr0` is assigned an MTU matching the smallest MTU of an enabled normal interface on the host.

The plugin requires a few things:

* The standard CNI `bridge`, `lo` and `host-local` plugins are required, at minimum version 0.2.0. Kubenet will first search for them in `/opt/cni/bin`. Specify `cni-bin-dir` to supply additional search path. The first found match will take effect.
* Kubelet must be run with the `--network-plugin=kubenet` argument to enable the plugin
* Kubelet should also be run with the `--non-masquerade-cidr=<clusterCidr>` argument to ensure traffic to IPs outside this range will use IP masquerade.
* The node must be assigned an IP subnet through either the `--pod-cidr` kubelet command-line option or the `--allocate-node-cidrs=true -cluster-cidr=<cidr>` controller-manager command-line options.
-->
<h3 id="kubenet">kubenet</h3>
<p>Kubenet 是一个非常基本的、简单的网络插件，仅适用于 Linux。
它本身并不实现更高级的功能，如跨节点网络或网络策略。
它通常与云驱动一起使用，云驱动为节点间或单节点环境中的通信设置路由规则。</p>
<p>Kubenet 创建名为 <code>cbr0</code> 的网桥，并为每个 pod 创建了一个 veth 对，
每个 Pod 的主机端都连接到 <code>cbr0</code>。
这个 veth 对的 Pod 端会被分配一个 IP 地址，该 IP 地址隶属于节点所被分配的 IP
地址范围内。节点的 IP 地址范围则通过配置或控制器管理器来设置。
<code>cbr0</code> 被分配一个 MTU，该 MTU 匹配主机上已启用的正常接口的最小 MTU。</p>
<p>使用此插件还需要一些其他条件：</p>
<ul>
<li>需要标准的 CNI <code>bridge</code>、<code>lo</code> 以及 <code>host-local</code> 插件，最低版本是0.2.0。
kubenet 首先在 <code>/opt/cni/bin</code> 中搜索它们。 指定 <code>cni-bin-dir</code> 以提供
其它搜索路径。首次找到的匹配将生效。</li>
<li>Kubelet 必须和 <code>--network-plugin=kubenet</code> 参数一起运行，才能启用该插件。</li>
<li>Kubelet 还应该和 <code>--non-masquerade-cidr=&lt;clusterCidr&gt;</code> 参数一起运行，
以确保超出此范围的 IP 流量将使用 IP 伪装。</li>
<li>节点必须被分配一个 IP 子网，通过kubelet 命令行的 <code>--pod-cidr</code> 选项或
控制器管理器的命令行选项 <code>--allocate-node-cidrs=true --cluster-cidr=&lt;cidr&gt;</code>
来设置。</li>
</ul>
<!--
### Customizing the MTU (with kubenet)

The MTU should always be configured correctly to get the best networking performance.  Network plugins will usually try
to infer a sensible MTU, but sometimes the logic will not result in an optimal MTU.  For example, if the
Docker bridge or another interface has a small MTU, kubenet will currently select that MTU.  Or if you are
using IPSEC encapsulation, the MTU must be reduced, and this calculation is out-of-scope for
most network plugins.

Where needed, you can specify the MTU explicitly with the `network-plugin-mtu` kubelet option.  For example,
on AWS the `eth0` MTU is typically 9001, so you might specify `--network-plugin-mtu=9001`.  If you're using IPSEC you
might reduce it to allow for encapsulation overhead e.g. `--network-plugin-mtu=8873`.

This option is provided to the network-plugin; currently **only kubenet supports `network-plugin-mtu`**.
-->
<h3 id="自定义-mtu-使用-kubenet">自定义 MTU（使用 kubenet）</h3>
<p>要获得最佳的网络性能，必须确保 MTU 的取值配置正确。
网络插件通常会尝试推断出一个合理的 MTU，但有时候这个逻辑不会产生一个最优的 MTU。
例如，如果 Docker 网桥或其他接口有一个小的 MTU, kubenet 当前将选择该 MTU。
或者如果你正在使用 IPSEC 封装，则必须减少 MTU，并且这种计算超出了大多数网络插件的能力范围。</p>
<p>如果需要，你可以使用 <code>network-plugin-mtu</code> kubelet 选项显式的指定 MTU。
例如：在 AWS 上 <code>eth0</code> MTU 通常是 9001，因此你可以指定 <code>--network-plugin-mtu=9001</code>。
如果你正在使用 IPSEC ，你可以减少它以允许封装开销，例如 <code>--network-plugin-mtu=8873</code>。</p>
<p>此选项会传递给网络插件； 当前 <strong>仅 kubenet 支持 <code>network-plugin-mtu</code></strong>。</p>
<!--
## Usage Summary

* `--network-plugin=cni` specifies that we use the `cni` network plugin with actual CNI plugin binaries located in `--cni-bin-dir` (default `/opt/cni/bin`) and CNI plugin configuration located in `--cni-conf-dir` (default `/etc/cni/net.d`).
* `--network-plugin=kubenet` specifies that we use the `kubenet` network plugin with CNI `bridge`, `lo` and `host-local` plugins placed in `/opt/cni/bin` or `cni-bin-dir`.
* `--network-plugin-mtu=9001` specifies the MTU to use, currently only used by the `kubenet` network plugin.
-->
<h2 id="用法总结">用法总结</h2>
<ul>
<li><code>--network-plugin=cni</code> 用来表明我们要使用 <code>cni</code> 网络插件，实际的 CNI 插件
可执行文件位于 <code>--cni-bin-dir</code>（默认是 <code>/opt/cni/bin</code>）下， CNI 插件配置位于
<code>--cni-conf-dir</code>（默认是 <code>/etc/cni/net.d</code>）下。</li>
<li><code>--network-plugin=kubenet</code> 用来表明我们要使用 <code>kubenet</code> 网络插件，CNI <code>bridge</code>，<code>lo</code>
和 <code>host-local</code> 插件位于 <code>/opt/cni/bin</code> 或 <code>cni-bin-dir</code> 中。</li>
<li><code>--network-plugin-mtu=9001</code> 指定了我们使用的 MTU，当前仅被 <code>kubenet</code> 网络插件使用。</li>
</ul>
<h2 id="what-s-next">What's next</h2>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-53e1ea8892ceca307ba19e8d6a7b8d32">12.3.2 - 设备插件</h1>
    <div class="lead">使用 Kubernetes 设备插件框架来实现适用于 GPU、NIC、FPGA、InfiniBand 以及类似的需要特定于供应商设置的资源的插件。</div>
	<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.10 [beta]</code>
</div>


<!--
Kubernetes provides a [device plugin framework](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md)
that you can use to advertise system hardware resources to the
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='Kubelet'>Kubelet</a>.

Instead of customizing the code for Kubernetes itself, vendors can implement a
device plugin that you deploy either manually or as a <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>.
The targeted devices include GPUs, high-performance NICs, FPGAs, InfiniBand adapters,
and other similar computing resources that may require vendor specific initialization
and setup.
-->
<p>Kubernetes 提供了一个
<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md">设备插件框架</a>，你可以用它来将系统硬件资源发布到 <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='Kubelet'>Kubelet</a>。</p>
<p>供应商可以实现设备插件，由你手动部署或作为 <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>
来部署，而不必定制 Kubernetes 本身的代码。目标设备包括 GPU、高性能 NIC、FPGA、
InfiniBand 适配器以及其他类似的、可能需要特定于供应商的初始化和设置的计算资源。</p>
<!-- body -->
<!--
## Device plugin registration
-->
<h2 id="device-plugin-registration">注册设备插件   </h2>
<!--
The kubelet exports a `Registration` gRPC service:
-->
<p><code>kubelet</code> 提供了一个 <code>Registration</code> 的 gRPC 服务：</p>
<pre><code class="language-gRPC" data-lang="gRPC">service Registration {
 rpc Register(RegisterRequest) returns (Empty) {}
}
</code></pre><!--
A device plugin can register itself with the kubelet through this gRPC service.
During the registration, the device plugin needs to send:

  * The name of its Unix socket.
  * The Device Plugin API version against which it was built.
  * The `ResourceName` it wants to advertise. Here `ResourceName` needs to follow the
    [extended resource naming scheme](/docs/concepts/configuration/manage-resources-containers/#extended-resources)
    as `vendor-domain/resourcetype`.
    (For example, an NVIDIA GPU is advertised as `nvidia.com/gpu`.)

Following a successful registration, the device plugin sends the kubelet the
list of devices it manages, and the kubelet is then in charge of advertising those
resources to the API server as part of the kubelet node status update.
For example, after a device plugin registers `hardware-vendor.example/foo` with the kubelet
and reports two healthy devices on a node, the node status is updated
to advertise that the node has 2 "Foo" devices installed and available.
-->
<p>设备插件可以通过此 gRPC 服务在 kubelet 进行注册。在注册期间，设备插件需要发送下面几样内容：</p>
<ul>
<li>设备插件的 Unix 套接字。</li>
<li>设备插件的 API 版本。</li>
<li><code>ResourceName</code> 是需要公布的。这里 <code>ResourceName</code> 需要遵循
<a href="/zh/docs/concepts/configuration/manage-resources-containers/#extended-resources">扩展资源命名方案</a>，
类似于 <code>vendor-domain/resourcetype</code>。（比如 NVIDIA GPU 就被公布为 <code>nvidia.com/gpu</code>。）</li>
</ul>
<p>成功注册后，设备插件就向 kubelet 发送它所管理的设备列表，然后 kubelet
负责将这些资源发布到 API 服务器，作为 kubelet 节点状态更新的一部分。</p>
<p>比如，设备插件在 kubelet 中注册了 <code>hardware-vendor.example/foo</code> 并报告了
节点上的两个运行状况良好的设备后，节点状态将更新以通告该节点已安装 2 个
&quot;Foo&quot; 设备并且是可用的。</p>
<!--
Then, users can request devices in a
[Container](/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core)
specification as they request other types of resources, with the following limitations:

* Extended resources are only supported as integer resources and cannot be overcommitted.
* Devices cannot be shared among Containers.
-->
<p>然后用户需要请求其他类型的资源的时候，就可以在
<a href="/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core">Container</a>
规范请求这类设备，但是有以下的限制：</p>
<ul>
<li>扩展资源仅可作为整数资源使用，并且不能被过量使用</li>
<li>设备不能在容器之间共享</li>
</ul>
<h3 id="example-pod">示例</h3>
<!--
Suppose a Kubernetes cluster is running a device plugin that advertises resource `hardware-vendor.example/foo`
on certain nodes. Here is an example of a pod requesting this resource to run a demo workload:
-->
<p>假设 Kubernetes 集群正在运行一个设备插件，该插件在一些节点上公布的资源为 <code>hardware-vendor.example/foo</code>。
下面就是一个 Pod 示例，请求此资源以运行一个工作负载的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>demo-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>demo-container-1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:2.0<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">hardware-vendor.example/foo</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 这个 pod 需要两个 hardware-vendor.example/foo 设备</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 而且只能够调度到满足需求的节点上</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 如果该节点中有 2 个以上的设备可用，其余的可供其他 Pod 使用</span><span style="color:#bbb">
</span></code></pre></div><!--
## Device plugin implementation

The general workflow of a device plugin includes the following steps:

* Initialization. During this phase, the device plugin performs vendor specific
  initialization and setup to make sure the devices are in a ready state.

* The plugin starts a gRPC service, with a Unix socket under host path
  `/var/lib/kubelet/device-plugins/`, that implements the following interfaces:
-->
<h2 id="device-plugin-implementation">设备插件的实现   </h2>
<p>设备插件的常规工作流程包括以下几个步骤：</p>
<ul>
<li>
<p>初始化。在这个阶段，设备插件将执行供应商特定的初始化和设置，
以确保设备处于就绪状态。</p>
</li>
<li>
<p>插件使用主机路径 <code>/var/lib/kubelet/device-plugins/</code> 下的 Unix 套接字启动
一个 gRPC 服务，该服务实现以下接口：</p>
<!--

```gRPC
service DevicePlugin {
      // GetDevicePluginOptions returns options to be communicated with Device Manager.
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}

      // ListAndWatch returns a stream of List of Devices
      // Whenever a Device state change or a Device disappears, ListAndWatch
      // returns the new list
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}

      // Allocate is called during container creation so that the Device
      // Plugin can run device specific operations and instruct Kubelet
      // of the steps to make the Device available in the container
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}

      // GetPreferredAllocation returns a preferred set of devices to allocate
      // from a list of available ones. The resulting preferred allocation is not
      // guaranteed to be the allocation ultimately performed by the
      // devicemanager. It is only designed to help the devicemanager make a more
      // informed allocation decision when possible.
      rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}

      // PreStartContainer is called, if indicated by Device Plugin during registeration phase,
      // before each container start. Device plugin can run device specific operations
      // such as resetting the device before making devices available to the container.
      rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}
}
```
-->
<pre><code class="language-gRPC" data-lang="gRPC">service DevicePlugin {
      // GetDevicePluginOptions 返回与设备管理器沟通的选项。
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}

      // ListAndWatch 返回 Device 列表构成的数据流。
      // 当 Device 状态发生变化或者 Device 消失时，ListAndWatch
      // 会返回新的列表。
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}

      // Allocate 在容器创建期间调用，这样设备插件可以运行一些特定于设备的操作，
      // 并告诉 kubelet 如何令 Device 可在容器中访问的所需执行的具体步骤
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}

      // GetPreferredAllocation 从一组可用的设备中返回一些优选的设备用来分配，
      // 所返回的优选分配结果不一定会是设备管理器的最终分配方案。
      // 此接口的设计仅是为了让设备管理器能够在可能的情况下做出更有意义的决定。
      rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}

      // PreStartContainer 在设备插件注册阶段根据需要被调用，调用发生在容器启动之前。
      // 在将设备提供给容器使用之前，设备插件可以运行一些诸如重置设备之类的特定于
      // 具体设备的操作，
      rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}
}
</code></pre><div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <!--
  Plugins are not required to provide useful implementations for
  `GetPreferredAllocation()` or `PreStartContainer()`. Flags indicating which
  (if any) of these calls are available should be set in the `DevicePluginOptions`
  message sent back by a call to `GetDevicePluginOptions()`. The `kubelet` will
  always call `GetDevicePluginOptions()` to see which optional functions are
  available, before calling any of them directly.
  -->
<p>插件并非必须为 <code>GetPreferredAllocation()</code> 或 <code>PreStartContainer()</code> 提供有用
的实现逻辑，调用 <code>GetDevicePluginOptions()</code> 时所返回的 <code>DevicePluginOptions</code>
消息中应该设置这些调用是否可用。<code>kubelet</code> 在真正调用这些函数之前，总会调用
<code>GetDevicePluginOptions()</code> 来查看是否存在这些可选的函数。
</div>
</li>
</ul>
<!--
* The plugin registers itself with the kubelet through the Unix socket at host
  path `/var/lib/kubelet/device-plugins/kubelet.sock`.

* After successfully registering itself, the device plugin runs in serving mode, during which it keeps
monitoring device health and reports back to the kubelet upon any device state changes.
It is also responsible for serving `Allocate` gRPC requests. During `Allocate`, the device plugin may
do device-specific preparation; for example, GPU cleanup or QRNG initialization.
If the operations succeed, the device plugin returns an `AllocateResponse` that contains container
runtime configurations for accessing the allocated devices. The kubelet passes this information
to the container runtime.
-->
<ul>
<li>插件通过 Unix socket 在主机路径 <code>/var/lib/kubelet/device-plugins/kubelet.sock</code>
处向 kubelet 注册自身。</li>
<li>成功注册自身后，设备插件将以服务模式运行，在此期间，它将持续监控设备运行状况，
并在设备状态发生任何变化时向 kubelet 报告。它还负责响应 <code>Allocate</code> gRPC 请求。
在 <code>Allocate</code> 期间，设备插件可能还会做一些设备特定的准备；例如 GPU 清理或 QRNG 初始化。
如果操作成功，则设备插件将返回 <code>AllocateResponse</code>，其中包含用于访问被分配的设备容器运行时的配置。
kubelet 将此信息传递到容器运行时。</li>
</ul>
<!--
### Handling kubelet restarts

A device plugin is expected to detect kubelet restarts and re-register itself with the new
kubelet instance. In the current implementation, a new kubelet instance deletes all the existing Unix sockets
under `/var/lib/kubelet/device-plugins` when it starts. A device plugin can monitor the deletion
of its Unix socket and re-register itself upon such an event.
-->
<h3 id="处理-kubelet-重启">处理 kubelet 重启</h3>
<p>设备插件应能监测到 kubelet 重启，并且向新的 kubelet 实例来重新注册自己。
在当前实现中，当 kubelet 重启的时候，新的 kubelet 实例会删除 <code>/var/lib/kubelet/device-plugins</code>
下所有已经存在的 Unix 套接字。
设备插件需要能够监控到它的 Unix 套接字被删除，并且当发生此类事件时重新注册自己。</p>
<!--
## Device plugin deployment

You can deploy a device plugin as a DaemonSet, as a package for your node's operating system,
or manually.

The canonical directory `/var/lib/kubelet/device-plugins` requires privileged access,
so a device plugin must run in a privileged security context.
If you're deploying a device plugin as a DaemonSet, `/var/lib/kubelet/device-plugins`
must be mounted as a <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷（Volume）'>卷（Volume）</a>
in the plugin's
[PodSpec](/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core).

If you choose the DaemonSet approach you can rely on Kubernetes to: place the device plugin's
Pod onto Nodes, to restart the daemon Pod after failure, and to help automate upgrades.
-->
<h2 id="设备插件部署">设备插件部署</h2>
<p>你可以将你的设备插件作为节点操作系统的软件包来部署、作为 DaemonSet 来部署或者手动部署。</p>
<p>规范目录 <code>/var/lib/kubelet/device-plugins</code> 是需要特权访问的，所以设备插件
必须要在被授权的安全的上下文中运行。
如果你将设备插件部署为 DaemonSet，<code>/var/lib/kubelet/device-plugins</code> 目录必须要在插件的
<a href="/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core">PodSpec</a>
中声明作为 <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷（Volume）'>卷（Volume）</a> 被挂载到插件中。</p>
<p>如果你选择 DaemonSet 方法，你可以通过 Kubernetes 进行以下操作：
将设备插件的 Pod 放置在节点上，在出现故障后重新启动守护进程 Pod，来进行自动升级。</p>
<!--
## API compatibility

Kubernetes device plugin support is in beta. The API may change before stabilization,
in incompatible ways. As a project, Kubernetes recommends that device plugin developers:

* Watch for changes in future releases.
* Support multiple versions of the device plugin API for backward/forward compatibility.

If you enable the DevicePlugins feature and run device plugins on nodes that need to be upgraded to
a Kubernetes release with a newer device plugin API version, upgrade your device plugins
to support both versions before upgrading these nodes. Taking that approach will
ensure the continuous functioning of the device allocations during the upgrade.
-->
<h2 id="api-兼容性">API 兼容性</h2>
<p>Kubernetes 设备插件支持还处于 beta 版本。所以在稳定版本出来之前 API 会以不兼容的方式进行更改。
作为一个项目，Kubernetes 建议设备插件开发者：</p>
<ul>
<li>注意未来版本的更改</li>
<li>支持多个版本的设备插件 API，以实现向后/向前兼容性。</li>
</ul>
<p>如果你启用 DevicePlugins 功能，并在需要升级到 Kubernetes 版本来获得较新的设备插件 API
版本的节点上运行设备插件，请在升级这些节点之前先升级设备插件以支持这两个版本。
采用该方法将确保升级期间设备分配的连续运行。</p>
<!--
## Monitoring Device Plugin Resources






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.15 [beta]</code>
</div>



In order to monitor resources provided by device plugins, monitoring agents need to be able to
discover the set of devices that are in-use on the node and obtain metadata to describe which
container the metric should be associated with. [Prometheus](https://prometheus.io/) metrics
exposed by device monitoring agents should follow the
[Kubernetes Instrumentation Guidelines](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/instrumentation.md),
identifying containers using `pod`, `namespace`, and `container` prometheus labels.
-->
<h2 id="监控设备插件资源">监控设备插件资源</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.15 [beta]</code>
</div>


<p>为了监控设备插件提供的资源，监控代理程序需要能够发现节点上正在使用的设备，
并获取元数据来描述哪个指标与容器相关联。
设备监控代理暴露给 <a href="https://prometheus.io/">Prometheus</a> 的指标应该遵循
<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/instrumentation.md">Kubernetes Instrumentation Guidelines</a>，
使用 <code>pod</code>、<code>namespace</code> 和 <code>container</code> 标签来标识容器。</p>
<!--
The kubelet provides a gRPC service to enable discovery of in-use devices, and to provide metadata
for these devices:
-->
<p>kubelet 提供了 gRPC 服务来使得正在使用中的设备被发现，并且还未这些设备提供了元数据：</p>
<pre><code class="language-gRPC" data-lang="gRPC">// PodResourcesLister 是一个由 kubelet 提供的服务，用来提供供节点上 
// Pods 和容器使用的节点资源的信息
service PodResourcesLister {
    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
    rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}
}
</code></pre><h3 id="grpc-endpoint-list"><code>List</code> gRPC 端点</h3>
<!--
The `List` endpoint provides information on resources of running pods, with details such as the
id of exclusively allocated CPUs, device id as it was reported by device plugins and id of
the NUMA node where these devices are allocated. Also, for NUMA-based machines, it contains
the information about memory and hugepages reserved for a container.
-->
<p>这一 <code>List</code> 端点提供运行中 Pods 的资源信息，包括类似独占式分配的
CPU ID、设备插件所报告的设备 ID 以及这些设备分配所处的 NUMA 节点 ID。
此外，对于基于 NUMA 的机器，它还会包含为容器保留的内存和大页的信息。</p>
<pre><code class="language-gRPC" data-lang="gRPC">// ListPodResourcesResponse 是 List 函数的响应
message ListPodResourcesResponse {
    repeated PodResources pod_resources = 1;
}

// PodResources 包含关于分配给 Pod 的节点资源的信息
message PodResources {
    string name = 1;
    string namespace = 2;
    repeated ContainerResources containers = 3;
}

// ContainerResources 包含分配给容器的资源的信息
message ContainerResources {
    string name = 1;
    repeated ContainerDevices devices = 2;
    repeated int64 cpu_ids = 3;
    repeated ContainerMemory memory = 4;
}

// ContainerMemory 包含分配给容器的内存和大页信息
message ContainerMemory {
    string memory_type = 1;
    uint64 size = 2;
    TopologyInfo topology = 3;
}

// Topology 描述资源的硬件拓扑结构
message TopologyInfo {
        repeated NUMANode nodes = 1;
}

// NUMA 代表的是 NUMA 节点
message NUMANode {
        int64 ID = 1;
}

// ContainerDevices 包含分配给容器的设备信息
message ContainerDevices {
    string resource_name = 1;
    repeated string device_ids = 2;
    TopologyInfo topology = 3;
}
</code></pre><!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p>cpu_ids in the <code>ContainerResources</code> in the <code>List</code> endpoint correspond to exclusive CPUs allocated
to a partilar container. If the goal is to evaluate CPUs that belong to the shared pool, the <code>List</code>
endpoint needs to be used in conjunction with the <code>GetAllocatableResources</code> endpoint as explained
below:</p>
<ol>
<li>Call <code>GetAllocatableResources</code> to get a list of all the allocatable CPUs</li>
<li>Call <code>GetCpuIds</code> on all <code>ContainerResources</code> in the system</li>
<li>Subtract out all of the CPUs from the <code>GetCpuIds</code> calls from the <code>GetAllocatableResources</code> call</li>
</ol>

</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <p><code>List</code> 端点中的 <code>ContainerResources</code> 中的 cpu_ids 对应于分配给某个容器的专属 CPU。
如果要统计共享池中的 CPU，<code>List</code> 端点需要与 <code>GetAllocatableResources</code> 端点一起使用，如下所述:</p>
<ol>
<li>调用 <code>GetAllocatableResources</code> 获取所有可用的 CPUs。</li>
<li>在系统中所有的 <code>ContainerResources</code> 上调用 <code>GetCpuIds</code>。</li>
<li>用 <code>GetAllocatableResources</code> 获取的 CPU 数减去 <code>GetCpuIds</code> 获取的 CPU 数。</li>
</ol>

</div>
<h3 id="grpc-endpoint-getallocatableresources"><code>GetAllocatableResources</code> gRPC 端点</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code>
</div>


<!--
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <code>GetAllocatableResources</code> should only be used to evaluate <a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">allocatable</a>
resources on a node. If the goal is to evaluate free/unallocated resources it should be used in
conjunction with the List() endpoint. The result obtained by <code>GetAllocatableResources</code> would remain
the same unless the underlying resources exposed to kubelet change. This happens rarely but when
it does (for example: hotplug/hotunplug, device health changes), client is expected to call
<code>GetAlloctableResources</code> endpoint.
However, calling <code>GetAllocatableResources</code> endpoint is not sufficient in case of cpu and/or memory
update and Kubelet needs to be restarted to reflect the correct resource capacity and allocatable.
</div>
-->
<div class="alert alert-info note callout" role="alert">
  <strong>Note:</strong> <code>GetAllocatableResources</code> 应该仅被用于评估一个节点上的<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">可分配的</a>
资源。如果目标是评估空闲/未分配的资源，此调用应该与 List() 端点一起使用。
除非暴露给 kubelet 的底层资源发生变化 否则 <code>GetAllocatableResources</code> 得到的结果将保持不变。
这种情况很少发生，但当发生时（例如：热插拔，设备健康状况改变），客户端应该调用 <code>GetAlloctableResources</code> 端点。
然而，调用 <code>GetAllocatableResources</code> 端点在 cpu、内存被更新的情况下是不够的，
Kubelet 需要重新启动以获取正确的资源容量和可分配的资源。
</div>
<!--
GetAllocatableResources provides information on resources initially available on the worker node.
It provides more information than kubelet exports to APIServer.
-->
<p>端点 <code>GetAllocatableResources</code> 提供最初在工作节点上可用的资源的信息。
此端点所提供的信息比导出给 API 服务器的信息更丰富。</p>
<pre><code class="language-gRPC" data-lang="gRPC">// AllocatableResourcesResponses 包含 kubelet 所了解到的所有设备的信息
message AllocatableResourcesResponse {
    repeated ContainerDevices devices = 1;
    repeated int64 cpu_ids = 2;
    repeated ContainerMemory memory = 3;
}

</code></pre><!--
Starting from Kubernetes v1.23, the `GetAllocatableResources` is enabled by default.
You can disable it by turning off the
`KubeletPodResourcesGetAllocatable` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).

Preceding Kubernetes v1.23, to enable this feature `kubelet` must be started with the following flag:

`--feature-gates=KubeletPodResourcesGetAllocatable=true`
-->
<p>从 Kubernetes v1.23 开始，<code>GetAllocatableResources</code> 被默认启用。
你可以通过关闭 <code>KubeletPodResourcesGetAllocatable</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a> 来禁用。</p>
<p>在 Kubernetes v1.23 之前，要启用这一功能，<code>kubelet</code> 必须用以下标志启动：</p>
<p><code>--feature-gates=KubeletPodResourcesGetAllocatable=true</code></p>
<!--
`ContainerDevices` do expose the topology information declaring to which NUMA cells the device is affine.
The NUMA cells are identified using a opaque integer ID, which value is consistent to what device
plugins report [when they register themselves to the kubelet](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager).
-->
<p><code>ContainerDevices</code> 会向外提供各个设备所隶属的 NUMA 单元这类拓扑信息。
NUMA 单元通过一个整数 ID 来标识，其取值与设备插件所报告的一致。
<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">设备插件注册到 kubelet 时</a>
会报告这类信息。</p>
<!--
The gRPC service is served over a unix socket at `/var/lib/kubelet/pod-resources/kubelet.sock`.
Monitoring agents for device plugin resources can be deployed as a daemon, or as a DaemonSet.
The canonical directory `/var/lib/kubelet/pod-resources` requires privileged access, so monitoring
agents must run in a privileged security context.  If a device monitoring agent is running as a
DaemonSet, `/var/lib/kubelet/pod-resources` must be mounted as a
<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷（Volume）'>卷（Volume）</a> in the device monitoring agent's
[PodSpec](/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core).

Support for the "PodResourcesLister service" requires `KubeletPodResources` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be enabled.
It is enabled by default starting with Kubernetes 1.15 and is v1 since Kubernetes 1.20.
-->
<p>gRPC 服务通过 <code>/var/lib/kubelet/pod-resources/kubelet.sock</code> 的 UNIX 套接字来提供服务。
设备插件资源的监控代理程序可以部署为守护进程或者 DaemonSet。
规范的路径 <code>/var/lib/kubelet/pod-resources</code> 需要特权来进入，
所以监控代理程序必须要在获得授权的安全的上下文中运行。
如果设备监控代理以 DaemonSet 形式运行，必须要在插件的
<a href="/docs/reference/generated/kubernetes-api/v1.23/#podspec-v1-core">PodSpec</a>
中声明将 <code>/var/lib/kubelet/pod-resources</code> 目录以
<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>的形式被挂载到设备监控代理中。</p>
<p>对“PodResourcesLister 服务”的支持要求启用 <code>KubeletPodResources</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
从 Kubernetes 1.15 开始默认启用，自从 Kubernetes 1.20 开始为 v1。</p>
<!--
## Device Plugin integration with the Topology Manager






<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.17 [alpha]</code>
</div>




The Topology Manager is a Kubelet component that allows resources to be co-ordinated in a Topology aligned manner. In order to do this, the Device Plugin API was extended to include a `TopologyInfo` struct.
-->
<h2 id="设备插件与拓扑管理器的集成">设备插件与拓扑管理器的集成</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
  <b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>


<p>拓扑管理器是 Kubelet 的一个组件，它允许以拓扑对齐方式来调度资源。
为了做到这一点，设备插件 API 进行了扩展来包括一个 <code>TopologyInfo</code> 结构体。</p>
<pre><code class="language-gRPC" data-lang="gRPC">message TopologyInfo {
 repeated NUMANode nodes = 1;
}

message NUMANode {
    int64 ID = 1;
}
</code></pre><!--
Device Plugins that wish to leverage the Topology Manager can send back a populated TopologyInfo struct as part of the device registration, along with the device IDs and the health of the device. The device manager will then use this information to consult with the Topology Manager and make resource assignment decisions.

`TopologyInfo` supports a `nodes` field that is either `nil` (the default) or a list of NUMA nodes. This lets the Device Plugin publish that can span NUMA nodes.

An example `TopologyInfo` struct populated for a device by a Device Plugin:

```
pluginapi.Device{ID: "25102017", Health: pluginapi.Healthy, Topology:&pluginapi.TopologyInfo{Nodes: []*pluginapi.NUMANode{&pluginapi.NUMANode{ID: 0,},}}}
```
-->
<p>设备插件希望拓扑管理器可以将填充的 TopologyInfo 结构体作为设备注册的一部分以及设备 ID
和设备的运行状况发送回去。然后设备管理器将使用此信息来咨询拓扑管理器并做出资源分配决策。</p>
<p><code>TopologyInfo</code> 支持定义 <code>nodes</code> 字段，允许为 <code>nil</code>（默认）或者是一个 NUMA 节点的列表。
这样就可以使设备插件可以跨越 NUMA 节点去发布。</p>
<p>下面是一个由设备插件为设备填充 <code>TopologyInfo</code> 结构体的示例：</p>
<pre><code>pluginapi.Device{ID: &quot;25102017&quot;, Health: pluginapi.Healthy, Topology:&amp;pluginapi.TopologyInfo{Nodes: []*pluginapi.NUMANode{&amp;pluginapi.NUMANode{ID: 0,},}}}
</code></pre><!--
## Device plugin examples {#examples}

Here are some examples of device plugin implementations:

* The [AMD GPU device plugin](https://github.com/RadeonOpenCompute/k8s-device-plugin)
* The [Intel device plugins](https://github.com/intel/intel-device-plugins-for-kubernetes) for Intel GPU, FPGA and QuickAssist devices
* The [KubeVirt device plugins](https://github.com/kubevirt/kubernetes-device-plugins) for hardware-assisted virtualization
* The [NVIDIA GPU device plugin](https://github.com/NVIDIA/k8s-device-plugin)
    * Requires [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) 2.0, which allows you to run GPU-enabled Docker containers.
* The [NVIDIA GPU device plugin for Container-Optimized OS](https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu)
* The [RDMA device plugin](https://github.com/hustcat/k8s-rdma-device-plugin)
* The [Solarflare device plugin](https://github.com/vikaschoudhary16/sfc-device-plugin)
* The [SR-IOV Network device plugin](https://github.com/intel/sriov-network-device-plugin)
* The [Xilinx FPGA device plugins](https://github.com/Xilinx/FPGA_as_a_Service/tree/master/k8s-fpga-device-plugin) for Xilinx FPGA devices
-->
<h2 id="examples">设备插件示例</h2>
<p>下面是一些设备插件实现的示例：</p>
<ul>
<li><a href="https://github.com/RadeonOpenCompute/k8s-device-plugin">AMD GPU 设备插件</a></li>
<li><a href="https://github.com/intel/intel-device-plugins-for-kubernetes">Intel 设备插件</a> 支持 Intel GPU、FPGA 和 QuickAssist 设备</li>
<li><a href="https://github.com/kubevirt/kubernetes-device-plugins">KubeVirt 设备插件</a> 用于硬件辅助的虚拟化</li>
<li>The <a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA GPU 设备插件</a></li>
<li>需要 <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a> 2.0，以允许运行 Docker 容器的时候启用 GPU。</li>
<li><a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu">为 Container-Optimized OS 所提供的 NVIDIA GPU 设备插件</a></li>
<li><a href="https://github.com/hustcat/k8s-rdma-device-plugin">RDMA 设备插件</a></li>
<li><a href="https://github.com/collabora/k8s-socketcan">SocketCAN 设备插件</a></li>
<li><a href="https://github.com/vikaschoudhary16/sfc-device-plugin">Solarflare 设备插件</a></li>
<li><a href="https://github.com/intel/sriov-network-device-plugin">SR-IOV 网络设备插件</a></li>
<li><a href="https://github.com/Xilinx/FPGA_as_a_Service/tree/master/k8s-fpga-device-plugin">Xilinx FPGA 设备插件</a></li>
</ul>
<h2 id="what-s-next">What's next</h2>
<!--
* Learn about [scheduling GPU resources](/docs/tasks/manage-gpus/scheduling-gpus/) using device plugins
* Learn about [advertising extended resources](/docs/tasks/administer-cluster/extended-resource-node/) on a node
* Read about using [hardware acceleration for TLS ingress](https://kubernetes.io/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/) with Kubernetes
* Learn about the [Topology Manager] (/docs/tasks/adminster-cluster/topology-manager/)
-->
<ul>
<li>查看<a href="/zh/docs/tasks/manage-gpus/scheduling-gpus/">调度 GPU 资源</a> 来学习使用设备插件</li>
<li>查看在上如何<a href="/zh/docs/tasks/administer-cluster/extended-resource-node/">公布节点上的扩展资源</a></li>
<li>阅读如何在 Kubernetes 中使用 <a href="https://kubernetes.io/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/">TLS Ingress 的硬件加速</a></li>
<li>学习<a href="/zh/docs/tasks/administer-cluster/topology-manager/">拓扑管理器</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b26fcf43d01abc16c8110766026dafed">12.4 - 服务目录</h1>
    
	<!--
title: Service Catalog
reviewers:
- chenopis
content_type: concept
weight: 40
-->
<!-- overview -->
<!--
---
title: Service Catalog
id: service-catalog
date: 2018-04-12
full_link: 
short_description: >
  An extension API that enables applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.

aka: 
tags:
- extension
---
-->
<!--
 An extension API that enables applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.
-->
<p><p>服务目录（Service Catalog）是 服务目录是一种扩展 API，它能让 Kubernetes 集群中运行的应用易于使用外部托管的的软件服务，例如云供应商提供的数据仓库服务。</p></p>
<!--
It provides a way to list, provision, and bind with external <a class='glossary-tooltip' title='由第三方供应商负责维护的一种软件产品。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-managed-service' target='_blank' aria-label='Managed Services'>Managed Services</a> from <a class='glossary-tooltip' title='由第三方提供并维护的一组托管服务的访问端点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-service-broker' target='_blank' aria-label='Service Brokers'>Service Brokers</a> without needing detailed knowledge about how those services are created or managed.
-->
<p>服务目录可以检索、供应、和绑定由 <a class='glossary-tooltip' title='由第三方提供并维护的一组托管服务的访问端点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-service-broker' target='_blank' aria-label='服务代理人（Service Brokers）'>服务代理人（Service Brokers）</a>
提供的外部<a class='glossary-tooltip' title='由第三方供应商负责维护的一种软件产品。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-managed-service' target='_blank' aria-label='托管服务（Managed Services）'>托管服务（Managed Services）</a>，
而无需知道那些服务具体是怎样创建和托管的。</p>
<!--
A service broker, as defined by the [Open service broker API spec](https://github.com/openservicebrokerapi/servicebroker/blob/v2.13/spec.md), is an endpoint for a set of managed services offered and maintained by a third-party, which could be a cloud provider such as AWS, GCP, or Azure.
Some examples of managed services are Microsoft Azure Cloud Queue, Amazon Simple Queue Service, and Google Cloud Pub/Sub, but they can be any software offering that can be used by an application.

Using Service Catalog, a <a class='glossary-tooltip' title='配置、控制、监控集群的人。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cluster-operator' target='_blank' aria-label='cluster operator'>cluster operator</a> can browse the list of managed services offered by a service broker, provision an instance of a managed service, and bind with it to make it available to an application in the Kubernetes cluster.
-->
<p>服务代理（Service Broker）是由<a href="https://github.com/openservicebrokerapi/servicebroker/blob/v2.13/spec.md">Open Service Broker API 规范</a>定义的一组托管服务的端点，这些服务由第三方提供并维护，其中的第三方可以是 AWS、GCP 或 Azure 等云服务提供商。
托管服务的一些示例是 Microsoft Azure Cloud Queue、Amazon Simple Queue Service 和 Google Cloud Pub/Sub，但它们可以是应用程序能够使用的任何软件交付物。</p>
<p>使用服务目录，<a class='glossary-tooltip' title='配置、控制、监控集群的人。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-cluster-operator' target='_blank' aria-label='集群操作员'>集群操作员</a>
可以浏览某服务代理所提供的托管服务列表，供应托管服务实例并与之绑定，
以使其可以被 Kubernetes 集群中的应用程序使用。</p>
<!-- body -->
<!--
## Example use case

An <a class='glossary-tooltip' title='编写可以在 Kubernetes 集群上运行的应用的人。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-application-developer' target='_blank' aria-label='application developer'>application developer</a> wants to use message queuing as part of their application running in a Kubernetes cluster.
However, they do not want to deal with the overhead of setting such a service up and administering it themselves.
Fortunately, there is a cloud provider that offers message queuing as a managed service through its service broker.

A cluster operator can setup Service Catalog and use it to communicate with the cloud provider's service broker to provision an instance of the message queuing service and make it available to the application within the Kubernetes cluster.
The application developer therefore does not need to be concerned with the implementation details or management of the message queue.
The application can simply use it as a service.
-->
<h2 id="示例用例">示例用例</h2>
<p><a class='glossary-tooltip' title='编写可以在 Kubernetes 集群上运行的应用的人。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-application-developer' target='_blank' aria-label='应用开发人员'>应用开发人员</a>，
希望使用消息队列，作为其在 Kubernetes 集群中运行的应用程序的一部分。
但是，他们不想承受构造这种服务的开销，也不想自行管理。
幸运的是，有一家云服务提供商通过其服务代理以托管服务的形式提供消息队列服务。</p>
<p>集群操作员可以设置服务目录并使用它与云服务提供商的服务代理通信，进而部署消息队列服务的实例
并使其对 Kubernetes 中的应用程序可用。
应用开发者于是可以不关心消息队列的实现细节，也不用对其进行管理。
他们的应用程序可以简单的将其作为服务使用。</p>
<!--
## Architecture

Service Catalog uses the [Open service broker API](https://github.com/openservicebrokerapi/servicebroker) to communicate with service brokers, acting as an intermediary for the Kubernetes API Server to negotiate the initial provisioning and retrieve the credentials necessary for the application to use a managed service.

It is implemented using a [CRDs-based](/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-resources) architecture.

![Service Catalog Architecture](/images/docs/service-catalog-architecture.svg)
-->
<h2 id="architecture">架构 </h2>
<p>服务目录使用<a href="https://github.com/openservicebrokerapi/servicebroker">Open Service Broker API</a>
与服务代理进行通信，并作为 Kubernetes API 服务器的中介，以便协商启动部署和获取
应用程序使用托管服务时必须的凭据。</p>
<p>它是<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-resources">基于 CRDs</a>
架构实现的。</p>
<p><img src="/images/docs/service-catalog-architecture.svg" alt="服务目录架构"></p>
<!--
### API Resources

Service Catalog installs the `servicecatalog.k8s.io` API and provides the following Kubernetes resources:

* `ClusterServiceBroker`: An in-cluster representation of a service broker, encapsulating its server connection details.
These are created and managed by cluster operators who wish to use that broker server to make new types of managed services available within their cluster.
* `ClusterServiceClass`: A managed service offered by a particular service broker.
When a new `ClusterServiceBroker` resource is added to the cluster, the Service Catalog controller connects to the service broker to obtain a list of available managed services. It then creates a new `ClusterServiceClass` resource corresponding to each managed service.
* `ClusterServicePlan`: A specific offering of a managed service. For example, a managed service may have different plans available, such as a free tier or paid tier, or it may have different configuration options, such as using SSD storage or having more resources. Similar to `ClusterServiceClass`, when a new `ClusterServiceBroker` is added to the cluster, Service Catalog creates a new `ClusterServicePlan` resource corresponding to each Service Plan available for each managed service.
* `ServiceInstance`: A provisioned instance of a `ClusterServiceClass`.
These are created by cluster operators to make a specific instance of a managed service available for use by one or more in-cluster applications.
When a new `ServiceInstance` resource is created, the Service Catalog controller connects to the appropriate service broker and instruct it to provision the service instance.
* `ServiceBinding`: Access credentials to a `ServiceInstance`.
These are created by cluster operators who want their applications to make use of a `ServiceInstance`.
Upon creation, the Service Catalog controller creates a Kubernetes `Secret` containing connection details and credentials for the Service Instance, which can be mounted into Pods.
-->
<h2 id="api-resources">API 资源</h2>
<p>服务目录安装 <code>servicecatalog.k8s.io</code> API 并提供以下 Kubernetes 资源：</p>
<ul>
<li><code>ClusterServiceBroker</code>：服务目录的集群内表现形式，封装了其服务连接细节。集群运维人员创建和管理这些资源，并希望使用该代理服务在集群中提供新类型的托管服务。</li>
<li><code>ClusterServiceClass</code>：由特定服务代理提供的托管服务。当新的 <code>ClusterServiceBroker</code> 资源被添加到集群时，服务目录控制器将连接到服务代理以获取可用的托管服务列表。然后为每个托管服务创建对应的新 <code>ClusterServiceClass</code> 资源。</li>
<li><code>ClusterServicePlan</code>：托管服务的特定产品。例如托管服务可能有不同的计划可用，如免费版本和付费版本，或者可能有不同的配置选项，例如使用 SSD 存储或拥有更多资源。与 <code>ClusterServiceClass</code> 类似，当一个新的 <code>ClusterServiceBroker</code> 被添加到集群时，服务目录会为每个托管服务的每个可用服务计划创建对应的新 <code>ClusterServicePlan</code> 资源。</li>
<li><code>ServiceInstance</code>：<code>ClusterServiceClass</code> 提供的示例。由集群运维人员创建，以使托管服务的特定实例可供一个或多个集群内应用程序使用。当创建一个新的 <code>ServiceInstance</code> 资源时，服务目录控制器将连接到相应的服务代理并指示它调配服务实例。</li>
<li><code>ServiceBinding</code>：<code>ServiceInstance</code> 的访问凭据。由希望其应用程序使用服务 <code>ServiceInstance</code> 的集群运维人员创建。创建之后，服务目录控制器将创建一个 Kubernetes <code>Secret</code>，其中包含服务实例的连接细节和凭据，可以挂载到 Pod 中。</li>
</ul>
<!--
### Authentication

Service Catalog supports these methods of authentication:

* Basic (username/password)
* [OAuth 2.0 Bearer Token](https://tools.ietf.org/html/rfc6750)
-->
<h3 id="authentication">认证 </h3>
<p>服务目录支持这些认证方法：</p>
<ul>
<li>基本认证（用户名/密码）</li>
<li><a href="https://tools.ietf.org/html/rfc6750">OAuth 2.0 不记名令牌</a></li>
</ul>
<!--
## Usage

A cluster operator can use Service Catalog API Resources to provision managed services and make them available within a Kubernetes cluster. The steps involved are:

1. Listing the managed services and Service Plans available from a service broker.
2. Provisioning a new instance of the managed service.
3. Binding to the managed service, which returns the connection credentials.
4. Mapping the connection credentials into the application.
-->
<h2 id="使用方式">使用方式</h2>
<p>集群运维人员可以使用服务目录 API 资源来供应托管服务并使其在 Kubernetes 集群内可用。涉及的步骤有：</p>
<ol>
<li>列出服务代理提供的托管服务和服务计划。</li>
<li>配置托管服务的新实例。</li>
<li>绑定到托管服务，它将返回连接凭证。</li>
<li>将连接凭证映射到应用程序中。</li>
</ol>
<!--
### Listing managed services and Service Plans

First, a cluster operator must create a `ClusterServiceBroker` resource within the `servicecatalog.k8s.io` group. This resource contains the URL and connection details necessary to access a service broker endpoint.

This is an example of a `ClusterServiceBroker` resource:

```yaml
apiVersion: servicecatalog.k8s.io/v1beta1
kind: ClusterServiceBroker
metadata:
  name: cloud-broker
spec:
  # Points to the endpoint of a service broker. (This example is not a working URL.)
  url:  https://servicebroker.somecloudprovider.com/v1alpha1/projects/service-catalog/brokers/default
  #####
  # Additional values can be added here, which may be used to communicate
  # with the service broker, such as bearer token info or a caBundle for TLS.
  #####
```
-->
<h3 id="列出托管服务和服务计划">列出托管服务和服务计划</h3>
<p>首先，集群运维人员在 <code>servicecatalog.k8s.io</code> 组内创建一个 <code>ClusterServiceBroker</code> 资源。此资源包含访问服务代理终结点所需的 URL 和连接详细信息。</p>
<p>这是一个 <code>ClusterServiceBroker</code> 资源的例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>servicecatalog.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterServiceBroker<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-broker<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 指向服务代理的末端。(这里的 URL 是无法使用的。)</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">url</span>:<span style="color:#bbb">  </span>https://servicebroker.somecloudprovider.com/v1alpha1/projects/service-catalog/brokers/default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic">#####</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 这里可以添加额外的用来与服务代理通信的属性值,</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 例如持有者令牌信息或者 TLS 的 CA 包。</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic">#####</span><span style="color:#bbb">
</span></code></pre></div><!--
The following is a sequence diagram illustrating the steps involved in listing managed services and Plans available from a service broker:

![List Services](/images/docs/service-catalog-list.svg)

1. Once the `ClusterServiceBroker` resource is added to Service Catalog, it triggers a call to the external service broker for a list of available services.
1. The service broker returns a list of available managed services and a list of Service Plans, which are cached locally as `ClusterServiceClass` and `ClusterServicePlan` resources respectively.
1. A cluster operator can then get the list of available managed services using the following command:
-->
<p>下面的时序图展示了从服务代理列出可用托管服务和计划所涉及的各个步骤：</p>
<p><img src="/images/docs/service-catalog-list.svg" alt="列举服务"></p>
<ol>
<li>一旦 <code>ClusterServiceBroker</code> 资源被添加到了服务目录之后，将会触发一个到外部服务代理的
调用，以列举所有可用服务；</li>
<li>服务代理返回可用的托管服务和服务计划列表，这些列表将本地缓存在 <code>ClusterServiceClass</code>
和 <code>ClusterServicePlan</code> 资源中。</li>
<li>集群运维人员接下来可以使用以下命令获取可用托管服务的列表：</li>
</ol>
<!--
        kubectl get clusterserviceclasses -o=custom-columns=SERVICE\ NAME:.metadata.name,EXTERNAL\ NAME:.spec.externalName

    It should output a list of service names with a format similar to:

        SERVICE NAME                           EXTERNAL NAME
        4f6e6cf6-ffdd-425f-a2c7-3c9258ad2468   cloud-provider-service
        ...                                    ...

    They can also view the Service Plans available using the following command:

        kubectl get clusterserviceplans -o=custom-columns=PLAN\ NAME:.metadata.name,EXTERNAL\ NAME:.spec.externalName

    It should output a list of plan names with a format similar to:

        PLAN NAME                              EXTERNAL NAME
        86064792-7ea2-467b-af93-ac9694d96d52   service-plan-name
        ...                                    ...
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get clusterserviceclasses <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  -o<span style="color:#666">=</span>custom-columns<span style="color:#666">=</span>SERVICE<span style="color:#b62;font-weight:bold">\ </span>NAME:.metadata.name,EXTERNAL<span style="color:#b62;font-weight:bold">\ </span>NAME:.spec.externalName
</code></pre></div><p>它应该输出一个和以下格式类似的服务名称列表：</p>
<pre><code>SERVICE NAME                           EXTERNAL NAME
4f6e6cf6-ffdd-425f-a2c7-3c9258ad2468   cloud-provider-service
...                                    ...
</code></pre><p>他们还可以使用以下命令查看可用的服务计划：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get clusterserviceplans <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   -o<span style="color:#666">=</span>custom-columns<span style="color:#666">=</span>PLAN<span style="color:#b62;font-weight:bold">\ </span>NAME:.metadata.name,EXTERNAL<span style="color:#b62;font-weight:bold">\ </span>NAME:.spec.externalName
</code></pre></div><pre><code>它应该输出一个和以下格式类似的服务计划列表：
</code></pre>
<pre><code>PLAN NAME                              EXTERNAL NAME
86064792-7ea2-467b-af93-ac9694d96d52   service-plan-name
...                                    ...
</code></pre><!--
### Provisioning a new instance

A cluster operator can initiate the provisioning of a new instance by creating a `ServiceInstance` resource.

This is an example of a `ServiceInstance` resource:

```yaml
apiVersion: servicecatalog.k8s.io/v1beta1
kind: ServiceInstance
metadata:
  name: cloud-queue-instance
  namespace: cloud-apps
spec:
  # References one of the previously returned services
  clusterServiceClassExternalName: cloud-provider-service
  clusterServicePlanExternalName: service-plan-name
  #####
  # Additional parameters can be added here,
  # which may be used by the service broker.
  #####
```
-->
<h3 id="供应一个新实例">供应一个新实例</h3>
<p>集群运维人员 可以通过创建一个 <code>ServiceInstance</code> 资源来启动一个新实例的配置。</p>
<p>下面是一个 <code>ServiceInstance</code> 资源的例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>servicecatalog.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceInstance<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-queue-instance<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>cloud-apps<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 引用之前返回的服务之一</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterServiceClassExternalName</span>:<span style="color:#bbb"> </span>cloud-provider-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterServicePlanExternalName</span>:<span style="color:#bbb"> </span>service-plan-name<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic">#####</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 这里可添加额外的参数，供服务代理使用</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic">#####</span><span style="color:#bbb">
</span></code></pre></div><!--
The following sequence diagram illustrates the steps involved in provisioning a new instance of a managed service:

![Provision a Service](/images/docs/service-catalog-provision.svg)

1. When the `ServiceInstance` resource is created, Service Catalog initiates a call to the external service broker to provision an instance of the service.
1. The service broker creates a new instance of the managed service and returns an HTTP response.
1. A cluster operator can then check the status of the instance to see if it is ready.
-->
<p>以下时序图展示了配置托管服务新实例所涉及的步骤：</p>
<p><img src="/images/docs/service-catalog-provision.svg" alt="供应服务"></p>
<ol>
<li>创建 <code>ServiceInstance</code> 资源时，服务目录将启动一个到外部服务代理的调用，
请求供应一个实例。</li>
<li>服务代理创建一个托管服务的新实例并返回 HTTP 响应。</li>
<li>接下来，集群运维人员可以检查实例的状态是否就绪。</li>
</ol>
<!--
### Binding to a managed service

After a new instance has been provisioned, a cluster operator must bind to the managed service to get the connection credentials and service account details necessary for the application to use the service. This is done by creating a `ServiceBinding` resource.

The following is an example of a `ServiceBinding` resource:

```yaml
apiVersion: servicecatalog.k8s.io/v1beta1
kind: ServiceBinding
metadata:
  name: cloud-queue-binding
  namespace: cloud-apps
spec:
  instanceRef:
    name: cloud-queue-instance
  #####
  # Additional information can be added here, such as a secretName or
  # service account parameters, which may be used by the service broker.
  #####
```
-->
<h3 id="绑定到托管服务">绑定到托管服务</h3>
<p>在设置新实例之后，集群运维人员必须绑定到托管服务才能获取应用程序使用服务所需的连接凭据和服务账户的详细信息。该操作通过创建一个 <code>ServiceBinding</code> 资源完成。</p>
<p>以下是 <code>ServiceBinding</code> 资源的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>servicecatalog.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceBinding<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-queue-binding<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>cloud-apps<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">instanceRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-queue-instance<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic">#####</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 这里可以添加供服务代理使用的额外信息，例如 Secret 名称或者服务账号参数，</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic">#####</span><span style="color:#bbb">
</span></code></pre></div><!--
The following sequence diagram illustrates the steps involved in binding to a managed service instance:

![Bind to a managed service](/images/docs/service-catalog-bind.svg)

1. After the `ServiceBinding` is created, Service Catalog makes a call to the external service broker requesting the information necessary to bind with the service instance.
1. The service broker enables the application permissions/roles for the appropriate service account.
1. The service broker returns the information necessary to connect and access the managed service instance. This is provider and service-specific so the information returned may differ between Service Providers and their managed services.
-->
<p>以下顺序图展示了绑定到托管服务实例的步骤：</p>
<p><img src="/images/docs/service-catalog-bind.svg" alt="绑定到托管服务"></p>
<ol>
<li>在创建 <code>ServiceBinding</code> 之后，服务目录调用外部服务代理，请求绑定服务实例所需的信息。</li>
<li>服务代理为相应服务账户启用应用权限/角色。</li>
<li>服务代理返回连接和访问托管服务示例所需的信息。这是由提供商和服务特定的，故返回的信息可能因服务提供商和其托管服务而有所不同。</li>
</ol>
<!--
### Mapping the connection credentials

After binding, the final step involves mapping the connection credentials and service-specific information into the application.
These pieces of information are stored in secrets that the application in the cluster can access and use to connect directly with the managed service.

<br>

![Map connection credentials](/images/docs/service-catalog-map.svg)
-->
<h3 id="映射连接凭据">映射连接凭据</h3>
<p>完成绑定之后的最后一步就是将连接凭据和服务特定的信息映射到应用程序中。这些信息存储在 secret 中，集群中的应用程序可以访问并使用它们直接与托管服务进行连接。</p>
<br>
<p><img src="/images/docs/service-catalog-map.svg" alt="映射连接凭据"></p>
<!--
#### Pod configuration File

One method to perform this mapping is to use a declarative Pod configuration.

The following example describes how to map service account credentials into the application. A key called `sa-key` is stored in a volume named `provider-cloud-key`, and the application mounts this volume at `/var/secrets/provider/key.json`. The environment variable `PROVIDER_APPLICATION_CREDENTIALS` is mapped from the value of the mounted file.
-->
<h4 id="pod-配置文件">Pod 配置文件</h4>
<p>执行此映射的一种方法是使用声明式 Pod 配置。</p>
<p>以下示例描述了如何将服务账户凭据映射到应用程序中。名为 <code>sa-key</code> 的密钥保存在一个名为
<code>provider-cloud-key</code> 的卷中，应用程序会将该卷挂载在 <code>/var/secrets/provider/key.json</code>
路径下。环境变量 <code>PROVIDER_APPLICATION_CREDENTIALS</code> 将映射为挂载文件的路径。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>provider-cloud-key<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>sa-key<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>provider-cloud-key<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/secrets/provider<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>PROVIDER_APPLICATION_CREDENTIALS<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/var/secrets/provider/key.json&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
The following example describes how to map secret values into application environment variables. In this example, the messaging queue topic name is mapped from a secret named `provider-queue-credentials` with a key named `topic` to the environment variable `TOPIC`.
-->
<p>以下示例描述了如何将 Secret 值映射为应用程序的环境变量。
在这个示例中，消息队列的主题名从 Secret <code>provider-queue-credentials</code> 中名为
<code>topic</code> 的主键映射到环境变量 <code>TOPIC</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;TOPIC&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">secretKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                   </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>provider-queue-credentials<span style="color:#bbb">
</span><span style="color:#bbb">                   </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>topic<span style="color:#bbb">
</span></code></pre></div><h2 id="what-s-next">What's next</h2>
<!--
* If you are familiar with <a class='glossary-tooltip' title='Helm Chart 是一组预先配置的 Kubernetes 资源所构成的包，可以使用 Helm 工具对其进行管理。' data-toggle='tooltip' data-placement='top' href='https://github.com/kubernetes/helm/blob/master/docs/charts.md' target='_blank' aria-label='Helm Charts'>Helm Charts</a>, [install Service Catalog using Helm](/docs/tasks/service-catalog/install-service-catalog-using-helm/) into your Kubernetes cluster. Alternatively, you can [install Service Catalog using the SC tool](/docs/tasks/service-catalog/install-service-catalog-using-sc/).
* View [sample service brokers](https://github.com/openservicebrokerapi/servicebroker/blob/master/gettingStarted.md#sample-service-brokers).
* Explore the [kubernetes-sigs/service-catalog](https://github.com/kubernetes-sigs/service-catalog) project.
-->
<ul>
<li>如果你熟悉 <a class='glossary-tooltip' title='Helm Chart 是一组预先配置的 Kubernetes 资源所构成的包，可以使用 Helm 工具对其进行管理。' data-toggle='tooltip' data-placement='top' href='https://github.com/kubernetes/helm/blob/master/docs/charts.md' target='_blank' aria-label='Helm Charts'>Helm Charts</a>，
可以<a href="/zh/docs/tasks/service-catalog/install-service-catalog-using-helm/">使用 Helm 安装服务目录</a>
到 Kubernetes 集群中。或者，你可以
<a href="/zh/docs/tasks/service-catalog/install-service-catalog-using-sc/">使用 SC 工具安装服务目录</a>。</li>
<li>查看<a href="https://github.com/openservicebrokerapi/servicebroker/blob/master/gettingStarted.md#sample-service-brokers">服务代理示例</a></li>
<li>浏览 <a href="https://github.com/kubernetes-sigs/service-catalog">kubernetes-sigs/service-catalog</a> 项目</li>
</ul>

</div>



    
	
  

    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      
      <a class="text-white" href="/zh/docs/home/">主页</a>
      
      <a class="text-white" href="/zh/blog/">博客</a>
      
      <a class="text-white" href="/zh/training/">培训</a>
      
      <a class="text-white" href="/zh/partners/">合作伙伴</a>
      
      <a class="text-white" href="/zh/community/">社区</a>
      
      <a class="text-white" href="/zh/case-studies/">案例分析</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" href="https://discuss.kubernetes.io">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/kubernetesio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://youtube.com/kubernetescommunity">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/kubernetes/kubernetes">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://slack.k8s.io">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Contribute" aria-label="Contribute">
    <a class="text-white" target="_blank" href="https://git.k8s.io/community/contributors/guide">
      <i class="fas fa-edit"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" href="https://stackoverflow.com/questions/tagged/kubernetes">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2024 The Kubernetes Authors | Documentation Distributed under <a href="https://git.k8s.io/website/LICENSE" class="light-text">CC BY 4.0</a></small>
        <br/>
        <small class="text-white">Copyright &copy; 2024 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage" class="light-text">Trademark Usage page</a></small>
        <br/>
        <small class="text-white">ICP license: 京ICP备17074266号-3</small>
        
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>










<script src="/js/main.js"></script>






  </body>
</html>
